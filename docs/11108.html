<html>
<head>
<title>Web Scraping with Python and Beautiful Soup 4</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Python 和美汤 4 进行网页抓取</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/web-scraping-with-python-and-beautiful-soup-4-718c55106d13?source=collection_archive---------5-----------------------#2022-12-19">https://blog.devgenius.io/web-scraping-with-python-and-beautiful-soup-4-718c55106d13?source=collection_archive---------5-----------------------#2022-12-19</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><figure class="gl gn jl jm jn jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi jk"><img src="../Images/498cfc9d0d4d791c38b2b1cfa13f06ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*djDOPB9e8bGaxAbx"/></div></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">阿诺·弗朗西斯卡在<a class="ae jz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><h1 id="64f4" class="ka kb in bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">介绍</h1><p id="c366" class="pw-post-body-paragraph ky kz in la b lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv ig bi translated">Web 抓取是数据科学中的一项基本技能。缺乏高质量的数据是数据分析师或科学家经常遇到的问题，这最终会影响您的研究结果。互联网是获取各种用途数据的绝佳资源。我们如何收集这些数据？</p><p id="d87d" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">在本文中，我们将使用 Python 编程语言来浏览在互联网上搜集有价值的数据的过程。您将看到如何使用 BeautifulSoup 解析 HTML 并提取网页数据。</p><h1 id="c506" class="ka kb in bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated"><strong class="ak">概述</strong></h1><p id="f92b" class="pw-post-body-paragraph ky kz in la b lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv ig bi translated"><strong class="la io">简介</strong> <br/> -什么是网页抓取<br/> -检查 HTML 页面</p><p id="b08e" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated"><strong class="la io">使用 Python 和 BeautifulSoup 进行网页抓取的工具和技术</strong> <br/> -安装依赖关系<br/> -导入已安装的依赖关系<br/> -检查网页<br/> -解析 HTML 为 Beautiful Soup</p><p id="3685" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated"><strong class="la io">抓取多个网页并存储数据</strong> <br/> -识别 URL 中的模式来抓取多个网页<br/> <strong class="la io"> - </strong>存储数据</p><p id="7b7e" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated"><strong class="la io">结论</strong></p><h1 id="ef8c" class="ka kb in bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">什么是网络抓取</h1><p id="c136" class="pw-post-body-paragraph ky kz in la b lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv ig bi translated">网络抓取是从网站上提取和收集数据，并将它们存储在数据库或本地机器中。Python 为我们提供了与网站交互和管理这些数据的库。本文将使用 Python 的 BeautifulSoup 库和 pandas 抓取一个电子商务网站的多个页面。</p><p id="7ab1" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">我们开始吧！</p><h1 id="9944" class="ka kb in bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">检查 HTML 页面</h1><p id="4af7" class="pw-post-body-paragraph ky kz in la b lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv ig bi translated">在我们继续之前，您需要理解 HTML，因为目前大多数网页都是用 HTML 构建的。超文本标记语言是一种标准化系统，用于标记文本文档以指示结构和格式。它是用来创建和组织万维网内容的主要语言。</p><p id="51e7" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">它由一系列定义网页结构和内容的标签组成。在开始网页抓取之前，您必须检查您的网页；它帮助你更好地理解网页，知道你需要的数据在哪里。你不需要知道如何编写 HTML 代码。</p><p id="ecfe" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">要检查网页，您可以使用 web 浏览器中的开发工具。</p><figure class="mc md me mf gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi mb"><img src="../Images/908f01c5fbbc61fb9a7cf55f96e06e04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*35U40LHQkw8cR36tRIiTBg.jpeg"/></div></div></figure><p id="5a1a" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">这将打开开发人员控制台，并允许您查看页面的 HTML 源代码。在控制台中，您可以使用“Elements”选项卡查看页面的 HTML 源代码。HTML 代码以树状结构组织，每个元素由一个标签表示。您可以使用控制台检查页面上的不同标记和属性，包括应用于页面的 CSS 样式。</p><figure class="mc md me mf gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi mg"><img src="../Images/7de1a5f9a09c02f845a7c34e193f5a92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rDhKqPbTIMi8oGpS6qYmxg.png"/></div></div></figure><h1 id="9588" class="ka kb in bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">使用 Python 和 BeautifulSoup 进行 Web 抓取的工具和技术</h1><h1 id="306e" class="ka kb in bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">安装依赖项</h1><p id="f342" class="pw-post-body-paragraph ky kz in la b lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv ig bi translated">我们需要安装以下库来启动这个项目:</p><ul class=""><li id="8e8d" class="mh mi in la b lb lw lf lx lj mj ln mk lr ml lv mm mn mo mp bi translated"><code class="fe mq mr ms mt b">bs4</code>用于解析 HTML 和从网页中提取数据。</li><li id="5c26" class="mh mi in la b lb mu lf mv lj mw ln mx lr my lv mm mn mo mp bi translated"><code class="fe mq mr ms mt b">requests</code>用于向电子商务 URL 发送 GET 请求</li><li id="1e77" class="mh mi in la b lb mu lf mv lj mw ln mx lr my lv mm mn mo mp bi translated"><code class="fe mq mr ms mt b">pandas</code>用于将提取的数据存储到 CSV 文件中</li></ul><pre class="mc md me mf gt mz mt na bn nb nc bi"><span id="ce99" class="nd kb in mt b be ne nf l ng nh">pip3 install requests<br/>pip3 install bs4 <br/>pip3 install pandas</span></pre><h1 id="ef24" class="ka kb in bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">导入已安装的依赖项</h1><p id="8d0d" class="pw-post-body-paragraph ky kz in la b lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv ig bi translated">接下来，我们将导入刚刚安装的库。像添加其他 Python 库一样添加它们:</p><pre class="mc md me mf gt mz mt na bn nb nc bi"><span id="6ca5" class="nd kb in mt b be ne nf l ng nh">import requests<br/>import pandas as pd<br/>import bs4 as BeautifulSoup<br/>import re #regular expression</span></pre><h1 id="44a4" class="ka kb in bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">检查网页</h1><p id="52c6" class="pw-post-body-paragraph ky kz in la b lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv ig bi translated">接下来，我们将检查用 Python 抓取的网页。我们将利用<code class="fe mq mr ms mt b">requests</code>库从网站获取数据。然后，我们将 URL 的内容存储在一个响应变量中。</p><pre class="mc md me mf gt mz mt na bn nb nc bi"><span id="4203" class="nd kb in mt b be ne nf l ng nh">url = "https://www.newegg.com/PS5-Systems/SubCategory/ID-3762/Page-2?Tid=1696840"<br/>response = requests.get(url)<br/># read content of the website<br/>response = response.content</span></pre><h1 id="03ea" class="ka kb in bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">使用 BeautifulSoup 解析 HTML</h1><p id="12ca" class="pw-post-body-paragraph ky kz in la b lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv ig bi translated">接下来，我们将把<code class="fe mq mr ms mt b">response</code>变量的内容连同 HTML 解析器一起传递给 BeautifulSoup 构造函数，如下所示:</p><pre class="mc md me mf gt mz mt ni nj aw nk bi"><span id="736e" class="nl kb in mt b gy nm nn l no nh">soup = BeautifulSoup(response, 'html.parser’)</span></pre><h1 id="7590" class="ka kb in bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">抓取多个网页</h1><p id="4933" class="pw-post-body-paragraph ky kz in la b lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv ig bi translated">对于这个项目，我们将在 Newegg.com 网站上删除 15 页的 PS5 子类别。</p><h1 id="1aea" class="ka kb in bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">识别 URL 中的模式以抓取多个页面</h1><p id="b3a0" class="pw-post-body-paragraph ky kz in la b lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv ig bi translated">在本节中，我们将了解如何从各种运营商获取数据。因此，我们在检查时会在商务网站上寻找商品卡片的 HTML 标签。</p><p id="cce7" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">这是<a class="ae jz" href="http://newegg.com/" rel="noopener ugc nofollow" target="_blank">Newegg.com</a>网站，显示了三个物品卡片。</p><p id="85d5" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">突出显示的 HTML 标签包含了这个页面上的整个卡片网格。我们使用 Python 的<code class="fe mq mr ms mt b">find</code>方法来搜索<code class="fe mq mr ms mt b">div</code>和指定的<code class="fe mq mr ms mt b">class</code>。该方法返回它找到的所有元素。</p><pre class="mc md me mf gt mz mt na bn nb nc bi"><span id="09db" class="nd kb in mt b be ne nf l ng nh">items = soup.find('div', class_='item-cells-wrap border-cells items-grid-view four-cells expulsion-one-cell'</span></pre><p id="ba11" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">我们希望获得存储数据的卡片网格中每件商品的名称、价格、运输类型和价格折扣(如果适用)。我们如何做到这一点？</p><p id="7899" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">首先，回到网站，搜索嵌套数据的标签。例如，产品名称可在此处找到:</p><p id="ca78" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">接下来，我们使用 find 方法获取产品的名称</p><pre class="mc md me mf gt mz mt na bn nb nc bi"><span id="4faa" class="nd kb in mt b be ne nf l ng nh">for item in items:<br/>	title= item.find('a', class_="item-title").get_text()</span></pre><p id="2ec8" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">注意下划线在课堂上的使用；我们添加这个是因为我们想让程序知道它是一个 HTML 类，而不是 Python 类。我们通过查找数据中的嵌套标签，对想要获取的其他数据进行同样的操作。</p><pre class="mc md me mf gt mz mt na bn nb nc bi"><span id="7a89" class="nd kb in mt b be ne nf l ng nh">for item in items:<br/>	title= item.find('a', class_="item-title").get_text()<br/>	ship_type = item.find('li', class_= 'price-ship').get_text()<br/>	product_discount = item.find('span', class_= 'price-save-percent')</span></pre><p id="f1f3" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">product_discount 返回一些 None 值，使用 get_text 方法返回一个属性错误；我们使用三元运算符来解决这个问题</p><pre class="mc md me mf gt mz mt na bn nb nc bi"><span id="2980" class="nd kb in mt b be ne nf l ng nh">for item in items:<br/>	title= item.find('a', class_="item-title").get_text()<br/>	ship_type = item.find('li', class_= 'price-ship').get_text()<br/>	product_discount = item.find('span', class_= 'price-save-percent')<br/>	discount = 'NaN' if product_discount == None else product_discount.get_text() #tenary operator</span></pre><p id="dd14" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">接下来，我们想得到我们的产品价格</p><pre class="mc md me mf gt mz mt na bn nb nc bi"><span id="8c97" class="nd kb in mt b be ne nf l ng nh">for item in items:<br/>	title= item.find('a', class_="item-title").get_text()<br/>	ship_type = item.find('li', class_= 'price-ship').get_text()<br/>	product_discount = item.find('span', class_= 'price-save-percent')<br/>	discount = 'NaN' if product_discount == None else product_discount.get_text()<br/>	price = item.find('li', class_= 'price-current').get_text()<br/>	print(price)</span></pre><pre class="np mz mt ni nj aw nk bi"><span id="f68f" class="nl kb in mt b gy nm nn l no nh">Output:</span><span id="64e3" class="nl kb in mt b gy nq nn l no nh">"$729.89 (6 Offers)–"<br/>"$719.99 –"<br/>"$758.95 (3 Offers)–"<br/>"$735.00 (8 Offers)–]"</span></pre><p id="1613" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">价格返回额外的字符，这是我们不需要的。我们将使用正则表达式来搜索并返回我们想要的东西。</p><pre class="mc md me mf gt mz mt na bn nb nc bi"><span id="a735" class="nd kb in mt b be ne nf l ng nh">for item in items:<br/>	title= item.find('a', class_="item-title").get_text()<br/>	ship_type = item.find('li', class_= 'price-ship').get_text()<br/>	product_discount = item.find('span', class_= 'price-save-percent')<br/>	discount = 'NaN' if product_discount == None else product_discount.get_text()<br/>	price = item.find('li', class_= 'price-current').get_text()<br/>	print(price)<br/>	Match = re.search(r"^\\$[\\d,]+(\\.\\d+)?", price).group()</span></pre><p id="53e5" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated"><code class="fe mq mr ms mt b">group()</code>方法从搜索结果中提取匹配的子组。如果我们检查变量匹配中的值，我们将有一个属性错误</p><pre class="mc md me mf gt mz mt na bn nb nc bi"><span id="311f" class="nd kb in mt b be ne nf l ng nh">AttributeError: 'NoneType' object has no attribute 'group'</span></pre><p id="b3c5" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">我们将使用 try 和 except 关键字来预测属性错误</p><pre class="mc md me mf gt mz mt na bn nb nc bi"><span id="35fa" class="nd kb in mt b be ne nf l ng nh">for item in items:<br/>	title= item.find('a', class_="item-title").get_text()<br/>	ship_type = item.find('li', class_= 'price-ship').get_text()<br/>	product_discount = item.find('span', class_= 'price-save-percent')<br/>	discount = 'NaN' if product_discount == None else product_discount.get_text()<br/>	price = item.find('li', class_= 'price-current').get_text()<br/>	try:<br/>			price_x = re.search(r"^\\$[\\d,]+(\\.\\d+)?", price).group()<br/>	except AttributeError:<br/>			price_x = re.search(r"^\\$[\\d,]+(\\.\\d+)?", price)<br/> <br/> print(title, price_x, ship_type, discount)</span></pre><p id="56a4" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">这将返回第一页的产品名称、价格、运输类型和折扣。让我们把它们列成一个列表。</p><p id="2d86" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">创建一个空列表，然后将你收集的数据添加到信息列表中。</p><pre class="mc md me mf gt mz mt na bn nb nc bi"><span id="69dd" class="nd kb in mt b be ne nf l ng nh">info = []<br/><br/>for item in items:<br/> title= item.find('a', class_="item-title").get_text()<br/> ship_type = item.find('li', class_= 'price-ship').get_text()<br/> product_discount = item.find('span', class_= 'price-save-percent')<br/> discount = 'NaN' if product_discount == None else product_discount.get_text()<br/> price = item.find('li', class_= 'price-current').get_text()<br/> try:<br/>   price_x = re.search(r"^\$[\d,]+(\.\d+)?", price).group()<br/> except AttributeError:<br/>   price_x = re.search(r"^\$[\d,]+(\.\d+)?", price)<br/> <br/> info.append([title, price_x, ship_type, discount])</span></pre><p id="56e1" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">请记住:我们希望从多个网站页面获得相同的信息。为了实现这一点，我们将遍历所有页面；对于每一个，我们的程序得到我们想要的数据。</p><pre class="mc md me mf gt mz mt na bn nb nc bi"><span id="dd5e" class="nd kb in mt b be ne nf l ng nh">info= []<br/>#We are extracting from 15 pages of the website<br/>for i in range(1, 16):<br/>	#in the  website URL, replace the page number with i<br/>	#use the format method<br/>	url = f"https://www.newegg.com/PS5-Systems/SubCategory/ID-3762/Page-{i}"</span></pre><p id="8b18" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">现在，我们将缩进循环中的项目，因为我们希望获得 url 中每个页面的项目。</p><pre class="mc md me mf gt mz mt na bn nb nc bi"><span id="5c7c" class="nd kb in mt b be ne nf l ng nh">info= []<br/>#We are extracting from 15 pages of the website<br/>for i in range(1, 16):<br/> #in the  website URL replace the page number with i<br/> #use the format method<br/> url = f"https://www.newegg.com/PS5-Systems/SubCategory/ID-3762/Page-{i}"<br/> response = requests.get(url)<br/> response = response.content<br/> soup = BeautifulSoup(response, 'html_parser')<br/> items = soup.find('div', class_='item-cells-wrap border-cells items-grid-view four-cells expulsion-one-cell')<br/><br/>  <br/><br/>   for item in items:<br/>     title= item.find('a', class_="item-title").get_text()<br/>     ship_type = item.find('li', class_= 'price-ship').get_text()<br/>     product_discount = item.find('span', class_= 'price-save-percent')<br/>     discount = 'NaN' if product_discount == None else product_discount.get_text()<br/>     price = item.find('li', class_= 'price-current').get_text()<br/>     try:<br/>      price_x = re.search(r"^\$[\d,]+(\.\d+)?", price).group()<br/>     except AttributeError:<br/>      price_x = re.search(r"^\$[\d,]+(\.\d+)?", price)<br/> <br/>      info.append([title, price_x, ship_type, discount])</span></pre><p id="feec" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">我们希望以逗号分隔值(CSV)格式存储数据。我们可以通过熊猫的图书馆来实现</p><pre class="mc md me mf gt mz mt na bn nb nc bi"><span id="c5ff" class="nd kb in mt b be ne nf l ng nh">#create a Dataframe with info as values and<br/>	# columns to specify the name of each column<br/>df = pd.DataFrame(info, columns=['Product_Name', 'Price', 'Shipment Type', 'Discount'])<br/></span></pre><pre class="np mz mt na bn nb nc bi"><span id="08c3" class="nd kb in mt b be ne nf l ng nh">#store as a CSV file with the file's name as an argument.<br/>df = pd.to_csv('product_csv')</span></pre><p id="12b2" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">这将返回一个 CSV 文件，其中包含您从 Newegg 网站上收集的数据。</p><h1 id="e92b" class="ka kb in bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">结论</h1><p id="b238" class="pw-post-body-paragraph ky kz in la b lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv ig bi translated">在本文中，我们使用 BeautifulSoup、Python requests、pandas 和正则表达式收集了 Newegg 网站 PS5 子类别中销售的 500 多件商品。我们可以使用这些数据来分析或开发推荐引擎。</p></div></div>    
</body>
</html>