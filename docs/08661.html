<html>
<head>
<title>Migrating Spark Jobs to Google Cloud &amp; File event sensor to Dynamically Create Spark Cluster (DataProc)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">将 Spark 作业迁移到 Google 云和文件事件传感器，以动态创建 Spark 集群(DataProc)</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/migrating-spark-jobs-to-google-cloud-file-event-sensor-to-dynamically-create-spark-cluster-7eff2c75423d?source=collection_archive---------3-----------------------#2022-07-01">https://blog.devgenius.io/migrating-spark-jobs-to-google-cloud-file-event-sensor-to-dynamically-create-spark-cluster-7eff2c75423d?source=collection_archive---------3-----------------------#2022-07-01</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><p id="bbd5" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">谷歌云分析的主要方面是<strong class="jm io">分离存储和计算</strong>和<strong class="jm io">为你使用的</strong>付费。本地 Hadoop 和 Spark 应用程序让服务器始终运行，无论您是否在使用，这意味着您实际上也在为空闲时间付费。</p><p id="2086" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">Google 想出了一种思想(<strong class="jm io">创建-运行-删除</strong>)来创建短暂的 spark 集群，每当需要运行 ETL 批处理作业时，完成作业后杀死集群。有了这种方法，就不会有任何 Hadoop 和 Spark 集群永远运行，您也不会有巨大的行政和管理成本。为了实现这一点，在迁移 spark 和 Hadoop 应用时要记住两件事。</p><p id="336b" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">1)将 HDFS 数据加载到 GCS</p><p id="bccc" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">2)将 Spark 作业替换为 hdfs://到 gs://</p><p id="cdcb" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">GCS 默认与 Hadoop 文件系统兼容。我们可以将 HDFS 数据复制并存储到 GCS 桶中。与 HDFS 相比，GCS 有很多优势，它便宜，可扩展，几乎没有管理开销。更重要的是，GCS 与所有 GCP 产品(Biquery、DataProc、Dataflow、DataFusion)无缝集成..等等)</p><p id="5ee4" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">一旦 HDFS 的数据被放入 GCS，下一步就是将 spark 的工作转移到 GCP。根据执行频率(每天、每周、每小时)定义和分割现有的 ETL 批处理作业。需要时触发火花作业。在批处理作业执行时创建 spark 集群。将 spark 作业迁移到 GCP 需要进行更改，将 HDFS 路径替换为 GCS 存储桶路径。</p><p id="d919" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">让我们开始实际地做这件事吧。</p><p id="a325" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io"> <em class="ki">文件监视事件:在任何 ETL/大数据世界中，当一个新文件到达时，必须立即触发 ETL 作业来处理该文件。ETL 作业不断极点并检查新文件</em>和</strong></p><p id="d7e9" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">议程:</strong>将一个入站文件放入 GCS 存储桶，该存储桶会在一分钟内触发 spark 集群，处理该文件并将结果存储回 GCS 存储桶并自动销毁集群。</p><ol class=""><li id="9387" class="kj kk in jm b jn jo jr js jv kl jz km kd kn kh ko kp kq kr bi translated">将文件摄取到 GCS 存储桶</li><li id="fc1d" class="kj kk in jm b jn ks jr kt jv ku jz kv kd kw kh ko kp kq kr bi translated">动态触发 ETL 火花作业</li><li id="65cf" class="kj kk in jm b jn ks jr kt jv ku jz kv kd kw kh ko kp kq kr bi translated">创建火花簇</li><li id="7e82" class="kj kk in jm b jn ks jr kt jv ku jz kv kd kw kh ko kp kq kr bi translated">处理文件</li><li id="c70f" class="kj kk in jm b jn ks jr kt jv ku jz kv kd kw kh ko kp kq kr bi translated">删除火花簇</li><li id="d832" class="kj kk in jm b jn ks jr kt jv ku jz kv kd kw kh ko kp kq kr bi translated">存档已处理的文件</li></ol><p id="9490" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">使用的技术:GCS、pyspark、Airflow(Composer)</p><p id="8151" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io"> <em class="ki">入站文件</em> </strong>:公开的犯罪 2020 CSV 数据集</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi kx"><img src="../Images/b04c26773c01969a79562c0d6ffaf458.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OC7710rYAV6Z1Rn_7ATBdg.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">犯罪公共数据集</figcaption></figure></div><div class="ab cl ln lo hr lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ig ih ii ij ik"><p id="6c66" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io"> <em class="ki">火花作业</em> </strong>:非常简单的改造。读取入站 CSV 文件并转换为拼花文件，然后保存回 GCS bucket 出站文件夹</p><pre class="ky kz la lb gt lu lv lw lx aw ly bi"><span id="edf2" class="lz ma in lv b gy mb mc l md me">from pyspark.sql import SparkSession<br/>from pyspark.sql import Row<br/>from pyspark.sql.functions import *</span><span id="19fc" class="lz ma in lv b gy mf mc l md me">#Define inbound and outbound folders<br/>inbound_file='gs://inbound/Crimes2020.csv'<br/>outbound_file='gs://outbound/crimes_modified'</span><span id="79ab" class="lz ma in lv b gy mf mc l md me">#Create Spark Session<br/>spark = SparkSession.builder.appName('DataProc-PySpark-Test').getOrCreate()</span><span id="0a30" class="lz ma in lv b gy mf mc l md me">#Read inbound file<br/>df = spark.read.format('csv').load(inbound_file)</span><span id="1de3" class="lz ma in lv b gy mf mc l md me">#Transform and save df.write.mode('append').format('parquet').save(outbound_file)</span></pre><p id="9cb8" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io"> <em class="ki"> Composer(Airflow)编排工作:ETL 管道构造为 Airflow DAG。</em> </strong>它把所有的组件集合在一起，按顺序运行任务。让我们检查每个组件。</p><p id="c34e" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io"> <em class="ki">文件传感器事件</em> </strong> : Google 提供 GCS 对象传感器操作员来验证特定文件的存在。基本上，它查看特定的 GCS 路径和对象。如果找到了对象，它将返回 True。</p><pre class="ky kz la lb gt lu lv lw lx aw ly bi"><span id="69b3" class="lz ma in lv b gy mb mc l md me">check_inbound_file = GCSObjectExistenceSensor(task_id='check_inbound_file',bucket=inbound_bucket,object='Crimes2020.csv',dag=dag)</span></pre><p id="7e79" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io"> <em class="ki">创建 Spark Cluster </em> </strong> : GCP 提供所有 DataProc 操作员，通过气流创建/管理和删除 Hadoop 生态系统和 Spark Cluster。下面的代码片段创建了包含 1 个主节点和 2 个工作节点的 Spark 集群。</p><pre class="ky kz la lb gt lu lv lw lx aw ly bi"><span id="02d5" class="lz ma in lv b gy mb mc l md me">create_cluster = dataproc_operator.DataprocClusterCreateOperator(task_id='create_cluster',<br/>dag=dag,<br/>region = models.Variable.get('dataproc_region'),<br/>zone=models.Variable.get('dataproc_zone'),<br/>project_id=models.Variable.get('project_id'),<br/>cluster_name='dataproc-spark-pipeline-{{ds}}',<br/>num_workers=2,<br/>master_machine_type='n1-standard-2',<br/>worker_machine_type='n1-standard-2'<br/>)</span></pre><p id="b0ad" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io"> <em class="ki">提交 spark 作业</em> </strong>:在上面的步骤中，我们创建了 spark cluster，现在是时候向 spark cluster 提交 Spark 作业了。DataProcPySparkOperator 操作符允许您输入 spark 文件位置(GCS 路径)和集群，该集群将向集群提交 spark 作业。</p><pre class="ky kz la lb gt lu lv lw lx aw ly bi"><span id="ba17" class="lz ma in lv b gy mb mc l md me">spark_transformation = dataproc_operator.DataProcPySparkOperator(task_id='spark_transformation',<br/><strong class="lv io"><em class="ki">main=spark_file, </em></strong>#Specify pyspark file location <br/>job_name='spark-transformations',<br/>cluster_name='dataproc-spark-pipeline-{{ds}}',<br/>dag=dag</span><span id="78f7" class="lz ma in lv b gy mf mc l md me">)</span></pre><p id="d25f" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io"> <em class="ki">删除集群</em> </strong>:一旦 spark 作业完成，结果将被保存到 GCS bucket outbound 文件夹中，现在不需要保持 Spark 集群运行。我们可以摧毁 spark 集群，并免除空闲时间的计算成本</p><pre class="ky kz la lb gt lu lv lw lx aw ly bi"><span id="8a34" class="lz ma in lv b gy mb mc l md me">delete_cluster = dataproc_operator.DataprocClusterDeleteOperator(task_id='delete_cluster',dag=dag,<br/>cluster_name='dataproc-spark-pipeline-{{ds}}',<br/>project_id=models.Variable.get('project_id')</span><span id="1802" class="lz ma in lv b gy mf mc l md me">)</span></pre><p id="3c36" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io"> <em class="ki">归档文件</em> </strong>:归档或者您可以从 GCS 入站桶中删除已处理的入站文件。一个简单的 Bash 命令</p><pre class="ky kz la lb gt lu lv lw lx aw ly bi"><span id="8064" class="lz ma in lv b gy mb mc l md me">remove_files = BashOperator(task_id='remove_files',<br/>bash_command='gsutil rm gs://inbound/Crimes2020.csv',dag=dag)</span></pre><p id="afd6" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">让我们来看看完整的气流 dag 代码片段</p><pre class="ky kz la lb gt lu lv lw lx aw ly bi"><span id="ae04" class="lz ma in lv b gy mb mc l md me">import airflow<br/>from airflow import DAG,models<br/>from datetime import datetime,timedelta<br/>from airflow.operators.bash_operator import BashOperator<br/>from airflow.contrib.operators import dataproc_operator<br/>from airflow.providers.google.cloud.sensors.gcs <br/>import GCSObjectExistenceSensor</span><span id="17e5" class="lz ma in lv b gy mf mc l md me">#Define pyspark file location and inbound bucket<br/>spark_file = ('gs://spark-warehouse/Spark_Transformation.py')<br/>inbound_bucket = models.Variable.get('inbound_bucket')</span><span id="1df1" class="lz ma in lv b gy mf mc l md me">#DAG default arguments<br/>default_arguments = {'start_date':airflow.utils.dates.days_ago(1),<br/>'retries':1,<br/>'retries_delay':timedelta(minutes=1)<br/>}</span><span id="8766" class="lz ma in lv b gy mf mc l md me">#Define DAG<br/>dag = DAG('dataproc_spark_pipeline',<br/>description='DataProc-Spark-Pipeline',<br/>schedule_interval=timedelta(days=1),<br/>default_args=default_arguments)</span><span id="f0b9" class="lz ma in lv b gy mf mc l md me">#File Sensor Event Task<br/>check_inbound_file = GCSObjectExistenceSensor(task_id='check_inbound_file',<br/>bucket=inbound_bucket,<br/>object='Crimes2020.csv',<br/>dag=dag)</span><span id="a652" class="lz ma in lv b gy mf mc l md me">#Create DataProc Spark Cluster<br/>create_cluster = dataproc_operator.DataprocClusterCreateOperator(<br/>task_id='create_cluster',<br/>dag=dag,<br/>region=models.Variable.get('dataproc_region'),<br/>zone=models.Variable.get('dataproc_zone'),<br/>project_id=models.Variable.get('project_id'),<br/>cluster_name='dataproc-spark-pipeline-{{ds}}',<br/>num_workers=2,<br/>master_machine_type='n1-standard-2',<br/>worker_machine_type='n1-standard-2')</span><span id="6527" class="lz ma in lv b gy mf mc l md me">#Submit Spark job <br/>spark_transformation = dataproc_operator.DataProcPySparkOperator(<br/>task_id='spark_transformation',<br/>main=spark_file,job_name='spark-transformations',<br/>region=models.Variable.get('dataproc_region'),<br/>zone=models.Variable.get('dataproc_zone'),<br/>project_id=models.Variable.get('project_id'),</span><span id="faa3" class="lz ma in lv b gy mf mc l md me">cluster_name='dataproc-spark-pipeline-{{ds}}',<br/>dag=dag)</span><span id="29b0" class="lz ma in lv b gy mf mc l md me">#Delete DataProc Spark Cluster<br/>delete_cluster = dataproc_operator.DataprocClusterDeleteOperator(<br/>task_id='delete_cluster',<br/>dag=dag,<br/>project_id=models.Variable.get('project_id'),<br/>region=models.Variable.get('dataproc_region'),<br/>zone=models.Variable.get('dataproc_zone'),<br/>cluster_name='dataproc-spark-pipeline-{{ds}}')</span><span id="5e24" class="lz ma in lv b gy mf mc l md me">#Delete Processed files<br/>remove_files = BashOperator(task_id='remove_files',<br/>bash_command='gsutil rm gs://inbound/Crimes2020.csv',<br/>dag=dag)</span><span id="c428" class="lz ma in lv b gy mf mc l md me">start = BashOperator(task_id='start',<br/>bash_command='echo date',<br/>dag=dag)</span><span id="c378" class="lz ma in lv b gy mf mc l md me">end = BashOperator(task_id='end',<br/>bash_command='echo date',<br/>dag=dag)</span><span id="d69e" class="lz ma in lv b gy mf mc l md me">sleep_process = BashOperator(task_id='sleep',<br/>bash_command='sleep 30',<br/>dag=dag)</span><span id="289f" class="lz ma in lv b gy mf mc l md me">#ETL pipeline <br/>start&gt;&gt;check_inbound_file<br/>&gt;&gt;create_cluster<br/>&gt;&gt;sleep_process<br/>&gt;&gt;spark_transformation<br/>&gt;&gt;delete_cluster<br/>&gt;&gt;remove_files<br/>&gt;&gt;end</span></pre><p id="9baf" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">看看这是怎么回事</p><p id="b204" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">上传入站 CSV 文件</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi mg"><img src="../Images/63ddd10b8c463d17769950aaabad6886.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C2unCifmtizc88rqlGer1g.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">上传入站文件</figcaption></figure><p id="3067" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">文件传感器事件任务:识别犯罪 2020.csv 并触发下一个任务，即创建火花簇</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi mh"><img src="../Images/2d12187613339d07fa3c7f77055084c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a2QZwt-ZWmE68EYURkcUgw.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">文件传感器任务</figcaption></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi mi"><img src="../Images/43ba69f6bce341967db876a20affe578.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*niT2bpMHLJv39J9Ka5ZOIg.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">文件传感器任务日志</figcaption></figure><p id="8acd" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">Spark 集群创建:创建带有 1 个主节点和 2 个工作节点的 DataProc Spark 集群</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi mj"><img src="../Images/09b8ab4d676967ac357545428201b427.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*47m2q70C_BWMr-IozZLH0w.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">创建 DataProc 火花簇</figcaption></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi mk"><img src="../Images/a8b2dbdbd9805c5e0b1e76e0c9740816.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wEvghPV9WJbNhe8JIz0yNg.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">供应 Spark 集群</figcaption></figure><p id="8704" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">执行火花作业</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi ml"><img src="../Images/d761df774fe396488dfc9959e386fc6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hj1qZPwMZkeEP4K7BvS0QA.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">向 Spark 集群提交 Spark 作业</figcaption></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi mm"><img src="../Images/e8f1cc651e4529d1a51ea5868d394fe3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ibHREWQDTN2NDGgdojxpRA.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">火花作业结果保存到出站存储桶</figcaption></figure><p id="50f5" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">删除火花簇</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi mn"><img src="../Images/26245df6e808ab931e5953be360e2def.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WtCRbzsExsxRQvWhfz9deQ.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">删除集群</figcaption></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi mo"><img src="../Images/04da44196c8bca15531e5ae1f0d4095b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BmOv0TUYHEgk82B-H0JTxQ.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">删除集群</figcaption></figure><p id="c0b5" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">结论</strong> : <em class="ki">我们已经看到谷歌云中的文件传感器事件和短暂火花簇在降低成本方面非常有用</em></p></div></div>    
</body>
</html>