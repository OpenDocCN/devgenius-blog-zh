<html>
<head>
<title>Beginner’s Introduction to Big Data in PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark 大数据初学者入门</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/beginners-introduction-to-big-data-in-pyspark-9931d919521c?source=collection_archive---------6-----------------------#2022-11-15">https://blog.devgenius.io/beginners-introduction-to-big-data-in-pyspark-9931d919521c?source=collection_archive---------6-----------------------#2022-11-15</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><figure class="gl gn jl jm jn jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi jk"><img src="../Images/5eb5838885568f0f044db78edf229432.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_KfLLedNUaL8gjQ3_lydIg.jpeg"/></div></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">帕斯卡尔·德布鲁纳在<a class="ae jz" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="8905" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">在这篇博客中，我们将深入探讨 PySpark 大数据基础知识。Python 一直是最流行的编程语言之一，因此集成 Python 和 Spark 只是一个好主意。它具有处理大数据的优势和 Python 的易解释性。</p><figure class="kz la lb lc gt jo gh gi paragraph-image"><div class="gh gi ky"><img src="../Images/261f852977737d18c7cc75605851d328.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*o4GIuUuZXttDvByats3P9g.png"/></div></figure><h2 id="c455" class="ld le in bd lf lg lh dn li lj lk dp ll kl lm ln lo kp lp lq lr kt ls lt lu lv bi translated">配置</h2><p id="0008" class="pw-post-body-paragraph ka kb in kc b kd lw kf kg kh lx kj kk kl ly kn ko kp lz kr ks kt ma kv kw kx ig bi translated">PySpark 可以在许多不同的平台上运行，对初学者来说最方便的是 Google colab。在 colab 中安装 PySpark 最简单的方法是</p><pre class="kz la lb lc gt mb mc md bn me mf bi"><span id="013d" class="mg le in mc b be mh mi l mj mk">!pip install pyspark</span></pre><h1 id="35c7" class="ml le in bd lf mm mn mo li mp mq mr ll ms mt mu lo mv mw mx lr my mz na lu nb bi translated">Spark 大数据分析简介</h1><h1 id="ab65" class="ml le in bd lf mm mn mo li mp mq mr ll ms mt mu lo mv mw mx lr my mz na lu nb bi translated">什么是大数据？</h1><p id="df83" class="pw-post-body-paragraph ka kb in kc b kd lw kf kg kh lx kj kk kl ly kn ko kp lz kr ks kt ma kv kw kx ig bi translated">根据维基百科的说法，大数据是一个术语，用于指对传统数据处理软件来说过于复杂的数据集的研究和应用。</p><p id="b07c" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">大数据的 3 个 V</strong></p><ol class=""><li id="ea58" class="nc nd in kc b kd ke kh ki kl ne kp nf kt ng kx nh ni nj nk bi translated">卷—指数据的大小</li><li id="3d86" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">多样性—指收集/生成数据的不同来源和格式</li><li id="9ec9" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">速度—指生成数据的速度</li></ol><p id="0f22" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">术语</strong></p><ol class=""><li id="8c05" class="nc nd in kc b kd ke kh ki kl ne kp nf kt ng kx nh ni nj nk bi translated">集群计算——汇集多台机器的资源来完成作业</li><li id="901c" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">并行处理—许多计算同时进行的处理</li><li id="bb36" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">分布式计算——并行运行作业的网络计算机(又名节点)的集合</li><li id="c111" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">批处理—指的是将数据分成更小的部分，并在单独的机器上运行每个部分</li><li id="c843" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">实时处理—意味着必须立即处理数据</li></ol><p id="6d17" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">了解大数据处理系统</strong></p><p id="b338" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">Hadoop 用于处理大数据。它是解决大数据问题的框架。它的工作原理是数据局部性，这意味着数据在保存的地方被处理。</p><p id="9b63" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">Hadoop 的核心组件</p><ol class=""><li id="3158" class="nc nd in kc b kd ke kh ki kl ne kp nf kt ng kx nh ni nj nk bi translated">HDFS —用于分布式文件存储</li><li id="3f5c" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">MapReduce——用于分布式处理(在许多地方使用 PySpark 代替 MapReduce——请进一步阅读)</li><li id="8cb0" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">纱线——资源管理</li></ol><p id="1549" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">Apache Spark 的特性</strong></p><ol class=""><li id="a21d" class="nc nd in kc b kd ke kh ki kl ne kp nf kt ng kx nh ni nj nk bi translated">它是一个分布式集群计算框架，这意味着它跨多个集群分布数据和计算</li><li id="05d7" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">Spark 的计算发生在内存中，这使得它对于大型数据集来说很快</li><li id="b1bd" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">Spark 可以提供比内存运行快 100 倍的计算速度</li><li id="c8c1" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">Spark 是用 Scala 编写的，但是支持 Python、Java、R 和 SQL</li></ol><p id="4c04" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">火花的成分</strong></p><ol class=""><li id="5825" class="nc nd in kc b kd ke kh ki kl ne kp nf kt ng kx nh ni nj nk bi translated">spark 核心—包含 spark 的基本功能</li><li id="721d" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">spark SQL——用于处理 python、java 和 Scala 中的结构化和半结构化数据的库</li><li id="052a" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">MLib——通用机器学习算法库</li><li id="b473" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">GraphX —操纵图形并执行并行图形计算</li><li id="8596" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">Spark Streaming —用于实时数据的可扩展、高吞吐量(吞吐量是对系统在给定时间内可以处理多少信息单元的衡量)库</li></ol><h1 id="97b7" class="ml le in bd lf mm mn mo li mp mq mr ll ms mt mu lo mv mw mx lr my mz na lu nb bi translated">PySpark</h1><p id="fe2a" class="pw-post-body-paragraph ka kb in kc b kd lw kf kg kh lx kj kk kl ly kn ko kp lz kr ks kt ma kv kw kx ig bi translated">Apache Spark 最初是用 Scala 编写的。为了支持 Python 实现，开发了 PySpark</p><p id="83a1" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">火花壳</strong></p><ol class=""><li id="20d4" class="nc nd in kc b kd ke kh ki kl ne kp nf kt ng kx nh ni nj nk bi translated">运行 Spark 作业的交互式环境</li><li id="3b3f" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">有助于交互式原型制作</li><li id="603f" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">Spark 允许在各种机器上运行进程，并自动负责分发进程</li><li id="f87c" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">PySpark shell 是 Python 语言中的 Spark shell</li></ol><p id="4743" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">星火语境</strong></p><ol class=""><li id="73fa" class="nc nd in kc b kd ke kh ki kl ne kp nf kt ng kx nh ni nj nk bi translated">Spark 上下文是与 sparks 功能交互的入口点</li><li id="aa76" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">入口点是连接 Spark 集群的一种方式</li><li id="2e93" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">PySpark 有一个名为“sc”的默认 Spark 上下文</li></ol><pre class="kz la lb lc gt mb mc md bn me mf bi"><span id="5038" class="mg le in mc b be mh mi l mj mk"><br/># creating a spark context<br/>from pyspark import SparkContext<br/>sc = SparkContext("local", "Simple App")<br/><br/># Print the version of SparkContext<br/>print("The version of Spark Context in the PySpark shell is", sc.version)<br/><br/># Print the Python version of SparkContext<br/>print("The Python version of Spark Context in the PySpark shell is", sc.pythonVer)<br/><br/># Print the master of SparkContext<br/>print("The master of Spark Context in the PySpark shell is", sc.master)</span></pre><h1 id="aefc" class="ml le in bd lf mm mn mo li mp mq mr ll ms mt mu lo mv mw mx lr my mz na lu nb bi translated">PySpark 的 RDD 的编程</h1><h1 id="e86e" class="ml le in bd lf mm mn mo li mp mq mr ll ms mt mu lo mv mw mx lr my mz na lu nb bi translated">用 RDD 氏提取数据</h1><p id="7644" class="pw-post-body-paragraph ka kb in kc b kd lw kf kg kh lx kj kk kl ly kn ko kp lz kr ks kt ma kv kw kx ig bi translated"><strong class="kc io">什么是 RDD </strong></p><ol class=""><li id="cdb4" class="nc nd in kc b kd ke kh ki kl ne kp nf kt ng kx nh ni nj nk bi translated">RDD 代表弹性分布式数据集</li><li id="b43c" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">它是一个不可变的分布式对象集合</li><li id="6d3d" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">它是分布在集群中的数据集合</li><li id="a28d" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">Spark 中的基本数据类型</li></ol><p id="3e46" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">RDD 的结构</strong></p><ol class=""><li id="4b19" class="nc nd in kc b kd ke kh ki kl ne kp nf kt ng kx nh ni nj nk bi translated">弹性—承受失败的能力</li><li id="552c" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">分布式—意味着将作业跨越群集中的多个节点，以实现高效计算</li><li id="282d" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">数据集—分区数据的集合</li></ol><p id="d45b" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">创建 rdd</strong></p><ol class=""><li id="5b30" class="nc nd in kc b kd ke kh ki kl ne kp nf kt ng kx nh ni nj nk bi translated">并行处理现有的对象集合</li><li id="53ac" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">外部数据集和文本文件方法</li><li id="8dbb" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">从现有 rdd</li></ol><pre class="kz la lb lc gt mb mc md bn me mf bi"><span id="f28c" class="mg le in mc b be mh mi l mj mk">filename = '/content/sample1.txt'<br/>rdd = sc.textFile(filename)</span></pre><p id="08c2" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">py spark 中的分区</strong></p><ol class=""><li id="2222" class="nc nd in kc b kd ke kh ki kl ne kp nf kt ng kx nh ni nj nk bi translated">分区是大型分布式数据集的逻辑划分</li></ol><pre class="kz la lb lc gt mb mc md bn me mf bi"><span id="1c1e" class="mg le in mc b be mh mi l mj mk"># Print the file_path<br/>print("The file_path is", file_path)<br/><br/># Create a fileRDD from file_path<br/>fileRDD = sc.textFile(file_path)<br/><br/># Check the type of fileRDD<br/>print("The file type of fileRDD is", type(fileRDD))<br/><br/># Check the number of partitions in fileRDD<br/>print("Number of partitions in fileRDD is", fileRDD.getNumPartitions())<br/><br/># Create a fileRDD_part from file_path with 5 partitions<br/>fileRDD_part = sc.textFile(file_path, minPartitions = 5)<br/><br/># Check the number of partitions in fileRDD_part<br/>print("Number of partitions in fileRDD_part is", fileRDD_part.getNumPartitions())</span></pre><h1 id="b185" class="ml le in bd lf mm mn mo li mp mq mr ll ms mt mu lo mv mw mx lr my mz na lu nb bi translated">基本 RDD 变换和行动</h1><p id="27d1" class="pw-post-body-paragraph ka kb in kc b kd lw kf kg kh lx kj kk kl ly kn ko kp lz kr ks kt ma kv kw kx ig bi translated"><strong class="kc io">py spark 操作概述</strong></p><ol class=""><li id="0488" class="nc nd in kc b kd ke kh ki kl ne kp nf kt ng kx nh ni nj nk bi translated">转换创建新的 rdd</li><li id="cdb9" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">转换遵循惰性求值，这意味着 Spark 会根据您在 RDD 上执行的所有操作创建一个图，并且只有在 RDD 上执行某个操作时，才会开始执行图</li><li id="7b52" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">基本的 RDD 变换</li></ol><blockquote class="nq nr ns"><p id="32a2" class="ka kb nt kc b kd ke kf kg kh ki kj kk nu km kn ko nv kq kr ks nw ku kv kw kx ig bi translated">map()-将函数应用于 RDD 中的所有元素</p><p id="cd3d" class="ka kb nt kc b kd ke kf kg kh ki kj kk nu km kn ko nv kq kr ks nw ku kv kw kx ig bi translated">filter()-返回 RDD 中满足条件的元素</p><p id="89e4" class="ka kb nt kc b kd ke kf kg kh ki kj kk nu km kn ko nv kq kr ks nw ku kv kw kx ig bi translated">flat map()-为原始 RDD 中的每个元素返回多个值</p><p id="71c2" class="ka kb nt kc b kd ke kf kg kh ki kj kk nu km kn ko nv kq kr ks nw ku kv kw kx ig bi translated">联合()—一个以上 RDD 的联合</p></blockquote><ol class=""><li id="2d6a" class="nc nd in kc b kd ke kh ki kl ne kp nf kt ng kx nh ni nj nk bi translated">操作对 RDD 执行计算并返回值</li></ol><blockquote class="nq nr ns"><p id="b0a7" class="ka kb nt kc b kd ke kf kg kh ki kj kk nu km kn ko nv kq kr ks nw ku kv kw kx ig bi translated">collect()-以数组形式返回 RDD 的所有元素</p><p id="8e9e" class="ka kb nt kc b kd ke kf kg kh ki kj kk nu km kn ko nv kq kr ks nw ku kv kw kx ig bi translated">take(N)-从 RDD 返回前 N 个元素</p><p id="2f11" class="ka kb nt kc b kd ke kf kg kh ki kj kk nu km kn ko nv kq kr ks nw ku kv kw kx ig bi translated">first()-返回 RDD 中的第一个元素</p><p id="a197" class="ka kb nt kc b kd ke kf kg kh ki kj kk nu km kn ko nv kq kr ks nw ku kv kw kx ig bi translated">count()-返回 RDD 中元素的数量</p></blockquote><pre class="kz la lb lc gt mb mc md bn me mf bi"><span id="29f7" class="mg le in mc b be mh mi l mj mk">#  Create map() transformation to cube numbers<br/>numbRDD = sc.parallelize(range(1,11))<br/>cubedRDD = numbRDD.map(lambda x: x**3)<br/><br/># Collect the results<br/>numbers_all = cubedRDD.collect()<br/><br/># Print the numbers from numbers_all<br/>for numb in numbers_all:<br/> print(numb)</span></pre><pre class="nx mb mc md bn me mf bi"><span id="7c6c" class="mg le in mc b be mh mi l mj mk"># Filter the fileRDD to select lines with Spark keyword<br/>fileRDD_filter = fileRDD.filter(lambda line: 'est' in line)<br/><br/># How many lines are there in fileRDD?<br/>print("The total number of lines with the keyword Spark is", fileRDD_filter.count())<br/><br/># Print the first four lines of fileRDD<br/>for line in fileRDD_filter.take(4): <br/>  print(line)</span></pre><h1 id="2333" class="ml le in bd lf mm mn mo li mp mq mr ll ms mt mu lo mv mw mx lr my mz na lu nb bi translated">PySpark 中的配对 rdd</h1><ol class=""><li id="0f72" class="nc nd in kc b kd lw kh lx kl ny kp nz kt oa kx nh ni nj nk bi translated">键是标识符，值是数据</li></ol><p id="9f35" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">创建配对 rdd</strong></p><ol class=""><li id="c2dc" class="nc nd in kc b kd ke kh ki kl ne kp nf kt ng kx nh ni nj nk bi translated">从键值元组列表中</li><li id="32b2" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">来自一个普通的 RDD</li></ol><p id="2dc1" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">对 rdd 的转换</strong></p><ol class=""><li id="7eef" class="nc nd in kc b kd ke kh ki kl ne kp nf kt ng kx nh ni nj nk bi translated">所有 RDD 变换都作用于 RDD 对</li><li id="3558" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">必须传递操作键值对而不是单个元素的函数</li><li id="73e2" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">一些功能-</li></ol><blockquote class="nq nr ns"><p id="bb37" class="ka kb nt kc b kd ke kf kg kh ki kj kk nu km kn ko nv kq kr ks nw ku kv kw kx ig bi translated">reduce by key(func)-组合具有相同关键字的值</p><p id="9893" class="ka kb nt kc b kd ke kf kg kh ki kj kk nu km kn ko nv kq kr ks nw ku kv kw kx ig bi translated">group by key(func)-对具有相同关键字的值进行分组</p><p id="f58f" class="ka kb nt kc b kd ke kf kg kh ki kj kk nu km kn ko nv kq kr ks nw ku kv kw kx ig bi translated">sort by key(func)-返回按关键字排序的 RDD</p><p id="8362" class="ka kb nt kc b kd ke kf kg kh ki kj kk nu km kn ko nv kq kr ks nw ku kv kw kx ig bi translated">join() —根据关键字连接两对 rdd</p></blockquote><pre class="kz la lb lc gt mb mc md bn me mf bi"><span id="0879" class="mg le in mc b be mh mi l mj mk"># Create PairRDD Rdd with key value pairs<br/>Rdd = sc.parallelize([(1,2),(3,4),(3,6),(4,5)])<br/><br/># Apply reduceByKey() operation on Rdd<br/>Rdd_Reduced = Rdd.reduceByKey(lambda x, y: x + y)<br/><br/># Iterate over the result and print the output<br/>for num in Rdd_Reduced.collect(): <br/>  print("Key {} has {} Counts".format(num[0], num[1]))</span></pre><pre class="nx mb mc md bn me mf bi"><span id="3235" class="mg le in mc b be mh mi l mj mk"># Sort the reduced RDD with the key by descending order<br/>Rdd_Reduced_Sort = Rdd_Reduced.sortByKey(ascending=False)<br/><br/># Iterate over the result and retrieve all the elements of the RDD<br/>for num in Rdd_Reduced_Sort.collect():<br/>  print("Key {} has {} Counts".format(num[0], num[1]))</span></pre><h1 id="9376" class="ml le in bd lf mm mn mo li mp mq mr ll ms mt mu lo mv mw mx lr my mz na lu nb bi translated">高级 RDD 行动</h1><ul class=""><li id="d689" class="nc nd in kc b kd lw kh lx kl ny kp nz kt oa kx ob ni nj nk bi translated">reduce()操作</li></ul><ol class=""><li id="32b3" class="nc nd in kc b kd ke kh ki kl ne kp nf kt ng kx nh ni nj nk bi translated">reduce(func)用于聚合常规 RDD 的元素</li><li id="5e28" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">函数应该是可交换的和可结合的</li></ol><ul class=""><li id="2417" class="nc nd in kc b kd ke kh ki kl ne kp nf kt ng kx ob ni nj nk bi translated">saveAsTextFile()操作</li></ul><ol class=""><li id="234e" class="nc nd in kc b kd ke kh ki kl ne kp nf kt ng kx nh ni nj nk bi translated">将 RDD 保存到一个文本文件中，每个分区作为一个单独的文件</li><li id="9f54" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">coalesce()可以用来将 RDD 保存为一个文本文件</li></ol><ul class=""><li id="c58d" class="nc nd in kc b kd ke kh ki kl ne kp nf kt ng kx ob ni nj nk bi translated">对 rdd 对的操作操作</li></ul><blockquote class="nq nr ns"><p id="f941" class="ka kb nt kc b kd ke kf kg kh ki kj kk nu km kn ko nv kq kr ks nw ku kv kw kx ig bi translated">countByKey() —计算每个键的元素数并返回一个列表</p><p id="b6dc" class="ka kb nt kc b kd ke kf kg kh ki kj kk nu km kn ko nv kq kr ks nw ku kv kw kx ig bi translated">collectAsMap() —以字典形式返回键值对</p></blockquote><pre class="kz la lb lc gt mb mc md bn me mf bi"><span id="e32a" class="mg le in mc b be mh mi l mj mk"># Count the unique keys<br/>total = Rdd.countByKey()<br/><br/># What is the type of total?<br/>print("The type of total is", type(total))<br/><br/># Iterate over the total and print the output<br/>for k, v in total.items(): <br/>  print("key", k, "has", v, "counts")</span></pre><h1 id="869d" class="ml le in bd lf mm mn mo li mp mq mr ll ms mt mu lo mv mw mx lr my mz na lu nb bi translated">PySpark SQL 和数据框架</h1><h2 id="c2ae" class="ld le in bd lf lg lh dn li lj lk dp ll kl lm ln lo kp lp lq lr kt ls lt lu lv bi translated">用数据帧提取数据</h2><p id="ca20" class="pw-post-body-paragraph ka kb in kc b kd lw kf kg kh lx kj kk kl ly kn ko kp lz kr ks kt ma kv kw kx ig bi translated"><strong class="kc io"> PySpark 数据帧</strong></p><ol class=""><li id="9a1f" class="nc nd in kc b kd ke kh ki kl ne kp nf kt ng kx nh ni nj nk bi translated">PySpark SQL 是一个用于结构化和半结构化数据的 Spark 库。它提供了比 RDD 更多的关于数据结构的信息</li><li id="1f64" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">它们是不可变的分布式集合</li><li id="2d3e" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">Pyspark 中的数据帧支持 SQL 查询或表达式方法</li></ol><p id="ab60" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">火花会议</p><ol class=""><li id="aa1b" class="nc nd in kc b kd ke kh ki kl ne kp nf kt ng kx nh ni nj nk bi translated">就像 SparkContext 是创建 rdd 的主要入口点一样，SparkSession 是 Spark 数据帧的单一入口点。</li><li id="f13f" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">Sparksession 可用于创建数据帧、注册数据帧、执行 SQL 查询</li><li id="92df" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">它在 PySpark 外壳中作为 Spark 提供</li></ol><p id="287c" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">在 PySpark 中创建数据帧</strong></p><ol class=""><li id="256c" class="nc nd in kc b kd ke kh ki kl ne kp nf kt ng kx nh ni nj nk bi translated">使用 SparkSession 的 createDataFrame()方法从现有 rdd</li><li id="df8c" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">使用 SparkSession 的读取方法[spark.read.csv()]从数据源(CSV、JSON、TXT)读取</li></ol><pre class="kz la lb lc gt mb mc md bn me mf bi"><span id="289e" class="mg le in mc b be mh mi l mj mk">sample_list  = [('Mona', 20), ('Jennifer', 34), ('John', 20), ('Jim', 26)]<br/><br/># Create an RDD from the list<br/>rdd = sc.parallelize(sample_list)<br/><br/># Create a PySpark DataFrame<br/>names_df = spark.createDataFrame(rdd, schema=['Name', 'Age'])<br/><br/># Check the type of names_df<br/>print("The type of names_df is", type(names_df))</span></pre><pre class="nx mb mc md bn me mf bi"><span id="9562" class="mg le in mc b be mh mi l mj mk">file_path = 'FILENAME.csv'<br/># Create a DataFrame from file_path<br/>people_df = spark.read.csv(file_path, header=False, inferSchema=True)<br/><br/># Check the type of people_df<br/>print("The type of people_df is", type(people_df))</span></pre><pre class="nx mb mc md bn me mf bi"><span id="f0d7" class="mg le in mc b be mh mi l mj mk">This is what our data looks like<br/><br/>+---+--------------------+------------------+---+-------+------+-----+-------+--------------------+----+<br/>|_c0|                 _c1|               _c2|_c3|    _c4|   _c5|  _c6|    _c7|                 _c8| _c9|<br/>+---+--------------------+------------------+---+-------+------+-----+-------+--------------------+----+<br/>|  1|Eldon Base for st...|Muhammed MacIntyre|  3|-213.25| 38.94| 35.0|Nunavut|Storage &amp; Organiz...| 0.8|<br/>|  2|"1.7 Cubic Foot C...|      Barry French|293| 457.81|208.16|68.02|Nunavut|          Appliances|0.58|<br/>|  3|Cardinal Slant-D�...|      Barry French|293|  46.71|  8.69| 2.99|Nunavut|Binders and Binde...|0.39|<br/>|  4|                R380|     Clay Rozendal|483|1198.97|195.99| 3.99|Nunavut|Telephones and Co...|0.58|<br/>|  5|Holmes HEPA Air P...|    Carlos Soltero|515|  30.94| 21.78| 5.94|Nunavut|          Appliances| 0.5|<br/>|  6|G.E. Longer-Life ...|    Carlos Soltero|515|   4.43|  6.64| 4.95|Nunavut|  Office Furnishings|0.37|<br/>|  7|Angle-D Binders w...|      Carl Jackson|613| -54.04|   7.3| 7.72|Nunavut|Binders and Binde...|0.38|<br/>|  8|SAFCO Mobile Desk...|      Carl Jackson|613|  127.7| 42.76| 6.22|Nunavut|Storage &amp; Organiz...|null|<br/>|  9|SAFCO Commercial ...|    Monica Federle|643|-695.26|138.14| 35.0|Nunavut|Storage &amp; Organiz...|null|<br/>| 10|           Xerox 198|   Dorothy Badders|678|-226.36|  4.98| 8.33|Nunavut|               Paper|0.38|<br/>+---+--------------------+------------------+---+-------+------+-----+-------+--------------------+----+</span></pre><h1 id="0934" class="ml le in bd lf mm mn mo li mp mq mr ll ms mt mu lo mv mw mx lr my mz na lu nb bi translated">在 PySpark 中操作数据帧</h1><p id="6bed" class="pw-post-body-paragraph ka kb in kc b kd lw kf kg kh lx kj kk kl ly kn ko kp lz kr ks kt ma kv kw kx ig bi translated"><strong class="kc io">PySpark 中的 Dataframe 运算符</strong></p><ol class=""><li id="3c7d" class="nc nd in kc b kd ke kh ki kl ne kp nf kt ng kx nh ni nj nk bi translated">数据帧转换-</li></ol><blockquote class="nq nr ns"><p id="89a9" class="ka kb nt kc b kd ke kf kg kh ki kj kk nu km kn ko nv kq kr ks nw ku kv kw kx ig bi translated">select()-对数据帧中的列进行子集划分</p><p id="7d0b" class="ka kb nt kc b kd ke kf kg kh ki kj kk nu km kn ko nv kq kr ks nw ku kv kw kx ig bi translated">filter()-基于条件的选择</p><p id="45eb" class="ka kb nt kc b kd ke kf kg kh ki kj kk nu km kn ko nv kq kr ks nw ku kv kw kx ig bi translated">group by()-使用指定的列对数据帧进行分组</p><p id="2056" class="ka kb nt kc b kd ke kf kg kh ki kj kk nu km kn ko nv kq kr ks nw ku kv kw kx ig bi translated">orderby() —返回按一列或多列排序的数据帧</p><p id="7c8c" class="ka kb nt kc b kd ke kf kg kh ki kj kk nu km kn ko nv kq kr ks nw ku kv kw kx ig bi translated">drop duplicates()-删除重复的行</p><p id="5970" class="ka kb nt kc b kd ke kf kg kh ki kj kk nu km kn ko nv kq kr ks nw ku kv kw kx ig bi translated">with column renamed()-重命名列</p></blockquote><ol class=""><li id="4c7a" class="nc nd in kc b kd ke kh ki kl ne kp nf kt ng kx nh ni nj nk bi translated">数据框架操作-</li></ol><blockquote class="nq nr ns"><p id="7fe6" class="ka kb nt kc b kd ke kf kg kh ki kj kk nu km kn ko nv kq kr ks nw ku kv kw kx ig bi translated">head() —将前 n 行作为行对象返回</p><p id="eb8f" class="ka kb nt kc b kd ke kf kg kh ki kj kk nu km kn ko nv kq kr ks nw ku kv kw kx ig bi translated">printSchema() —返回 dataframe 中列的数据类型</p><p id="c893" class="ka kb nt kc b kd ke kf kg kh ki kj kk nu km kn ko nv kq kr ks nw ku kv kw kx ig bi translated">columns()-打印数据帧的列</p><p id="0dda" class="ka kb nt kc b kd ke kf kg kh ki kj kk nu km kn ko nv kq kr ks nw ku kv kw kx ig bi translated">show()-打印数据帧中的前 20 行</p><p id="516d" class="ka kb nt kc b kd ke kf kg kh ki kj kk nu km kn ko nv kq kr ks nw ku kv kw kx ig bi translated">count()-计算数据帧中的行数</p><p id="68d2" class="ka kb nt kc b kd ke kf kg kh ki kj kk nu km kn ko nv kq kr ks nw ku kv kw kx ig bi translated">describe()-数字列的汇总统计信息</p></blockquote><pre class="kz la lb lc gt mb mc md bn me mf bi"><span id="9922" class="mg le in mc b be mh mi l mj mk"># Print the first 10 observations <br/>people_df.show(10)<br/><br/># Count the number of rows <br/>print("There are {} rows in the people_df DataFrame.".format(people_df.count()))<br/><br/># Count the number of columns and their names<br/>print("There are {} columns in the people_df DataFrame and their names are {}"\<br/>      .format(len(people_df.columns), people_df.columns))</span></pre><pre class="nx mb mc md bn me mf bi"><span id="95f1" class="mg le in mc b be mh mi l mj mk"># Select _c0,_c1,_c2 of birth columns<br/>people_df_sub = people_df.select('_c0', "_c1", "_c2")<br/><br/># Print the first 10 observations from people_df_sub<br/>people_df_sub.show(10)<br/><br/># Remove duplicate entries from people_df_sub<br/>people_df_sub_nodup = people_df_sub.dropDuplicates()<br/><br/># Count the number of rows<br/>print("There were {} rows before removing duplicates, and {} rows after removing duplicates"\<br/>      .format(people_df_sub.count(), people_df_sub_nodup.count()))</span></pre><p id="a705" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">上述查询的输出-</p><figure class="kz la lb lc gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi oc"><img src="../Images/fa5922faa971bc5f0f6631b75bdc3b39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b573CNvkjxt-zsa4k59cuA.png"/></div></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">图片作者使用<a class="ae jz" href="https://carbon.now" rel="noopener ugc nofollow" target="_blank"> https://carbon.now </a></figcaption></figure><pre class="kz la lb lc gt mb mc md bn me mf bi"><span id="1486" class="mg le in mc b be mh mi l mj mk"># Filter people_df to select Nunavut <br/>people_df_Nunavut = people_df.filter(people_df._c7 == "Nunavut")<br/><br/># Filter people_df to select Northwest Territories<br/>people_df_nt = people_df.filter(people_df._c7 == "Northwest Territories")<br/><br/># Count the number of rows <br/>print("There are {} rows in the people_df_female DataFrame and {} rows in the people_df_male DataFrame"\<br/>      .format(people_df_Nunavut.count(), people_df_nt.count()))</span></pre><h1 id="b595" class="ml le in bd lf mm mn mo li mp mq mr ll ms mt mu lo mv mw mx lr my mz na lu nb bi translated">PySpark SQL</h1><p id="457b" class="pw-post-body-paragraph ka kb in kc b kd lw kf kg kh lx kj kk kl ly kn ko kp lz kr ks kt ma kv kw kx ig bi translated"><strong class="kc io">数据框架 API 与 SQL 查询</strong></p><ol class=""><li id="1b79" class="nc nd in kc b kd ke kh ki kl ne kp nf kt ng kx nh ni nj nk bi translated">使用编程领域特定语言的数据框架 API</li><li id="041f" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">SQL 查询可以很简洁，更容易理解</li></ol><p id="880b" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">执行 SQL 查询</p><ol class=""><li id="c673" class="nc nd in kc b kd ke kh ki kl ne kp nf kt ng kx nh ni nj nk bi translated">SparkSession sql()方法执行 sql 查询</li><li id="e74f" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">SQL 不能针对数据帧运行，因此我们必须使用 createOrReplaceTempView()命令创建一个临时视图，并运行 SQL 查询，它将返回一个数据帧</li><li id="d300" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">语法-df . createorreplacetenview(" test ")</li></ol><p id="c7a1" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">&gt; &gt; test_df = spark.sql(来自测试的查询报价)</p><p id="d447" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">&gt; &gt; test_df.show()</p><pre class="kz la lb lc gt mb mc md bn me mf bi"><span id="011f" class="mg le in mc b be mh mi l mj mk"># Create a temporary table "people"<br/>people_df.createOrReplaceTempView("people")<br/><br/># Construct a query to select the names of the people from the temporary table "people"<br/>query = '''SELECT _c2 FROM people'''<br/><br/># Assign the result of Spark's query to people_df_names<br/>people_df_names = spark.sql(query)<br/><br/># Print the top 10 names of the people<br/>people_df_names.show(10)</span></pre><p id="af56" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">上述查询的输出—</p><figure class="kz la lb lc gt jo gh gi paragraph-image"><div class="gh gi od"><img src="../Images/cf2938e480121cff511d957dc2e1a4d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*KGGaVHapl3luNb-0vVd1jQ.png"/></div></figure><pre class="kz la lb lc gt mb mc md bn me mf bi"><span id="2814" class="mg le in mc b be mh mi l mj mk"># Filter the people table to select female sex <br/>people_Nunavut_df = spark.sql('SELECT * FROM people WHERE _c7=="Nunavut"')<br/><br/># Filter the people table DataFrame to select male sex<br/>people_nt_df = spark.sql('SELECT * FROM people WHERE _c7=="Northwest Territories"')<br/><br/># Count the number of rows in both DataFrames<br/>print("There are {} rows in the people_female_df and {} rows in the people_male_df DataFrames"\<br/>      .format(people_Nunavut_df.count(), people_nt_df.count()))</span></pre><h1 id="14f6" class="ml le in bd lf mm mn mo li mp mq mr ll ms mt mu lo mv mw mx lr my mz na lu nb bi translated">PySpark 中的数据可视化</h1><p id="ac5d" class="pw-post-body-paragraph ka kb in kc b kd lw kf kg kh lx kj kk kl ly kn ko kp lz kr ks kt ma kv kw kx ig bi translated"><strong class="kc io">使用 PySpark 数据帧绘制图表可通过以下方法完成</strong></p><ol class=""><li id="c444" class="nc nd in kc b kd ke kh ki kl ne kp nf kt ng kx nh ni nj nk bi translated">Pyspark_dist_explore 库</li></ol><blockquote class="nq nr ns"><p id="3baa" class="ka kb nt kc b kd ke kf kg kh ki kj kk nu km kn ko nv kq kr ks nw ku kv kw kx ig bi translated">历史()</p><p id="dcdb" class="ka kb nt kc b kd ke kf kg kh ki kj kk nu km kn ko nv kq kr ks nw ku kv kw kx ig bi translated">距离图()</p><p id="1016" class="ka kb nt kc b kd ke kf kg kh ki kj kk nu km kn ko nv kq kr ks nw ku kv kw kx ig bi translated">熊猫 _ 直方图()-</p></blockquote><p id="21f0" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">2.托潘达斯</p><blockquote class="nq nr ns"><p id="a636" class="ka kb nt kc b kd ke kf kg kh ki kj kk nu km kn ko nv kq kr ks nw ku kv kw kx ig bi translated">使用 df.toPandas()将数据帧转换为 pandas 数据帧，然后绘图</p></blockquote><p id="86e3" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">3.HandySpark 图书馆</p><blockquote class="nq nr ns"><p id="8c28" class="ka kb nt kc b kd ke kf kg kh ki kj kk nu km kn ko nv kq kr ks nw ku kv kw kx ig bi translated">使用 df.toHandy()将 df 转换成方便的数据帧，然后绘图</p></blockquote><pre class="kz la lb lc gt mb mc md bn me mf bi"><span id="8618" class="mg le in mc b be mh mi l mj mk"># Check the column names of names_df<br/>print("The column names of names_df are", names_df.columns)<br/><br/># Convert to Pandas DataFrame  <br/>df_pandas = names_df.toPandas()<br/><br/>import matplotlib.pyplot as plt<br/><br/># Create a horizontal bar plot<br/>df_pandas.plot(kind='barh', x='Name', y='Age', colormap='winter_r')<br/>plt.show()</span></pre><figure class="kz la lb lc gt jo gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/4233da35adf4fe98302589bb6d49e68c.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*PcgoiidMzvcXUeOrbWP59A.png"/></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">上述查询的输出</figcaption></figure><p id="36f4" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">熊猫数据帧到 PySpark 数据帧</strong></p><ol class=""><li id="4d8a" class="nc nd in kc b kd ke kh ki kl ne kp nf kt ng kx nh ni nj nk bi translated">Pandas 是基于单服务器的内存结构，而 Pyspark 操作是并行运行的</li><li id="fda4" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">熊猫数据帧是可变的，Pyspark 数据帧是不可变的</li></ol><h1 id="218c" class="ml le in bd lf mm mn mo li mp mq mr ll ms mt mu lo mv mw mx lr my mz na lu nb bi translated">结论</h1><p id="2271" class="pw-post-body-paragraph ka kb in kc b kd lw kf kg kh lx kj kk kl ly kn ko kp lz kr ks kt ma kv kw kx ig bi translated">PySpark 对于数据分析和处理大量数据来说是一项重要的技能。我希望这篇博客能帮助你了解 PySpark 的基础知识和它的工作原理。PySpark 也有一个名为 MLib 的机器学习库，我将在另一个博客中介绍。</p><h1 id="5d6d" class="ml le in bd lf mm mn mo li mp mq mr ll ms mt mu lo mv mw mx lr my mz na lu nb bi translated">参考资料和资源-</h1><ol class=""><li id="a9ee" class="nc nd in kc b kd lw kh lx kl ny kp nz kt oa kx nh ni nj nk bi translated">我的 PySpark 代码—<a class="ae jz" href="https://github.com/RaghuMadhavTiwari/PySpark-practice/blob/main/IntroToPySpark.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/RaghuMadhavTiwari/py spark-practice/blob/main/introtopyspark . ipynb</a></li><li id="b01d" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">我学习 PySpark 的 Datacamp 课程—<a class="ae jz" href="https://app.datacamp.com/learn/courses/big-data-fundamentals-with-pyspark" rel="noopener ugc nofollow" target="_blank">https://app . data camp . com/learn/courses/big-data-fundamentals-with py spark</a></li><li id="6789" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">PySpark 博客—<a class="ae jz" href="https://jacobcelestine.com/knowledge_repo/colab_and_pyspark/#installing-spark" rel="noopener ugc nofollow" target="_blank">https://Jacob celestine . com/knowledge _ repo/colab _ and _ py spark/# installing-spark</a></li><li id="823a" class="nc nd in kc b kd nl kh nm kl nn kp no kt np kx nh ni nj nk bi translated">面向傻瓜的大数据，用于理解大数据基础知识。</li></ol><h1 id="16be" class="ml le in bd lf mm mn mo li mp mq mr ll ms mt mu lo mv mw mx lr my mz na lu nb bi translated">祝你好运！</h1><p id="4557" class="pw-post-body-paragraph ka kb in kc b kd lw kf kg kh lx kj kk kl ly kn ko kp lz kr ks kt ma kv kw kx ig bi translated">如果你想进一步讨论这个问题，请通过 LinkedIn 联系我。<strong class="kc io">在下面留下掌声和评论支持博客！关注更多。</strong></p></div></div>    
</body>
</html>