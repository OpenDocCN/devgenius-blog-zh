<html>
<head>
<title>Distributed Model Training</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分布式模型训练</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/distributed-model-training-db295ea890ca?source=collection_archive---------7-----------------------#2022-07-01">https://blog.devgenius.io/distributed-model-training-db295ea890ca?source=collection_archive---------7-----------------------#2022-07-01</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><figure class="gl gn jl jm jn jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi jk"><img src="../Images/bbc67df0c4b9cbf3420de596df9c6e26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*54lQQNfQLL2b_mMr.png"/></div></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">红猪。</figcaption></figure><h1 id="6e58" class="jz ka in bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">分布式培训</h1><p id="81a7" class="pw-post-body-paragraph kx ky in kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu ig bi translated">深度学习是机器学习的一个子集，是人工智能通过经验执行任务的一个分支。深度学习算法非常适合，在大型数据集上表现最佳，更不用说对高计算能力的需求了。使用按使用付费的无服务器服务模式，如谷歌协作实验室，在云上训练大型神经网络比以往任何时候都更容易。虽然在一台多核 GPU 机器上训练大型模型是可能的，但这可能需要几天甚至几周的时间。因此，这导致了减少训练时间的根本问题。</p><p id="1195" class="pw-post-body-paragraph kx ky in kz b la lv lc ld le lw lg lh li lx lk ll lm ly lo lp lq lz ls lt lu ig bi translated">通常，任何扩展问题都可以通过向上扩展或向外扩展来解决，即水平和垂直扩展。根据不同的使用案例，垂直扩展在某一点上有最大限度的限制，并且从长远来看，在价格和技术储备方面往往会更加昂贵。</p><p id="4276" class="pw-post-body-paragraph kx ky in kz b la lv lc ld le lw lg lh li lx lk ll lm ly lo lp lq lz ls lt lu ig bi translated"><strong class="kz io">一行程序:</strong>分布式训练将训练工作量分布在多个计算处理器上。在工作节点集群并行工作以加速训练过程的情况下，并行性通过数据并行性或模型并行性来实现。</p><h1 id="7e87" class="jz ka in bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">分布式培训的类型</h1><h1 id="c0c1" class="jz ka in bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">数据并行性</h1><p id="8929" class="pw-post-body-paragraph kx ky in kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu ig bi translated">顾名思义，数据集是水平/垂直分片并并行处理的。群集中的每个工作节点根据不同批次的训练数据训练模型的副本，传递计算结果以保持模型参数和梯度在所有节点上同步。计算结果可以同步共享，即在每次批处理计算结束时共享，也可以异步共享。</p><figure class="mb mc md me gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi ma"><img src="../Images/5418f392687fac62907f6ff872de365b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ag1Le5jE0M4wRxE3.png"/></div></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">图 1:数据并行训练。</figcaption></figure><p id="ae6c" class="pw-post-body-paragraph kx ky in kz b la lv lc ld le lw lg lh li lx lk ll lm ly lo lp lq lz ls lt lu ig bi translated"><strong class="kz io">一行代码:</strong>整个模型被部署到集群的多个节点上，每个节点代表分片数据集和模型的水平/垂直分割。</p><h1 id="0dad" class="jz ka in bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">模型并行性</h1><p id="4f90" class="pw-post-body-paragraph kx ky in kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu ig bi translated">相反，在模型并行性中，在模型大小对于单个工作者来说太大的情况下，模型本身被分成部分/层；因此，跨不同的工作者节点同时训练一组层。整个数据集被复制/可用于所有工作节点，并且它们仅与其他工作节点共享全局模型参数-通常就在向前或向后传播之前。此外，这些层可以垂直或水平划分。</p><figure class="mb mc md me gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi mf"><img src="../Images/b0d5548d7725c7c5b4c44f8dd1bb0054.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*AWH-az3CtjkuRBTA.png"/></div></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">图 2:模型并行训练。</figcaption></figure><p id="0943" class="pw-post-body-paragraph kx ky in kz b la lv lc ld le lw lg lh li lx lk ll lm ly lo lp lq lz ls lt lu ig bi translated"><strong class="kz io">一行:</strong>模型的一层或一组层被部署到集群的多个节点上，整个数据集被复制到每个节点上。</p><figure class="mb mc md me gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi mg"><img src="../Images/dd29e4de31562e2a8fcc4ba346f5e2a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SUsdKEcTRPMRRZy_.png"/></div></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">图 3:水平或垂直的模型划分。</figcaption></figure><p id="b3e7" class="pw-post-body-paragraph kx ky in kz b la lv lc ld le lw lg lh li lx lk ll lm ly lo lp lq lz ls lt lu ig bi translated">这两者中，数据并行比较常用，也比较容易实现。以批量数据(非顺序)训练模型并对模型的整体性能做出贡献的能力是解决方案的关键。换句话说，为工作者节点中的每一小批数据计算模型参数和梯度，并在结束时→将更新的权重发送回发起节点→将来自每个工作者节点的权重的加权平均值/均值应用于模型参数→将更新的模型参数发送回所有工作者节点，用于下一次迭代；这导致了关于模型参数如何以及何时被存储和更新的问题。</p><h1 id="9a3f" class="jz ka in bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">分布式训练循环</h1><p id="9a4b" class="pw-post-body-paragraph kx ky in kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu ig bi translated">执行分布式训练循环的两种方式如下:</p><h1 id="842b" class="jz ka in bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">同步训练</h1><p id="cbc2" class="pw-post-body-paragraph kx ky in kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu ig bi translated">再次以数据并行性为例，我们将数据划分为分区/批次，供每个工作节点处理。每个 worker 节点都有一个完整的模型副本和一批数据。</p><ul class=""><li id="4703" class="mh mi in kz b la lv le lw li mj lm mk lq ml lu mm mn mo mp bi translated">向前传递对所有工作器同时开始，每个工作器节点计算梯度(输出)。</li><li id="f57c" class="mh mi in kz b la mq le mr li ms lm mt lq mu lu mm mn mo mp bi translated">工人们一直等到所有其他工人都完成了训练循环。然后，一旦所有的工人都计算了梯度，他们就开始互相交流以聚集梯度。</li><li id="369d" class="mh mi in kz b la mq le mr li ms lm mt lq mu lu mm mn mo mp bi translated">在所有梯度被组合之后，更新的梯度的副本被发送给所有工人。</li><li id="9369" class="mh mi in kz b la mq le mr li ms lm mt lq mu lu mm mn mo mp bi translated">然后，每个工人继续向后传递，并更新权重的本地副本。</li><li id="9cd3" class="mh mi in kz b la mq le mr li ms lm mt lq mu lu mm mn mo mp bi translated">直到所有工人都更新了他们的重量，下一次向前传递才开始；因此得名“同步”。</li></ul><p id="a2a6" class="pw-post-body-paragraph kx ky in kz b la lv lc ld le lw lg lh li lx lk ll lm ly lo lp lq lz ls lt lu ig bi translated">注意:所有的工人产生不同的梯度，因为他们在不同的数据子集上被训练，并且最终，所有的工人有相同的重量。</p><h1 id="c4a7" class="jz ka in bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">归约算法</h1><p id="0143" class="pw-post-body-paragraph kx ky in kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu ig bi translated">通常，单个节点用于完成聚合。例如，在图 3 所示的情况下，机器 A 的带宽随着机器/参数数量的增加而增加。</p><figure class="mb mc md me gt jo gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/5c12aaf5799b2e70414fdcb09b7f14a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/0*9QI5Uy2HQIS2wy74.png"/></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">图 4:单节点聚合器。</figcaption></figure><p id="f430" class="pw-post-body-paragraph kx ky in kz b la lv lc ld le lw lg lh li lx lk ll lm ly lo lp lq lz ls lt lu ig bi translated">作为同步训练中提到的 reduce 算法的后续，all-reduce 算法背后的思想是分担存储和维护全局参数的负载，以克服使用参数服务器方法的限制。有几种 all-reduce 算法规定了如何计算和共享参数:</p><figure class="mb mc md me gt jo gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/2fd68b3c553e7489c68adbe5c48e8442.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/0*SjhQO-s50m4Ogu5w.png"/></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">图 5: All Reduce:将聚合任务分配给所有节点，而不是单个节点。</figcaption></figure><p id="4be6" class="pw-post-body-paragraph kx ky in kz b la lv lc ld le lw lg lh li lx lk ll lm ly lo lp lq lz ls lt lu ig bi translated">像 AllReduce 一样，每个节点对参数的子集执行聚合任务:机器 A —参数 1，机器 B —参数 2，等等。每个工作节点将自己的版本发送给下一个节点，而不是将其版本的参数发送给所有其他节点。</p><figure class="mb mc md me gt jo gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/6fe028871402dd49944a0933eb6c7fe5.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/0*5a_A_NgXxsOi9Tyn.png"/></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">图 6:环所有减少。</figcaption></figure><p id="8047" class="pw-post-body-paragraph kx ky in kz b la lv lc ld le lw lg lh li lx lk ll lm ly lo lp lq lz ls lt lu ig bi translated">类似地，在 tree-all-reduce 中，参数通过树结构共享。无论拓扑结构如何，all-reduce 算法都可以减少同步开销，并且更易于水平扩展。</p><figure class="mb mc md me gt jo gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/daf479d07f613166867a6a3e7ca100cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/0*a-ur3-3P287gFIw0.png"/></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">图 7:树全部减少。</figcaption></figure><p id="0c90" class="pw-post-body-paragraph kx ky in kz b la lv lc ld le lw lg lh li lx lk ll lm ly lo lp lq lz ls lt lu ig bi translated">每个工作者节点保存数据的子集并计算梯度；这些值沿树向上传递并聚合，直到在根节点中计算出全局聚合值。然后，全局值被传递给所有其他节点。</p><h1 id="aea7" class="jz ka in bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">异步训练</h1><p id="dfbd" class="pw-post-body-paragraph kx ky in kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu ig bi translated">同步方法的明显问题是缺乏有效的资源利用，因为一个工人必须等待集群中的所有其他工人在管道中向前移动。此外，当工作人员的计算时间明显不同时，问题会放大，这可能是因为数据集或计算能力的变化，因此整个过程的速度只与群集中最慢的工作人员一样快。因此，在异步培训中，工人独立工作，工人不需要等待集群中的任何其他工人。实现这一点的一种方法是使用参数服务器。</p><h1 id="d51d" class="jz ka in bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">沟通方式</h1><p id="232d" class="pw-post-body-paragraph kx ky in kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu ig bi translated">两种通信方法，集中和分散模式，适用于数据并行和模型并行训练。这里的关键是工作节点之间的通信，参数如何初始化，以及权重/偏差如何更新。</p><h1 id="72fe" class="jz ka in bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">集中训练</h1><p id="22df" class="pw-post-body-paragraph kx ky in kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu ig bi translated">在分布式培训中，工人集群只执行一项任务:培训。然而，在集中式通信模式中，我们为每个工作者分配不同的角色，其中一些工作者充当参数服务器，其余的充当培训工作者。</p><p id="6aad" class="pw-post-body-paragraph kx ky in kz b la lv lc ld le lw lg lh li lx lk ll lm ly lo lp lq lz ls lt lu ig bi translated">参数服务器负责保存模型的参数，并负责更新模型的全局状态。同时，训练工人运行实际的训练循环，并从分配给他们的数据批次中产生梯度。</p><figure class="mb mc md me gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi mf"><img src="../Images/efd2e70c1811cfb7a442c267f7f07e75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*cFgV0kf0koAOjh4M.png"/></div></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">图 8:集中培训。</figcaption></figure><p id="d0e0" class="pw-post-body-paragraph kx ky in kz b la lv lc ld le lw lg lh li lx lk ll lm ly lo lp lq lz ls lt lu ig bi translated">因此，集中式数据并行训练的整个过程如下:</p><ul class=""><li id="6005" class="mh mi in kz b la lv le lw li mj lm mk lq ml lu mm mn mo mp bi translated">跨培训工作者节点复制模型；每个工作者节点使用数据的子集。</li><li id="1580" class="mh mi in kz b la mq le mr li ms lm mt lq mu lu mm mn mo mp bi translated">每个培训工作者从参数服务器获取参数。</li><li id="97ed" class="mh mi in kz b la mq le mr li ms lm mt lq mu lu mm mn mo mp bi translated">每个训练工作者节点执行训练循环，并将梯度发送回所有参数服务器。</li><li id="f5b1" class="mh mi in kz b la mq le mr li ms lm mt lq mu lu mm mn mo mp bi translated">参数服务器更新模型参数，并确保所有工人模型同步。</li></ul><p id="a1a6" class="pw-post-body-paragraph kx ky in kz b la lv lc ld le lw lg lh li lx lk ll lm ly lo lp lq lz ls lt lu ig bi translated">一些已知的缺点是:</p><ul class=""><li id="eda0" class="mh mi in kz b la lv le lw li mj lm mk lq ml lu mm mn mo mp bi translated">在给定的时间点，可能只有一个工人使用模型的更新版本，而其他人使用旧版本。</li><li id="7024" class="mh mi in kz b la mq le mr li ms lm mt lq mu lu mm mn mo mp bi translated">仅使用一个工作器作为参数服务器可能会成为瓶颈，并导致单点故障。</li></ul><h1 id="ecbb" class="jz ka in bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">分散培训</h1><p id="a273" class="pw-post-body-paragraph kx ky in kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu ig bi translated">另一方面，在去中心化的通信模式中，每个工作者节点与每个其他节点通信以更新模型参数。这种方法的优点是对等更新更快，并且没有单点故障。</p><figure class="mb mc md me gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi my"><img src="../Images/236183e917c51d2c00849e643bfbf964.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tgIfds-TNNI7oan6.png"/></div></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">图 9:去中心化的培训。</figcaption></figure><h1 id="d3bb" class="jz ka in bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">结论</h1><p id="b047" class="pw-post-body-paragraph kx ky in kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu ig bi translated">深度学习模型变得日益雄心勃勃，其支持基础设施难以跟上。采用分布式模型训练技术来解决在巨大数据集上训练复杂机器学习模型的问题只是时间问题。此外，这些优势取代了开发时间/带宽，具有更好的容错性和可靠性，更高的效率，可横向扩展以处理大规模，并且从长远来看具有成本效益。</p><h1 id="122d" class="jz ka in bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">参考</h1><pre class="mb mc md me gt mz na nb nc aw nd bi"><span id="d00a" class="ne ka in na b gy nf ng l nh ni">[1] “Distributed Training: Guide for Data Scientists,” neptune.ai, Jan. 19, 2022. https://neptune.ai/blog/distributed-training (accessed Jun. 23, 2022).</span><span id="d44a" class="ne ka in na b gy nj ng l nh ni">[2] “Distributed Training,” www.run.ai. https://www.run.ai/guides/gpu-deep-learning/distributed-training (accessed Jun. 24, 2022).</span><span id="ffdb" class="ne ka in na b gy nj ng l nh ni">[3] “Distributed Training for Machine Learning – Amazon Web Services,” Amazon Web Services, Inc. https://aws.amazon.com/sagemaker/distributed-training/ (accessed Jun. 26, 2022).</span><span id="f0c1" class="ne ka in na b gy nj ng l nh ni">[4] “Distributed model training II: Parameter Server and AllReduce – Ju Yang.” http://www.juyang.co/distributed-model-training-ii-parameter-server-and-allreduce/ (accessed Jun. 26, 2022).</span></pre></div></div>    
</body>
</html>