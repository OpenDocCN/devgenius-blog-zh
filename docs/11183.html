<html>
<head>
<title>Spark Map, Reduce &amp; Shuffle Magic</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">火花地图，减少和洗牌魔术</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/spark-map-reduce-shuffle-magic-4946cbb300ec?source=collection_archive---------9-----------------------#2022-12-23">https://blog.devgenius.io/spark-map-reduce-shuffle-magic-4946cbb300ec?source=collection_archive---------9-----------------------#2022-12-23</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><p id="2177" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">在这篇文章中，我将解释地图，减少和洗牌的内在魔力。</p><p id="c128" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">Map，reduce 是一种用于分布式系统的代码范例，可以解决某些类型的问题。记住不是所有的程序都可以用 Map，reduce 解决。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/535a2c182897608d9de4ccffa9f020b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7RSm1Ftcv3ZvGHbNFWZxQQ.png"/></div></div></figure><p id="7b04" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">让我们借助一个例子来理解地图，洗牌，减魔。下面是大数据<strong class="jm io"> HelloWord </strong>的 spark 代码——字数统计程序:</p><pre class="kj kk kl km gt ku kv kw bn kx ky bi"><span id="e06a" class="kz la in kv b be lb lc l ld le">package org.apache.spark.examples;<br/><br/>import scala.Tuple2;<br/><br/>import org.apache.spark.api.java.JavaPairRDD;<br/>import org.apache.spark.api.java.JavaRDD;<br/>import org.apache.spark.sql.SparkSession;<br/><br/>import java.util.Arrays;<br/>import java.util.List;<br/>import java.util.regex.Pattern;<br/><br/>public final class JavaWordCount {<br/>  private static final Pattern SPACE = Pattern.compile(" ");<br/><br/>  public static void main(String[] args) throws Exception {<br/><br/>    if (args.length &lt; 1) {<br/>      System.err.println("Usage: JavaWordCount &lt;file&gt;");<br/>      System.exit(1);<br/>    }<br/><br/>    SparkSession spark = SparkSession<br/>      .builder()<br/>      .appName("JavaWordCount")<br/>      .getOrCreate();<br/><br/>    // This reads the text file<br/>    JavaRDD&lt;String&gt; lines = spark.read().textFile(args[0]).javaRDD();<br/><br/>    JavaRDD&lt;String&gt; words = lines.flatMap(s -&gt; Arrays.asList(SPACE.split(s)).iterator());<br/><br/>    JavaPairRDD&lt;String, Integer&gt; ones = words.mapToPair(s -&gt; new Tuple2&lt;&gt;(s, 1));<br/><br/>    JavaPairRDD&lt;String, Integer&gt; counts = ones.reduceByKey((i1, i2) -&gt; i1 + i2);<br/><br/>    List&lt;Tuple2&lt;String, Integer&gt;&gt; output = counts.collect();<br/>    for (Tuple2&lt;?,?&gt; tuple : output) {<br/>      System.out.println(tuple._1() + ": " + tuple._2());<br/>    }<br/>    spark.stop();<br/>  }<br/>}</span></pre><p id="ffc6" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">Apache spark 程序的目标是处理大数据，这意味着它需要自己处理的所有数据或文件都无法放入主内存。想想 1 GB 的文件，如果我们需要处理它，整个文件将放不进内存。</p><p id="d5b4" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">现在，让我们布置小组工作。假设给我们统计单词的文件如下:</p><pre class="kj kk kl km gt ku kv kw bn kx ky bi"><span id="162e" class="kz la in kv b be lb lc l lf le">Mary had a little lamb,<br/>Its fleece was white as snow;<br/>And everywhere that Mary went<br/>The lamb was sure to go.<br/><br/>It followed her to school one day,<br/>Which was against the rule;<br/>It made the children laugh and play<br/>To see a lamb at school.<br/><br/>And so the teacher turned it out,<br/>But still it lingered near,<br/>And waited patiently about<br/>Till Mary did appear.<br/><br/>Why does the lamb love Mary so?<br/>The eager children cry;<br/>Why, Mary loves the lamb, you know,<br/>The teacher did reply.</span></pre><p id="017d" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">假设 spark 程序将它分成三个分区(实际上，理论上我们可以告诉 spark 程序它应该分成多少个分区，但稍后会详细说明)。请注意，分区会将数据文件分割得足够大，以便可以同时将所有数据文件放入内存中。</p><p id="6e15" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">分区 1 </strong></p><pre class="kj kk kl km gt ku kv kw bn kx ky bi"><span id="079e" class="kz la in kv b be lb lc l lf le">Mary had a little lamb,<br/>Its fleece was white as snow;<br/>And everywhere that Mary went<br/>The lamb was sure to go.<br/><br/>It followed her to school one day,<br/>Which was against the rule;<br/>It made the children laugh and play<br/>To see a lamb at school.</span></pre><p id="1536" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">隔断 2 </strong></p><pre class="kj kk kl km gt ku kv kw bn kx ky bi"><span id="0cb0" class="kz la in kv b be lb lc l ld le">And so the teacher turned it out,<br/>But still it lingered near,<br/>And waited patiently about<br/>Till Mary did appear.</span></pre><p id="0691" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">分区 3 </strong></p><pre class="kj kk kl km gt ku kv kw bn kx ky bi"><span id="faed" class="kz la in kv b be lb lc l ld le">Why does the lamb love Mary so?<br/>The eager children cry;<br/>Why, Mary loves the lamb, you know,<br/>The teacher did reply.</span></pre><p id="1487" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">分区完成后，映射阶段开始执行。</p><h1 id="0e30" class="lg la in bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">地图</h1><p id="7504" class="pw-post-body-paragraph jk jl in jm b jn md jp jq jr me jt ju jv mf jx jy jz mg kb kc kd mh kf kg kh ig bi translated">Map 是 spark 计划中通常运行的第一个阶段。它的目标是将输入从一种形式转换成另一种形式，这种形式对于 spark 程序的计算操作来说更容易管理。在上面的 JavaWordCount 程序中，映射阶段发生在这两行中:</p><pre class="kj kk kl km gt ku kv kw bn kx ky bi"><span id="0335" class="kz la in kv b be lb lc l ld le">JavaRDD&lt;String&gt; words = lines.flatMap(s -&gt; Arrays.asList(SPACE.split(s)).iterator());<br/><br/>JavaPairRDD&lt;String, Integer&gt; ones = words.mapToPair(s -&gt; new Tuple2&lt;&gt;(s, 1));</span></pre><p id="3868" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">请记住，由于这是一个分布式系统问题，Map 阶段在所有计算节点上以完全相同的方式发生。执行器(spark 的计算单元)获取文件块，并将数据转换成 apache spark 程序可以处理的其他有意义的方式。在本例中，在贴图阶段首先删除空格。然后，每个映射器创建三个混排文件(针对 reduce 阶段的每个 reduce)。</p><p id="2e38" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">我们怎么知道哪个词元组会去哪个 shuffle 文件？</strong></p><p id="8967" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">这是通过哈希分区方案实现的。键(在我们的例子中是字符串)是以归约器的数量为模散列的。所以，</p><pre class="kj kk kl km gt ku kv kw bn kx ky bi"><span id="6ed9" class="kz la in kv b be lb lc l lf le">int index = hash(word) % num_of_reducers<br/><br/>int index = hash(Mary) % 3<br/><br/>Let's say this index value comes out to be 0.</span></pre><p id="3603" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">因此我们看到文件—<strong class="jm io">executor 1-index 1-chunk</strong>看起来像这样</p><pre class="kj kk kl km gt ku kv kw bn kx ky bi"><span id="c43a" class="kz la in kv b be lb lc l ld le">(Mary, 1),<br/>(lamb, 1),<br/>(was,1),<br/>(lamb, 1),<br/>(sure, 1),<br/>(Mary, 1),<br/>(lamb, 1)<br/>....</span></pre><p id="287d" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">这一过程对其他执行者也同样重复。最后，每个遗嘱执行人将有 3 个文件。这也可以被称为混洗写入过程。注意，这些只是临时文件，一旦相应的减少阶段完成，这些文件将被 spark 驱动程序删除。有关更多详细信息，请参见上图中的详细信息。</p><h1 id="bc29" class="lg la in bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">洗牌</h1><p id="61dd" class="pw-post-body-paragraph jk jl in jm b jn md jp jq jr me jt ju jv mf jx jy jz mg kb kc kd mh kf kg kh ig bi translated">当映射阶段完成时，会发生随机写入，这会创建与缩减器数量相等的临时(键、值)文件。现在，每个 reducer 都需要相应的相同的索引混洗文件。所以，假设 spark 驱动程序分配了执行器 1 —索引 1 文件，执行器 2 —索引 2 文件和执行器 3 —执行器 3 文件。这些文件需要被传送到每个执行者 1。例如，执行程序 1 已经有了执行程序 1-索引 1-块文件，只需要执行程序 2-索引 1-块和执行程序 3-索引 1-块就可以进行归约阶段。一旦每个执行者都有了所有的文件，就可以开始 reduce 阶段了。</p><h1 id="65f0" class="lg la in bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">减少</h1><p id="3a39" class="pw-post-body-paragraph jk jl in jm b jn md jp jq jr me jt ju jv mf jx jy jz mg kb kc kd mh kf kg kh ig bi translated">reduce 阶段逐个值地聚合数据。在我们的代码简化操作中</p><pre class="kj kk kl km gt ku kv kw bn kx ky bi"><span id="f20b" class="kz la in kv b be lb lc l ld le">JavaPairRDD&lt;String, Integer&gt; counts = ones.reduceByKey((i1, i2) -&gt; i1 + i2)</span></pre><p id="8895" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">从随机播放阶段收集的每个文件现在应该有相同的键。它的意思是，如果执行人一上有“玛丽”这个词，那么其他的缩减者就不会得到这个词。当然，元组(“Mary，1”)可以在同一个执行程序的不同文件中，但不会在任何其他执行程序中重复。这就是它的妙处！现在，reducers 将开始聚合单词并更新它们的计数。在内部，spark 可以实现所有键的映射，并在每次看到相同的单词时更新计数器。因此，最终我们会在每个执行器上看到一个最终输出文件，如下所示:</p><pre class="kj kk kl km gt ku kv kw bn kx ky bi"><span id="4a3e" class="kz la in kv b be lb lc l lf le">(Mary, 4),<br/>(lamb, 5),<br/>(was, 2),<br/>(sure, 1)<br/><br/>...</span></pre><p id="ca3c" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">忠太..这就是火花贴图和减少的工作原理！！</p></div></div>    
</body>
</html>