# æå–è¶³çƒç»Ÿè®¡æ•°æ®å¹¶åŠ è½½åˆ° Google äº‘å­˜å‚¨& BigQuery with Airflow

> åŸæ–‡ï¼š<https://blog.devgenius.io/extract-and-load-football-statistics-to-google-cloud-storage-bigquery-with-airflow-1a217227dbd1?source=collection_archive---------6----------------------->

## *åˆ©ç”¨æ°”æµè‡ªåŠ¨æå–å’Œè£…è½½ç®¡é“çš„æ¼”ç¤º*

![](img/0c61bd9d120209526a9a6382c9cbe7ca.png)

å›¾ç‰‡æ¥è‡ªé©¬ä¿®Â·æ–½ç“¦ç‰¹çš„ Unsplash

å¦‚æœä½ ç¢°å·§çœ‹åˆ°è¿™ç¯‡æ–‡ç« ï¼Œé‚£ä¹ˆæˆ‘æƒ³ä½ ç›®å‰æ­£åœ¨å­¦ä¹ æˆ–ä»äº‹ä¸€ä¸ªæ¶‰åŠ Apache Airflow çš„é¡¹ç›®ã€‚ **Apache Airflow** æ˜¯æœ€æµè¡Œçš„å¼€æºå·¥ä½œæµç¼–æ’å·¥å…·ã€‚å®ƒå°†æ‚¨çš„ä»£ç è½¬æ¢æˆä¸€ä¸ªæ‚¨å¯ä»¥è®¡åˆ’ã€è¿è¡Œå’Œè§‚å¯Ÿçš„å·¥ä½œæµã€‚åœ¨æ•°æ®é¢†åŸŸï¼Œè®¸å¤šæ•°æ®å›¢é˜Ÿä½¿ç”¨è¿™ä¸ªå·¥å…·æ¥**è‡ªåŠ¨åŒ–ä»–ä»¬çš„ ETL ç®¡é“**(æå–ã€ä¼ è¾“å’ŒåŠ è½½)ï¼Œä¾‹å¦‚åœ¨æ•°æ®åº“ã€æ•°æ®æ¹–å’Œæ•°æ®ä»“åº“ä¹‹é—´å¤åˆ¶æˆ–ä¼ è¾“æ•°æ®ã€‚

## ç›®æ ‡

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†å±•ç¤ºå¦‚ä½•ä»å¤–éƒ¨æ•°æ®æºæå–æ•°æ®å¹¶åŠ è½½åˆ° Google äº‘å­˜å‚¨(GCS)å’Œ BigQuery (BQ)ä¸­ã€‚ä¸ºæ­¤ï¼Œæˆ‘å°†ä½¿ç”¨ Astro CLIã€‚Astro CLI æ˜¯ä¸€ä¸ªå¼€æºçš„ CLI å·¥å…·ï¼Œç”¨äº Apache Airflow çš„æ•°æ®ç¼–æ’ã€‚å®ƒä¸ºæ„å»ºã€æµ‹è¯•å’Œéƒ¨ç½²æ°”æµ Dag å’Œä»»åŠ¡æä¾›äº†ä¸€ä¸ªè‡ªåŒ…å«çš„æœ¬åœ°å¼€å‘ç¯å¢ƒã€‚å…³äº Astro CLI çš„æ›´å¤šä¿¡æ¯ï¼Œæ‚¨å¯ä»¥è®¿é—®è¿™ç¯‡[åšå®¢æ–‡ç« ](https://www.astronomer.io/blog/astro-cli-the-easiest-way-to-install-apache-airflow/)ã€‚

è¿™ç¯‡æ–‡ç« ä¸æ˜¯å…³äºæ°”æµçš„ä»‹ç»æ€§é˜…è¯»ã€‚å› æ­¤ï¼Œæˆ‘å°†ä¸æ¶‰åŠè¯¸å¦‚*â€œä»€ä¹ˆæ˜¯æ°”æµâ€*æˆ–*â€œä»€ä¹ˆæ˜¯ DAGâ€*ä¹‹ç±»çš„å†…å®¹ã€‚ä¸ºäº†äº†è§£æ°”æµã€æ¦‚å¿µå’ŒåŠŸèƒ½ï¼Œæˆ‘å¼ºçƒˆæ¨èä½ è®¿é—®è¿™ä¸ªç”±å¤©æ–‡å­¦å®¶æ’°å†™çš„å…³äºæ°”æµçš„ä¼Ÿå¤§çš„[æŒ‡å—å’Œæ•™ç¨‹](https://docs.astronomer.io/learn)ã€‚

**ä»¥ä¸‹æ˜¯å·¥ä½œæµç¨‹æ¦‚è¦:**

![](img/ce1ab7222e3d2bc3a7a1e29bed8c90a8.png)

æŒ‰ä½œè€…åˆ†ç±»çš„å›¾åƒ:å·¥ä½œæµæ‘˜è¦

## **å…ˆå†³æ¡ä»¶:**

1.  Astro CLI & Dockerã€‚æŒ‰ç…§æœ¬æŒ‡å—å®‰è£… Astro CLI ã€‚
2.  ä¸€ä¸ªè°·æ­Œäº‘å¸æˆ·å’Œé¡¹ç›®æ¥è®¿é—® GCS & BigQueryã€‚ä½ å¯ä»¥åœ¨è¿™é‡Œå…è´¹è¯•ç”¨ã€‚
3.  è®¿é—® GCS å’Œ BigQuery çš„æœåŠ¡å¸æˆ·æƒé™(JSON)ã€‚
    é€‰æ‹©ä½ çš„é¡¹ç›®ï¼Œè¿›å…¥ IAM &ç®¡ç†ï¼Œç‚¹å‡» IAM ã€‚ç‚¹å‡»æ‚¨çš„æœåŠ¡å¸æˆ·ä¸Šçš„ç¼–è¾‘ä¸»ä½“å›¾æ ‡ï¼Œæ·»åŠ è¿™äº›è§’è‰²(é™¤äº†*æŸ¥çœ‹è€…çš„è§’è‰²*)ï¼›*å­˜å‚¨ç®¡ç†å‘˜ã€å­˜å‚¨å¯¹è±¡ç®¡ç†å‘˜å’Œå¤§æŸ¥è¯¢ç®¡ç†å‘˜ã€‚*æŒ‰ç…§è¿™äº›è¯´æ˜[åˆ›å»ºæœåŠ¡è´¦æˆ·å¯†é’¥](https://cloud.google.com/iam/docs/creating-managing-service-account-keys)ã€‚

## **æ•°æ®çš„ç®€è¦æè¿°**ğŸ“

æˆ‘è¦æå–çš„æ•°æ®æ˜¯ [datahub](https://datahub.io/) ä¸Šæ¬§æ´²äº”å¤§è¶³çƒè”èµ›çš„è¶³çƒé˜Ÿç»Ÿè®¡æ•°æ®ã€‚æœ‰ 5 ä¸ªè”èµ›(è‹±è¶…ã€è¥¿ç”²ã€å¾·ç”²ã€æ„ç”²ã€æ³•ç”²)å’Œ 10 ä¸ªèµ›å­£(2009/2010â€“2018/2019)çš„æ•°æ®ä»¥ csv æ ¼å¼å­˜å‚¨ã€‚æˆ‘å°†ä»æ•°æ®æºä¸­æå– 50 ä¸ª csv æ–‡ä»¶[ã€‚](https://datahub.io/collections/football#football-datasets-on-datahub)

![](img/ec16f0f4c9cd5e20b5f1ad0d5f938d6f.png)

å›¾ç‰‡ä½œè€…:æ•°æ®é›†ä½ç½®-->[https://data hub . io/collections/football # football-datasets-on-data hub](https://datahub.io/collections/football#football-datasets-on-datahub)

ä»¥ä¸‹æ˜¯æ•°æ®é›†çš„ç®€è¦ä»‹ç»:

![](img/804a84922353a650e856e84b8ec20de1.png)

ä½œè€…å›¾ç‰‡

å¦‚ä¸Šæ‰€ç¤ºï¼Œä»æ¯ä¸ªæ•°æ®é›†ä¸­ï¼Œæˆ‘å°†åªæå–å¦‚ä¸Šæ‰€ç¤ºçš„ 22 åˆ—ï¼Œå¹¶æ·»åŠ  1 åˆ—ä½œä¸ºå­£èŠ‚å·çš„æ ‡å¿—(å°†åœ¨æ•°æ®æå–è¿‡ç¨‹ä¸­ç”Ÿæˆ)ã€‚ä»¥ä¸‹æ˜¯æ¯åˆ—çš„ç®€çŸ­æè¿°å’Œæ•°æ®ç±»å‹ã€‚

## å¼€å§‹å§â€¦ğŸš€

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦åœ¨ä¸€ä¸ªç©ºçš„æœ¬åœ°ç›®å½•ä¸­åˆå§‹åŒ–ä¸€ä¸ªæ–°çš„ Astro é¡¹ç›®ã€‚åœ¨ Astro CLI ä¸­ï¼Œæˆ‘ä»¬ç”¨`astro dev init`å‘½ä»¤åˆ›å»ºä¸€ä¸ªæ–°é¡¹ç›®ã€‚

```
$ mkdir airflow_astrocli_demo && cd airflow_astrocli_demo
$ astro dev init
```

å®ƒå°†ç”Ÿæˆä¸€ä¸ªåŸºæœ¬çš„é¡¹ç›®ç›®å½•ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚`dags`æ–‡ä»¶å¤¹å°†åŒ…å«æ‚¨å°†è¦ç¼–å†™çš„å·¥ä½œæµ(Dag)ã€‚

```
 â”œâ”€â”€ dags
â”‚   â”œâ”€â”€ example_dag_advanced.py
â”‚   â”œâ”€â”€ example_dag_basic.py
â”œâ”€â”€ tests
â”‚   â”œâ”€â”€ dags
â”‚       â”œâ”€â”€ test_dag_integrity.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ include
â”œâ”€â”€ packages.txt
â”œâ”€â”€ plugins
â””â”€â”€ requirements.txt
```

è¦åœ¨æ‚¨çš„æœºå™¨ä¸Šå¯åŠ¨æœ¬åœ°ç‰ˆæœ¬çš„æ°”æµï¼Œè¯·ä½¿ç”¨`astro dev start`ã€‚è¿™å°†å¯åŠ¨ä¸€ä¸ªæ°”æµè°ƒåº¦å™¨ã€webserver å’Œ postgres(éƒ½è¿è¡Œåœ¨ docker å®¹å™¨ä¸­)ã€‚ä½ å¯ä»¥ç”¨`astro dev ps`è¿›ä¸€æ­¥éªŒè¯è¿™ä¸€ç‚¹ã€‚

æ¥ä¸‹æ¥ï¼Œåœ¨æ‚¨çš„`Dockerfile`ä¸­ï¼Œæ‚¨éœ€è¦æŒ‡å®šä¸€äº›ç¯å¢ƒå˜é‡ï¼ŒAirflow å°†ä½¿ç”¨è¿™äº›å˜é‡æ¥è®¿é—®æ‚¨çš„ GCS å’Œ BigQueryã€‚

```
FROM quay.io/astronomer/astro-runtime:6.0.3

# Insert your gcp project id and gcs bucket name
ENV GCP_PROJECT_ID='your_gcp_project_id'
ENV GCP_GCS_BUCKET='your_gcs_bucket_name'

# Create a dataset on your BQ, in my case I name it as european_football_leagues
ENV BIGQUERY_DATASET='your_dataset_name'

# Path to the location where you store your JSON service account key
ENV GOOGLE_APPLICATION_CREDENTIALS=/usr/local/airflow/google_credentials.json
ENV AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT='google-cloud-platform://?extra__google_cloud_platform__key_path=/usr/local/airflow/google_credentials.json'
```

æœ€åï¼Œç”±äºæˆ‘ä»¬å°†ä½¿ç”¨ BigQueryï¼Œæˆ‘ä»¬éœ€è¦ä¸º`google` provider å®‰è£…ä¸€ä¸ª provider åŒ…ï¼Œè¿™æ · Airflow å°±å¯ä»¥ä¸ BigQuery äº¤äº’å¹¶ä¸ºæˆ‘ä»¬åˆ›å»ºå¤–éƒ¨è¡¨ã€‚åœ¨`requirements.txt`ä¸­ï¼Œæ·»åŠ ä»¥ä¸‹å†…å®¹æ¥å®‰è£…æä¾›ç¨‹åº:`apache-airflow-providers-google==8.4.0`ã€‚

åœ¨æˆ‘ä»¬è¿›å…¥ä¸‹ä¸€éƒ¨åˆ†ä¹‹å‰ï¼Œè¿™äº›æ˜¯æˆ‘åœ¨è¿™ä¸ªé¡¹ç›®ä¸­å‘ç°æœ‰ç”¨çš„å…¶ä»– Astro CLI å‘½ä»¤:

*   `astro dev restart`:åœæ­¢ä½ çš„ Airflow ç¯å¢ƒï¼ŒæŠŠä½ çš„ Astro é¡¹ç›®é‡å»ºæˆ Docker é•œåƒï¼Œç”¨æ–°çš„ Docker é•œåƒé‡å¯ä½ çš„ Airflow ç¯å¢ƒã€‚
*   `astro dev stop`:æš‚åœæ‰€æœ‰è¿è¡Œæœ¬åœ°æ°”æµç¯å¢ƒçš„ Docker å®¹å™¨ã€‚
*   `astro dev kill`:é’ˆå¯¹æ‚¨å½“åœ°çš„æ°”æµç¯å¢ƒï¼Œå¼ºåˆ¶åœæ­¢å¹¶ç§»é™¤æ‰€æœ‰æ­£åœ¨è¿è¡Œçš„å®¹å™¨ã€‚
*   `astro dev logs`:æ˜¾ç¤ºæœ¬åœ° Airflow ç¯å¢ƒä¸­çš„ Airflow web æœåŠ¡å™¨ã€è°ƒåº¦ç¨‹åºå’Œè§¦å‘å™¨æ—¥å¿—ã€‚

è¿™ä¸ªé¡¹ç›®æˆ‘åˆ›å»ºäº† 2 ä¸ª Dagï¼Œä¸€ä¸ªç”¨äºæå–æ•°æ®å¹¶åŠ è½½åˆ° GCSï¼Œå¦ä¸€ä¸ªç”¨äºå°†æ•°æ®ä» GCS åŠ è½½åˆ° BigQueryã€‚

## DAG 1:æå–å¹¶åŠ è½½æ•°æ®åˆ° GCSâ€¦â³

ä¸ºäº†å¼€å§‹æå–æ•°æ®ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦å¯¼å…¥æˆ‘ä»¬ä¹‹å‰åœ¨`Dockerfile`ä¸­å®šä¹‰çš„ç¯å¢ƒå˜é‡ï¼Œå¹¶åˆ›å»ºä¸¤ä¸ª [python å‡½æ•°ï¼Œç”¨äºä¸‹è½½å’Œä¸Šä¼ æ•°æ®åˆ° GCS](https://github.com/Balurc/airflow_astrocli_demo/blob/main/dags/data_ingestion_gcs_dag.py) ( `download_upload_data` & `upload_to_gcs`)ã€‚

```
PROJECT_ID = os.environ.get("GCP_PROJECT_ID")
BUCKET = os.environ.get("GCP_GCS_BUCKET")
AIRFLOW_HOME = os.environ.get("AIRFLOW_HOME", "/opt/airflow/")
BIGQUERY_DATASET = os.environ.get("BIGQUERY_DATASET", "european_football_leagues")
```

`download_upload_data`å‡½æ•°åŒ…å«**ä¸‰ä¸ªæ°”æµä»»åŠ¡**ï¼ŒåŒæ—¶ä½¿ç”¨ bash æ“ä½œç¬¦å’Œ python æ“ä½œç¬¦ã€‚ä»»åŠ¡(åœ¨æ°”æµä¸­)æ˜¯ä¸€ä¸ªå·¥ä½œå•å…ƒï¼Œå®ƒè¢«å®‰æ’åˆ°ä¸€ä¸ª DAG ä¸­ã€‚ä»»åŠ¡ä¹‹é—´è®¾ç½®äº†ä¸Šæ¸¸å’Œä¸‹æ¸¸ä¾èµ–å…³ç³»ï¼Œä»¥è¡¨ç¤ºå®ƒä»¬åº”è¯¥è¿è¡Œçš„é¡ºåºã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä»»åŠ¡ä¾èµ–å…³ç³»å’Œæ‰§è¡Œé¡ºåºå¦‚ä¸‹ã€‚

![](img/b4d08b222afd10767782c37c32764dce.png)

ä½œè€…å›¾ç‰‡:æ‰§è¡Œé¡ºåº

## ä»»åŠ¡ 1:ä¸‹è½½æ•°æ®â†’ä¸‹è½½æ•°æ®é›†ä»»åŠ¡ğŸ“¥

è¯¥ä»»åŠ¡å°†ä½¿ç”¨`BashOperator`ä»æ•°æ®æºä¸­æå–æ•°æ®ã€‚`BashOperator`æ˜¯ä¸€ä¸ª Airflow æ“ä½œç¬¦ï¼Œå®ƒä» Airflow DAG ä¸­æ‰§è¡Œ bash å‘½ä»¤æˆ– bash è„šæœ¬ã€‚è¯¥å‘½ä»¤æœ‰ 4 ä¸ªéƒ¨åˆ†ï¼Œç¬¬ä¸€éƒ¨åˆ†æ˜¯ç”¨`curl -sSLF`æå– csv æ ¼å¼çš„æ•°æ®ï¼Œç„¶åç”¨`cut -f 1-22 -d,`é€‰æ‹©ç›¸å…³å­—æ®µï¼Œç”¨`sed`æ·»åŠ å­£èŠ‚å¹´ä»½ï¼Œæœ€åå°†æ–°ä¸‹è½½å’Œç¼–è¾‘çš„æ–‡ä»¶ä¿å­˜(`>`)åˆ°æ°”æµæ•°æ®åº“ã€‚

```
download_dataset_task = BashOperator(
         task_id="download_dataset_task",
         bash_command=f"curl -sSLf https://datahub.io/sports-data/english-premier-league/r/season-0910.csv \
                        | cut -f 1-22 -d, \
                        | sed '1s/$/,\season/; 2,$s/$/,\season-0910/' \
                        > usr/local/airflow/english-premier-league_season-0910.csv"
    )
```

## ä»»åŠ¡ 2:å°†æ•°æ®æ‘„å–åˆ° GCS â†’ local_to_gcs_taskğŸ’‰

ä¸€æ—¦æå–äº†æ•°æ®ï¼Œä¸‹ä¸€ä¸ªä»»åŠ¡å°±æ˜¯ç”¨ python ä»£ç å°†å®ƒä»¬æ¥æ”¶åˆ° GCS ä¸­ã€‚åœ¨ Airflow ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`PythonOperator`æ¥æ‰§è¡Œ python ä»£ç ã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘åˆ›å»ºäº†ä¸€ä¸ª python å‡½æ•°ï¼Œ`upload_to_gcs`ï¼Œå®ƒæ˜¯ä¸€ä¸ªå°†æ•°æ®è¿æ¥å¹¶ä¼ è¾“åˆ° GCS çš„å‡½æ•°ã€‚è¯¥åŠŸèƒ½å°†é€šè¿‡ä½¿ç”¨`PythonOperator`ä¸­çš„`python_callable`å‚æ•°ä¼ é€’ç»™æ°”æµ DAGã€‚ç¡®ä¿åœ¨æ‚¨çš„`ops_kwargs`å‚æ•°ä¸­å®šä¹‰æ‚¨çš„ GCS å­˜å‚¨æ¡¶åç§°ã€ä¿å­˜ GCS å­˜å‚¨æ¡¶ä¸­æ–‡ä»¶çš„è·¯å¾„ä»¥åŠæ–‡ä»¶æ‰€åœ¨çš„æœ¬åœ°è·¯å¾„ã€‚

```
local_to_gcs_task = PythonOperator(
            task_id="local_to_gcs_task",
            python_callable=upload_to_gcs,
            op_kwargs={
                "bucket": BUCKET,
                "object_name": gcs_path_template,
                "local_file": local_csv_path_template,
            },
    )
```

## ä»»åŠ¡ 3:ä»æœ¬åœ°æ¸…é™¤æ•°æ®â†’ rm_task ğŸ§¹

è¯¥ä»»åŠ¡å°†ä½¿ç”¨`BashOperator`å’Œ bash å‘½ä»¤(`rm`)ä» Airflow æ•°æ®åº“ä¸­åˆ é™¤æ‰€æœ‰æ–‡ä»¶ã€‚

```
rm_task = BashOperator(
            task_id="rm_task",
            bash_command=f"rm usr/local/airflow/english-premier-league_season-0910.csv"
    )
```

å®šä¹‰å®Œä»»åŠ¡åï¼Œè®©æˆ‘ä»¬å¼€å§‹åˆ›å»ºä¸€ä¸ª DAG æ¥è‡ªåŠ¨åŒ–æ•´ä¸ªè¿‡ç¨‹ã€‚æœ‰ 50 ä¸ª csv æ–‡ä»¶(æ¥è‡ª 5 ä¸ªè¶³çƒè”èµ›ï¼Œæ¯ä¸ªæœ‰ 10 ä¸ª csv æ–‡ä»¶)ï¼Œæˆ‘ä»¬æ­£åœ¨å¤„ç†ã€‚ä¸ºæ­¤ï¼Œæˆ‘å°†å¾ªç¯æ¯ä¸ªè”ç›Ÿå’Œæ¯ä¸ªèµ›å­£çš„æ¯ä¸ªæ•°æ®ï¼Œå¹¶ä½¿ç”¨`TaskGroup`åœ¨æ°”æµä¸­ç»„ç»‡è¿™äº›ä»»åŠ¡ã€‚`TaskGroup`å¸®åŠ©æ‚¨æ›´å¥½åœ°ç»„ç»‡æ‚¨çš„ä»»åŠ¡å’Œç»´æŠ¤æ‚¨çš„ Dagï¼Œè€Œæ²¡æœ‰è‹›åˆ»çš„å­ Dagã€‚æˆ‘ä»¬æ€»å…±å°†æœ‰ 150 ä¸ªä»»åŠ¡åœ¨`extract_load_gcs_dag`è¿è¡Œã€‚

![](img/ef60bb22bbd86f6667623bb82380d565.png)

ä½œè€…å›¾ç‰‡

```
leagues = ["english-premier-league", "spanish-la-liga", "german-bundesliga", "italian-serie-a", "french-ligue-1"]
seasons = ["season-0910", "season-1011", "season-1112", "season-1213", "season-1314", "season-1415", "season-1516", "season-1617", "season-1718", "season-1819"]

   for league in leagues:
       for season in seasons:
           with TaskGroup(group_id=f'group_{league}_{season}') as tg1:
               URL_TEMPLATE = f"{URL_PREFIX}/{league}/r/{season}.csv"
               CSV_FILE_TEMPLATE = f"{AIRFLOW_HOME}/{league}_{season}.csv"
               GCS_PATH_TEMPLATE = f"football_stats/{league}/{league}_{season}.csv"

               if league == "english-premier-league":
                   download_upload_data(url_template=URL_TEMPLATE,
                                       filter="cut -f 1-10,12-23 -d,",
                                       local_csv_path_template=CSV_FILE_TEMPLATE,
                                       season=season,
                                       gcs_path_template=GCS_PATH_TEMPLATE
                                   )
               else:
                   download_upload_data(url_template=URL_TEMPLATE,
                                       filter="cut -f 1-22 -d,",
                                       local_csv_path_template=CSV_FILE_TEMPLATE,
                                       season=season,
                                       gcs_path_template=GCS_PATH_TEMPLATE
                                   )
```

![](img/c2a19782c79159ef0c4a77cf244d5aa6.png)

ä½œè€…æä¾›çš„å›¾ç‰‡:DAG è¿è¡Œç»“æŸåï¼Œæ‰€æœ‰æ•°æ®éƒ½å·²è¢«çº³å…¥ GCS

ä¸€æ—¦æ•°æ®å·²ç»åœ¨ GCS å†…éƒ¨ï¼Œä¸‹ä¸€ä¸ªä»»åŠ¡å°±æ˜¯å°†å®ƒä»¬åŠ è½½åˆ° BigQuery å¤–éƒ¨è¡¨ä¸­ã€‚å¤–éƒ¨è¡¨éå¸¸ç±»ä¼¼äºæ ‡å‡†çš„ BigQuery è¡¨ã€‚å®ƒåœ¨ BigQuery å­˜å‚¨ä¸­å­˜å‚¨å…ƒæ•°æ®å’Œæ¨¡å¼ã€‚ç„¶è€Œï¼Œä¸»è¦çš„åŒºåˆ«æ˜¯ï¼Œå®ƒçš„æ•°æ®é©»ç•™åœ¨å¤–éƒ¨æºä¸­ã€‚å¤–éƒ¨è¡¨åŒ…å«åœ¨æ•°æ®é›†ä¸­ï¼Œæ‚¨å¯ä»¥åƒç®¡ç†æ ‡å‡†çš„ BigQuery è¡¨ä¸€æ ·ç®¡ç†å®ƒä»¬ã€‚

## DAG 2:å°†æ•°æ®åŠ è½½åˆ° Bigueryâ€¦â³

ä¸ºäº†æ‰§è¡Œè¿™ä¸ªä»»åŠ¡ï¼Œæˆ‘åˆ›å»ºäº†ä¸€ä¸ªå•ç‹¬çš„ DAG ( `load_to_bq_dag`)ï¼Œå®ƒå°†æ•°æ®ä» GCS bucket åŠ è½½åˆ° BigQuery å¤–éƒ¨è¡¨ï¼Œåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä½¿ç”¨äº†`BigQueryCreateExternalTableOperator`ï¼Œä¸€ä¸ªæ°”æµæ“ä½œç¬¦ã€‚åœ¨è¿™ä¸ªæ“ä½œç¬¦ä¸­ï¼Œæ‚¨éœ€è¦åœ¨å‚æ•°ä¸­å®šä¹‰æºè·¯å¾„(GCS)ã€ç›®æ ‡è·¯å¾„(BigQuery)å’Œæ¨¡å¼ã€‚æ€»å…±ï¼Œæˆ‘ä»¬å°†æœ‰ 5 ä¸ªä»»åŠ¡åœ¨è¿™ä¸ª DAG ä¸­è¿è¡Œï¼Œæ¯ä¸ªè¶³çƒè”èµ›ä¸€ä¸ªä»»åŠ¡ã€‚ä¸€æ—¦ DAG è¿è¡Œå®Œæˆï¼Œæ‚¨åº”è¯¥åœ¨ BigQuery ä¸­çœ‹åˆ°æ‚¨çš„æ•°æ®ï¼Œå¹¶ä¸ºæ‚¨çš„æŸ¥è¯¢åšå¥½å‡†å¤‡ã€‚

![](img/ba8e77ec0cdcce9e5fd81209458b92f8.png)

ä½œè€…å›¾ç‰‡:DAGs è¿è¡ŒæˆåŠŸ

```
for league in leagues:
     with TaskGroup(group_id=f'group_{league}') as tg2:
         bigquery_external_table_task = BigQueryCreateExternalTableOperator(
                    task_id="bigquery_external_table_task",
                    destination_project_dataset_table=f"{BIGQUERY_DATASET}.{league}_external_table",
                    bucket=BUCKET,
                    source_objects=[f"football_stats/{league}/*"],
                    skip_leading_rows=1,
                    schema_fields=[
                        {"name": "Div", "type": "STRING"},
                        {"name": "Date", "type": "STRING"},
                        {"name": "HomeTeam", "type": "STRING"},
                        {"name": "AwayTeam", "type": "STRING"},
                        {"name": "FTHG", "type": "INTEGER"},
                        {"name": "FTAG", "type": "INTEGER"},
                        .......
                      ]
```

æˆ‘ä»¬å·²ç»è¾¾åˆ°äº†å¸–å­çš„ç»“å°¾ï¼Œè¯·[æ£€æŸ¥æˆ‘çš„å›è´­](https://github.com/Balurc/airflow_astrocli_demo)é¡¹ç›®çš„å…¨é¢å®æ–½ã€‚æˆ‘å¸Œæœ›è¿™ç¯‡æ–‡ç« å€¼å¾—ä½ èŠ±æ¯ä¸€ç§’é’Ÿçš„æ—¶é—´ï¼Œå¦‚æœä½ æœ‰ä»»ä½•åé¦ˆï¼Œè¯·éšæ—¶å†™åœ¨è¯„è®ºåŒºã€‚è°¢è°¢ä½ ã€‚

## é™„åŠ èµ„æºğŸ“š

*   [è·Ÿå¤©æ–‡å­¦å®¶](https://docs.astronomer.io/learn)å­¦ä¹ æ°”æµã€‚
*   [å¸¦é˜¿å¸•å¥‡æ°”æµçš„æ•°æ®ç®¡é“](https://www.amazon.com/Data-Pipelines-Apache-Airflow-Harenslak/dp/1617296902)ã€‚
*   [å…³äº DataTalksClub æ•°æ®æ‘„å–çš„ç¬¬ 2 å‘¨è¯¾ç¨‹](https://github.com/Balurc/data_eng_zoomcamp/tree/main/week2_data_ingestion)ã€‚