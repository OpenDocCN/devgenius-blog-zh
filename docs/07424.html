<html>
<head>
<title>Build A Machine Learning Model with PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 PySpark 构建机器学习模型</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/build-a-machine-learning-model-with-pyspark-641cd70b7923?source=collection_archive---------3-----------------------#2022-03-24">https://blog.devgenius.io/build-a-machine-learning-model-with-pyspark-641cd70b7923?source=collection_archive---------3-----------------------#2022-03-24</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><figure class="gl gn jl jm jn jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi jk"><img src="../Images/edf4f40c358c62cddb48b25c310eb7ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*EcdCVd_gKC0UnXTw.jpg"/></div></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">来源:<a class="ae jz" href="https://realpython.com/pyspark-intro/" rel="noopener ugc nofollow" target="_blank"> RealPython </a></figcaption></figure><p id="87e1" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">非分布式的机器学习库已经无法满足企业的业务需求(尤其是流量驱动的技术行业)，将数据库部署到多个节点甚至云端是不可避免的。然而，分布式机器学习对数据、硬件和任务管理提出了挑战。现有的大数据引擎(Spark)已经接管了这些挑战，并且它还为机器学习开发提供了 python 库。与传统的大数据平台相比，Spark 具有以下优势:</p><ul class=""><li id="b0fd" class="ky kz in kc b kd ke kh ki kl la kp lb kt lc kx ld le lf lg bi translated">Spark 通常比 Hadroop 快得多，这是因为 Hadoop 将中间结果写入磁盘，而 Spark 尽可能将中间结果保存在内存中。</li><li id="91ce" class="ky kz in kc b kd lh kh li kl lj kp lk kt ll kx ld le lf lg bi translated">Spark 拥有多种 API，可以支持 java、python 和 scala 等。</li><li id="03bb" class="ky kz in kc b kd lh kh li kl lj kp lk kt ll kx ld le lf lg bi translated">Spark 还包含多个其他应用程序的库，如数据分析、图形分析和流式实时数据处理。</li><li id="dbd3" class="ky kz in kc b kd lh kh li kl lj kp lk kt ll kx ld le lf lg bi translated">Spark 不仅可以使用 Hadoop 分布式文件系统<strong class="kc io"> ( </strong> HDFS)，还可以使用其他一些数据存储系统，如亚马逊网络服务(AWS) S3</li><li id="d199" class="ky kz in kc b kd lh kh li kl lj kp lk kt ll kx ld le lf lg bi">…</li></ul><p id="c6d0" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">本博客将展示 PySpark 如何用于建立机器学习模型，并将使用虚拟流媒体公司的客户流失(取消服务)示例进行演示。本博客感谢 Udacity 提供数据。这个项目的代码可以在<a class="ae jz" href="https://github.com/xujiang1993/Customer_Churn_with_PySpark" rel="noopener ugc nofollow" target="_blank">这里</a>找到，代码会不断更新，所以如果你喜欢我的 Github，你不会错过任何更新。</p><h1 id="0869" class="lm ln in bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">客户流失预测</h1><h2 id="d9b4" class="mk ln in bd lo ml mm dn ls mn mo dp lw kl mp mq ma kp mr ms me kt mt mu mi mv bi translated">问题陈述</h2><p id="9a3b" class="pw-post-body-paragraph ka kb in kc b kd mw kf kg kh mx kj kk kl my kn ko kp mz kr ks kt na kv kw kx ig bi translated">Sportify 和腾讯音乐等流媒体公司很容易加入或取消服务，从而导致客户流失(客户停止使用服务或产品)。如果能提前预测到客户会流失，就可以提前采取送优惠券等行动留住客户。这个博客将使用 PySpark 来预测客户是否会流失。</p><h2 id="4560" class="mk ln in bd lo ml mm dn ls mn mo dp lw kl mp mq ma kp mr ms me kt mt mu mi mv bi translated">数据</h2><p id="300f" class="pw-post-body-paragraph ka kb in kc b kd mw kf kg kh mx kj kk kl my kn ko kp mz kr ks kt na kv kw kx ig bi translated">因为这个演示只在本地工作，所以我为这项工作选择了一个相对较小的数据子集(128MB)(完整的数据集将是 12GB)。本博客中使用的数据包含客户行为的记录(例如，添加朋友、添加到播放列表、下一首歌曲、滚动广告、拇指向下和拇指向上等)、歌曲的详细信息、艺术家、登录设备和事件时间戳。下面是这些记录的简要介绍。</p><figure class="nc nd ne nf gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi nb"><img src="../Images/40af2ed77e74191ceb1159b5f331f047.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uRJjtpmHbEc4s-vp2gmVSw.png"/></div></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">数据的快照</figcaption></figure><p id="62f0" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">每行数据代表一个用户操作日志，时间戳可以帮助我们对事件进行排序。</p><h2 id="829a" class="mk ln in bd lo ml mm dn ls mn mo dp lw kl mp mq ma kp mr ms me kt mt mu mi mv bi translated">图书馆</h2><p id="9eca" class="pw-post-body-paragraph ka kb in kc b kd mw kf kg kh mx kj kk kl my kn ko kp mz kr ks kt na kv kw kx ig bi translated">PySpark libaries 包含多个模块。在本博客中，<strong class="kc io"> pyspark.sql </strong>和<strong class="kc io"> pyspark.ml </strong>是数据处理和建模主要使用的库。<strong class="kc io"> pyspark.sql </strong>用于数据查询、数据挖掘和数据分析。<strong class="kc io"> pyspark.ml </strong>用于管道/模型开发、评估和数据工程。</p><h2 id="fe4f" class="mk ln in bd lo ml mm dn ls mn mo dp lw kl mp mq ma kp mr ms me kt mt mu mi mv bi translated">工作流程</h2><p id="285c" class="pw-post-body-paragraph ka kb in kc b kd mw kf kg kh mx kj kk kl my kn ko kp mz kr ks kt na kv kw kx ig bi translated">使用 PySpark 构建机器学习模型需要以下步骤:</p><ul class=""><li id="f410" class="ky kz in kc b kd ke kh ki kl la kp lb kt lc kx ld le lf lg bi translated"><strong class="kc io">创建一个火花对象</strong></li></ul><p id="5453" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">需要创建一个 Spark 会话，它可以用下面的代码创建</p><pre class="nc nd ne nf gt ng nh ni nj aw nk bi"><span id="dcc1" class="mk ln in nh b gy nl nm l nn no">spark = SparkSession.builder \<br/> .master(“local”) \<br/> .appName(“Sparkify Project”) \<br/> .getOrCreate()</span></pre><p id="b539" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">值得注意的是，这个项目是建立在本地模式，因为这是更容易演示和测试。</p><p id="4fa9" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">然后，创建的 Spark 对象可以用来加载数据</p><pre class="nc nd ne nf gt ng nh ni nj aw nk bi"><span id="f2a1" class="mk ln in nh b gy nl nm l nn no">stack_overflow_data = 'data.json'<br/>df = spark.read.json(stack_overflow_data)<br/>df.persist()</span></pre><p id="e15f" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">persist()是一种优化技术，用于捕获内存中的数据，以便在 PySpark 中进行数据处理</p><pre class="nc nd ne nf gt ng nh ni nj aw nk bi"><span id="3b4a" class="mk ln in nh b gy nl nm l nn no">df.show()  <br/>df.collect()</span></pre><p id="8aea" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">为了减少计算量，spark 通常使用声明式编程(惰性求值)而不是命令式编程。大多数代码用于建立逻辑/工作流的抽象。如果您想查看代码运行的结果，show()或 collect()函数是触发计算的命令。</p><ul class=""><li id="6112" class="ky kz in kc b kd ke kh ki kl la kp lb kt lc kx ld le lf lg bi translated"><strong class="kc io">数据探索</strong></li></ul><p id="9c6d" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">此阶段通常会探索一些描述性的统计特征，并将一些特征可视化。常见命令有:</p><p id="2500" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">观察变量的数据类型</p><pre class="nc nd ne nf gt ng nh ni nj aw nk bi"><span id="cf48" class="mk ln in nh b gy nl nm l nn no">df.printSchema()</span></pre><figure class="nc nd ne nf gt jo gh gi paragraph-image"><div class="gh gi np"><img src="../Images/d0e38c535229f7458afb8a34804b1ec8.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*mjCUqcRhTV_0DvyXMdXNaQ.png"/></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">printSchema()的输出</figcaption></figure><p id="9e6b" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">获取描述性的统计摘要</p><pre class="nc nd ne nf gt ng nh ni nj aw nk bi"><span id="4ac2" class="mk ln in nh b gy nl nm l nn no">df.describe(['age'])</span></pre><figure class="nc nd ne nf gt jo gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/41328870e66f2fc9d827fb3fb005c045.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/1*jzTLglwNtGCbGZW0MOZ1yw.gif"/></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">describe()的输出</figcaption></figure><p id="ec75" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">获取分类特征的计数</p><pre class="nc nd ne nf gt ng nh ni nj aw nk bi"><span id="0157" class="mk ln in nh b gy nl nm l nn no">df.dropDuplicates()\<br/>    .groupBy('auth')\<br/>    .count()\<br/>    .show()</span></pre><figure class="nc nd ne nf gt jo gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/69cbfdbb93d84e10169ce52ba11a3493.png" data-original-src="https://miro.medium.com/v2/resize:fit:352/format:webp/1*_6UCgawH_XPlRL5Vd8tNHg.png"/></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">groupBy()和 count()的输出</figcaption></figure><p id="b0fc" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">Pyspark 还包含多个内置绘图功能，包括散点图、条形图、饼图、折线图等。他们可以很容易地被谷歌找到。</p><ul class=""><li id="5bd8" class="ky kz in kc b kd ke kh ki kl la kp lb kt lc kx ld le lf lg bi translated"><strong class="kc io">数据角力</strong></li></ul><p id="823c" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">有两种方式进行数据争论:pandas dataframe 或 sql commands 方式。</p><p id="01bb" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">熊猫数据帧</strong>:</p><p id="006a" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">这种方式类似于 pandas dataframe，我们可以使用上传的 spark dataframe 数据调用各种函数，包含<strong class="kc io"> select() </strong>，<strong class="kc io"> filter()/where() </strong>，<strong class="kc io"> groupBy() </strong>，<strong class="kc io"> sort() </strong>，<strong class="kc io"> dropDuplicates() </strong>，<strong class="kc io"> withColumn() </strong>和<strong class="kc io"> pivot() </strong>等。它还包含一些聚合函数，可以用来计算一些统计汇总，如<strong class="kc io"> count() </strong>，<strong class="kc io"> countDistinct() </strong>，<strong class="kc io"> avg() </strong>，<strong class="kc io"> max() </strong>，<strong class="kc io"> min() </strong>。最后，PySpark 还为我们提供了用户自定义函数(UDF)并通常与 lambda 函数一起工作(它也可以与自定义函数一起工作)。下面给出了一个例子</p><pre class="nc nd ne nf gt ng nh ni nj aw nk bi"><span id="b98d" class="mk ln in nh b gy nl nm l nn no">slen = udf(<strong class="nh io">lambda</strong> s: len(s), IntegerType())</span></pre><p id="372e" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io"> SQL 方式</strong>:</p><p id="2944" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">这种方式可以调用<strong class="kc io"> sql() </strong>函数，方便地使用 sql 命令查询数据，对于熟悉 sql 语法的人来说，可以使数据争论的步骤变得更容易。下面给出了一个例子</p><pre class="nc nd ne nf gt ng nh ni nj aw nk bi"><span id="e203" class="mk ln in nh b gy nl nm l nn no">spark.sql('''<br/>          SELECT * <br/>          FROM data_table <br/>          LIMIT 5<br/>          '''<br/>          ).show()</span></pre><p id="a68c" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">在这个项目中，我使用 dataframe 的方式进行数据角力，使用大量的聚合函数提取数字特征，使用<strong class="kc io"> pivot() </strong>函数生成虚拟列。</p><p id="71bd" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">值得注意的是，这个项目的任务是预测用户是否有流失的趋势。因此，在进行数据角力时，我们需要将表转换成以<strong class="kc io"> user_Id </strong>为主键的表。</p><ul class=""><li id="581f" class="ky kz in kc b kd ke kh ki kl la kp lb kt lc kx ld le lf lg bi translated"><strong class="kc io">标签搅拌</strong></li></ul><p id="81a0" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">下一步是生成客户流失的标签。数据的<strong class="kc io">页</strong>栏记录了客户的<strong class="kc io">取消确认</strong>和<strong class="kc io">降级</strong>事件。因此，选择这两个事件作为流失的标签，代码如下</p><pre class="nc nd ne nf gt ng nh ni nj aw nk bi"><span id="4c46" class="mk ln in nh b gy nl nm l nn no">#define the flag event udf to transform event to 0 or 1<br/>flag_event = udf(lambda x : 1 if x in {'Cancellation Confirmation', 'Downgrade'} else 0, IntegerType())<br/>#define the current churn or not state<br/>df = df.withColumn('Churn',flag_event('page'))</span></pre><p id="b1c4" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">因为数据是用户操作日志，所以在客户流失之前标记活动是值得的，并且 Window()函数对于跟踪这些活动是有用的。下面给出了一个代码示例</p><pre class="nc nd ne nf gt ng nh ni nj aw nk bi"><span id="0598" class="mk ln in nh b gy nl nm l nn no"># label users who have churned<br/>churn_window = Window.partitionBy("userId").rangeBetween(Window.unboundedPreceding, Window.unboundedFollowing)<br/>df = df.withColumn("user_churned", sum('Churn').over(churn_window))<br/>df.show()</span></pre><ul class=""><li id="cef6" class="ky kz in kc b kd ke kh ki kl la kp lb kt lc kx ld le lf lg bi translated"><strong class="kc io">数据收集</strong></li></ul><p id="6ac2" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">创建完所有特征后，下一步是将所有特征和标签收集到一个变量中，<strong class="kc io"> join() </strong>函数很有用。的代码如下所示</p><pre class="nc nd ne nf gt ng nh ni nj aw nk bi"><span id="f27c" class="mk ln in nh b gy nl nm l nn no"># numeric features gathering<br/>numeric_features=num_songs_df.join(user_page_df,on='userId',how='outer').join(song_length_df,'userId','outer').join(num_artists_user_df,'userId','outer').join(num_sessions_user_df,'userId','outer').join(reg_df,'userId','outer')</span><span id="6e9d" class="mk ln in nh b gy ns nm l nn no"># categorical features gathering<br/>categorical_features=userAgent_df.join(gender_df,'userId','outer').join(level_df,'userId','outer').join(location_df,'userId','outer')</span><span id="79aa" class="mk ln in nh b gy ns nm l nn no"># Gather all<br/>data=categorical_features.join(numeric_features.select('userId','Numeric_Features'),on='userId',how='outer'.join(label,on='userId',how='outer').drop('userId').fillna(0)</span></pre><p id="6449" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">值得注意的是，在我们将所有的特性和标签组合在一起之后，我们需要删除<strong class="kc io"> userId </strong>。</p><p id="99e8" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">此外，数字特征通常具有不同的标度，这可能误导机器学习模型。因此，我们通常也希望在将数字特征与其他特征结合之前对其进行规范化。可以使用<strong class="kc io"> StandardScaler() </strong>函数进行归一化。在 PySpark 中，大多数变压器要求输入必须是矢量，因此通常需要<strong class="kc io"> VectorAssembler() </strong>。也有一些其他的软件包可以用于特征转换，如<strong class="kc io"> RegexTokenizer() </strong>、<strong class="kc io"> Normalizer() </strong>、<strong class="kc io"> StandardScaler() </strong>和<strong class="kc io"> MinMaxScaler() </strong>等。本项目中使用的变压器代码如下</p><pre class="nc nd ne nf gt ng nh ni nj aw nk bi"><span id="fccd" class="mk ln in nh b gy nl nm l nn no">assembler = VectorAssembler(inputCols=numeric_features.columns[1:], outputCol="Numerical_Features")<br/>numeric_features = assembler.transform(numeric_features)<br/>scaler2 = StandardScaler(inputCol="Numerical_Features", outputCol="Numeric_Features", withStd=True)<br/>scalerModel = scaler2.fit(numeric_features)<br/>numeric_features = scalerModel.transform(numeric_features)</span></pre><ul class=""><li id="ce41" class="ky kz in kc b kd ke kh ki kl la kp lb kt lc kx ld le lf lg bi translated"><strong class="kc io">建模和评估</strong></li></ul><p id="d435" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">在所有准备活动之后，最后的工作是建立一个机器学习模型来预测用户流失，该任务可以进一步分为以下子任务。</p><p id="1e5d" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">步骤 1 数据集分割</strong></p><p id="c137" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">randomSplit()函数可用于将收集的数据分成训练集和测试集。</p><pre class="nc nd ne nf gt ng nh ni nj aw nk bi"><span id="b49c" class="mk ln in nh b gy nl nm l nn no">train,test = data.randomSplit([0.8, 0.2], seed=50)</span></pre><p id="0888" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">第二步构建管道</strong></p><p id="8f2f" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">那么可以用 pipeline()函数来构造管道</p><pre class="nc nd ne nf gt ng nh ni nj aw nk bi"><span id="f87b" class="mk ln in nh b gy nl nm l nn no">pipeline = Pipeline(stages=[transformer1, transformer2, estimator])</span></pre><p id="871b" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">由于时间有限，本项目直接使用估算器而不是流水线。但是，这个项目已经测试了多个估计器，如逻辑回归、决策树、梯度提升树、随机森林模型、多层感知器分类器和线性向量机。</p><p id="e378" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">步骤 3 调整模型</strong></p><p id="4a75" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">这一步是为管道或估计器设置验证方法和参数。为了选择最佳模型，通常使用 gridsearch。为此，使用了 ParamGridBuilder()和 CrossValidator()函数。</p><pre class="nc nd ne nf gt ng nh ni nj aw nk bi"><span id="5064" class="mk ln in nh b gy nl nm l nn no"># parameters<br/>param_grid = ParamGridBuilder().addGrid(model.regParam,[0.0, 0.5]).build()</span><span id="985f" class="mk ln in nh b gy ns nm l nn no"># validator<br/>crossval=CrossValidator(estimator=estimator,estimatorParamMaps=paramGrid,evaluator=MulticlassClassificationEvaluator(),numFolds=3)</span></pre><p id="0f5e" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">第四步评估</strong></p><p id="1dd7" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">最后一步是评估，通常有两个评估器:<strong class="kc io">多类分类评估器()</strong>和<strong class="kc io">二元分类评估器()</strong>。在评估器中，可以选择不同的度量标准，如 f1 值、准确度和假阳性率等。</p><pre class="nc nd ne nf gt ng nh ni nj aw nk bi"><span id="3b5d" class="mk ln in nh b gy nl nm l nn no">evaluator=BinaryClassificationEvaluator()<br/>    model_results['f1_score'] = evaluator.evaluate(results.select('label','prediction'),{evaluator.metricName: 'f1'})<br/>    model_results['accuracy'] = evaluator.evaluate(results.select('label','prediction'), {evaluator.metricName: "accuracy"})</span></pre><h1 id="a682" class="lm ln in bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">调试和优化</h1><p id="7c8c" class="pw-post-body-paragraph ka kb in kc b kd mw kf kg kh mx kj kk kl my kn ko kp mz kr ks kt na kv kw kx ig bi translated">由于 PySpark ML 是一个分布式库和惰性评估，代码调试比 scikit-learn 库更困难。数据子集通常会被发送到不同的工作节点进行处理，并且很难跟踪本地情况。其中一个叫做累加器的函数可以用来创建全局变量来跟踪不正确的记录。下面给出了一个例子</p><pre class="nc nd ne nf gt ng nh ni nj aw nk bi"><span id="3280" class="mk ln in nh b gy nl nm l nn no">incorrect_records = SparkContext.accumulator(0,0)<br/>def add_incorrect_record():<br/>   global incorrect_records<br/>   incorrect_records +=1<br/>correct_ts = udf(lambda x: 1 if x.isdigit() else add_incorrect_record())<br/>data = data.withColumn("ts_digit",correct_ts(data.ts))</span></pre><p id="8c06" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">除了 accumulator 之外，Web UI(Spark 内置用户界面)是另一种调试方式，它显示集群的状态、配置和任何最近作业的状态。端口 8080 可用于连接 WebUI。</p><p id="2593" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">这个博客使用本地模式，所以在这个项目中不需要调试方法，但它对其他模式非常有用。我将在未来创建另一个博客来讨论模式和调试的事情。</p><h1 id="7987" class="lm ln in bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">结论</h1><p id="df4f" class="pw-post-body-paragraph ka kb in kc b kd mw kf kg kh mx kj kk kl my kn ko kp mz kr ks kt na kv kw kx ig bi translated">随着数据爆炸或昂贵但有效的算法出现，分布式机器学习对于商业和数据分析将变得更加重要。这篇博客展示了如何使用 PySpark 来建立和找到一个最佳机器学习模型来预测客户流失。这项工作仅用于指导，因此准确性仍有提高的空间(特别是对于完整的 12GB 数据集)。订阅我，我将尝试创建另一个博客来描述云的后续工作。最后，如果你能点击<strong class="kc io">鼓掌</strong>来支持我的博客，我会非常感激，我会继续围绕数据科学或一些热点领域制作更多有趣的内容。</p></div></div>    
</body>
</html>