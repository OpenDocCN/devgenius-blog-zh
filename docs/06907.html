<html>
<head>
<title>Fake Job prediction with Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用机器学习进行假工作预测</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/fake-job-prediction-with-machine-learning-76b238c2d2e8?source=collection_archive---------8-----------------------#2022-02-10">https://blog.devgenius.io/fake-job-prediction-with-machine-learning-76b238c2d2e8?source=collection_archive---------8-----------------------#2022-02-10</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><figure class="gl gn jl jm jn jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi jk"><img src="../Images/7397e2b5b2c3b2f53760c000addf5863.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lXojpmw9SNwH-XN-RMOioA.jpeg"/></div></div></figure><p id="669b" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">在机器学习中，分类问题可能是人们第一次学习该领域时首先学习的问题。我是作为数据科学家开始我的旅程的，所以为了练习，今天我们将使用著名的机器学习问题库，它提供了十几个内置模型，Scikit-learn。对于我们的数据，我们将使用来自 Kaggle 的<a class="ae kt" href="https://www.kaggle.com/shivamb/real-or-fake-fake-jobposting-prediction" rel="noopener ugc nofollow" target="_blank"> Fake Jobs 数据集来查看我迄今为止的一些学习。这个想法是为了预测一些在互联网上发布的工作是真是假。我们将涵盖数据分析、数据清理、建模和改进超参数技术的基础知识。</a></p><p id="96ef" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">让我们开始吧，我们的第一步是导入一些在数据处理过程中对我们有用的包，并从 Kaggle 下载数据集。</p><pre class="ku kv kw kx gt ky kz la lb aw lc bi"><span id="fe83" class="ld le in kz b gy lf lg l lh li"># Import tools we need</span><span id="2b32" class="ld le in kz b gy lj lg l lh li">import pandas as pd</span><span id="e3f5" class="ld le in kz b gy lj lg l lh li">import numpy as np</span><span id="6fe6" class="ld le in kz b gy lj lg l lh li">import matplotlib.pyplot as plt</span><span id="e262" class="ld le in kz b gy lj lg l lh li">from sklearn.model_selection import train_test_split</span><span id="485d" class="ld le in kz b gy lj lg l lh li">from sklearn.metrics import accuracy_score</span><span id="5fc9" class="ld le in kz b gy lj lg l lh li"># Import data</span><span id="fd3c" class="ld le in kz b gy lj lg l lh li">data = pd.read_csv("fake_job_postings.csv")</span><span id="7695" class="ld le in kz b gy lj lg l lh li">data.head()</span></pre><figure class="ku kv kw kx gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi lk"><img src="../Images/28b1d0460b9a69c11c3d7682d1372b88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PFBFvtXQsr7cGEBmYq5buQ.png"/></div></div></figure><p id="b5d2" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">正如我们所看到的，我们的数据有一些 NaN 值，这是一个大问题。让我们验证有多少列有 NaN 值。</p><pre class="ku kv kw kx gt ky kz la lb aw lc bi"><span id="edee" class="ld le in kz b gy lf lg l lh li">data.isna().sum()</span></pre><figure class="ku kv kw kx gt jo gh gi paragraph-image"><div class="gh gi ll"><img src="../Images/b409a8e3ba0b81acd76209c7e6d4bbc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*pi8Kqryei4ohUkVnjwhiZQ.png"/></div></figure><p id="625c" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">我们可以用一系列技术来解决这个问题，其中之一就是用零填充缺失的数据。</p><pre class="ku kv kw kx gt ky kz la lb aw lc bi"><span id="2fee" class="ld le in kz b gy lf lg l lh li">data = data.fillna(0)</span></pre><p id="2328" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">正如我们在下图中看到的，我们在每个标签的数据量上有很大的差异，如果与“1-假工作”相比，标签“0-真工作”的数据大约多 20 倍。</p><figure class="ku kv kw kx gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi lm"><img src="../Images/627d16780c0954f9fb475968b8d26890.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*RVbWewA-vHW5uEinhAPURQ.png"/></div></div></figure><p id="275e" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">有多种技术可以解决这个问题，其中一种称为“非抽样”，即从标签 0 开始减少样本数，直到与标签 1 的大小相同。</p><pre class="ku kv kw kx gt ky kz la lb aw lc bi"><span id="0c42" class="ld le in kz b gy lf lg l lh li"># Shuffle the Dataset</span><span id="d2df" class="ld le in kz b gy lj lg l lh li">shuffled_df = data.sample(frac=1,random_state=4)</span><span id="6ff9" class="ld le in kz b gy lj lg l lh li"># Get only fraudulent values</span><span id="56dc" class="ld le in kz b gy lj lg l lh li">fraudulent_df = shuffled_df.loc[shuffled_df["fraudulent"] == 1]</span><span id="0790" class="ld le in kz b gy lj lg l lh li"># Get only non-fraudulent values</span><span id="ebc8" class="ld le in kz b gy lj lg l lh li">non_fraudulent_df = shuffled_df.loc[shuffled_df["fraudulent"] == 0].sample(n=len(fraudulent_df),random_state=42)</span><span id="e831" class="ld le in kz b gy lj lg l lh li"># Concat the fraudulent and non-fraudulent values in one dataset</span><span id="ad65" class="ld le in kz b gy lj lg l lh li">df = pd.concat([fraudulent_df, non_fraudulent_df])</span></pre><p id="e1ec" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">我们已经解决了数据中的两个问题，但还剩下一个。我们数据的某些列不是数值，这将很快成为训练我们机器学习模型的问题。</p><pre class="ku kv kw kx gt ky kz la lb aw lc bi"><span id="b86d" class="ld le in kz b gy lf lg l lh li">df.head().info()</span></pre><p id="7e27" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">我们可以看到，非数字数据的类型为“object”。</p><figure class="ku kv kw kx gt jo gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/784d8e107b0997e8987963386c4bbe29.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*ip5izD1-0MtTgFs-qPGrJA.png"/></div></figure><p id="b466" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">让我们将数据转换成分类格式，这将为列中的每个不同值生成一些“代码”。</p><pre class="ku kv kw kx gt ky kz la lb aw lc bi"><span id="f766" class="ld le in kz b gy lf lg l lh li"># Turn into categorical <br/>for label, content in df.items():</span><span id="b3b3" class="ld le in kz b gy lj lg l lh li">if pd.api.types.is_string_dtype(content):</span><span id="b6e6" class="ld le in kz b gy lj lg l lh li">df[label] = content.astype("category").cat.as_ordered()</span><span id="9061" class="ld le in kz b gy lj lg l lh li"># Turn categorical variables into numbers</span><span id="6808" class="ld le in kz b gy lj lg l lh li">for label, content in df.items():</span><span id="db31" class="ld le in kz b gy lj lg l lh li">if not pd.api.types.is_numeric_dtype(content):</span><span id="20a8" class="ld le in kz b gy lj lg l lh li">df[label] = pd.Categorical(content).codes+1</span></pre><figure class="ku kv kw kx gt jo gh gi paragraph-image"><div class="gh gi lo"><img src="../Images/a87fcfcd0767a466c18719c195d0f689.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*x_zlmSMwwJ-7lpTnu0Sf7g.png"/></div></figure><p id="a37e" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">正如我们所看到的，我们的数据现在是数字格式，我们可以在将数据分成训练集和测试集之后建模。</p><pre class="ku kv kw kx gt ky kz la lb aw lc bi"><span id="201c" class="ld le in kz b gy lf lg l lh li"># Split data into train and test set</span><span id="a39d" class="ld le in kz b gy lj lg l lh li">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</span></pre><p id="abad" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">对于建模部分，让我们开始检查哪些算法在我们的数据中执行得最好，然后选择最好的，并调整它以获得可能的最佳精度。</p><pre class="ku kv kw kx gt ky kz la lb aw lc bi"><span id="fb05" class="ld le in kz b gy lf lg l lh li"># Put models in a dictionary</span><span id="6bcb" class="ld le in kz b gy lj lg l lh li">models = {"KNN": KNeighborsClassifier(),</span><span id="48e6" class="ld le in kz b gy lj lg l lh li">"LinearSVC": LinearSVC(),</span><span id="1a4b" class="ld le in kz b gy lj lg l lh li">"NaiveBayes": GaussianNB(),</span><span id="9292" class="ld le in kz b gy lj lg l lh li">"Logistic Regression": LogisticRegression(),</span><span id="084d" class="ld le in kz b gy lj lg l lh li">"Random Forest": RandomForestClassifier()}<br/></span><span id="ae1b" class="ld le in kz b gy lj lg l lh li"># Create function to fit and score models</span><span id="fe36" class="ld le in kz b gy lj lg l lh li">def fit_and_score(models, X_train, X_test, y_train, y_test):</span><span id="79ce" class="ld le in kz b gy lj lg l lh li"># Random seed for reproducible results</span><span id="96cb" class="ld le in kz b gy lj lg l lh li">np.random.seed(42)</span><span id="3ffa" class="ld le in kz b gy lj lg l lh li"># Make a list to keep model scores</span><span id="468a" class="ld le in kz b gy lj lg l lh li">model_scores = {}</span><span id="7b32" class="ld le in kz b gy lj lg l lh li"># Loop through models</span><span id="7d91" class="ld le in kz b gy lj lg l lh li">for name, model in models.items():</span><span id="450a" class="ld le in kz b gy lj lg l lh li"># Fit the model to the data</span><span id="c917" class="ld le in kz b gy lj lg l lh li">model.fit(X_train, y_train)</span><span id="447c" class="ld le in kz b gy lj lg l lh li"># Evaluate the model and append its score to model_scores</span><span id="d336" class="ld le in kz b gy lj lg l lh li">model_scores[name] = model.score(X_test, y_test)</span><span id="5a6d" class="ld le in kz b gy lj lg l lh li">return model_scores<br/></span><span id="74d2" class="ld le in kz b gy lj lg l lh li">fit_and_score(models=models, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)</span></pre><figure class="ku kv kw kx gt jo gh gi paragraph-image"><div class="gh gi lp"><img src="../Images/dc31a8ba66d4034e889f9fc17f1821ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*ibZurZiStYtrkdCKoon4dg.png"/></div></figure><p id="d1d6" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">我们用 RandomForestClassifier 在测试数据中获得了大约 90%的准确率<strong class="jx io">，这是相当不错的。但是我们仍然可以调整模型超参数，看看是否在我们的数据中表现得更好。超参数调整有很多方法，但我们将使用<a class="ae kt" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html" rel="noopener ugc nofollow" target="_blank"> RandomizedSearchCV </a>，它让我们为超参数提供一系列值，然后随机选择性能最佳的一个。</strong></p><pre class="ku kv kw kx gt ky kz la lb aw lc bi"><span id="a291" class="ld le in kz b gy lf lg l lh li">from sklearn.model_selection import RandomizedSearchCV</span><span id="b763" class="ld le in kz b gy lj lg l lh li">n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]</span><span id="2400" class="ld le in kz b gy lj lg l lh li">max_features = ['auto', 'sqrt']</span><span id="29a9" class="ld le in kz b gy lj lg l lh li">max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]</span><span id="eec3" class="ld le in kz b gy lj lg l lh li">max_depth.append(None)</span><span id="4f3e" class="ld le in kz b gy lj lg l lh li">min_samples_split = [2, 5, 10]</span><span id="e760" class="ld le in kz b gy lj lg l lh li">min_samples_leaf = [1, 2, 4]</span><span id="eb53" class="ld le in kz b gy lj lg l lh li">random_grid = {</span><span id="2c89" class="ld le in kz b gy lj lg l lh li">'n_estimators': n_estimators,</span><span id="3218" class="ld le in kz b gy lj lg l lh li">'max_features': max_features,</span><span id="0f2d" class="ld le in kz b gy lj lg l lh li">'max_depth': max_depth,</span><span id="5983" class="ld le in kz b gy lj lg l lh li">'min_samples_split': min_samples_split,</span><span id="0350" class="ld le in kz b gy lj lg l lh li">'min_samples_leaf': min_samples_leaf</span><span id="861f" class="ld le in kz b gy lj lg l lh li">}</span><span id="5cf3" class="ld le in kz b gy lj lg l lh li">rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)</span><span id="e397" class="ld le in kz b gy lj lg l lh li">rf_random = rf_random.fit(X_train, y_train)</span><span id="7845" class="ld le in kz b gy lj lg l lh li"># Best random model</span><span id="01d5" class="ld le in kz b gy lj lg l lh li">preds_best_random = rf_random.predict(X_test)</span><span id="daf6" class="ld le in kz b gy lj lg l lh li">print(f"Best model accuracy: {accuracy_score(y_test, preds_best_random)}")</span></pre><figure class="ku kv kw kx gt jo gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/955969d626fa491a421129c62ba980fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*Y_bmiJX5Je9CAPEW8LBwRA.png"/></div></figure><p id="ac7e" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">我们有将近 92%的准确率！！！这很好，如果我在随机搜索中使用更多的超参数，可能会更好或更差，这取决于每个问题。</p><p id="0b07" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">我在 medium 上完成了我的第一个完整的机器学习实现教程/过程，听起来不错…</p><p id="6000" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated"><strong class="jx io">对我帮助很大的资源:</strong></p><p id="9127" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated"><a class="ae kt" href="https://www.udemy.com/course/complete-machine-learning-and-data-science-zero-to-mastery/" rel="noopener ugc nofollow" target="_blank">完全机器学习&amp;数据科学训练营 2022 </a></p><p id="b48e" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated"><a class="ae kt" href="https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74?gi=a2dc21967f4a" rel="noopener" target="_blank">超参数调优 Python 中的随机森林</a></p></div></div>    
</body>
</html>