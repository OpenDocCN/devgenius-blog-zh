<html>
<head>
<title>Why initialize weights randomly(numpy.random.randn()) in Hidden Layered Neural Networks ?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">隐层神经网络为什么要随机初始化权重(numpy.random.randn())？</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/why-initialize-weights-randomly-numpy-random-randn-in-hidden-layered-neural-networks-5187263f2ed1?source=collection_archive---------8-----------------------#2022-02-18">https://blog.devgenius.io/why-initialize-weights-randomly-numpy-random-randn-in-hidden-layered-neural-networks-5187263f2ed1?source=collection_archive---------8-----------------------#2022-02-18</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><p id="75fe" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">有线性回归为什么还要 Logistic 回归？</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/e8cb209a119b6c1f9d1c60ec7e05d750.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*Y1UH7y5WR2aDldvPj5hI0g.png"/></div></figure><p id="fb68" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">来源:www。Mathworks.com</p><p id="039a" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">基本上，分类是预测离散类别标签的任务，而回归是预测连续数量的任务。回归问题涉及到数量的预测。</p><p id="3906" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">线性回归</strong>:建立输入与输出之间的线性关系。它适用于连续变量。使用最小平方估计方法。</p><p id="ec99" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">逻辑回归:</strong>这里不需要线性关系，因为在方法中使用了 Logit 函数，所以称之为逻辑回归。它可以输出 0 到 1 之间的预测值，它对分类变量有效。s 曲线(sigmoid)输出介于 0 和 1 之间。使用 s 曲线是因为逻辑回归问题的输出是事件发生的概率。这里使用最大似然估计。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kq"><img src="../Images/d9b15cc271633451fba7fe01d16ec01a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XgAlWcV-GwhipVkDwATUoA.jpeg"/></div></div></figure><p id="d4b6" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">来源:谷歌</p><p id="1cff" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">为什么？:</strong>线性回归不适用于二元响应变量，结果变量不会预测 0 到 1 之间的值，因为它适用于连续因变量，而逻辑回归适用于分类因变量。</p><p id="c61c" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">逻辑回归存在时为什么要隐藏分层神经网络？</strong></p><p id="db9d" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">为了更好地逼近大数据，逻辑回归不足以得出近似值。因此，分层神经网络被发现来实现这种情况。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kv"><img src="../Images/bed60d8f56b4db4bbdebb2cf9d7c68a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-2YWGYfAy0KacPF_Sp9WTQ.png"/></div></div></figure><p id="fbb6" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">资料来源:Coursera Andrew NG</p><p id="3e4f" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">为什么命名为隐藏层？</strong></p><p id="ca4e" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">让我们来看看神经网络的体系结构——一个输入层，大小等于输入的维数，一个输出层每个类一个节点或一个实值结果的节点，即隐藏层。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kw"><img src="../Images/b01a5c745c4dd912050da7cd499d38a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jqdkatC1IRRMOYW5OVn3AQ.jpeg"/></div></div></figure><p id="d597" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">来源:谷歌</p><p id="0513" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">输入和输出层都是不言自明的。现在需要给中间层命名了。在所有的选择中间，未知，潜在，隐藏。隐藏是最好的选择。</p><p id="c775" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">为什么神经网络和非线性网络都需要激活函数？</strong></p><p id="b6ed" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">神经网络中的激活函数意味着如何将输入的加权和转换成来自神经网络层中的一个或多个节点的输出。</p><p id="633f" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">为什么？:</strong>将具有无限范围(-inf 到+inf)的值投影到概率设置，其中值的范围是从 0 到 1(有时是从-1 到+1)。这引入了多层网络中期望的非线性，以便检测数据中的非线性特征。如果只应用线性激活函数，那么网络将只是线性函数的组合，其也是线性函数。</p><p id="19e5" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">为什么要随机初始化权重？</strong></p><p id="9721" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">重点是<strong class="jm io">打破对称性</strong>。因为如果你将所有权重初始化为零，那么神经网络中所有隐藏的神经元(单元)将进行完全相同的计算。当我们将权重和偏差初始化为零时，这使得神经网络问题成为一个死问题。</p><p id="7a92" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">在梯度下降迭代中，前一层的梯度是现有权重乘以从下一层的反向传播获得的梯度。如果初始权重设置为零，权重和梯度的乘积也将为零。由于梯度为零，梯度下降不会改变或优化每次迭代中的权重。这不是我们在神经网络问题中想要做的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kx"><img src="../Images/4d31c08416d8e88840ab1596aa648fb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1hFsvG0GEIWaq5hLlzNaAg.png"/></div></div></figure><p id="e37c" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">来源:谷歌</p><p id="fcfc" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">当层中的所有权重都相同时，每个神经元都表示相同，并且层中存在对称性。因此，即使层中有更多的神经元也没有意义，它几乎类似于一个神经元。为了避免这种情况，神经网络问题层中的初始权重必须随机初始化。最常用的随机化初始化是 numpy.random.randn()</p><p id="1e51" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">最后得出了一个完美的结论——当随机初始化权重时，系统的熵增加，这有助于找到更低的点，如局部或全局最小点。</p><p id="5300" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">引文:直接和/或修改的一些句子或图像摘自 DeepLearning AI，Medium，Mathworks，Steven L.Brunton 数据驱动的科学与工程</strong></p><p id="d27d" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我希望你能愉快地阅读这篇文章。</p></div></div>    
</body>
</html>