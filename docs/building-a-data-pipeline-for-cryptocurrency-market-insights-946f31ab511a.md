# ä¸ºåŠ å¯†è´§å¸å¸‚åœºæ´å¯Ÿæ„å»ºæ•°æ®ç®¡é“

> åŸæ–‡ï¼š<https://blog.devgenius.io/building-a-data-pipeline-for-cryptocurrency-market-insights-946f31ab511a?source=collection_archive---------9----------------------->

![](img/ab8a3c9dd82dc2e64ae279fb9e49d054.png)

## ä»‹ç»

åˆ©æ¶¦ï¼è¿™æ˜¯ä¸€ä¸ªéå¸¸æœ‰è¶£çš„è¯ï¼Œä½†å®ƒæ˜¯æ„æˆæˆ‘ä»¬ä¸–ç•Œå¸‚åœºç»æµçš„æœ¬è´¨ã€‚ç®€è€Œè¨€ä¹‹ï¼Œå®ƒæ˜¯ä¸€ç§è®©ä½ æŠ•èµ„çš„èµ„é‡‘äº§ç”Ÿç›ˆä½™çš„èƒ½åŠ›ã€‚æ‰€ä»¥ï¼Œå‡è®¾ä½ æŠ•èµ„ 10 ç¾å…ƒï¼Œèµšäº† 15 ç¾å…ƒï¼Œé‚£ä¹ˆä½ å°±èµšäº† 5 ç¾å…ƒâ€”â€”å¾ˆç®€å•ï¼Œå¯¹å§ï¼Ÿ

ç°åœ¨â€¦ä½ ä¸€ç”Ÿä¸­é‡åˆ°çš„æ¯ä¸€é¡¹æœåŠ¡ã€æ¯ä¸€ä¸ªæœºæ„éƒ½æ˜¯åŸºäºè¿™ä¸ªåŸåˆ™ï¼Œä»å­¦æ ¡ã€äº¤é€šã€å¨±ä¹åˆ°åŸºæœ¬çš„ç”Ÿå­˜ï¼Œæ¯”å¦‚é£Ÿå“å·¥ä¸šã€‚å¦‚æœè¿™äº›è¡Œä¸šçš„æˆæœ¬é«˜äºå®ƒä»¬åº”è¯¥è·å¾—çš„åˆ©æ¶¦ï¼Œå®ƒä»¬å°±ä¼šå€’é—­ã€‚

è®©æˆ‘ä»¬è°ˆä¸€è°ˆäº¤æ˜“ï¼Œå› ä¸ºè¯¥è®¡åˆ’æ—¨åœ¨è§£å†³åŠ å¯†è´§å¸å¸‚åœºä¸­çš„ä¸€ä¸ªç‰¹å®šç”¨ä¾‹ã€‚

## é‚£ä¹ˆï¼Œä»€ä¹ˆæ˜¯äº¤æ˜“ï¼Ÿ

å®ƒå»ºç«‹åœ¨ä»¥ç›ˆåˆ©ä¸ºç›®çš„çš„å¸‚åœºç»æµä½“åˆ¶çš„åŒæ ·åŸåˆ™åŸºç¡€ä¸Šã€‚ä½†æ˜¯å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„å‘¢ï¼Ÿ

ä½ ä»¥æŸä¸€ä»·æ ¼ä¹°å…¥ä¸€ç§è´§å¸ï¼Œå½“è¿™ç§è´§å¸å‡å€¼æ—¶ï¼Œä½ å°±è·åˆ©äº†ğŸ˜Šè¯¶å“ˆâ€¦ä½†æ˜¯å¦‚æœå½“ä½ çš„é’±åœ¨å¸‚åœºä¸Šæ—¶å®ƒä¸‹è·Œäº†ï¼Œä½ å°±äºäº†ã€‚å¦‚æœä½ çš„é’±ä¸åœ¨å¸‚åœºä¸Šï¼Œé‚£ä¹ˆè¿™æ˜¯ä¸€ä¸ªä½ä»·ä¹°å…¥çš„æœºä¼šï¼Œè¿™æ ·å½“ä»·æ ¼ä¸Šæ¶¨æ—¶ï¼Œä½ å°±è·åˆ©äº†ã€‚äº¤æ˜“çš„å­˜åœ¨æ˜¯å› ä¸ºä»·å€¼æ— æ—¶æ— åˆ»ä¸åœ¨æ³¢åŠ¨â€¦**ï¼å®ƒçš„æ³¢åŠ¨åŸºäºå¾ˆå¤šå› ç´ ï¼Œæ ¸å¿ƒæ˜¯å…³äºå¸‚åœºä¸Šèµ„äº§çš„ä¾›ç»™å’Œéœ€æ±‚ï¼Œæ›´å¤šçš„ä¾›ç»™å’Œæ›´å¼±çš„éœ€æ±‚æ¨åŠ¨ä»·æ ¼ä¸‹è·Œï¼Œç›¸åæ¨åŠ¨ä»·æ ¼ä¸Šæ¶¨ã€‚**

**æ‰€ä»¥ï¼Œä½œä¸ºä¸€ä¸ªäº¤æ˜“è€…ï¼Œå°¤å…¶æ˜¯çŸ­çº¿äº¤æ˜“è€…ï¼Œä½ æƒ³è¦ä¸€ä¸ªæ³¢åŠ¨å¾ˆå¤§çš„è´§å¸ï¼Œå› ä¸ºä½ æƒ³åšå°½å¯èƒ½å¤šçš„ç›ˆåˆ©äº¤æ˜“ã€‚ä½ çŸ¥é“æ¯”ç‰¹å¸ä¸æ˜¯åŠ å¯†è´§å¸å¸‚åœºä¸­å”¯ä¸€çš„è´§å¸èµ„äº§å—ï¼ŸğŸ¤”æœ‰å¾ˆå¤šï¼Œä½ è¦åšçš„æ˜¯ç¡®å®šå“ªäº›éšç€æ—¶é—´çš„æ¨ç§»æ³¢åŠ¨æœ€å¤§ï¼Œä½ è¦æŠŠä½ çš„æŠ•èµ„åˆ†æ•£åˆ°ä½ å·²ç»ç¡®å®šçš„è‚¡ç¥¨ä¸Šã€‚ä¸ºæ­¤ï¼Œæ‚¨éœ€è¦å¯¹å¸‚åœºè¿›è¡Œåˆ†æï¼Œä»¥äº†è§£ ***è¶‹åŠ¿*** å’Œ ***æ¨¡å¼*** ï¼Œè¿™å°±æ˜¯ä¸‹é¢çš„**æ•°æ®è§£å†³æ–¹æ¡ˆå¹³å°**å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚**

**åœ¨ä»»ä½•ç»™å®šçš„æ—¶é—´å›ç­”ä¸€äº›å…³é”®é—®é¢˜çš„èƒ½åŠ›åœ¨è¿™é‡Œæ˜¯è‡³å…³é‡è¦çš„ã€‚**

*   **éšç€æ—¶é—´çš„æ¨ç§»ï¼ŒåŠ å¯†è´§å¸çš„å¹³å‡ä»·æ ¼æ˜¯å¤šå°‘ï¼Ÿä¸€å¹´ã€ä¸¤å¹´ç­‰ã€‚ä½ æƒ³åœ¨è¿™é‡ŒæŠ“ä½ ***æ¨¡å¼***â€¦â€¦*æˆ‘ä»¬å¾ˆå¯èƒ½ä¼šåšæˆ‘ä»¬åœ¨ä¸€æ®µæ—¶é—´å†…é€šå¸¸ä¼šåšçš„äº‹æƒ…ã€‚â€***
*   **ä»·æ ¼å¤šä¹…æ³¢åŠ¨ä¸€æ¬¡ï¼Œæ³¢åŠ¨å¹…åº¦æ˜¯å¤šå°‘ï¼Ÿåœ¨è¿‡å» x æ®µæ—¶é—´å†…(ä¸€å¹´ã€ä¸¤å¹´ç­‰)ï¼Œè¯¥å€¼ä¸Šå‡æˆ–ä¸‹é™ 1%ã€5%ã€10%ç­‰çš„é¢‘ç‡**

**ä½œä¸ºæŠ•èµ„è€…ï¼Œè¿™äº›é—®é¢˜çš„ç­”æ¡ˆä¼šè®©ä½ å¤„äºæ›´æœ‰åˆ©çš„ä½ç½®ã€‚**

**å¥½äº†ï¼Œç°åœ¨æˆ‘ä»¬å·²ç»è§£å†³äº†è¿™ä¸ªé—®é¢˜â€”â€”è®©æˆ‘ä»¬æ¥è°ˆè°ˆæŠ€æœ¯é—®é¢˜ğŸ˜‹**

**ç»¼åˆèµ·æ¥çš„è§£å†³æ–¹æ¡ˆæ˜¯ç”¨ Python ç¼–å†™çš„ï¼Œå®ƒå°†æ•°æ®å‘é€åˆ°ä¸€ä¸ªè¿‘ä¹å®æ—¶çš„æµåª’ä½“å¹³å°( ***ã€å¡å¤«å¡ã€‘*** )ï¼Œç„¶åè¯¥å¹³å°å°†æ•°æ®è½¬å‘åˆ°ä¸€ä¸ªåˆ†æå¹³å°( ***Elasticsearch*** )ä»¥è·å¾—å¯æ“ä½œçš„è§è§£ã€‚**

## **æ‘„å…¥ Python Kafka åˆ¶ä½œç¨‹åº**

```
 ################################ Get ALL cryptocurrencies (array) #######################################

#==============================# Import required modules for this program #==============================

from confluent_kafka import Producer, KafkaError
import json
import os
import requests
from time import sleep
from datetime import datetime
from datetime import timezone
from requests.exceptions import ConnectionError, ConnectTimeout
import socket
import boto3
import json
import requests
import logging

#==============================# This section runs our python code as a TCP server #==============================

# Create a socket
sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

# Ensure that you can restart your server quickly when it terminates
sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)

# Set the client socket's TCP "well-known port" number
well_known_port = int(os.environ['PYTHON_PRODUCER_SERVER_PORT'])
sock.bind(('localhost', well_known_port))

# Set the number of clients waiting for connection that can be queued
sock.listen(5)

#==============================# This section sets up Kafka Consumer world #==============================

#Set your kafka topic you will publishing into
topic = os.environ['KAFKA_TOPIC']
#Set kafka Producer settings
settings = {
    'bootstrap.servers': os.environ['KAFKA_BROKERS_HOST_PORT'],
    'client.id': os.environ['KAFKA_PRODUCER_CLIENT_ID']
}

#Function to check connectivity to kafka cluster
def isOpen(ip,port):
   s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
   try:
      s.connect((ip, int(port)))
      s.settimeout(30)
      print("Connection successfully established with kafka cluster. We may proceed")
      return True
   except:
      return False
   finally:
        s.close()

# Check connectivity to kafka cluster before proceding any further
def check_kafka():
    global check_port
    check_port_wait = 30
    while True:
        check_port =  isOpen(os.environ['KAFKA_HOST'], int(os.environ['KAFKA_BROKERS_PORT']))
        if check_port == False:
            for x in range(check_port_wait):
                print("Retrying connection to kafka cluster...in " + str(x) + " seconds", end="\r")
                sleep(1)
        else:
            break

# Execute function    
check_kafka()

# Here is our Producer variable 
if check_port == True:
    p = Producer(settings)

# API poll interval for the dataset we're streaming
# We got it at every 2 minutes - you can adjust to make as real-time as possible
interval = int(os.environ['KAFKA_PRODUCER_BATCH_INTERVAL'])

#Wait interval to retry a connection
waiting = int(os.environ['KAFKA_PRODUCER_RETRIES_INTERVAL'])

## Functions ## 
#Called once for each message produced to indicate delivery result.
#Triggered by poll() or flush().
def delivery_report(err, msg):
    if err is not None:
        print('Message delivery failed: {}'.format(err))
    else:
        print('1 Message delivered to {} [{}]'.format(msg.topic(), msg.partition()))

#==================================#            AWS S3 setup            #==================================

#Creating Session With Boto3.
session = boto3.Session(
aws_access_key_id= os.environ['AWS_S3_ACCESS_KEY'],
aws_secret_access_key= os.environ['AWS_S3_SECRET_KEY'],
region_name= os.environ['AWS_S3_REGION']
)

#Function for AWS S3 connectivity 
def send_to_s3():
    try:
        s3 = session.resource('s3')
        run = s3.meta.client.put_object(Body=x,Bucket=os.environ['AWS_S3_BUCKET'],Key=os.environ['AWS_S3_FILENAME_PREFIX'] + str(datetime.now().strftime("%Y-%m-%dT%H-%M-%S.json")))
        res = run.get('ResponseMetadata')

        if res.get('HTTPStatusCode') == 200:
            print('Data Uploaded Succesfully to the Lake AWS S3 at ' + str(datetime.now().strftime("%Y-%m-%dT%H:%M:%S")))
        else:
            print('Data Not Uploaded  to the Lake AWS S3 at ' + str(datetime.now().strftime("%Y-%m-%dT%H:%M:%S"))) 
            print("Exiting program until fixed")
            exit()
    except Exception as e:
        print(e)

#==================================# This is the Main section which runs this program - Produce to kafka #==================================

#a countdown variable to keep track of the amount of batches processed
track = 1

#a continous loop to keep polling API endpoint for datasets
try:
    while True:
        count = 0
        ola = requests.get(os.environ['CRYPTO_API_ENDPOINT']).json()
        #We loop throught the array
        for x in ola["result"]:
            #The dataset does NOT have a timestamp associated with the event, we need to add one so we can do time series analytics  
            x["timestamp"] = datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%S.%f")
            #Because our get call returns a type of dictionary(json), we need to convert it to a string as kafka only takes string, bytes NOT dict
            x = json.dumps(x)
            #Kafka producer keeps polling for new events
            p.poll(0)
            #Kafka producer now produces the events received to kafka topic and prints out and executes the report function
            p.produce(topic,key="hello",value=x, callback=delivery_report)
            #We increment our count variable to keep track of the amount of events processed
            count += 1
        #Then producer queue is flushed
        p.flush()
        #Once we run through the array, we print out message with total amount of events processed in a batch     
        print("Batch number " + str(track) + " - " + str(count) + " Messages delivered to " + topic + " at " + str(datetime.now().strftime("%Y-%m-%dT%H:%M:%S")))
        #Now, we're sending dataset to the lake (AWS S3)
        #Creating S3 Resource From the Session.
        print("Sending datasets to the Data lake AWS S3...")
        send_to_s3()
        #Here we pause for our next batch 
        for x in range(interval):
            print("Pausing for next batch in ..." + str(interval) + "sec - " + str(x))
            sleep(1)
        #The batch count is incremented here
        track += 1
        #Ensure kafka is reachable 
        check_kafka()
except ConnectionError:
    while True:
        print("Unable to establish connection to crypto API endpoint.Verify URL")
        for x in range(waiting):
            print("Retrying connection to crypto API in ... " + str(waiting) + " seconds ", str(x), end="\r")
            sleep(1)
except Exception as e:
    print(e) 
```

**æˆ‘å·²ç»å°½åŠ›åœ¨ä»£ç ä¸­åŒ…å«å°½å¯èƒ½å¤šçš„æ³¨é‡Šï¼Œè¿™æ ·ä½ å°±å¯ä»¥ç†è§£äº†ã€‚æˆ‘æƒ³åœ¨è¿™é‡Œæåˆ°çš„å‡ ä»¶äº‹æ˜¯â€¦å› ä¸ºä»£ç ä¾èµ–äºå…¶ä»–å±‚æ¥å®ç°ä¸€ä¸ªæœ‰æ•ˆä¸”å‡†ç¡®çš„è§£å†³æ–¹æ¡ˆï¼Œæˆ‘éœ€è¦è€ƒè™‘åˆ°è¿™ä¸€ç‚¹ï¼Œå¹¶åœ¨ä¾èµ–å±‚( **kafka** ã€ **aws s3** )ç”±äºæŸäº›åŸå› ä¸å¯ç”¨æ—¶åœæ­¢ä»£ç æ‰§è¡Œã€‚**

## **ETL å¤„ç†â€” Python Kafka æ¶ˆè´¹è€…**

```
#Consume from kafka topic and send to ES

import requests
import json
from decimal import Decimal
from confluent_kafka import Consumer, KafkaError
import os
import socket
from time import sleep

from datetime import datetime
from elasticsearch import Elasticsearch

#==============================# This section runs our python code as a TCP server #==============================
# Create a socket
sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

# Ensure that you can restart your server quickly when it terminates
sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)

# Set the client socket's TCP "well-known port" number
well_known_port = int(os.environ['PYTHON_CONSUMER_SERVER_PORT'])
sock.bind(('localhost', well_known_port))

# Set the number of clients waiting for connection that can be queued
sock.listen(5)

#==============================# This section sets up Elasticsearch world #==============================

#Function to check connectivity to ES cluster
def isOpenES(ip,port):
   s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
   try:
      print("Attempting connectio to ES cluster...")
      s.settimeout(30)
      s.connect((ip, int(port)))
      print("Connection successfully established with ES cluster. We may proceed")
      sleep(1)
      return True
   except:
      return False
   finally:
        s.close()

# Check connectivity to ES cluster before proceding any further
def check_elastic():
  global check_port_wait
  check_port_wait = 30
  while True:
    global check_port
    check_port =  isOpenES(os.environ['ES_HOST'], int(os.environ['ES_HOST_PORT']))
    if check_port == False:
        for x in range(check_port_wait):
            print("Retrying connection to ES cluster...in " + str(x) + " seconds")
            sleep(1)
    else:
      break

# Execute elasticsearch connection check
check_elastic()

#Only when successfull connection to ES cluster established then we can setup ES client 
if check_port == True:
  es = Elasticsearch(
      hosts= os.environ['ES_URL_ENDPOINT'],
      basic_auth= (os.environ['ES_USER'], os.environ['ES_PASSWORD']),
      request_timeout= 60
      )

# Set up index settings and fields mappings
es_settings = {
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0
  },
  "mappings": {
    "properties": {
      "next_funding_time": {
        "type": "keyword"
      },
      "delivery_time": {
        "type": "keyword"
      },
      "ask_price": {
        "type": "float"
      },
      "bid_price": {
        "type": "float"
      },
      "index_price": {
        "type": "float"
      },
      "last_price": {
        "type": "float"
      },
      "low_price_24h": {
        "type": "float"
      },
      "last_price": {
        "type": "float"
      },
      "mark_price": {
        "type": "float"
      },
      "open_interest": {
        "type": "float"
      },
      "prev_price_1h": {
        "type": "float"
      },
      "prev_price_24h": {
        "type": "float"
      },
      "price_1h_pcnt": {
        "type": "float"
      },
      "price_24h_pcnt": {
        "type": "float"
      },
      "total_volume": {
        "type": "float"
      },
      "turnover_24h": {
        "type": "float"
      },
      "volume_24h": {
        "type": "float"
      }
    }
  }
}

#Check if if index exists before creating it:
try:
  check_indices = es.indices.exists(index=os.environ['ES_INDEX'])
except:
  while True:
    if es.cluster.health()["status"] != ("green" or "yellow"):
      for x in range(15):
        print("ES cluster is not healthy yet, we retry in " + str(x) + " seconds")
    else:
      break

#If index doesnt exist, create it
if check_indices == False:
  es.indices.create(index=os.environ['ES_INDEX'], body=es_settings)

#==============================# This section sets up Kafka Consumer world #==============================

#Kafka consumer settings
settings = {
    'bootstrap.servers': os.environ['KAFKA_BROKERS_HOST_PORT'],
    'group.id': os.environ['KAFKA_CONSUMER_GROUP_ID'],
    'default.topic.config': {'auto.offset.reset': os.environ['KAFKA_OFFSET_RESET']},
    'auto.offset.reset': os.environ['KAFKA_OFFSET_RESET']
}

#Function to check connectivity to Kafka cluster
def isOpenKafka(ip,port):
   s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
   try:
      s.connect((ip, int(port)))
      s.settimeout(30)
      print("Connection successfully established with Kafka cluster. We may proceed, awaiting incoming events ...")
      sleep(1)
      return True
   except:
      return False
   finally:
        s.close()

# Check connectivity to kafka cluster before proceding any further
def check_kafka():
  while True:
    check_port =  isOpenKafka(os.environ['KAFKA_HOST'], int(os.environ['KAFKA_BROKERS_PORT']))
    if check_port == False:
        for x in range(check_port_wait):
            print("Retrying connection to kafka cluster...in " + str(x) + " seconds")
            sleep(1)
    else:
      break

# Execute kafka check inception
check_kafka()

#Only when successfull connection is established with kafka then we can setup Consumer 
if check_port == True:
  c = Consumer(settings)
  c.subscribe([os.environ['KAFKA_TOPIC']])

#==============================# This is the Main section which runs this program - Consume from kafka #==============================

#Set up the batch counter
count = 0

#Kick off the consumer loop to continously consume as per poll interval.
#We need to have it in a 'try' section so we can handle exceptions errors
try:
    while True:
      #Initiates the consumer with a poll interval of milliseconds - price_nowtime poll
      msg = c.poll(0.1)
      #If there are no messages, continue waiting for incoming events
      if msg is None:
          continue
      #If there are no msg.error then we've got messages and can print what we've received
      elif not msg.error():
          count += 1
          print('Received message: {0}'.format(msg.value()))
          #print('Received message: {}'.format(msg.value().decode('utf-8')))
          data = msg.value().decode()
          #If the message value is None, continue 
          if msg.value() is None:
              continue

          # Handle UTF
          try:
              data = msg.value().decode()
          except Exception:
              data = msg.value()
              print(data)
          #At this point we have the messages so we can send them to ES cluster
          try: 
            resp = es.index(index="elastic-index", id=datetime.now(), document=data)
            print(resp['result'])
          except:
            check_elastic()
      #Keeping track of the amount of message processed
      print(str(count) + " messages received so far...at " + str(datetime.now().strftime("%Y-%m-%dT%H:%M:%S")))

      if msg.error():
        print("Consumer error: {}".format(msg.error()))
        continue

except Exception as e:
  print(e)

finally:
    c.close() 
```

**å…³äºæ¶ˆè´¹è€…éœ€è¦è¡¥å……çš„æ˜¯ï¼Œè¿™ä¸ªç¨‹åºä½œä¸º TCP æœåŠ¡å™¨åœ¨æ‚¨é€‰æ‹©çš„ç‰¹å®š TCP ç«¯å£ä¸Šè¿è¡Œã€‚åŸå› æ˜¯æˆ‘ä»¬æƒ³é€šè¿‡å®ƒçš„ TCP ç«¯å£çŸ¥é“è¿™ä¸ªåº”ç”¨ç¨‹åºæ˜¯å¯åŠ¨è¿˜æ˜¯æ­»äº¡ã€‚æˆ‘ä»¬å°†ä½œä¸º docker å®¹å™¨è¿è¡Œï¼Œå¹¶é€šè¿‡ TCP ç«¯å£ç›‘æ§å…¶å¥åº·çŠ¶å†µã€‚é¡ºä¾¿è¯´ä¸€å¥ï¼ŒåŒæ ·çš„åŸåˆ™åº”è¯¥é€‚ç”¨äºç”Ÿäº§è€…ã€‚**

**ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸¤ä¸ªä»£ç è¿è¡Œåçš„æ ·å­ã€‚**

## **Bootstrapping â€¦ä»£ç æ­£åœ¨è¿è¡Œï¼Œä½†ä¼šåœæ­¢è¿›ä¸€æ­¥æ‰§è¡Œï¼Œç›´åˆ°å…¶ä¾èµ–å±‚( *Kafka* )å¯è®¿é—®**

**![](img/6ab7a6b3e46e6550a3f1fd8ea93d16fe.png)**

**ä¸€æ—¦ä¾èµ–æ˜¯å¯è¾¾çš„å’Œå¯æ“ä½œçš„ï¼Œå®ƒå°±å¯ä»¥äº§ç”Ÿå¡å¤«å¡ä¸»é¢˜**

**![](img/cee3173878aa479950282bd200d3cb97.png)****![](img/91ad63e2e7973be1c1db399227e989ed.png)**

**æˆ‘ç¡®ä¿æˆ‘ä»¬å¾—åˆ°äº†äº‹ä»¶æµçš„å®æ—¶æŠ¥å‘Šï¼Œä»¥ä¾¿è·Ÿè¸ªåœ¨è¿™ä¸ªæµä¸­å‘ç”Ÿäº†ä»€ä¹ˆã€‚æˆ‘ä»¬æ­£åœ¨åˆ†æ‰¹æ‘„å–ï¼Œä½œä¸ºæ¯æ¬¡æ‘„å–çš„ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†æ•°æ®é›†å‘é€åˆ°æ•°æ®æ¹–( **AWS S3** )è¿›è¡Œé•¿æœŸä¿å­˜ï¼Œå¹¶åœ¨éœ€è¦æ—¶ç”¨äºæ»¡è¶³ä»»ä½•å…¶ä»–æ¶ˆè´¹è€…çš„éœ€æ±‚ã€‚**

## **ä¸‹é¢æ˜¯åç»­æ‰¹å¤„ç†çš„æ ·å­:**

**![](img/837a5f28bdec5ba45fb1d217343d3a02.png)**

## **æ˜¯æ—¶å€™å¯åŠ¨æˆ‘ä»¬çš„ Kafka æ¶ˆè´¹è€…äº†â€¦â€¦æ­£å¦‚æ‚¨åœ¨è¿™é‡Œæ³¨æ„åˆ°çš„ï¼Œæ¶ˆè´¹è€…çš„ä»£ç æ‰§è¡Œæš‚åœäº†ï¼Œå› ä¸ºä»–çš„ä¾èµ–å±‚(***elastic search***)ä¸å¯è¾¾ã€‚è¿™å°†å¾ªç¯è¿›è¡Œï¼Œç›´åˆ°å¯ä»¥åˆ°è¾¾:**

**![](img/87cdc8376542ab1436f138bcb96151c8.png)**

**è®©æˆ‘ä»¬å¯åŠ¨ elastic searchâ€¦â€¦ä¸€æ—¦å¯åŠ¨ï¼Œä»£ç å°†ç»§ç»­æ‰§è¡Œã€‚**

**![](img/3ca09093523e0f47dfc108c73654c023.png)**

**è§£å†³æ–¹æ¡ˆä¸­åŒ…å«çš„ä¸€ä¸ªé‡è¦æ³¨æ„äº‹é¡¹æ˜¯ï¼Œæˆ‘ä»¬**è·Ÿè¸ª**å’Œ**æ—¶é—´æˆ³**æ­¤å·¥ä½œæµæ¥æ”¶ã€å¤„ç†çš„æ¯æ¡è®°å½•:**

**![](img/310c9f5628ffd04e24f1f89872db427e.png)**

**å¾ˆå¥½ï¼Œç°åœ¨æˆ‘ä»¬æ­£åœ¨å¤„ç†äº‹ä»¶å¹¶å°†å®ƒä»¬å‘å¸ƒåˆ°æˆ‘ä»¬çš„åˆ†æå¹³å°(elasticsearch)ã€‚æˆ‘ä»¬å»çœ‹çœ‹å®ƒæ˜¯ä»€ä¹ˆæ ·å­ã€‚**

**![](img/820e67f0e392353d1f613bd46d4e420c.png)**

**åœ¨æˆ‘ä»¬å¼€å§‹è¿›ä¸€æ­¥åˆ†æå®ƒä¹‹å‰ï¼Œå®ƒçœ‹èµ·æ¥å¹¶ä¸æ€ä¹ˆæ ·â€¦**

**![](img/29b9719350d7ee281650d17cedb1340d.png)**

**åœ¨è¿‡å»çš„ 7 å¤©é‡Œï¼Œæ¯ç§è´§å¸çš„ä»·æ ¼å’Œæ³¢åŠ¨ç™¾åˆ†æ¯”æ˜¯ä»€ä¹ˆæ ·çš„:**

**![](img/23a20ab4e933a0895f509e3ceb1ea9ae.png)**

**å“¼ğŸ¤”â€¦æˆ‘ä»¬å¼€å§‹äº†è§£è¿™ç§æƒ…å†µï¼Œä¸Šæ¶¨ 2.5%ååˆä¸‹è·Œçš„è´§å¸æ˜¯ä»€ä¹ˆï¼Ÿè°åœ¨è¿™é‡Œç»å¸¸å˜åŠ¨ï¼Ÿ**

**è®©æˆ‘ä»¬è·å¾— 10 ç§åŠ å¯†è´§å¸åœ¨åŒä¸€æ—¶æœŸ(7 å¤©)çš„ç™¾åˆ†æ¯”å˜åŒ–æƒé‡:**

```
GET elastic-index/_search
{
  "size": 0,
  "aggs": {
    "MICKA": {
      "terms": {
        "field": "symbol.keyword"
        , "size": 10
      },
      "aggs": {
        "NAME": {
          "percentiles": {
            "field": "price_24h_pcnt",
            "percents": [
              25,
              50,
              75,
              95,
              99
            ]
          }
        }
      }
    }
  }
}
```

```
{
  "took" : 59,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 10000,
      "relation" : "gte"
    },
    "max_score" : null,
    "hits" : [ ]
  },
  "aggregations" : {
    "MICKA" : {
      "doc_count_error_upper_bound" : 0,
      "sum_other_doc_count" : 72986,
      "buckets" : [
        {
          "key" : "SKLUSDT",
          "doc_count" : 397,
          "NAME" : {
            "values" : {
              "25.0" : -0.017249249387532473,
              "50.0" : 0.013384000398218632,
              "75.0" : 0.02213199995458126,
              "95.0" : 0.0325398001819849,
              "99.0" : 0.04053437914699315
            }
          }
        },
        {
          "key" : "SLPUSDT",
          "doc_count" : 397,
          "NAME" : {
            "values" : {
              "25.0" : -0.01976200006902218,
              "50.0" : 0.003984000068157911,
              "75.0" : 0.0206344376783818,
              "95.0" : 0.041492998600006104,
              "99.0" : 0.04583299905061722
            }
          }
        },
        {
          "key" : "SNXUSDT",
          "doc_count" : 397,
          "NAME" : {
            "values" : {
              "25.0" : -0.04339925153180957,
              "50.0" : -0.002605666678088407,
              "75.0" : 0.034786999225616455,
              "95.0" : 0.05091765057295561,
              "99.0" : 0.056417589783668516
            }
          }
        },
        {
          "key" : "SOLUSDT",
          "doc_count" : 397,
          "NAME" : {
            "values" : {
              "25.0" : -0.018149999901652336,
              "50.0" : 0.020818000038464863,
              "75.0" : 0.08763474971055984,
              "95.0" : 0.11487374790012836,
              "99.0" : 0.12104500085115433
            }
          }
        },
        {
          "key" : "STGUSDT",
          "doc_count" : 397,
          "NAME" : {
            "values" : {
              "25.0" : -0.024567687651142478,
              "50.0" : -0.01639300025999546,
              "75.0" : 0.00449066663471361,
              "95.0" : 0.025875399820506545,
              "99.0" : 0.06390500068664551
            }
          }
        },
        {
          "key" : "STMXUSDT",
          "doc_count" : 397,
          "NAME" : {
            "values" : {
              "25.0" : -0.03550550062209368,
              "50.0" : 9.619999909773469E-4,
              "75.0" : 0.03224662481807172,
              "95.0" : 0.05798500031232834,
              "99.0" : 0.06297794189304108
            }
          }
        },
        {
          "key" : "STORJUSDT",
          "doc_count" : 397,
          "NAME" : {
            "values" : {
              "25.0" : -0.03473616577684879,
              "50.0" : -0.007122000213712454,
              "75.0" : 0.014991999603807926,
              "95.0" : 0.033587001264095306,
              "99.0" : 0.03975500166416168
            }
          }
        },
        {
          "key" : "STXUSDT",
          "doc_count" : 397,
          "NAME" : {
            "values" : {
              "25.0" : 0.004346999805420637,
              "50.0" : 0.013100000098347664,
              "75.0" : 0.025890000785390537,
              "95.0" : 0.03912999853491783,
              "99.0" : 0.07264900207519531
            }
          }
        },
        {
          "key" : "SUNUSDT",
          "doc_count" : 397,
          "NAME" : {
            "values" : {
              "25.0" : 0.015608999878168106,
              "50.0" : 0.028155000880360603,
              "75.0" : 0.042449667739371456,
              "95.0" : 0.06716989986598491,
              "99.0" : 0.07128699868917465
            }
          }
        },
        {
          "key" : "SUSHIUSDT",
          "doc_count" : 397,
          "NAME" : {
            "values" : {
              "25.0" : -0.049969250336289406,
              "50.0" : -0.020973000675439835,
              "75.0" : 0.010279750218614936,
              "95.0" : 0.09280899912118912,
              "99.0" : 0.10080005057156086
            }
          }
        }
      ]
    }
  }
}
```

**ä½ æœ€å–œæ¬¢çš„åŠ å¯†è´§å¸ğŸ™‚**

```
GET elastic-index/_search
{
  "size": 0,
  "query": {
    "match": {
      "symbol": "BTCUSD"
    }
  }, 
  "aggs": {
    "MICKA": {
      "terms": {
        "field": "symbol.keyword"
      },
      "aggs": {
        "NAME": {
          "percentiles": {
            "field": "price_24h_pcnt",
            "percents": [
              1,
              5,
              25,
              50,
              75,
              95,
              99
            ]
          }
        }
      }
    }
  }
}
```

```
{
  "took" : 740,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 398,
      "relation" : "eq"
    },
    "max_score" : null,
    "hits" : [ ]
  },
  "aggregations" : {
    "MICKA" : {
      "doc_count_error_upper_bound" : 0,
      "sum_other_doc_count" : 0,
      "buckets" : [
        {
          "key" : "BTCUSD",
          "doc_count" : 398,
          "NAME" : {
            "values" : {
              "1.0" : -0.018621680364012717,
              "5.0" : -0.017968399822711947,
              "25.0" : -0.013923999853432178,
              "50.0" : 0.0018339999951422215,
              "75.0" : 0.010238000191748142,
              "95.0" : 0.022255999967455864,
              "99.0" : 0.02323047965764999
            }
          }
        }
      ]
    }
  }
}
```

**è¿™æ˜¯ä¸€ä¸ª **7 å¤©**çš„æ•°æ®é›†åˆ†æï¼è¿‡å»ä¸€ä¸¤å¹´çš„å›¾åƒæ•°æ®é›†â€¦å¯èƒ½ä¼šè®©ä½ å¯¹å¸‚åœºæœ‰ä¸€ä¸ªæ›´å¥½çš„äº†è§£ï¼Œä¸æ˜¯å—ï¼Ÿ**

**é…·ï¼Œå°±æ˜¯è¿™æ ·ï¼Œä¼™è®¡ä»¬ï¼åœ¨æˆ‘çš„ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†åšä¸€äº› **devops** å¹¶å°†å…¶å­˜æ”¾åœ¨ docker å®¹å™¨ä¸­ï¼Œä»¥ä¾¿ä¸ ansible ä¸€èµ·éƒ¨ç½²åœ¨ [**AWS EC2**](https://aws.amazon.com/ec2/) å·¥ä½œè´Ÿè½½ä¸Šâ€¦â€¦æ•¬è¯·å…³æ³¨ğŸ˜‰**

## **å“¦ï¼ä»¥ä¸‹æ˜¯æˆ‘ä»¬çš„æ•°æ®æ¹– AWS S3 ä¸­çš„æ•°æ®é›†:**

**![](img/b0d12ffa826b13bc71d6381e561d959e.png)**