# Kafka 和 ActiveMQ 之间的一些细节差异

> 原文：<https://blog.devgenius.io/some-detailed-differences-between-kafka-and-activemq-d40163cb2ac4?source=collection_archive---------0----------------------->

![](img/ee2b28be2d81726b24e78bfcb42cb40a.png)

激发我写这篇文章的原因很简单——好奇心和主要为自己澄清差异的愿望。

基于松散耦合服务的异步交互，转向分布式架构的想法已经强烈地吸引了大众。自古以来，MQ 中间件(例如 ActiveMQ)就为开发人员所熟知。对 Kafka 提供的通用功能的简要描述允许将其视为传统消息传递系统的现代高级版本。

因此，许多团队受 Kafka 在知名项目中的应用的大肆宣传和令人印象深刻的例子的影响，不加思索地做出不太明智的决定，将其用作公共项目基础设施，这并不奇怪。此后，他们面临着许多由两种制度之间的哲学差异引起的困难。

我想马上承认，除了我自己基于 10 多年 JMS 中间件(主要是 Apache ActiveMQ 和 IBM WebSphere MQ)经验得出的高度主观的观点和结论之外，这篇文章几乎没有什么新内容，主要是在与银行相关的项目中，但不仅仅是在项目中。

对于那些想要深入理解这个主题的人，我坚持推荐阅读下面两本书:

*   Neha Narkhede，Gwen Shapira，Todd Palino《卡夫卡:权威指南》(非常基础，但不幸的是已经略显过时)
*   Jakub Korab“了解消息代理”(很好，但需要有一些以前的经验)

如果你不想浪费你的时间，我可以从这些书中提供一个简短的摘录和我自己的卑微经历。

# 消息的存储

也许这两个系统之间最根本的区别是下一个:Kafka 不是传输，而是具有有限寻址能力的数据的流存储(流或时间戳中的抽象偏移量，可以粗略地转换为偏移量)。

*   对于许多应用的多次消费尝试(在连接新应用、纠错、添加新处理功能的情况下)，数据不仅被传输，而且被永久存储。
*   通过定义 KSQL，可以使用这种分布式存储来执行查询。与传统的 SQL 查询不同，它们不是每次都“按需”执行，而是作为状态过滤器工作，根据通过它们的数据流更新它们的内部状态。它们可以被认为是来自经典关系数据库的物化视图的一些类似物。

同时，MQ 最初并没有被设计成长期数据存储:它的口号是“只要消费—删除”:

*   不断拥挤(当然，还有不断增长)的队列通常是一些问题的征兆
*   耐用性和持久性会显著降低处理速度，并且在实现高吞吐量和低延迟时通常不得不做出牺牲

## 逻辑结构

**ActiveMQ**

JMS API 实现了队列(点对点)和主题(发布-订阅)目的地之间的基本语义差异。ActiveMQ 为每种类型提供可选的持久性和持久性。

所有消息都以一个流的形式传入，并在潜在的无限数量的消费者之间循环分发，每个消费者都可能接收任何消息。

最后，一旦消息被传递给一个/所有订户，它们就被从服务器存储中删除。可以按时间配置消息“过时”,在这种情况下，它们根本不会被发送。消费者根本无法再次阅读已经确认的消息。

存储功能或多或少是一个自动过程，不需要高级管理和监控

**卡夫卡**

它只支持具有强制持久性和持久性的主题，不可能实现具有 P2P 语义的经典队列。您可以玩存储容量和可靠性，但数据将始终被保存并在一段时间内可用。

消息可重复地分布在一组配置好的分区上，均匀地分布或在生产者的控制下分布。单个组(某些功能的实现)中的每个消费者可以同时读取主题的几个分区，并严格按照生产者发送的顺序从每个分区接收消息。在任何时候，每个分区只能由组中的一个使用者处理。因此，组中活动消费者的最大数量受限于分区的数量。

Kafka 消息不是在它们已经被传递(被消费者接收和确认)时被删除，而是根据存储策略(最大配置存储时间/容量)被删除，即使它们根本没有被传递到任何地方。任何消费者在任何时候都可以从存储的开始或使用某种偏移/超时来再次读取先前已经接收的消息。原则上不可能“删除”该消息，使其对消费者不再可用。

Kafka 的一个独特特性是“日志压缩”主题，它只存储每个键的最后一个值(消息的标准可选属性)。

## 物理存储

**ActiveMQ**

将消息物理存储在磁盘上根本不是强制性的——我们完全可以只使用内存。对于某些数据量，它允许(如果内存足够并且用户接收消息很快)达到低延迟和高吞吐量。

所有主题的消息都存储在一个日志中，位于一个逻辑设备上。为了将数据写入该日志，线程被排成一行。在任何给定时间，只有一个代理使用存储。

代理可以选择通过强制刷新每个消息(一个事务中的一组消息)的所有缓存来提供可靠的数据存储策略，这将显著降低速度。

逻辑存储的带宽受限于一个磁盘的带宽(您可以为几个磁盘使用 raid 或特殊的存储插件，但有一些限制，特别是关于事务)。因此，这个选项不太适合所有文件系统都通过网络连接的云，因此速度很慢。

永久数据存储有多种插件策略(每个多磁盘、JDBC 等)

**卡夫卡**

卡夫卡的信息总是被永久保存，没有其他选择。当然，我们可以使存储的最大时间/大小最小，但不能使其为零。因此，我们还有一个额外的功能——免费的备份数据存储。在架构和配置正确的情况下，所有其他数据存储都可以从初始消息流中恢复。

来自每个主题的消息分布在几个分区中，每个分区都是集群的，用于读/写/存储负载平衡。因此，只有对一个主题的一个分区的一个副本的写(Kafka 的最新版本允许用于读取分区的最近副本)操作被排队。其余的在一个/不同的集群节点上并行执行

Kafka broker 依赖于将消息灵活地复制到集群中的其他节点，而不是强制刷新每个消息的所有缓存。这大大增加了带宽(有时增加了一千倍),因为消息是以内存更新速率提交的，并且代理消耗更多的文件系统缓存而不是应用程序内存。堆的使用很少，代理不会遇到典型的 JVM 应用程序问题，如垃圾收集期间的 STW 暂停。此外，这种架构非常适合云部署。

通过将每个主题分成分布在整个集群中的多个部分，消息的流量或多或少地均匀分布在整个集群中

存储数据的唯一策略是——在分段日志中。数据总是大量写入包含活动段的文件的末尾，并通过系统文件缓存按页检索。不存在对文件碎片的随机访问，这显著提高了文件 I/O 的性能。

它需要全面的配置和持续的监控，以在集群节点之间保持数据和负载的平均分布。

# 一般建筑

## 根本区别是“智能代理与智能客户端”

**MQ 模型专注于高度功能化的代理**

它负责接收和分发消息的集中逻辑，但是客户端应该只关心发送、接收和确认单个消息。因此，随着连接的客户端数量的增加，速度会明显变慢

它通常支持许多协议和更复杂的路由/应用集成模板。例如临时队列、同步请求-响应、由另一个消费者自动重新处理单个损坏的消息+死信队列、延迟消息等等。

除了有效负载之外，消息还可以包含任意报头。其中一些可以携带 JMS(或特定实现)系统元数据的标准，告诉代理如何处理消息。例如，过时时间，在哪里发送响应，是否有必要存储它可靠等等。

可以根据标题内容的条件进行订阅。由于这一点以及主题/队列的分层名称系统，可以实现许多企业集成模式，用于 SOA 架构中应用程序/服务之间的复杂数据路由

**卡夫卡**

它面向“智能”客户端，这些客户端(有时一起)负责传统代理的许多功能:定义用于传递消息的分区、在消费者之间分发和重新分发分区、处理异常、跟踪已使用的消息、根据条件过滤已使用的消息等等。

代替“智能”代理的是一个简单、快速、可伸缩的代理，当连接的客户端数量和数据处理量增加时，它的速度只会略微变慢。

Kafka 更多地出现在微服务架构中，并作为分布式处理数据流的平台。

描述消息的标准元数据非常有限。根本没有任何标准的头来控制代理/阅读器的行为。例如，它不能只使用具有某些特定头值的消息。只有在消费完消息之后，才可能处理或丢弃它们。

## 交互协议

**ActiveMQ**

除了本地 OpenWire，它还支持 AMQP，STOMP，MQTT，XMPP。

一些协议可以同时基于 TCP 和 WebSocket，它允许直接从例如基于 Java 脚本的 UI 调用。

消息大小可能是无限的，或者拆分/收集是自动执行的。

Broker 实现了一种“推送”方法，即将消息从存储设备推送到客户端。也可以配置拉动方法。

**卡夫卡**

原则上，它不支持任何东西，但为了保持对实现的完全控制，它拥有一个二进制协议。由于它是一个“永不停机”的系统，因此随着软件的发展和集群节点的逐步更新，保持协议兼容性至关重要。

它还允许将接收到的消息“按原样”写在磁盘上而不进行转换，并根据消费者的请求将它们直接从磁盘高速缓存页面传递到网络套接字(零拷贝),从而节省了大量资源。

因此，由于长期存储，最新版本的软件可以通过副本接收最早格式的消息。

默认邮件大小为 1MB，不建议增加。“索赔检查”集成模板可以提供帮助。

代理实现了一种“拉”方法——客户端从代理拉消息，根本不需要背压。

## 发送消息

**ActiveMQ**

它通常(有选项，但仅作为配置实现，不在 API 中提供)通过来自供应商的同步阻塞调用来执行，直到将数据保存到内存或代理的磁盘存储中。

物理存储后，客户可以立即看到更改

**卡夫卡**

除了前一种情况，这是一个更加复杂和灵活的过程:

*   成功地向代理发送消息并不意味着它已经被物理地存储在某个地方。供应商可以在网络呼叫后立即释放。通常，broker 只将收到的消息保存在文件系统缓存中，而不会刷新它。
*   不仅分区的主代理参与保存过程，而且其随后副本的代理也参与保存过程。
*   在更改被分发到集群的节点之前，消费者不会看到这些更改。这需要一些时间，具体取决于集群节点的加载情况。

默认情况下，发送过程是异步的，这允许自动修复可重试的问题——代理的不可访问性、分区的领导者暂时不存在等。但同时，我们牺牲了发送消息的固定顺序。也可以使这个过程同步，应用配置，但是这会导致低吞吐量。

因此，正如我们所看到的，在“吞吐量”和“数据存储的可靠性”之间存在着广泛的权衡，从“发送并忘记”到等到集群的所有代理都确认收到消息。

## 消息消费

**ActiveMQ**

消息向连接消费者的集中分发是由代理执行的，代理负载很重

在正常情况下，每个消息只能被消费者接收一次。消费者成功确认指定的消息后，基本上不可能再收到第二次消息。但是，例如，如果确认丢失，消息将被再次发送。所以，真正的语义是“作为最小一次”。

消费者可以在任何时候连接/断开代理，而不会暂停消息处理。在这种情况下，我们面对的是一组非常有弹性的用户，这有利于循环负载平衡。

代理控制消息流，理论上可以让消费者超载

使用 JMS 选择器，消费者可以在代理端过滤接收的消息。这在 MQ 架构特性中非常重要，下面是一些有用的用例:

*   通过一对队列在两个组件之间执行同步请求-响应。
*   从 QoS 已经接近临界的旧消息队列中提前获取—某种双端队列，它允许您将消息从过载或失效的处理程序重定向到健康或刚刚开始的处理程序，以修复交通堵塞。

这种情况的最大缺点是，如果几个应用程序连接到同一个队列(这很正常，例如，对于事件总线模式)，每个应用程序都将接收(甚至可能反序列化)所有消息。相应的，我们有多余的网络带宽和 CPU。

**卡夫卡**

消费者之间就谁将从哪些分区接收消息达成一致。此外，它们通过定期记录它们的当前流位置并与“同事”共享该信息来确认消息，以便能够在重新分区的情况下应用该信息。确认信息可以保存在 Kafka 中(在 ZooKeeper 中为老客户保存，在一个特殊的服务主题中为新客户保存)或它们的存储器中(例如，与数据库中的消息处理结果一起保存，这提供了某种类型的事务性)。

通过传递先前保存的偏移量或时间戳，它们可以接收最后一条消息以及已经被再次读取的消息。

消费者不像 JMS 那样确认收到的单个消息，而是保存他们最后收到的消息的偏移量。因此，消息 X 的固定(在 Kafka 或另一个存储器中写入偏移量 X)自动意味着消息 X-1、X-2 等也被安全地接收。如果出于加速的目的将接收到的消息包传递到工作线程池，可能会出现问题。

消费者的连接/断开启动了重新分区的过程——属于所有受影响主题的所有分区在所有相关消费者之间的重新分配，这导致数据消耗的暂停，并可能导致消息的重复。因此，它们的数量应该尽可能稳定。

接收信息的强度完全由消费者控制。此外，他必须做得足够快，不违反配置的超时，之后，他的分区将被传递给其他人。

消费者从分配的分区接收所有消息。不可能在代理端过滤它们。例如，它使得已经提到的“同步请求-响应”模式难以实现。

## 消息的分发

**卡夫卡**

生产者在主题的分区中发送消息时，强制将消息分发给潜在的消费者。

在每个分区中，消息以有序的方式存储，并以相同的顺序到达消费者。这极大地简化了许多业务操作的实现，但非常适合数量稳定的用户，因为更改用户数量会导致用户之间的分区重新分配暂停，在此期间没有人会读取任何内容。

相同/不同主题的不同分区中的消息的相对顺序没有定义，即使它们被连续发送到代理。

此外，固定数量的分区(以及相应的消费者)及其扩展的复杂性限制了消息处理的并行度。例如，Kafka 不太适合处理程序之间的弹性负载平衡。

**ActiveMQ**

在消费时，它在所有注册的消费者之间循环分发消息。

分区是一个很少使用的特性。如果有必要，可以通过向消息中添加特殊的元数据来实现，但它不是 JMS 标准的一部分。

# 一些常见的功能需求

# 交易支持

**卡夫卡**

原则上，它不参与与其他系统的共享事务

从版本 0.11 开始，可以在几个主题中事务性地发送几个消息，同时确认已经收到的消息。

原则上，如果一个消息的处理出现问题，它不会自动回滚。根据我们在处理“之前”或“之后”确认收到的消息的时间，消费者的崩溃会导致整个消息集的丢失或重复接收。

没有内置的“死信队列”模拟

**ActiveMQ**

它不仅允许组织几个代理调用之间的事务，还允许作为资源全面参与本地/ XA 事务。

处理一个消息时的异常自动导致仅重复接收该消息

您可以限制这种尝试的次数，并为未送达的邮件组织一个“死信队列”

# 消息排序

**ActiveMQ**

由于同步流程的分布式特性，无法保证所传递消息的有序性，同步流程处理一组并行线程，将消息传递给消费者。这意味着处理每几个相邻消息的相对顺序没有被定义，但是这个不确定性窗口本身相当小。

但是在大多数情况下，消息的严格有序性根本不重要:

*   一些事件(例如，系统指标或日志流)对单个消息的接收顺序不敏感，因为每个事件都有时间戳，并且每个事件的处理都是独立的。
*   如果实体处理是基于状态机的，并且每个操作的执行只能由前一个状态发送的事件来启动，那么在流中只有一个消息同时寻址到实体。在这种情况下，发往不同实体的消息的相对顺序并不重要。
*   在某些情况下，两个不应交换的消息之间的时间“距离”相对较长，以至于冲突的可能性可以忽略不计。

根据我对每天生成数亿条消息的系统的经验，我可以说，消息的顺序几乎只有在我们传输分成独立片段的大部分数据时才是敏感的。但这不是一个好的做法，EIP 模式的“索赔检查”更可取。

**卡夫卡**

它保证了同一分区中的所有消息都将按照添加的顺序进行传递。这样做的代价是只能由一个线程使用来自一个分区的消息。如果为了加速数据处理，我们将接收到的消息传递给线程池进行处理，那么执行序列又会丢失。我们在准确确认收到的信息方面也有问题。

不保证来自不同分区的消息。由于存储的分布式特性，不同分区之间的数据吞吐量可能会有很大差异。因此，如果没有选择或错误地选择用于分区的业务键，则所有使用者处理消息的实际顺序可能会与最初生成消息的顺序有很大不同。

这意味着同一个应用程序中的各个数据消费者可以生活在完全不同的逻辑时代。这种时间差异取决于数据在整个集群中的不平衡分布，这种分布通常会随着集群的运行而增加，需要手动干预才能消除。

因此，在使用 ActiveMQ 时，不加考虑地希望减少关闭消息的微弱随机性，可能会导致整个数据片段的接收非常混乱。

# 恰好一次交货

**卡夫卡**

只有基于 Kafka Streams 框架，才可能实现恰好一次交付。它不是 API 方法调用的标准事务特性，而是架构的整体效果以及平台和单个任务的协作事件处理。因此，应用程序代码似乎与框架的体系结构和功能紧密相关。

*   严格地说，我们没有一次准确的消息传递，而是能够对每条消息只执行一次“读取-处理-写入”操作，前提是处理的结果也通过 Kafka 存储(不支持共享事务)。
*   这是另一个问题的来源:处理状态的可靠存储是通过将其所有更改复制到专用主题来执行的。如果存储空间很大，在处理器跌落的情况下，其恢复可能需要相当长的时间。

更高级但实施起来复杂的手动方法是 bloom filter +本地存储(或 RockDB 作为两者的组合)+分区(以减小本地存储的大小)+存储文件的定期备份。在重新启动的情况下，我们可以使用目标主题作为关于已经处理的消息的信息源，并在继续工作之前更新从备份过时存储恢复的内容。

**ActiveMQ**

还没有准备好使用 API 或恰好一次消息传递的模式。

它可以基于由供应商生成的唯一消息密钥以及以下方法之一来轻松实现:

*   将“流程”步骤实现为幂等运算。例如通过将密钥与消息处理的结果一起保存，同时检查数据的唯一性。容易，但不总是可能的。它还需要单独保存每条消息的结果，这增加了存储的负载。
*   将带有 TTL 的密钥保存在单独的 fast(通常是 NoSQL，例如 Redis)存储器中，并在处理每个消息之前进行检查。缺点是显而易见的:事务不可用，额外的远程调用，持续清理基于 TTL 的密钥需要额外的资源，集群存储是安装、管理、监控等基础设施的又一个元素。

第三个可能的机会是将队列实现移到数据库内部，这将进一步讨论。

# 分割

正如已经提到的，ActiveMQ 不支持分区，但是对于 Kafka 来说，它是唯一的和默认的工作方式。对于某些业务场景，尤其是与无限数据序列的处理相关的业务场景，这是一个实际上必要的特性。

在 ActiveMQ 的情况下，可以通过在提供者端生成分区键(最简单的方式是“散列( <business key="">) mod <count of="" consumers="">”)和两种常见的方法来实现:</count></business>

*   通过 JMS 选择器，消费者订阅从自己的分区接收唯一的消息。它要求每个消费者都有一个唯一的索引。
*   通过 JMS JMXGroupID 头代理组织与每个消费者的 HTTP stiky 会话的一些模拟，基于这个头的值，它用分区键或业务键初始化

同时，许多场景根本不需要完全分区。由于各个消息之间的独立性，任何商业实体在任何时刻都以独占模式更新就足够了。

它可以基于在处理之前应用于实体的“选择更新”查询来实现。这是一种非常简单、直接和透明的方法，但是它也有一些缺点:

*   你必须考虑到死锁的风险。例如，为了在银行帐户之间传输数据，总是需要以相同的顺序(例如，按字母顺序)获取它们的锁。
*   通过锁定，进行远程调用是非常不可取的，因为这会增加平均事务时间，而正常情况下这应该是最小的。这种情况可以通过为业务实体引入中间状态(READY _ FOR _ UPDATE = > LOCKED _ FOR _ UPDATE)来改善，但是这会使解决方案的实现变得非常复杂。例如，它需要引入超时和救援管理器，救援管理器会拾取由于处理程序失败而长期处于中间状态的实体。

因此，乍看起来相当简单的业务实体独占更新任务，随着业务逻辑的不断发展，很容易变得复杂得多。

# 系统的一致备份

**卡夫卡**

具有多个数据存储的分布式系统的一致备份的任务本身是复杂的。在 Kafka 的案例中，情况变得更加复杂，因为我们获得了额外的分布式存储，而单个代理无法得到一致的备份。

幸运的是，Kafka 集群本身相当稳定，本身就是一个备份。通常还会组织一个特殊的灾难恢复集群，并使用 Mirror Maker 将所有更改复制到集群中。切换到备份集群并不那么简单，如今，切换到备份集群而不丢失数据和/或复制事件，以及不出现数据不一致的工件，基本上是不可能的。当一个集群同时安装在几个数据中心时，一个很好的替代方案是“扩展集群”,并且切换的应用程序不会出现不一致。但是这个选择需要两个地区之间有一个好的渠道。

不幸的是，这并没有解决系统能够回滚到先前状态的问题。一旦发送到 Kafka，数据将永远无法更新或删除。

**ActiveMQ**

对于 ActiveMQ，情况更简单—代理的存储可以一致地存储为备份，而不会丢失数据。因此，我们唯一要做的是在其他存储之后备份它，并实现一次性处理“孤儿”消息，这些消息是由数据产生的，在其他数据存储中还不存在。是的，它可能是一个复杂的业务任务，不能自动执行，但它更简单。

**可能的替代方案**

能够对所有应用程序数据执行一致备份的唯一简单通用的方法是，基于传统的关系数据库，只有一个存储。在这种情况下，可以基于专用表来实现队列，其中消息和接收过程被实现为“select for update skip locked”查询。

只要每个操作的持续时间不超过 10 毫秒(长事务会降低数据库性能)，它就非常简单、灵活和可靠，每秒大约可以处理 1000 次操作。

如果同时在队列中的消息量不大，数据库服务器实际上把它们都保存在内存中，几乎像一个真正的代理一样工作。因此，建议通过队列传输所需的最小数据量。

主要优势(除了 subj):

*   我们可以实现非常灵活的消息优先级策略，不仅基于时间戳，还基于一些与业务相关的属性
*   数据处理一方面是多线程的，另一方面是严格的事务性的
*   出现异常时自动重试
*   我们还获得了强大的持久性、任务交付保证和至少一次交付语义

这种方法的缺点很明显:

*   每条信息都必须快速处理。这意味着无法在事务或复杂的处理逻辑中进行远程调用，因为存在中间状态和锁超时
*   即使消息队列为空，也不断重新请求数据的消费者。
*   就吞吐量和参与者数量而言，它的可扩展性不是很好。通过特定于供应商的选项对表进行调优和分区，这种情况可以得到改善。例如，可以将单个表分成几个表，分别位于不同的物理服务器上，并通过类似 Oracle 的“dblink”或 PostgreSQL 的“外部数据包装器”进行逻辑连接

这种方法有一个现成的实现——“d b-queue”，它是 Yandex 支付系统的核心。

# 可扩展性和容错性

**卡夫卡**

它最初是为具有冗余数据复制和在节点间分配用户请求的集群而设计的。这允许几乎无限制地增加容量和吞吐量。

访问主题数据的并行度受分区数量(每个分区最多一个组读取器)的限制，最好在设计时预先确定。增加分区的数量很复杂，会导致整个主题的阅读暂停，并且只影响新数据。只有通过创建新主题并将所有现有数据复制到其中，才能在扩展的分区数量中公平地重新分配数据。

可扩展性和容错性可以基于普通硬件水平实现，这些硬件可以根据需要在不停机的情况下重建/删除/添加到集群中(永不停机系统)。

一个话题的问题或多或少是孤立的，只导致一些经纪人的问题。由于这个原因，系统更加健壮，但是 Kafka 集群的问题的存在和不存在之间的区别更加模糊。

如果集群的单个代理定期出现故障，集群上负载的不平衡分布就会增加，只能通过手动操作来修复。

**ActiveMQ**

它只允许基于一个共享存储的主从配置。因此，可伸缩性是有限的，并且几乎只能是垂直的。随着流量的增长，it 需要部署具有独立存储的独立服务器，并在它们之间手动分配应用程序的目的地。

并行度受到代理资源的限制。理论上，它支持每个队列/主题无限数量的读取器，而不需要为了增加并行度而重新分配数据。

“动态”扩展是困难的，并且从根本上受到限制。维修时需要暂停。增加容量只能纵向实现，需要越来越强大的硬件。

一个队列/主题的问题通常是由磁盘空间不足引起的，并立即成为整个代理的问题。它使基础设施变得更脆弱但更简单，并鼓励管理员更加关注。

# 生产费用

**卡夫卡**

低级 API 是独一无二的，也足够复杂。消费者/生产者/顶层/代理的行为可以由每种类型的大约 30 个配置参数来配置，这些参数中的许多从根本上影响其行为，而不仅仅是非功能特性，例如吞吐量/延迟平衡。

实施可靠的解决方案需要很好地理解系统架构。所以几乎需要基于 Streams / Connectors / KSQL 框架，增加了本来就不低的入门成本。应用卡夫卡需要开发者和建筑师更多的经验。

对于生产用途来说，安装和管理是相当困难的。通常，成功的维护需要先进的 DevOps 文化。

缺少现成的管理和监控工具，而这些工具是操作系统和满足 QOS 要求所必需的。

常见企业应用程序的关键障碍—无法参与分布式事务。它要求开发人员转移到基础架构，并与业务分析师进行痛苦的讨论。

**ActiveMQ**

它很容易安装。它甚至可以在嵌入式模式下执行，这对于组件测试来说非常好。

大多数情况下，它不需要任何复杂的管理和持续的监控。

JMS API 是经典的、紧凑的、足够简单的、众所周知的和可预测的。

分布式事务没有任何问题，因此企业开发人员可以安全地遵循对他们来说舒适的 ACID 范例。

# 摘要

总的来说，卡夫卡可以被粗略地描述为古代 MQ 世界中的“NoSQL 式”解决方案:

*   这是一个现代的，美妙的，轻量级的，相当灵活的工具，但它也提供了一个很好的机会，通过拍摄自己的腿来获得 reach 体验。
*   然而，有了正确的体系结构和对其工作原理的深刻理解，就有可能通过资源的最佳加载获得令人印象深刻的(就 QoS 而言)结果。

不可避免的代价是应用程序的复杂性，但是框架的**连接器**和 **Kafka Streams** 部分可以显著简化实现(当然，由于下一层抽象，这可以使系统行为比以前更加神秘)。

**卡夫卡**

当我们需要以非常高(10-100 倍的差异)的吞吐量和相对简单的交付逻辑组织非常大量的数据的可靠存储时，它是非常好的。

ready 系统的入门、开发和维护的价格相对较高。

非常适合大量服务项目，特别是在公共互联网和物联网领域，采用基于云的部署

**ActiveMQ**

对于实现复杂的消息路由流来说，它是一个非常好的智能传输工具

它允许在动态数量的并发消费者之间有效地平衡请求

潜在的吞吐量不是很高，并且会随着存储数据量和连接的客户端数量的增加而降低。

Well 可以与传统的企业环境集成，特别是在财务和高级文档工作流领域，并具有内部部署

总的来说，如果你不是脸书或谷歌的开发人员，并且你对大数据存储、无止境的数据流处理或诸如真正的分区、严格的消息顺序或一次性交付等高级功能没有严格的需求，那么 Kafka 可能对你来说太难了。