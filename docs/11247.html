<html>
<head>
<title>Machine Learning Algorithm Series: Fisher Kernel Algorithm with Python, Julia, and R code examples</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习算法系列:带有 Python、Julia 和 R 代码示例的 Fisher 核算法</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/machine-learning-algorithm-series-fisher-kernel-algorithm-with-python-julia-and-r-code-examples-9445d32512dd?source=collection_archive---------12-----------------------#2022-12-27">https://blog.devgenius.io/machine-learning-algorithm-series-fisher-kernel-algorithm-with-python-julia-and-r-code-examples-9445d32512dd?source=collection_archive---------12-----------------------#2022-12-27</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><figure class="gl gn jl jm jn jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi jk"><img src="../Images/d7b2df05489434fa776f63dc9fb87df1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kqL2Itlq8TJxUQJy"/></div></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">迈克尔·泽兹奇在<a class="ae jz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="f573" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">Fisher 核算法是一种比较两组数据统计特性的方法，通常用于机器学习和模式识别领域。它由安德鲁·费希尔在 1936 年首次提出，此后成为机器学习领域的一个重要工具。它基于 Fisher 信息矩阵的思想，该矩阵测量一组随机变量包含的关于统计模型参数的信息量。Fisher 核算法用于通过构造核函数来比较两组数据的统计特性，核函数是两组数据之间相似性的度量。</p></div><div class="ab cl ky kz hr la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ig ih ii ij ik"><h1 id="1b8d" class="lf lg in bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">内核函数是什么？</h1><p id="f355" class="pw-post-body-paragraph ka kb in kc b kd md kf kg kh me kj kk kl mf kn ko kp mg kr ks kt mh kv kw kx ig bi translated">为了理解 Fisher 核算法，首先需要理解核函数的概念。核函数是采用两个输入向量并返回标量值的函数，标量值是两个向量之间相似性的度量。核函数通常用于机器学习算法中，以测量数据点之间的相似性，因为它们允许比较在其原始形式下可能不可直接比较的数据点。</p></div><div class="ab cl ky kz hr la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ig ih ii ij ik"><h1 id="0d9c" class="lf lg in bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">什么是费雪信息矩阵？</h1><p id="2640" class="pw-post-body-paragraph ka kb in kc b kd md kf kg kh me kj kk kl mf kn ko kp mg kr ks kt mh kv kw kx ig bi translated">Fisher 核算法使用基于 Fisher 信息矩阵的核函数。费希尔信息矩阵是一种矩阵，它度量一组随机变量包含的关于统计模型参数的信息量。它被定义为对数似然函数的 Hessian 矩阵的期望值的负值，它衡量对数似然函数在给定点的曲率。</p></div><div class="ab cl ky kz hr la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ig ih ii ij ik"><p id="98b3" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">Fisher 核算法的工作原理是构建一个线性分类器，通过在特征空间中找到一个使两个类之间的距离最大化的超平面，将数据分成两个类。这个超平面被称为 Fisher 线性判别式，它用于根据新数据点落在超平面的哪一侧来对它们进行分类。</p><p id="50c5" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">Fisher 核算法的一个关键优势是其处理高维数据集的能力。它特别适用于维数远大于数据点数目的问题。</p><p id="bbbb" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">要使用 Fisher 核算法比较两组数据的统计特性，第一步是计算每组数据的 Fisher 信息矩阵。这是通过估计每组数据的统计模型的参数，然后在估计的参数下计算对数似然函数的 Hessian 矩阵来实现的。然后计算每组数据的 fisher 信息矩阵，作为 Hessian 矩阵的负矩阵。</p><p id="9d30" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">一旦计算出两组数据的 Fisher 信息矩阵，下一步就是构造核函数。这是通过对一组数据的费希尔信息矩阵和另一组数据的费希尔信息矩阵求逆的乘积的迹来实现的。轨迹是矩阵的对角元素之和的度量，它在核函数中用于捕获两组数据之间的总体相似性。</p><p id="6eca" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">然后，得到的核函数可用于比较两组数据的统计特性。例如，它可以在聚类算法中用作相似性的度量，或者在分类算法中用作距离度量。</p><figure class="mj mk ml mm gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi mi"><img src="../Images/e5cd1d09143b9493fb865dfe39e9eec3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V9blFH36bnWjAioBYgyNbw.png"/></div></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">Vijay Chandrasekhar，林杰，Olivier Morère，Hanlin Goh，Antoine Veillard，CNN 和 Fisher Vectors 用于图像实例检索的实用指南，信号处理，第 128 卷，2016 年，第 426-439 页，ISSN 0165-1684，https://doi.org/10.1016/j.sigpro.2016.05.021.</figcaption></figure></div><div class="ab cl ky kz hr la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ig ih ii ij ik"><p id="fbc3" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">为了理解 Fisher 核算法是如何工作的，让我们考虑一个简单的例子。假设我们有一个包含两个类的数据集，A 类和 b 类。每个数据点都由一组特征描述，如身高、体重和年龄。我们希望使用 Fisher 核算法找到一个尽可能精确地分隔这两类的超平面。</p><p id="d6ce" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">为此，我们首先需要计算每个类的平均向量。平均向量就是每个类别的所有特征值的平均值。例如，如果 A 类具有特征值为<code class="fe mn mo mp mq b">[5, 10, 15]</code>的三个数据点，B 类具有特征值为<code class="fe mn mo mp mq b">[1, 2, 3, 4]</code>的四个数据点，那么 A 类的平均向量将为<code class="fe mn mo mp mq b">[10]</code>，B 类的平均向量将为<code class="fe mn mo mp mq b">[2, 5]</code>。</p><p id="f875" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">接下来，我们需要计算类内散布矩阵，它测量每个类内数据的散布。为此，我们取每个数据点与其类别的均值向量之间的差值，然后对这些差值求平方并求和。</p><p id="18df" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">最后，我们计算类间散布矩阵，该矩阵测量两个类之间的分离。为此，我们取两个类的平均向量之间的差，然后将该差乘以类内散布矩阵的逆矩阵。</p><p id="d230" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">然后，Fisher 核算法找到类间散布矩阵的特征向量和特征值。具有最大特征值的特征向量对应于两个类别之间具有最大间隔的维度。这些特征向量用于构建 Fisher 线性判别式，该判别式用于对新数据点进行分类。</p></div><div class="ab cl ky kz hr la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ig ih ii ij ik"><h1 id="a2b3" class="lf lg in bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">代码示例</h1><p id="e62b" class="pw-post-body-paragraph ka kb in kc b kd md kf kg kh me kj kk kl mf kn ko kp mg kr ks kt mh kv kw kx ig bi translated">以下是用 Python 实现 Fisher 内核算法的代码示例:</p><pre class="mj mk ml mm gt mr mq ms bn mt mu bi"><span id="4937" class="mv lg in mq b be mw mx l my mz">import numpy as np<br/><br/>def fisher_kernel(X, Y):<br/>  """<br/>  Calculates the Fisher Kernel between two sets of data.<br/>  <br/>  Parameters<br/>  ----------<br/>  X : array-like, shape (n_samples, n_features)<br/>      The first set of data.<br/>  Y : array-like, shape (m_samples, m_features)<br/>      The second set of data.<br/>  <br/>  Returns<br/>  -------<br/>  kernel : float<br/>      The Fisher Kernel between X and Y.<br/>  """<br/>  <br/>  # Calculate the Fisher information matrix for X<br/>  X_params = estimate_parameters(X)<br/>  X_hessian = calculate_hessian(X, X_params)<br/>  X_fisher = -np.expect(X_hessian)<br/>  <br/>  # Calculate the Fisher information matrix for Y<br/>  Y_params = estimate_parameters(Y)<br/>  Y_hessian = calculate_hessian(Y, Y_params)<br/>  Y_fisher = -np.expect(Y_hessian)<br/>  <br/>  # Calculate the kernel function<br/>  kernel = np.trace(np.linalg.inv(X_fisher).dot(Y_fisher))<br/>  <br/>  return kernel</span></pre><p id="4553" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">下面是一个在 Julia 中实现 Fisher 内核算法的代码示例:</p><pre class="mj mk ml mm gt mr mq ms bn mt mu bi"><span id="0149" class="mv lg in mq b be mw mx l my mz">using LinearAlgebra<br/><br/>function fisher_kernel(X, Y)<br/>  # Calculate the Fisher information matrix for X<br/>  X_params = estimate_parameters(X)<br/>  X_hessian = calculate_hessian(X, X_params)<br/>  X_fisher = -expected_value(X_hessian)<br/>  <br/>  # Calculate the Fisher information matrix for Y<br/>  Y_params = estimate_parameters(Y)<br/>  Y_hessian = calculate_hessian(Y, Y_params)<br/>  Y_fisher = -expected_value(Y_hessian)<br/>  <br/>  # Calculate the kernel function<br/>  kernel = trace(inv(X_fisher) * Y_fisher)<br/>  <br/>  return kernel<br/>end</span></pre><p id="2eba" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">最后，这里有一个在 R 中实现 Fisher 核算法的代码示例:</p><pre class="mj mk ml mm gt mr mq ms bn mt mu bi"><span id="a14d" class="mv lg in mq b be mw mx l my mz">fisher_kernel &lt;- function(X, Y) {<br/>  # Calculate the Fisher information matrix for X<br/>  X_params &lt;- estimate_parameters(X)<br/>  X_hessian &lt;- calculate_hessian(X, X_params)<br/>  X_fisher &lt;- -E(X_hessian)<br/>  <br/>  # Calculate the Fisher information matrix for Y<br/>  Y_params &lt;- estimate_parameters(Y)<br/>  Y_hessian &lt;- calculate_hessian(Y, Y_params)<br/>  Y_fisher &lt;- -E(Y_hessian)<br/>  <br/>  # Calculate the kernel function<br/>  kernel &lt;- trace(solve(X_fisher) %*% Y_fisher)<br/>  <br/>  return(kernel)<br/>}</span></pre><blockquote class="na nb nc"><p id="a37c" class="ka kb nd kc b kd ke kf kg kh ki kj kk ne km kn ko nf kq kr ks ng ku kv kw kx ig bi translated">请注意，上面的代码示例只是为了说明 Fisher 内核算法的基本结构，并不打算成为功能完整的代码。您需要实现函数<code class="fe mn mo mp mq b">estimate_parameters</code>、<code class="fe mn mo mp mq b">calculate_hessian</code>和<code class="fe mn mo mp mq b">expected_value</code>才能正常工作。</p></blockquote></div><div class="ab cl ky kz hr la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ig ih ii ij ik"><figure class="mj mk ml mm gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi nh"><img src="../Images/1eeaefa6a07b788bd78c2d71c6c197fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*APO2DKWjhVNf3c3NRqErGg.png"/></div></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">拉普什金，塞巴斯蒂安&amp;宾德，亚历山大&amp;蒙塔冯，格雷瓜尔&amp;穆勒，克劳斯-罗伯特&amp;萨梅克，沃伊切赫。(2016).分析分类器:费希尔向量和深度神经网络。2912-2920.2016.318。</figcaption></figure><p id="3c7d" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">总之，Fisher 核算法是一种强大的机器学习技术，特别适合高维数据集。它的工作原理是构建一个线性分类器，将数据分成两类。这个超平面被称为 Fisher 线性判别式，用于根据新数据点落在超平面的哪一侧来对它们进行分类。</p></div><div class="ab cl ky kz hr la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ig ih ii ij ik"><h1 id="287f" class="lf lg in bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">阅读建议</h1><p id="ce69" class="pw-post-body-paragraph ka kb in kc b kd md kf kg kh me kj kk kl mf kn ko kp mg kr ks kt mh kv kw kx ig bi translated"><a class="ae jz" href="https://link.springer.com/chapter/10.1007/978-3-030-86271-8_38" rel="noopener ugc nofollow" target="_blank">一个非参数的费希尔核</a></p><figure class="mj mk ml mm gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi ni"><img src="../Images/12605ecb94d0f4fe0c2ef64613bdf6aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ezHz1ihRc14G7cOlBtgWXw.jpeg"/></div></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">菲格拉郡，布林加斯郡(2021 年)。非参数 Fisher 核。载于:Sanjurjo González，h .，Pastor López，I .，García Bringas，p .，Quintián，h .，Corchado，E. (eds)混合人工智能系统。HAIS 2021。计算机科学讲义()，第 12886 卷。斯普林格，查姆。https://doi.org/<a class="ae jz" href="https://doi.org/10.1007/978-3-030-86271-8_38" rel="noopener ugc nofollow" target="_blank">10.1007/978-3-030-86271-8 _ 38</a></figcaption></figure><p id="83ee" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><a class="ae jz" href="https://www.sciencedirect.com/science/article/abs/pii/S0165168416300846" rel="noopener ugc nofollow" target="_blank">图像实例检索的 CNN 和 Fisher 向量实用指南</a></p></div><div class="ab cl ky kz hr la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ig ih ii ij ik"><p id="2d17" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">感谢您阅读本文！对于任何建议，请留下评论！</p><p id="487e" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><a class="ae jz" href="https://www.linkedin.com/in/mertdemir0" rel="noopener ugc nofollow" target="_blank"> Linkedin </a> <a class="ae jz" href="https://www.mertdemir.org" rel="noopener ugc nofollow" target="_blank">网站</a> <a class="ae jz" href="https://www.github.com/mertdemir0" rel="noopener ugc nofollow" target="_blank"> Github </a></p><p id="4aec" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">电子邮件:info@mertdemir.org</p></div></div>    
</body>
</html>