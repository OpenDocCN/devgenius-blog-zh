<html>
<head>
<title>Hyperparameter Tuning with Python: Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Python 调优超参数:第 1 部分</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/hyperparameter-tuning-with-python-part-1-7aabc622140e?source=collection_archive---------8-----------------------#2022-09-08">https://blog.devgenius.io/hyperparameter-tuning-with-python-part-1-7aabc622140e?source=collection_archive---------8-----------------------#2022-09-08</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><div class=""><h2 id="b750" class="pw-subtitle-paragraph jk im in bd b jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb dk translated">在进行超参数<br/>调整实验之前，您需要了解的概念</h2></div><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi kc"><img src="../Images/8570cd878060281854398b9966b82522.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-6KefWe7iuE5F56o"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">由<a class="ae ks" href="https://unsplash.com/@fakurian?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">米拉德·法库里安</a>在<a class="ae ks" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</figcaption></figure><p id="d8e0" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">超参数调整是一个过程，通过该过程，我们从所有候选集合中搜索 ML 模型的最佳超参数集合。它是优化我们关心的技术指标的过程。</p><blockquote class="lp lq lr"><p id="64d8" class="kt ku ls kv b kw kx jo ky kz la jr lb lt ld le lf lu lh li lj lv ll lm ln lo ig bi translated">超参数调优的目标只是在验证集上获得最大的评估分数，而不会导致过度拟合问题。</p></blockquote><p id="440a" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">超参数调整是优化模型性能的<em class="ls">以模型为中心的</em>方法之一。实际上，在优化模型性能时，建议优先考虑以数据为中心的方法，而不是以模型为中心的方法。<em class="ls">以数据为中心</em>意味着我们专注于清理、<br/>采样、增加或修改数据，而<em class="ls">以模型为中心</em>意味着我们专注于模型及其配置。</p><p id="2d5b" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">在以模型为中心的方法中，超参数调优是在我们找到最合适的模型框架或架构之后进行的。因此，可以说超参数调整是优化模型性能的最终<em class="ls"/><em class="ls">步骤</em>。</p><p id="1b3b" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">本文总结了在进行超参数调优实验之前需要了解的概念和理论。本文基于我最近出版的书，<strong class="kv io"> <em class="ls">超参数调优用 Python </em> </strong>，用 Packt 出版。</p><div class="lw lx gp gr ly lz"><a href="https://www.amazon.com/Hyperparameter-Tuning-Python-performance-hyperparameter-ebook/dp/B0B2DNQHHG/ref=sr_1_1?crid=WHHGESP6AP4M&amp;keywords=Hyperparameter+Tuning+with+Python&amp;qid=1655194083&amp;sprefix=hyperparameter+tuning+with+python%2Caps%2C675&amp;sr=8-1" rel="noopener  ugc nofollow" target="_blank"><div class="ma ab fo"><div class="mb ab mc cl cj md"><h2 class="bd io gy z fp me fr fs mf fu fw im bi translated">使用 Python 进行超参数调整:通过以下方式提升机器学习模型的性能…</h2><div class="mg l"><h3 class="bd b gy z fp me fr fs mf fu fw dk translated">Amazon.com:使用 Python 进行超参数调整:通过超参数提升机器学习模型的性能…</h3></div><div class="mh l"><p class="bd b dl z fp me fr fs mf fu fw dk translated">www.amazon.com</p></div></div><div class="mi l"><div class="mj l mk ml mm mi mn km lz"/></div></div></a></div><p id="afe2" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">在本文中，我们将讨论本书第一部分的摘要，该部分由 6 章组成:</p><ul class=""><li id="e7ac" class="mo mp in kv b kw kx kz la lc mq lg mr lk ms lo mt mu mv mw bi translated">第 1 章，评估机器学习模型</li><li id="b6bd" class="mo mp in kv b kw mx kz my lc mz lg na lk nb lo mt mu mv mw bi translated">第 2 章，介绍超参数调整</li><li id="f4ad" class="mo mp in kv b kw mx kz my lc mz lg na lk nb lo mt mu mv mw bi translated">第三章，探索穷举搜索</li><li id="594c" class="mo mp in kv b kw mx kz my lc mz lg na lk nb lo mt mu mv mw bi translated">第四章，探索贝叶斯优化</li><li id="42a6" class="mo mp in kv b kw mx kz my lc mz lg na lk nb lo mt mu mv mw bi translated">第 5 章，探索启发式搜索</li><li id="e4dc" class="mo mp in kv b kw mx kz my lc mz lg na lk nb lo mt mu mv mw bi translated">第 6 章，探索多保真优化</li></ul><p id="1962" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">不再浪费时间，让我们深呼吸，让自己舒服一点，准备好在进行超参数调优实验之前，学习你需要知道的概念和理论有哪些！</p></div><div class="ab cl nc nd hr ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ig ih ii ij ik"><h1 id="fd20" class="nj nk in bd nl nm nn no np nq nr ns nt jt nu ju nv jw nw jx nx jz ny ka nz oa bi translated">评估机器学习模型</h1><p id="1f81" class="pw-post-body-paragraph kt ku in kv b kw ob jo ky kz oc jr lb lc od le lf lg oe li lj lk of lm ln lo ig bi translated">需要对 ML 模型进行彻底的评估，以确保它们能够在生产中发挥作用。我们必须确保模型不会记住训练数据，并确保它从给定的训练数据中学到足够多的东西。当我们希望在稍后阶段执行超参数调整时，选择合适的评估方法也很重要。</p><p id="c1b9" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">在这一章中，我们将学习评估 ML 模型时需要知道的所有重要事情。首先，我们需要理解过度拟合的概念。然后，我们将研究将数据分成训练集、验证集和测试集的想法。此外，我们将了解随机分割和分层分割之间的区别，以及何时使用它们。我们将讨论交叉验证的概念及其各种策略:k-fold、重复 k-fold、留一(LOO)、留 P(LPO)，以及处理时序数据时的一种特定策略，称为时序交叉验证。我们还将学习如何使用 Scikit-Learn 包实施每种评估策略。</p><p id="cab9" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">在本章结束时，你将会很好地理解为什么选择一个合适的评估策略在 ML 模型开发生命周期中是至关重要的。此外，您将了解众多评估策略，并能够选择最适合您情况的策略。此外，<br/>您还将能够使用 Scikit-Learn 软件包实施每种评估策略。</p></div><div class="ab cl nc nd hr ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ig ih ii ij ik"><h1 id="86a9" class="nj nk in bd nl nm nn no np nq nr ns nt jt nu ju nv jw nw jx nx jz ny ka nz oa bi translated">引入超参数调谐</h1><p id="588b" class="pw-post-body-paragraph kt ku in kv b kw ob jo ky kz oc jr lb lc od le lf lg oe li lj lk of lm ln lo ig bi translated">每个 ML 项目都应该有一个明确的目标和成功的衡量标准。成功度量可以是业务和/或技术度量的形式。评估业务度量是困难的，通常，它们只能在 ML 模型投入生产后才能被评估。另一方面，评估技术指标更加简单，可以在开发阶段完成。作为<br/>的数据科学家，我们希望达到我们能得到的最好的技术指标，因为这是我们可以优化的。</p><p id="d10f" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">在本章中，我们将学习优化所选技术指标的几种方法之一，称为超参数调优。我们将从理解什么是超参数调优及其目标开始这一章。然后，我们将讨论超参数和参数之间的区别。我们还将学习超参数空间的概念以及在实践中可能发现的超参数值的可能分布。</p><p id="4b3b" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">在本章结束时，你将理解超参数调整的概念和超参数本身。理解这些概念对你理解下一章将要讨论的内容至关重要。</p></div><div class="ab cl nc nd hr ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ig ih ii ij ik"><h1 id="ec19" class="nj nk in bd nl nm nn no np nq nr ns nt jt nu ju nv jw nw jx nx jz ny ka nz oa bi translated">探索穷举搜索</h1><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi og"><img src="../Images/ba07b1c807f9f437899744bbc03cbd76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*v_Md78ThX3NwHd9b"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">Nubelson Fernandes 在<a class="ae ks" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="25b4" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">超参数调整并不总是对应于复杂的搜索算法。事实上，也可以利用简单的 for 循环或基于开发人员直觉的手动搜索来实现超参数调优的目标，即在验证<br/>分数上获得最大的评估分数，而不会导致过拟合问题。</p><p id="ef8a" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">在本章中，我们将讨论四组超参数调整中的第一组，称为穷举搜索。这是实践中使用最广泛和最直接的超参数调整组。顾名思义，属于该组的超参数调整方法通过<br/>在超参数空间中进行彻底搜索来工作。除了一种方法，这一组中的所有方法都被归类为无信息搜索算法，这意味着它们没有从以前的迭代中学习，以便在未来拥有更好的搜索空间。本章将讨论三种方法:手动搜索、网格搜索和随机搜索。</p><p id="c974" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">在本章结束时，你将理解属于穷举搜索组的每个超参数调整方法的概念。当有人向你询问这些方法时，你将能够自信地以高层次和详细的方式解释它们，以及利弊。更重要的是，你将能够在实践中自信地运用所有的方法。您还将能够了解如果出现错误或意外结果会发生什么，并了解如何设置方法配置以匹配您的特定问题。</p></div><div class="ab cl nc nd hr ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ig ih ii ij ik"><h1 id="12cc" class="nj nk in bd nl nm nn no np nq nr ns nt jt nu ju nv jw nw jx nx jz ny ka nz oa bi translated">探索贝叶斯优化</h1><p id="0233" class="pw-post-body-paragraph kt ku in kv b kw ob jo ky kz oc jr lb lc od le lf lg oe li lj lk of lm ln lo ig bi translated">贝叶斯优化(BO)是四组超参数调整方法中的第二组。与被归类为无信息搜索方法的网格搜索和随机搜索不同，属于业务对象组的所有方法都被归类为有信息搜索方法，这意味着它们正在从以前的迭代中学习，以便(有希望)在未来提供更好的搜索空间。</p><p id="111f" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">在本章中，我们将讨论属于 BO 组的几种方法，包括高斯过程(GP)、基于序列模型的算法配置(SMAC)、树形结构 Parzen 估计器(TPE)和 Metis。类似于第 3 章，探索穷举搜索，我们将讨论每种方法的定义，它们之间的区别，它们如何工作，以及每种方法的优缺点。</p><p id="9ea6" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">在本章结束时，除了贝叶斯优化和它的变体，你将能够达到和第三章一样的效果。</p></div><div class="ab cl nc nd hr ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ig ih ii ij ik"><h1 id="0b1e" class="nj nk in bd nl nm nn no np nq nr ns nt jt nu ju nv jw nw jx nx jz ny ka nz oa bi translated">探索启发式搜索</h1><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi oh"><img src="../Images/6213e54628b83cff52540bb2da822435.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mspVbHIuTiDZVWYr"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">在<a class="ae ks" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae ks" href="https://unsplash.com/@_louisreed?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">路易斯·里德</a>拍摄的照片</figcaption></figure><p id="9113" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">启发式搜索是四组超参数调整方法中的第三组。这一组与其他组的关键区别在于，属于这一组的所有方法都是通过反复试验来实现最优解的。类似于贝叶斯<br/>优化中的获取功能(见第 4 章，探索贝叶斯优化)，这一组中的所有方法也采用了探索与利用的概念。探索意味着在未探索的空间中执行搜索，以降低陷入局部最优的概率，而利用意味着在已知很有可能包含最优解的局部空间中执行搜索。</p><p id="d17e" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">在本章中，我们将讨论属于启发式搜索组的几种方法，包括模拟退火(SA)、遗传算法(GAs)、粒子群优化(PSO)和基于群体的训练(PBT)。类似于第 4 章，我们将讨论每种方法的定义，它们之间的区别是什么，它们是如何工作的，以及每种方法的优缺点。</p><p id="78d0" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">在本章结束时，除了超参数调整方法的启发式搜索组，你将能够达到与第 4 章相同的效果。</p></div><div class="ab cl nc nd hr ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ig ih ii ij ik"><h1 id="c11e" class="nj nk in bd nl nm nn no np nq nr ns nt jt nu ju nv jw nw jx nx jz ny ka nz oa bi translated">探索多保真优化</h1><p id="f61b" class="pw-post-body-paragraph kt ku in kv b kw ob jo ky kz oc jr lb lc od le lf lg oe li lj lk of lm ln lo ig bi translated">多重保真优化(MFO)是四组超参数调整方法中的第四组。这一组的主要特点是，属于这一组的所有方法都利用整个超参数调谐管道的廉价近似，因此我们可以以低得多的计算成本和更快的实验时间获得类似的性能<br/>结果。当您有一个非常大的模型或非常多的样本时，例如，当您正在开发一个基于神经网络的模型时，这个组是合适的。</p><p id="a1e9" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">在这一章中，我们将讨论 MFO 小组的几种方法，包括从粗到细的搜索，连续减半，超波段，贝叶斯优化和超波段(BOHB)。如同第 5 章，探索启发式搜索，我们将讨论每种方法的定义，它们之间的区别，它们如何工作，以及每种方法的优缺点。</p><p id="6e54" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">本章结束时，除了超参数调谐方法的多保真度优化组，你将能够实现与第 5 章相同的效果。</p></div><div class="ab cl nc nd hr ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ig ih ii ij ik"><h1 id="0e3a" class="nj nk in bd nl nm nn no np nq nr ns nt jt nu ju nv jw nw jx nx jz ny ka nz oa bi translated">你能从书中期待什么</h1><div class="lw lx gp gr ly lz"><a href="https://www.amazon.com/Hyperparameter-Tuning-Python-performance-hyperparameter-ebook/dp/B0B2DNQHHG/ref=sr_1_1?crid=WHHGESP6AP4M&amp;keywords=Hyperparameter+Tuning+with+Python&amp;qid=1655194083&amp;sprefix=hyperparameter+tuning+with+python%2Caps%2C675&amp;sr=8-1" rel="noopener  ugc nofollow" target="_blank"><div class="ma ab fo"><div class="mb ab mc cl cj md"><h2 class="bd io gy z fp me fr fs mf fu fw im bi translated">使用 Python 进行超参数调整:通过以下方式提升机器学习模型的性能…</h2><div class="mg l"><h3 class="bd b gy z fp me fr fs mf fu fw dk translated">Amazon.com:使用 Python 进行超参数调整:通过超参数提升机器学习模型的性能…</h3></div><div class="mh l"><p class="bd b dl z fp me fr fs mf fu fw dk translated">www.amazon.com</p></div></div><div class="mi l"><div class="mj l mk ml mm mi mn km lz"/></div></div></a></div><p id="2422" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">这本书由 14 章组成，分为 3 个部分。这本书涵盖了你需要知道的关于超参数调谐的所有重要的事情，从概念和理论，实现，以及如何把事情付诸实践开始。除了对每种方法如何工作的深入解释之外，您还将使用一个<strong class="kv io"> <em class="ls">决策图</em> </strong>，它可以帮助您确定满足您需求的最佳调优方法。这本书还附有代码实现，您可以从 GitHub 免费获得！</p><div class="lw lx gp gr ly lz"><a href="https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python" rel="noopener  ugc nofollow" target="_blank"><div class="ma ab fo"><div class="mb ab mc cl cj md"><h2 class="bd io gy z fp me fr fs mf fu fw im bi translated">GitHub—packt publishing/使用 Python 进行超参数调整:使用 Python 进行超参数调整</h2><div class="mg l"><h3 class="bd b gy z fp me fr fs mf fu fw dk translated">这是用 Python 进行超参数调优的代码库，由 Packt 发布。促进您的机器学习…</h3></div><div class="mh l"><p class="bd b dl z fp me fr fs mf fu fw dk translated">github.com</p></div></div><div class="mi l"><div class="oi l mk ml mm mi mn km lz"/></div></div></a></div><p id="fa34" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">本书面向使用 Python 的数据科学家和机器学习工程师，他们希望通过利用适当的超参数调整方法来进一步提高 ML 模型的性能。你需要对 ML 和如何用 Python 编码有一个基本的了解，但是<br/>不需要 Python 中超参数调优的先验知识。</p></div><div class="ab cl nc nd hr ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ig ih ii ij ik"><h1 id="48ec" class="nj nk in bd nl nm nn no np nq nr ns nt jt nu ju nv jw nw jx nx jz ny ka nz oa bi translated">关于作者</h1><p id="0ccd" class="pw-post-body-paragraph kt ku in kv b kw ob jo ky kz oc jr lb lc od le lf lg oe li lj lk of lm ln lo ig bi translated">Louis Owen 是一名来自印度尼西亚的数据科学家/人工智能工程师，他总是渴望新知识。在他的职业生涯中，他曾在多个行业领域工作，包括非政府组织、电子商务、对话式人工智能、OTA、智能城市和金融科技。工作之外，他喜欢花时间帮助数据科学爱好者成为数据科学家，要么通过他的文章，要么通过辅导会议。他还喜欢在业余时间做自己的爱好:看电影和做兼职项目。</p><p id="d68b" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">目前，Louis 是世界领先的 CX 自动化平台 Yellow.ai <em class="ls">、</em>的 NLP 研究工程师。查看路易斯的网站以了解更多关于他的信息！最后，如果您有任何疑问或要讨论的话题，请通过<a class="ae ks" href="https://www.linkedin.com/in/louisowen/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>联系 Louis。</p></div></div>    
</body>
</html>