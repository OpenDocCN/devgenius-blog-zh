<html>
<head>
<title>Digital Avatars: The Next Leap in Remote Work?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数字化身:远程工作的下一次飞跃？</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/digital-avatars-the-next-leap-in-remote-work-c72c0cd369a7?source=collection_archive---------3-----------------------#2021-06-03">https://blog.devgenius.io/digital-avatars-the-next-leap-in-remote-work-c72c0cd369a7?source=collection_archive---------3-----------------------#2021-06-03</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><figure class="gl gn jl jm jn jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi jk"><img src="../Images/33d4429c9b43b2e89dfd731be88a7006.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PaiBpevkdLx9j_d9UinT7w.jpeg"/></div></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">我的朋友们，这种疲劳是真实的——也就是“变焦疲劳”。迪伦·费雷拉在<a class="ae jz" href="https://unsplash.com/s/photos/video-call?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><h1 id="79cd" class="ka kb in bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">远程工作失败的地方</h1><p id="d6c3" class="pw-post-body-paragraph ky kz in la b lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv ig bi translated">我的朋友们，这种疲劳是真实的——也就是“变焦疲劳”。</p><p id="b9dd" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">不管你喜欢还是讨厌，远程工作可能会持续一段时间，因为它仍然是在全球许多地方开展业务的最安全、最实惠、也可以说是最环保的<a class="ae jz" href="https://youtu.be/XswV_yqPq28" rel="noopener ugc nofollow" target="_blank">方式。</a></p><p id="3d57" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">然而，如果你曾经在一天的视频会议后感到精疲力尽，你并不孤单。今年2月，斯坦福大学的杰里米·贝伦森博士正式发表了一项关于变焦疲劳原因的研究。简而言之，贝伦森博士认为这些原因是“过多的近距离凝视、认知负荷、盯着自己的视频看而增加的自我评价，以及对身体流动性的限制”。</p><p id="bf19" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">随着疫情的继续，变焦是我们职业生涯中的生命线。但是有什么方法可以让知识工作者从变焦疲劳中拯救我们的头脑呢？</p><h1 id="7e7c" class="ka kb in bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">深度学习如何帮助:数字化身！</h1><figure class="mb mc md me gt jo"><div class="bz fp l di"><div class="mf mg l"/></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">有没有可能有一天，远程团队会依靠数字化身，如<a class="ae jz" href="https://zainraza.me/headsetsGoodbye/expressionTrackingDemos/robot/index.html" rel="noopener ugc nofollow" target="_blank">【Roberto】</a>(上面看到的机器人)在团队成员之间进行交流？</figcaption></figure><h2 id="01d5" class="mh kb in bd kc mi mj dn kg mk ml dp kk lj mm mn ko ln mo mp ks lr mq mr kw ms bi translated">但是什么是数字化身呢？</h2><p id="ef87" class="pw-post-body-paragraph ky kz in la b lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv ig bi translated">虚拟角色只不过是在计算机中创建的用户的一种表现形式。更正式地说，Ducheneaut教授等人描述(在他们2009年的ACM论文中，“<a class="ae jz" href="https://dl.acm.org/doi/10.1145/1518701.1518877" rel="noopener ugc nofollow" target="_blank">“身体和心灵:三个虚拟世界中的化身个性化研究”</a>)这些化身通常可以由用户控制和定制，以适应他们的个性化外观[ <a class="ae jz" href="https://dl.acm.org/doi/10.1145/1518701.1518877" rel="noopener ugc nofollow" target="_blank"> 2 </a> ]。</p><h2 id="86d6" class="mh kb in bd kc mi mj dn kg mk ml dp kk lj mm mn ko ln mo mp ks lr mq mr kw ms bi translated">这就是为什么它们很重要</h2><p id="cfb1" class="pw-post-body-paragraph ky kz in la b lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv ig bi translated">正如上面的GIF图所示，用户可以定制<a class="ae jz" href="https://zainraza.me/headsetsGoodbye/expressionTrackingDemos/robot/index.html" rel="noopener ugc nofollow" target="_blank">“罗伯托:移情机器人”</a>的面部表情，只需改变他们自己的表情。不像今天流行的XR应用程序中的化身，Roberto只需要一个网络摄像头就可以控制。这得益于利用了流行的面部识别库<a class="ae jz" href="https://justadudewhohacks.github.io/face-api.js/docs/index.html" rel="noopener ugc nofollow" target="_blank"> face-api.js </a>。它最初是由Vincent Mühler在2018年创建的，代码是开源的。</p><p id="c195" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">那么，有没有可能有一天，远程团队将依靠像罗伯托这样的数字化身，以一种有趣的、认知不那么详尽的方式定制团队成员？</p><p id="5455" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">在博客的其余部分，我将报告我在构建<a class="ae jz" href="https://zainraza.me/headsetsGoodbye/expressionTrackingDemos/robot/index.html" rel="noopener ugc nofollow" target="_blank">“罗伯托:移情机器人”</a>项目时是如何努力实现以下目标的，以及由此产生的结果。</p><h2 id="3e50" class="mh kb in bd kc mi mj dn kg mk ml dp kk lj mm mn ko ln mo mp ks lr mq mr kw ms bi translated"><strong class="ak">本项目目标:</strong></h2><ol class=""><li id="96de" class="mt mu in la b lb lc lf lg lj mv ln mw lr mx lv my mz na nb bi translated"><strong class="la io">演示</strong>如何通过网络浏览器中的3D模型<strong class="la io">传达情感</strong>。</li><li id="1561" class="mt mu in la b lb nc lf nd lj ne ln nf lr ng lv my mz na nb bi translated"><strong class="la io">测试TinyYoloV2模型能够<strong class="la io">有效识别混合情绪的限度</strong>。</strong></li><li id="5dab" class="mt mu in la b lb nc lf nd lj ne ln nf lr ng lv my mz na nb bi translated"><strong class="la io">在Three.js场景中，通过平衡稳定性和响应性之间的权衡，产生平滑的UI/UX </strong>。</li></ol><h1 id="fd0f" class="ka kb in bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">目标1:在网络上传达情感</h1><h2 id="1cd0" class="mh kb in bd kc mi mj dn kg mk ml dp kk lj mm mn ko ln mo mp ks lr mq mr kw ms bi translated">动画机器人:<strong class="ak">接近</strong></h2><p id="acd0" class="pw-post-body-paragraph ky kz in la b lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv ig bi translated">坦率地说，当我开始这个项目时，我是一个计算机图形学的新手，不知道从哪里开始。因此，我试着从一个简单的UX目标开始:制造一个机器人——一种人造生物——以人类自然的方式传达情感。</p><figure class="mb mc md me gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi nh"><img src="../Images/44ca77e2e7eb04da64b26c83578bbf4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BD0oL35CpmkfpE14o-0U4g.png"/></div></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">这7种面部表情的含义被认为是“通用的”——这意味着它们可以不受文化、种族、宗教、性别或任何其他人口统计标志的影响而被识别。这使得设计机器人的面部表情变得更容易，因为我们可以概括出它应该显示的特征，以便人类用户能够识别。资料来源:大卫松本，<a class="ae jz" href="https://www.humintell.com/2021/01/the-universality-of-facial-expressions-of-emotion/" rel="noopener ugc nofollow" target="_blank">休敏特尔</a>，2021年。</figcaption></figure><p id="6d1a" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">好消息是，心理学家已经对我们人类如何表达情感做了大量的研究。旧金山州立大学的大卫·松本博士在他的网站上写道，有几种情绪仅仅通过面部表情就可以被普遍识别[<a class="ae jz" href="https://www.humintell.com/2021/01/the-universality-of-facial-expressions-of-emotion/" rel="noopener ugc nofollow" target="_blank">3</a>。这使得理解3D机器人角色需要包含表达式的哪些方面变得更加容易。例如，如果一个只有眼睛和眉毛的机器人角色需要显示愤怒，那么他们必须做出愤怒的人类眉毛中常见的典型“V”形。</p><p id="4d11" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">下一步实际上是找到一个有表现力的机器人3D模型。幸运的是，有很多可供选择，这要感谢计算机图形学中现有的R&amp;D的几十年。我最终选择了<a class="ae jz" href="https://github.com/mrdoob/three.js/tree/master/examples/models/gltf/RobotExpressive" rel="noopener ugc nofollow" target="_blank">这个</a>，这是一个开源模型，最初由<a class="ae jz" href="https://www.patreon.com/quaternius" rel="noopener ugc nofollow" target="_blank">托马斯·劳埃</a>创建，并在<a class="ae jz" href="https://threejs.org/examples/#webgl_animation_skinning_morph" rel="noopener ugc nofollow" target="_blank"> Three.js网站</a>上作为“蒙皮和变形”的例子。除了是开源的，我选择这个模型是因为它是glTF格式的。正如Lewy Blue在<a class="ae jz" href="https://discoverthreejs.com/book/first-steps/load-models/" rel="noopener ugc nofollow" target="_blank"><em class="ni">DISCOVER three . js</em></a><em class="ni"/>(2021)电子书中所报告的那样，这是迄今为止用于在浏览器中加载3D模型的最流行的文件格式，因为文件大小往往更小，动画加载速度比其他文件类型快，如OBJ或FBX [ <a class="ae jz" href="https://discoverthreejs.com/book/first-steps/load-models/" rel="noopener ugc nofollow" target="_blank"> 4 </a> ]。</p><figure class="mb mc md me gt jo"><div class="bz fp l di"><div class="nj mg l"/></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">最初的机器人模型，通过鼠标控制，具有可点击的GUI(来源:<a class="ae jz" href="https://threejs.org/examples/#webgl_animation_skinning_morph" rel="noopener ugc nofollow" target="_blank"> Three.js </a>)。</figcaption></figure><p id="aba4" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">这些简短的动画，又名<em class="ni"> shape keys，</em>已经和模型一起出现了(感谢开源贡献者<a class="ae jz" href="https://www.donmccurdy.com" rel="noopener ugc nofollow" target="_blank">唐·麦克迪</a>的工作)，我从Three.js GitHub库下载了这些动画。然而，它们也可以通过第三方<a class="ae jz" href="https://en.wikipedia.org/wiki/3D_modeling" rel="noopener ugc nofollow" target="_blank">建模程序</a>如<a class="ae jz" href="https://www.blender.org/" rel="noopener ugc nofollow" target="_blank"> Blender </a>手动添加到glTF模型中。</p><h2 id="10d0" class="mh kb in bd kc mi mj dn kg mk ml dp kk lj mm mn ko ln mo mp ks lr mq mr kw ms bi translated"><strong class="ak">结果</strong></h2><p id="86cf" class="pw-post-body-paragraph ky kz in la b lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv ig bi translated">我还在为机器人的面部表情进行UX测试，反馈主要是定性的。当试图判断用户是否认为机器人的表情(对他们自己面部扭曲的反应)看起来“真实”时，我会问如下问题:</p><blockquote class="nk nl nm"><p id="298f" class="ky kz ni la b lb lw ld le lf lx lh li nn ly ll lm no lz lp lq np ma lt lu lv ig bi translated">“机器人让你想起了什么，如果有的话？”<br/>“看机器人的动画有什么感受？”</p></blockquote><p id="d9f2" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">对此，通常用户的反应是，虽然他们认为机器人看起来很酷，但它并没有强烈地提醒他们任何其他事情。在观看机器人时，他们同样表示动画没有任何强烈的联系感。</p><h1 id="9d0b" class="ka kb in bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">目标2:实时显示复杂的感觉</h1><h2 id="44ee" class="mh kb in bd kc mi mj dn kg mk ml dp kk lj mm mn ko ln mo mp ks lr mq mr kw ms bi translated">方法</h2><p id="bc43" class="pw-post-body-paragraph ky kz in la b lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv ig bi translated">既然我们有了一个表情丰富的机器人，下一步就是找到一种方法，只用我们的面部表情来控制机器人的脸。</p><p id="d085" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">这就是<a class="ae jz" href="https://justadudewhohacks.github.io/face-api.js/docs/index.html" rel="noopener ugc nofollow" target="_blank"> face-api.js </a>模块派上用场的地方:它提供了一个简单易懂的api来利用<em class="ni"> TinyYolov2 </em>模型，这是一个强大而轻量级的神经网络，用于执行一些计算机视觉任务。在我们的案例中，我们需要执行“面部表情识别”，即通过网络摄像头检测情绪。下面显示了执行此任务时模型输出的示例，使用了在<a class="ae jz" href="https://justadudewhohacks.github.io/face-api.js/docs/index.html" rel="noopener ugc nofollow" target="_blank"> face-api.js文档中找到的静态图像。</a></p><figure class="mb mc md me gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi nq"><img src="../Images/44ea0c6bf2fd77a31f3757a829f6433d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ftG1HjxPcToVx2RBCekb9w.png"/></div></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">正如你所看到的，这个来自<a class="ae jz" href="https://justadudewhohacks.github.io/face-api.js/docs/index.html" rel="noopener ugc nofollow" target="_blank"> face-api.js文档</a>的例子展示了TinyYolov2模型如何识别普遍的情感；并且它还提供了一个介于0–1之间的值，表示模型对其预测的信心程度(来源:<a class="ae jz" href="https://justadudewhohacks.github.io/face-api.js/docs/index.html" rel="noopener ugc nofollow" target="_blank"> face-api.js </a>)。</figcaption></figure><p id="135c" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">使用前面提到的API，我们可以使用<em class="ni"> TinyYolov2 </em>模型来识别在网络摄像头上捕捉到的用户表情，并返回该预测的置信度(0到1之间的值)。</p><p id="c371" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">这个知识仍然回避了这个问题:<strong class="la io">我们如何在机器人身上表现出复杂的感情？</strong></p><p id="4b67" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">这只是为了了解face-api.js的API是如何工作的。为了详细说明，我们可以调用下面的函数:</p><pre class="mb mc md me gt nr ns nt nu aw nv bi"><span id="a604" class="mh kb in ns b gy nw nx l ny nz">.withFaceExpressions();</span></pre><p id="adac" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">为了返回模型对<em class="ni">每一种可能的</em>情感的信心，它可以检测到，而不仅仅是在任何给定时刻最有可能的一种。巧合的是，我要提到的是，TinyYolov2 的创造者使用了七种普遍情绪作为模型分类的任何面部表情的潜在标签。</p><p id="bb58" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">每种情绪被模型预测的概率(同样，总共有七种)，总共也必须加起来达到100%。为了让您直观地看到这一点，它可能看起来像下面的饼图(请放大以查看百分比):</p><figure class="mb mc md me gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi oa"><img src="../Images/8122cfea4d6897675e8fb42146d5a19f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1oFGsTb-MheIsTg81XWT2A.png"/></div></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">来源:<a class="ae jz" href="https://chart-studio.plotly.com/~chrispiro/62.embed" rel="noopener ugc nofollow" target="_blank"> Chrispiro，图表工作室</a></figcaption></figure><p id="d4b9" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">因此，我显示混合情绪的方法是<em class="ni">使用每个预测情绪的置信度，作为该情绪的动画播放程度。</em>例如，如果模型检测到用户在给定时刻正经历类似于上面饼图的情绪组合，那么机器人将显示一个表情，主要是<strong class="la io">惊讶</strong>，然后是<strong class="la io">恐惧</strong>，接着是<strong class="la io">悲伤</strong>，等等。</p><p id="5ebc" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">下面，我将包含实际实现上述方法的<a class="ae jz" href="https://github.com/UPstartDeveloper/headsetsGoodbye/blob/main/expressionTrackingDemos/robot/resources/js/robot.js" rel="noopener ugc nofollow" target="_blank">代码片段</a>。为了使机器人的表情能够实时变化，最后一步是调用下面的函数作为一个更大的渲染循环的一部分(你可以在GitHub链接<a class="ae jz" href="https://github.com/UPstartDeveloper/headsetsGoodbye/blob/main/expressionTrackingDemos/robot/resources/js/robot.js" rel="noopener ugc nofollow" target="_blank">的底部找到这个函数</a>)。</p><figure class="mb mc md me gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi ob"><img src="../Images/7a52bac61ef5566e5cc8f2f007eba860.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1brthzhOC_FhC4qbt4_QVQ.png"/></div></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">通过<em class="oc">使用每个预测情感的置信度作为该情感动画的程度，动画化机器人面部的代码片段，允许基于网络摄像头显示复杂的情感。</em></figcaption></figure><h2 id="7b3d" class="mh kb in bd kc mi mj dn kg mk ml dp kk lj mm mn ko ln mo mp ks lr mq mr kw ms bi translated">结果呢</h2><p id="d26e" class="pw-post-body-paragraph ky kz in la b lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv ig bi translated">尽管这种方法相当直观，但还不是很健壮。例如，我在一项对该应用的非科学调查中注意到，在多次测试中，该应用在昏暗的光线条件下反应变得更慢。</p><h1 id="61f1" class="ka kb in bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">目标3:制作流畅的用户界面/UX</h1><h2 id="b40f" class="mh kb in bd kc mi mj dn kg mk ml dp kk lj mm mn ko ln mo mp ks lr mq mr kw ms bi translated">平衡稳定性和响应性之间的权衡</h2><p id="02af" class="pw-post-body-paragraph ky kz in la b lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv ig bi translated">到目前为止，我们解决的第三大问题是性能问题——最初机器人动画是如此滞后，以至于现实中的人会觉得这是一种令人愉快的体验是不现实的。</p><p id="296b" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">最终，我发现在机器人的面部动画上使用补间技术有助于我让应用程序对用户做出足够快的响应，以每秒<em class="ni">帧数</em> (FPS)来衡量，而不会太紧张。作为背景，“补间”仅仅是指通过填充中间步骤，使用Javascript平滑有助于屏幕显示的动画。你可以在华盛顿大学的Terry Brooks的讲座中了解更多关于补间<a class="ae jz" href="http://staff.washington.edu/tabrooks/343INFO/JSMotionTween/jsMotionTween.htm" rel="noopener ugc nofollow" target="_blank">的正式定义。总的来说，补间让我可以尝试在一个缓慢稳定的应用程序和一个敏感但反应灵敏的应用程序之间找到平衡。</a></p><h2 id="12c2" class="mh kb in bd kc mi mj dn kg mk ml dp kk lj mm mn ko ln mo mp ks lr mq mr kw ms bi translated">进行测量</h2><p id="15d3" class="pw-post-body-paragraph ky kz in la b lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv ig bi translated">我的目标是看看我们是否可以最小化应用程序中的抖动(定义为Roberto的表情不可预测地变化的次数，而用户的表情不变)，同时优化最高的响应能力(以我的FPS为衡量标准)。</p><p id="ca27" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">为了做到这一点，我决定通过11次测试来衡量这款应用的性能。在每个试验中，<em class="ni">独立变量</em>是补间动画的持续时间。我会将这个值调整0.10秒，从初始值0.05秒开始。</p><p id="da89" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">在每次试验中，我将计时器设置为5秒，在试验期间，我在代码中设置了一个全局Javascript变量<code class="fe od oe of ns b">counter</code>，它将从0开始，并在每次动画被触发时递增。然后它会被记录到控制台。然后，随着时间的推移，我会看着镜头，用“惊讶”的表情托住自己的脸——扬起眉毛，睁大眼睛。</p><p id="0d3f" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">总而言之，我们的<em class="ni">因变量</em>如下:</p><ol class=""><li id="b3f4" class="mt mu in la b lb lw lf lx lj og ln oh lr oi lv my mz na nb bi translated">“表情变化”:这是动画在5秒内被触发的次数，当我在“惊讶”表情中保持面部稳定时。它被记录到控制台，我用谷歌浏览器的Inspect工具查看了它。同样，这是使用名为<code class="fe od oe of ns b">counter</code>的全局Javascript变量测量的。</li><li id="94d4" class="mt mu in la b lb nc lf nd lj ne ln nf lr ng lv my mz na nb bi translated">最大值FPS —使用<code class="fe od oe of ns b">dat.gui.module.js</code>(又名“dat”)中的代码。GUI”)，我能够测量该应用程序在试用过程中达到的最高FPS(总共需要大约10秒)。对于感兴趣的读者，“dat。GUI”是另一个开源Javascript模块，它包含在我从Three.js存储库中获取的样板代码中。更多信息可以在<a class="ae jz" href="https://github.com/dataarts/dat.gui%5C" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</li></ol><p id="18f5" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">虽然我没有把这归结为一门科学，但试验结果如下所示:</p><figure class="mb mc md me gt jo gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/6eae5a67458dddb80d963cee0cf48643.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*8yKMHkJEM32EEMhA0tWMBw.png"/></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">来源:<a class="ae jz" href="https://colab.research.google.com/drive/1XXORTuzKL0vxjRqma2088x4omkMxrBjx?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Jupyter笔记本</a></figcaption></figure><p id="e941" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">如您所见，观察到的最佳补间持续时间约为0.85秒。该值与大约40处表达变化的最低值以及最高值相关。每秒11帧。为了更深入地了解我收集的数据，请查看我在Google Colab中的<a class="ae jz" href="https://colab.research.google.com/drive/1XXORTuzKL0vxjRqma2088x4omkMxrBjx?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Jupyter笔记本。</a></p><h1 id="7217" class="ka kb in bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">改进的后续步骤</h1><p id="eebc" class="pw-post-body-paragraph ky kz in la b lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv ig bi translated"><strong class="la io">目标1: </strong>关于给机器人制作动画，有两个主要建议可以让它的面部表情看起来更真实:</p><ol class=""><li id="d49a" class="mt mu in la b lb lw lf lx lj og ln oh lr oi lv my mz na nb bi translated"><strong class="la io">增加更多类似人类的特征:</strong>比如嘴巴，或者上下移动脖子的能力。这可以通过增加机器人创建的真实感来改善用户体验。</li><li id="ac17" class="mt mu in la b lb nc lf nd lj ne ln nf lr ng lv my mz na nb bi translated"><strong class="la io">继续测试，使用更多种类的面部表情:</strong>关于像Blender这样的建模程序的广泛可用性的好消息是，编辑或向机器人添加新种类的表情动画相当简单。因此，我们可能只需要继续寻找最能引起用户共鸣的设计。从通过面部表达的7种普遍情绪列表中，我为不同机器人表情的设计做了一些低逼真度的设计:</li></ol><figure class="mb mc md me gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi ok"><img src="../Images/d368c74d58e393dcfe2eace28fd41e8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iireJ54cLygdnUyN5Of5fg.jpeg"/></div></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">从通过面部表达的7种普遍情绪列表中，我为不同机器人表情的设计未来可能的样子做了一些低逼真度的设计。当然，这些需要使用建模程序如Blender来添加。</figcaption></figure><p id="aadd" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated"><strong class="la io">目标2: </strong>关于使情感识别更加鲁棒，最有可能起作用的选项是重新训练<em class="ni"> TinyYolov2 </em>模型，或者使用另一个模型，以便它将在昏暗的灯光或从不同摄像机角度看到的人脸的情况下工作得更好。</p><p id="e617" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated"><strong class="la io">目标3: </strong>虽然我只测试了11次，现在考虑优化性能还为时过早，但这款应用的平均速度已经达到了<strong class="la io"> 11 FPS左右(在11次测试中)。</strong>这比Three.js网站上的<a class="ae jz" href="https://threejs.org/examples/#webgl_animation_skinning_morph" rel="noopener ugc nofollow" target="_blank">原始机器人示例少了近5倍，后者可通过鼠标控制。这表明未来需要做更多的工作来优化GPU/CPU负载，因为这种滞后会在应用程序的其他地方出现，如机器人行走、挥手打招呼等。同时试图检测情绪，并恶化用户体验。</a></p><p id="addc" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">以下是一些需要改进的想法:</p><ol class=""><li id="bb67" class="mt mu in la b lb lw lf lx lj og ln oh lr oi lv my mz na nb bi translated"><strong class="la io">为WebGPU优化:</strong>据我所知，face-api.js库目前使用WebGL作为TinyYolov2模型的后端。然而，Tensorflow.js 2的较新的WebGPU后端最近已经发布，并承诺提供更好的性能，因此修改库代码以支持较新的后端可能是值得的。</li><li id="d76f" class="mt mu in la b lb nc lf nd lj ne ln nf lr ng lv my mz na nb bi translated"><strong class="la io">自适应补间</strong>:另一个想法，但需要更多的研究，是研究开发一种算法来动态调整硬件加速器(即GPU)上的负载。这可能会使不同机器之间的UX更加平滑，因为该应用程序可以优化补间动画所需的时间，而不管内存可用性、互联网连接等方面的约束条件如何变化。平衡响应性和稳定性。</li></ol><h1 id="a3f2" class="ka kb in bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">结论和未来应用</h1><p id="bae2" class="pw-post-body-paragraph ky kz in la b lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv ig bi translated">变焦疲劳不需要成为日常生活的事实。数字化身的使用可能会改变远程工作的动态，使其更加流畅，减少精神上的疲惫，就像面对面的会议一样具有协作性。</p><p id="e41c" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">下面，我将分享一些关于这项技术的想法，希望其他人也能受到启发，更多地关注这一领域:</p><ol class=""><li id="bb39" class="mt mu in la b lb lw lf lx lj og ln oh lr oi lv my mz na nb bi translated"><strong class="la io">在课堂上:</strong>有了数字化身，我们将有另一种方法来衡量员工对学习体验的反应，例如，训练你在工厂中导航的虚拟现实应用程序。这可以帮助向VR创作者提供反馈，读取用户观察到的面部表情，以查看课程是否可以理解。</li><li id="5ae9" class="mt mu in la b lb nc lf nd lj ne ln nf lr ng lv my mz na nb bi translated"><strong class="la io">在家中:</strong>与iOS设备上的Animoji类似，数字化身可以为家庭成员提供另一种方式，通过网络浏览器相互发送计算机生成的消息。</li><li id="3280" class="mt mu in la b lb nc lf nd lj ne ln nf lr ng lv my mz na nb bi translated"><strong class="la io">在办公室:</strong>长期以来，数字化身为远程团队成员提供了另一种沟通方式，以防他们不能出现在镜头前，但仍然希望在会议上直观地表达自己。</li></ol><h1 id="5cf9" class="ka kb in bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">参考</h1><p id="b8d1" class="pw-post-body-paragraph ky kz in la b lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv ig bi translated">非语言超载:变焦疲劳原因的理论论证。<em class="ni">技术、思想和行为</em>，美国心理协会，2021年2月23日。</p><p id="8ef8" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">[ <a class="ae jz" href="https://dl.acm.org/doi/10.1145/1518701.1518877" rel="noopener ugc nofollow" target="_blank"> 2 </a> ]杜兴瑙特，尼古拉斯等人<a class="ae jz" href="https://dl.acm.org/doi/10.1145/1518701.1518877" rel="noopener ugc nofollow" target="_blank">“身体与心灵:三个虚拟世界中的化身个性化研究。”</a> <em class="ni">池</em> (2009)。</p><p id="905d" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">【<a class="ae jz" href="https://www.humintell.com/2021/01/the-universality-of-facial-expressions-of-emotion/" rel="noopener ugc nofollow" target="_blank"> 3 </a>】松本，大卫。<a class="ae jz" href="http://www.humintell.com/2021/01/the-universality-of-facial-expressions-of-emotion/" rel="noopener ugc nofollow" target="_blank">《情感的面部表情的普遍性，2021年更新》</a> <em class="ni">休敏特尔</em>，休敏特尔，2021年1月3日<a class="ae jz" href="http://www.humintell.com/2021/01/the-universality-of-facial-expressions-of-emotion/." rel="noopener ugc nofollow" target="_blank">。</a></p><p id="f6d1" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated"><a class="ae jz" href="https://discoverthreejs.com/book/first-steps/load-models/" rel="noopener ugc nofollow" target="_blank"> 4 </a>蓝，路易。<a class="ae jz" href="https://discoverthreejs.com/book/first-steps/load-models/" rel="noopener ugc nofollow" target="_blank">“加载GlTF格式的3D模型”</a> <em class="ni">发现三个. js </em>，发现三个. js，2018年10月1日。</p><p id="8433" class="pw-post-body-paragraph ky kz in la b lb lw ld le lf lx lh li lj ly ll lm ln lz lp lq lr ma lt lu lv ig bi translated">布鲁克斯，特里。<a class="ae jz" href="http://staff.washington.edu/tabrooks/343INFO/JSMotionTween/jsMotionTween.htm" rel="noopener ugc nofollow" target="_blank"> JavaScript动画</a>，<em class="ni"> INFO 343 Web Technologies，</em>华盛顿大学，2011。</p></div></div>    
</body>
</html>