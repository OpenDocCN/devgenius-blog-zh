<html>
<head>
<title>3 Standard Activation Functions In Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中的 3 个标准激活函数</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/3-standard-activation-functions-in-neural-network-46b0a18fd14f?source=collection_archive---------24-----------------------#2020-07-05">https://blog.devgenius.io/3-standard-activation-functions-in-neural-network-46b0a18fd14f?source=collection_archive---------24-----------------------#2020-07-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d506" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">什么是激活功能，你如何应用它？</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/973939b9cdf680a6aa0615b0b65827d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*h7L-837misH5HD6d"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">克林特·王茂林在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="c723" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">神经网络分为三层，描述如下:</p><h2 id="2351" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">输入层</h2><blockquote class="ml"><p id="18bd" class="mm mn iq bd mo mp mq mr ms mt mu lr dk translated">被动层将相同的信息从单个节点传递到多个输出。这是神经网络的第一个入口。</p></blockquote></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><h2 id="8067" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">隐蔽层</h2><blockquote class="ml"><p id="227f" class="mm mn iq bd mo mp mq mr ms mt mu lr dk translated">对输入进行数学计算并产生净输入，然后应用激活函数产生输出。通常，这一层被视为一个黑盒，比其他层小得多。</p></blockquote></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><h2 id="7991" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">输出层</h2><blockquote class="ml"><p id="59d7" class="mm mn iq bd mo mp mq mr ms mt mu lr dk translated">合并并具体产生来自先前层的复杂计算的输出结果。</p></blockquote><p id="9fcc" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">如上所述，大多数神经网络由三层构成。让我们更深入地研究一下架构。每层由一个或多个<strong class="ky ir">节点组成。</strong>从输入层到输出层有一个<strong class="ky ir"> </strong>信息<strong class="ky ir"> </strong>的流程。在每个神经网络中，所使用的激活函数的类型决定了数据应该如何在神经网络中成形。激活功能的主要目标是:</p><h2 id="f9db" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">对数据应用某种非线性函数，以便输入数据不会直接映射到输出图层。</h2></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><h1 id="235c" class="nh lt iq bd lu ni nj nk lx nl nm nn ma jw no jx md jz np ka mg kc nq kd mj nr bi translated">整流线性单元</h1><p id="704a" class="pw-post-body-paragraph kw kx iq ky b kz ns jr lb lc nt ju le lf nu lh li lj nv ll lm ln nw lp lq lr ij bi translated">ReLu 是深度学习模型中最常用的激活函数。这也可以说是深度学习革命中为数不多的进步之一，例如，现在允许持续开发非常深度的神经网络的技术。</p><p id="244f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果 ReLu 函数接收到任何负的输入，它将返回 0，但是对于任何正值 x，它将返回该值。所以可以写成<em class="nx"> f(x)=max(0，x)。</em></p><p id="55e0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">ReLu 最初是由<strong class="ky ir"> Abien Fred M. Agarap </strong>在他的文章<em class="nx"> ' </em> <a class="ae kv" href="https://arxiv.org/pdf/1803.08375.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="nx">深度学习使用校正线性单元(ReLU) </em> </a>'中介绍的，作者将其描述为:</p><blockquote class="ny nz oa"><p id="feae" class="kw kx nx ky b kz la jr lb lc ld ju le ob lg lh li oc lk ll lm od lo lp lq lr ij bi translated">由此引入的激活函数具有强大的生物学和数学基础。2011 年，它被证明可以进一步改善深度神经网络的训练。它的工作原理是将阈值设为 0，即 f (x) = max(0，x)。简单来说，当 x &lt; 0, and conversely, it outputs a linear function when x ≥ 0 (<a class="ae kv" href="https://arxiv.org/pdf/1803.08375.pdf" rel="noopener ugc nofollow" target="_blank">源</a>时输出 0</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/8bbe5dd3187040efe200542693c3c047.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pRKP_SmejE_2J8t_cgCREQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">来源:https://arxiv.org/pdf/1803.08375.pdf</figcaption></figure><p id="85a5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在神经网络中使用 ReLu 有一些明显的好处:</p><ol class=""><li id="be30" class="of og iq ky b kz la lc ld lf oh lj oi ln oj lr ok ol om on bi translated">让我们再次回忆一下<em class="nx"> f(x)= max(0，x)。</em>意味着随着 x &gt; 0 <em class="nx">，</em>，<em class="nx"> </em>梯度消失的可能性降低。为了保持更快的学习，更恒定的梯度是优选的。</li><li id="c211" class="of og iq ky b kz oo lc op lf oq lj or ln os lr ok ol om on bi translated">在一些实践中，ReLu 往往表现出<strong class="ky ir">更快的收敛性能</strong>。如本文<a class="ae kv" href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf" rel="noopener ugc nofollow" target="_blank">所述，ReLu 实现的训练收敛速度比其他激活函数快六倍，以实现 25%的训练误差损失。</a></li></ol></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><h1 id="2a0c" class="nh lt iq bd lu ni nj nk lx nl nm nn ma jw no jx md jz np ka mg kc nq kd mj nr bi translated">Softmax</h1><p id="3dec" class="pw-post-body-paragraph kw kx iq ky b kz ns jr lb lc nt ju le lf nu lh li lj nv ll lm ln nw lp lq lr ij bi translated">在所有的神经网络中，输出层被设计成基于一定的置信度输出结果。让我们想象一下。</p><p id="72a7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设你有一个预测，得分为(0.04，0.96)，A 类为 0.04，B 类为 0.96。显然，网络有信心将预测识别为 a 类</p><p id="d54f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是(0.75，0.85)怎么样？我们可以说 B 类的预测有更高的“可能性”是正确的。因此，以上这些都不是概率，而 Softmax 函数恰恰提供了这一点。Softmax 函数定义为:</p><blockquote class="ny nz oa"><p id="8f85" class="kw kx nx ky b kz la jr lb lc ld ju le ob lg lh li oc lk ll lm od lo lp lq lr ij bi translated">将一个实数向量作为输入，并将其规范化为一个概率分布数组，该数组由与输入向量项的指数成比例的概率组成。这个概率分布总计为 1</p></blockquote><pre class="kg kh ki kj gt ot ou ov ow aw ox bi"><span id="70f5" class="ls lt iq ou b gy oy oz l pa pb"><strong class="ou ir">def</strong> softmax(x):                   # <em class="nx">x</em> is the input vector<br/>    e_x = np.exp(x)               # Taking the exponential of <em class="nx">x</em><br/>    <strong class="ou ir">return</strong> e_x / e_x.sum()        # Returns the probability vector</span></pre><p id="7f37" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在数学符号中，它被推导为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pc"><img src="../Images/5ff3bf4f3112de1df3e7a1cfca189865.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0fR5pqu0_30fTgs3-HcCqQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">资料来源:https://en.wikipedia.org/wiki/Softmax_function</figcaption></figure><p id="09ec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在神经网络中，Softmax 用作分类网络最后一层的激活。这是因为结果是概率分布。比如在最后一层，一旦出现[0.05，0.23，0.12，<strong class="ky ir"> 0.60 </strong>这样的概率，概率最高(0.60)的类就表示模型的最终预测。</p></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><h1 id="fb6b" class="nh lt iq bd lu ni nj nk lx nl nm nn ma jw no jx md jz np ka mg kc nq kd mj nr bi translated">双曲正切值</h1><p id="4ea0" class="pw-post-body-paragraph kw kx iq ky b kz ns jr lb lc nt ju le lf nu lh li lj nv ll lm ln nw lp lq lr ij bi translated">双曲正切激活函数有一个基本用途，它会将负输入强推至(-1，1)范围内的负输出。tanh 和 sigmoid 的区别在于 sigmoid 函数不能很好地处理负输入。在神经网络中，梯度在前馈传播过程中更新。</p><p id="914d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果输入不变，模型参数将不会持续更新，因为 sigmoid 不处理负输入，并将它们分配到接近零值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/ebb518be3440719d3a67550b90898eff.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*Q0nlcIXIrryYp8PMA0fJQg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">来源:<a class="ae kv" href="https://arxiv.org/pdf/1811.03378.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1811.03378.pdf</a></figcaption></figure><p id="7fb7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文<a class="ae kv" href="https://arxiv.org/pdf/1811.03378.pdf" rel="noopener ugc nofollow" target="_blank">来源于</a>中，作者解释了双曲正切函数对于多层网络具有更好的训练性能。并且该范围是从(-1，1)到非(0，1)的事实使得它对于大多数深度神经网络是优选的。由于双曲正切函数的平均值接近 0，意味着范围也更大，这导致更大的导数，从而使收敛性能更快地达到全局最小值，极大地减少了训练时间和复杂性。双曲正切函数通常用于递归神经网络和语音识别模型。</p></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><p id="2c86" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="nx">如果我已经设法让你注意到这一点，请留下你的评论，如果你对这个系列有任何建议，因为它将大大增加我的知识，改善我的写作方式。</em> <a class="ae kv" href="https://towardsdatascience.com/@premstroke95" rel="noopener" target="_blank"> <strong class="ky ir"> <em class="nx">普雷姆·库马尔</em> </strong> <em class="nx"> </em> </a> <em class="nx">是一个无私的学习者，对我们身边的日常数据充满热情。如果你想谈论这个故事和等待的未来发展，请在</em><a class="ae kv" href="https://www.linkedin.com/in/premstrk/" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir"><em class="nx">LinkedIn</em></strong></a><strong class="ky ir"><em class="nx"/></strong><em class="nx">上与我联系。</em></p></div></div>    
</body>
</html>