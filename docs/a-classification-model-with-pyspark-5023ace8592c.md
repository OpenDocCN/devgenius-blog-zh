# 基于 Pyspark 的分类模型

> 原文：<https://blog.devgenius.io/a-classification-model-with-pyspark-5023ace8592c?source=collection_archive---------10----------------------->

Sparkify 是 Udacity 出于教育目的创建的一个虚假流媒体音乐服务。这项流媒体服务可以免费使用(歌曲之间有广告)，也可以不订阅广告。然而，如果你订阅了付费服务，你可以随时降级或升级。这项工作的主要目标是预测用户是否会停止使用 Sparkify，不管他是免费用户还是付费用户。

![](img/5f30f2366e552299b48337a3201e39c3.png)

Sparkify 用户分布。红点表示流失用户，绿色表示活跃用户。

# 动机——项目概述

当你提供产品或服务时，你想让你的顾客关注你的品牌。出现这种情况是因为获得新客户比留住老客户成本更高。因此，理解客户为什么会停止消费服务或产品非常重要。要理解这种行为，有一个关键概念叫做*流失用户。*

流失用户是指在某个时候决定停止购买或消费某项服务的人。这个定义可以根据核心业务而改变。但是，很多时候，一个用户一个月不跟品牌互动，就会被称为流失用户。

您可能会注意到，了解客户成为流失用户的主要原因或在流失发生之前预测流失用户非常重要。这一信息变得相关，因为如果你知道用户流失，你会给他们折扣或礼物，以避免这些用户离开。

# 问题陈述

主要目标是尝试预测 Sparkify 客户是否会成为流失用户。为此，我使用了一个包含用户信息、流媒体服务(歌曲、艺术家等)和日志信息的数据库。为了分析数据，我在云上的 IBM 集群上使用了 pyspark。

主要的发现都暴露在这里，但是，要看到完整的工作，你可以查看 GitHub 资源库可用的[这里的](https://github.com/fravellaneda/sparkify-analysis)。

# 主轴承轴瓦

1.  收费和清理数据:我在数据中定义目标变量，并清理一些缺少值的列。
2.  探索性分析——数据可视化:我探索数据以理解变量是否对预测我们的目标变量有用。我也做了一些图表来更好地理解这个问题。
3.  特征工程:在回顾了变量之间的关系后，我对数据进行了一些转换，为建模做准备。
4.  建模:我在相同的数据中使用了不同的模型，看看哪个模型最合适。
5.  最佳模型—参数选择:我检查了模型在不同参数下的最佳性能。
6.  模型评估:我使用不同的度量标准来比较不同模型的性能。

# 充电和清洁数据

首先，我对数据集收费，并打印出它的模式

```
root
 |-- artist: string (nullable = true)
 |-- auth: string (nullable = true)
 |-- firstName: string (nullable = true)
 |-- gender: string (nullable = true)
 |-- itemInSession: long (nullable = true)
 |-- lastName: string (nullable = true)
 |-- length: double (nullable = true)
 |-- level: string (nullable = true)
 |-- location: string (nullable = true)
 |-- method: string (nullable = true)
 |-- page: string (nullable = true)
 |-- registration: long (nullable = true)
 |-- sessionId: long (nullable = true)
 |-- song: string (nullable = true)
 |-- status: long (nullable = true)
 |-- ts: long (nullable = true)
 |-- userAgent: string (nullable = true)
 |-- userId: string (nullable = true)
```

首先，我注意到我可以将变量分成三大组:用户、日志和歌曲艺术家信息。每一组都要进行不同的分析，因为每一组都需要特殊对待。

第二，我注意到用户信息中有大量缺失值(姓名、性别等)。我检查了这些值，发现发生这种情况是因为这些人已经注销或者是客人。我决定放弃这些值，因为它们没有给出关于流失用户的信息。

# 探索性分析—数据可视化

## 用户信息

关于用户信息，我分析了以下特征:

**会话:**这是用户拥有的唯一会话的数量。直觉告诉我们，许多会话意味着用户更关注服务，数据也证实了这一发现。

![](img/1f5cbf8e40240a39eaeb7a617e3ab880.png)

我用 t 检验来证明这是组间会议的相关证据。然而，p 值不足以证实这一假设。

**性别:**用户的生物性别。我检查了男性和女性用户的百分比。我注意到它们没有显著的区别。

![](img/03473092be01a199422d2d0a44f9ae64.png)

**位置:**不完全是位置，是用户的地址。对于这个变量，我使用了一个 *geopy* 包来获取位置(纬度和经度)。有了这些信息，我可以制作这张漂亮的地图。

![](img/cc5992fad9455a683a6e79953c7a9c4b.png)

红点是流失用户，绿色是活跃用户。

我注意到在一些州，比如旧金山，有更多的活跃用户。因此，我认为这个位置有助于了解流失用户。

**注册日期:**这是用户的注册日期。我在这个领域做了一点特征工程来提取群组(我只提取了 3 个最大的群组)。

![](img/b09fab623a07b0ec5d1a6e08ac209ed7.png)

正如你所注意到的，在团队中，流失人员的比例没有太大的差异。然而，我做了一个 t 检验来检验这一说法，p 值不够大，不足以证实这是一个显著的差异。

我还用报名日期计算了**报名天数**。这个新字段是从*2019–01–01*到注册日期之间的天数。我的直觉告诉我，新用户更容易流失。

![](img/d149bee28be298afc5a49f8a6d2c5a49.png)

这个事实是不正确的，人们平均有更多的注册天数。然而，这种差异在统计学上并不显著。

**用户代理:**该字段包含您使用的系统操作员和浏览器的信息。我用 python 包提取了两个重要特性:平台和浏览器。

![](img/7cb81e576347f5aa970d6dfd4cb9e97e.png)

关于平台差异，我使用 ANOVA 测试来检查差异是否显著。p 值非常接近低于 0.1。我认为这是因为 iOS 中的比例有不同的行为，这将有助于模型。

![](img/a2fec77fdfa5a78decb4d163af7f4234.png)

关于浏览器，两组之间没有显著的统计差异。

**级别:**我转换了这个变量，它的意思是:如果用户在任何时候使用了付费服务，则为“付费”；如果用户仅使用了免费服务，则为“无付费”。我注意到“付费”群体中有更多的流失用户。然而，两组之间没有统计学上的显著证据。

![](img/ebfd2c39a547417c7ccd6e0d9e061d11.png)

## 记录信息

关于记录信息，我对每天的会话数量进行了分组，我注意到大多数 Sparkify 客户大多在工作日使用流媒体服务。对我来说，这意味着用户可以在工作日去工作或去大学、学校等地方听音乐。另一方面，在周末，我认为用户很忙，在户外活动，不使用流媒体服务。

![](img/a5084e653e8dc6cefb1540f1ec96058f.png)

在日志记录信息中，有一个称为 page 的重要字段。此字段提供用户在会话中所做操作的相关信息。但是有 18 个不同的页面。出于这个原因，我只使用页面:拇指向上、拇指向下、添加朋友、降级、帮助和错误。

**竖起大拇指:**是竖起大拇指的次数。

![](img/aa45dbbb729102b82e3e8b8de716743b.png)

我用 t 检验来检验差异是否显著。p 值非常接近低于 0.1。我认为这是因为没有流失的用户比流失的用户更喜欢竖起大拇指。

**拇指朝下:**这是拇指朝下的数字。

![](img/6ca5714e69e1ec73fae2bca6c87ed18d.png)

**添加好友:**这是在 Sparkify 中添加的好友数量。

![](img/38329f3d96b71c2591d67a1abe635562.png)

在这个领域似乎是一个不同的客户群。然而，通过 t 检验，我发现差异并不显著。

**降级**、**帮助**和**错误**为指示变量。这意味着如果用户出错，变量 error 将为 1。这些变量不是计数变量。

**会话中的平均项目数:**这是会话中的平均项目数。

![](img/c49fd2e8528c0753ac8c3b1b2e8b5adb.png)

我用 t 检验来检验差异是否显著。p 值低于 0.05。这意味着每个会话的项目数量有显著差异。在这种情况下，在流失人群中每个会话有更多的项目。

## 艺术家-歌曲信息

为了分析这些信息，我必须做两个汇总。首先，我按用户和会话进行了汇总，统计了独特的艺术家、独特的歌曲以及歌曲长度的平均值。之后，我对这些领域的用户进行了第二次汇总，并取了平均值。

**平均艺术家:**该字段表示在一个会话中收听的不同艺术家的平均数量。

![](img/3f000dad623077ae8e240b6aea49fc95.png)

我用 t 检验来检验差异是否显著。p 值非常接近低于 0.1。这意味着流失用户平均听更多不同的艺术家。

**平均歌曲:**该字段表示在一个会话中收听的不同歌曲的平均数量。

![](img/403bc205cce6639903c86e5704dfe1a2.png)

我用 t 检验来检验差异是否显著。p 值低于 0.1。这意味着流失用户平均听更多不同的歌曲。

**平均长度:**该字段表示在一个会话中所听歌曲的平均长度的平均值。

![](img/f06c419bbb4b645d219feb150c208ed2.png)

我用 t 检验来检验差异是否显著。p 值非常高。这意味着各组歌曲的长度没有差别。

# 特征工程

在回顾了一些变量和目标变量之间的关系后，我做了一些特征工程来将所有变量转换成数字格式。最后，我将用于建模的变量是:

```
- sessions
- genderM
- latitude
- longitude
- registrationDays
- julyCohort
- augustCohort
- septemberCohort
- paid
- windows
- macOs
- linux
- ios
- firefox
- chrome
- safari
- microsoftInternetExplorer
- thumbsUp
- thumbsDown
- friends
- downgrade
- help
- error
- avgItemInSession
- avgArtist
- avgSong
- avgLength
- churn
```

# 建模

我用了不同的型号，看看哪一种性能最好。我使用的模型是:

*   逻辑回归，正则化参数[0，0.0001，0.001，0.01，0.1]
*   最大深度参数为[2，5，10，15，20]的决策树。
*   随机森林，树木数量的参数为[10，15，20]，最大深度为[2，3，4，5]。
*   基于梯度的，最大迭代的参数为[10，20，30]，步长为[0.001，0.01，0.1]。
*   树和多层感知器，最大迭代的密集层[27，16，8，4，2]和参数[100，200，300，500]以及步长[0.001，0.01，0.1，0.3]。

# 韵律学

我还使用了不同的指标来比较这些模型

*   准确(性)
*   精度(加权精度)
*   召回(加权召回)
*   F1 分数(加权 F1 分数)

在下图中，您将找到关于每一个的简要说明。

![](img/c7696432422265208c2eb6d460731ea6.png)

我将检查模型中的所有这些指标。但是，我会用 F1 来选择最好的模型。我将使用这个指标，因为它给了我精确度(假阳性)和召回率(假阴性)之间的良好平衡，并且它对于不平衡的数据非常有用。

# 最佳模型—参数选择

经过度量评估，每个模型的最佳参数是:

*   物流回归，用参数 **0** 进行正则化
*   决策树，最大深度参数 **2** 。
*   随机森林，参数 **10** 为树木数量，参数 **2** 为最大深度。
*   基于梯度的，最大迭代次数的参数 **10** ，步长的参数 **0.001** 。
*   树和多层感知器，具有最大迭代的密集层[27，16，8，4，2]和参数 **100** 以及步长的 **0.001** 。

# 模型评估

在下图中，您可以看到我使用的不同模型和指标的结果。

![](img/792bc75241683c201ab2e015406bdb68.png)

如您所见，根据 F1 得分，正则化参数为 0 的逻辑回归比其他模型表现更好。我做这个决定也是因为逻辑回归是一个简单的模型，你可以很容易地解释。随机森林、梯度树和多层感知器模型需要很长时间来运行。

# 正当理由；辩解

在这篇文章中，我分析并开发了一个预测用户流失的模型。为了完成这项工作，我使用一些可视化方法分析了数据集，并做了一些统计测试来证明一些假设。我使用了所有我认为有助于模型预测的变量。但是，我认为对于一个度量标准的改进来说，减少变量是必要的。

关于模型，我认为逻辑回归并没有过度拟合数据集。事实上，我认为这个模型很好地概括了用户流失行为。

# 结论和改进

在这个项目中，我研究了名为 Sparkify 的 udacity 流媒体服务的流失用户的行为。大部分工作是理解问题，并在数据中找到可能的转换，以创建或改进字段，从而获得更多信息或它们与目标变量之间的更多关系。

这项工作的另一个重要部分是考虑简单，因为大多数脚本需要很长时间来运行。然而，这还不是最糟糕的部分。对我来说，最糟糕的部分是有时集群停止或断开，您必须重新运行所有脚本。这是我花了很长时间做这个分析的原因之一。

另一方面，我认为有可能改进模型的性能。其中一种方法是减少模型中的变量。我认为在逻辑回归中使用 p 值的信息可以做到这一点。

如果你在这里，谢谢你阅读我。如果你有问题，可以发邮件到 fr_avellaneda@hotmail.com 联系我。还要记住，这里有一个 GitHub 存储库，其中有关于分析的所有信息。