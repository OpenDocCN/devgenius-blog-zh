<html>
<head>
<title>Getting started with Deep Learning : 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习入门:2</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/getting-started-with-deep-learning-2-4dfee5a631ab?source=collection_archive---------10-----------------------#2022-10-27">https://blog.devgenius.io/getting-started-with-deep-learning-2-4dfee5a631ab?source=collection_archive---------10-----------------------#2022-10-27</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><p id="f264" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">在本文中，我们将学习激活功能。</p><h2 id="2498" class="kj kk in bd kl km kn dn ko kp kq dp kr jv ks kt ku jz kv kw kx kd ky kz la lb bi translated">激活/转移功能</h2><p id="682c" class="pw-post-body-paragraph jk jl in jm b jn lc jp jq jr ld jt ju jv le jx jy jz lf kb kc kd lg kf kg kh ig bi translated">这些功能决定了神经元是否应该被激活。它将来自节点的加权输入之和转换为输出值，作为输出提供给下一层。</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi lh"><img src="../Images/a4b50ff381ebc61e5f90e2ea9f7cc387.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/0*5UTdbYKQOzsSD-vR"/></div></figure><h2 id="a653" class="kj kk in bd kl km kn dn ko kp kq dp kr jv ks kt ku jz kv kw kx kd ky kz la lb bi translated">激活功能的需求</h2><ul class=""><li id="299e" class="lp lq in jm b jn lc jr ld jv lr jz ls kd lt kh lu lv lw lx bi translated">向神经网络添加非线性；因为一次又一次地增加输入只会意味着输入之间的线性关系，而不会有学习。</li><li id="d9fc" class="lp lq in jm b jn ly jr lz jv ma jz mb kd mc kh lu lv lw lx bi translated">激活函数的选择对神经网络的能力和性能有很大的影响，并且不同的激活函数可以用于模型的不同部分。</li></ul><h1 id="36b5" class="md kk in bd kl me mf mg ko mh mi mj kr mk ml mm ku mn mo mp kx mq mr ms la mt bi translated">激活功能的类型</h1><ol class=""><li id="9d3f" class="lp lq in jm b jn lc jr ld jv lr jz ls kd lt kh mu lv lw lx bi translated">二元阶跃函数</li></ol><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/1b8941b33f9c8c5b3ce4c611475a033b.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/0*45vq96BireEdSwwx"/></div></figure><p id="7600" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">二元阶跃函数依赖于决定神经元是否应该被激活的阈值。<br/>馈入激活函数的输入与某个阈值进行比较；如果输入大于它，那么神经元被激活，否则它被停用，这意味着它的输出不会传递到下一个隐藏层。阶跃函数的梯度为零，这在反向传播过程中造成阻碍。</p><p id="167a" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">2.二元 Sigmoid 函数</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/81842ea7fc5809d99e74eb831bd232d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*K1t0x1_s0pQP116H.jpg"/></div><figcaption class="mx my gj gh gi mz na bd b be z dk translated">二元 Sigmoid 图</figcaption></figure><p id="dc64" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">也称为物流/挤压功能。该函数的范围从 0 到 1，呈 S 形。它用于 DNN 的输出图层，并用于基于概率的输出。因为任何事情的概率只存在于 0 和 1 之间，所以 sigmoid 是正确的选择，因为它的范围。</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/2606bbd51fb96df0ace3fdc3a08fecf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:340/format:webp/1*2DWp9Ls3ZNToRycn9KLGsw.png"/></div><figcaption class="mx my gj gh gi mz na bd b be z dk translated">二元 Sigmoid 函数</figcaption></figure><p id="67a2" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">缺点:反向传播期间的急剧阻尼梯度、梯度饱和、缓慢收敛以及非零中心输出，从而导致梯度更新在不同方向上传播。</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/6a0d981056cfd383ebbf96150711d9fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/0*METQ9ijIoPmFj9UB"/></div><figcaption class="mx my gj gh gi mz na bd b be z dk translated"><em class="nd">函数的导数为</em><strong class="bd kl">f '(x)= sigmoid(x)*(1-sigmoid(x))</strong></figcaption></figure><p id="2a36" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">从上图中我们可以看出，梯度值仅在<strong class="jm io">范围-3 到 3 </strong>内有意义，在其他区域图形变得更加平坦。这意味着对于大于 3 或小于-3 的值，函数将具有非常小的梯度。当梯度值接近零时，网络停止学习并遭受<strong class="jm io"> <em class="ki">消失梯度</em> </strong>问题。</p><p id="360c" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">3.双极 Sigmoid 函数</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi mw"><img src="../Images/dec34b70f172192be24699b7016419f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*zpfImuj1If_yktrD.jpg"/></div></div></figure><p id="c379" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">sigmoid 函数的值的范围可以根据应用而变化。但是，最常用的范围是(-1，+1)。</p><p id="2bc7" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">4.双曲正切函数</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/b050208a75f596513377a62b0cf1aceb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/0*6UHZ4H9dpghX6TAD"/></div></figure><p id="0dc2" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">它具有相同的 S 形(S 形/逻辑激活函数),输出范围的差异为-1 比 1。在 tanh 中，输入越大(越正)，输出值越接近 1.0，而输入越小(越负)，输出越接近-1.0。它本质上是两极的。这是一种被称为<strong class="jm io">反向传播网络的特殊类型的神经网络的广泛采用的激活函数。</strong></p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/3951cf4bb5da5376b34fb9544387b991.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/0*8T6aBo_ruAUaaIy-"/></div><figcaption class="mx my gj gh gi mz na bd b be z dk translated">tanh 函数</figcaption></figure><p id="5110" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">由于此函数以零为中心，因此更容易对具有强负值、中性值和强正值的输入进行建模。</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/6e14ebedbfb631ec0b10822d5ae0e958.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*53Z7ywCCKsG337onDGckPg.png"/></div><figcaption class="mx my gj gh gi mz na bd b be z dk translated">双曲正切函数的导数</figcaption></figure><p id="c740" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">5.ReLU 函数</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/0b7ee69b6c8b89b562b3ee2f5817f236.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*1VwgZSKPKePoxuhQfcTdhQ.png"/></div><figcaption class="mx my gj gh gi mz na bd b be z dk translated">Relu 函数</figcaption></figure><p id="7bf6" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">深度学习中使用最广泛的激活函数。它在性能和通用性方面更胜一筹，同时提高了计算和通信的整体速度，因为它不计算指数和除法。它很容易优化，因此在实践中它总是优于 Sigmoid 函数。</p><p id="c5c1" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">缺点:它还面临着类似于 sigmoid 激活函数的消失梯度问题。</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/136691b88a63fbc6c485a58afe1cec6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/0*uamCNQk5dqEq8pz2"/></div></figure><p id="f064" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">濒死热卢问题:</strong>图形的负边使渐变值为零。由于这个原因，在反向传播过程中，一些神经元的权重和偏差没有被更新。这会产生永远不会被激活的死亡神经元。所有负输入值立即变为零，这降低了模型根据数据进行适当拟合或训练的能力。</p><p id="73f8" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">6.泄漏 ReLU 函数</p><p id="64fe" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">该函数是 ReLUto solve 的改进版本，用于解决<strong class="jm io">死亡 ReLU 问题</strong>，因为它在负区域也有小的正斜率。它支持负值的反向传播，并且与 ReLU 一样具有优势。</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/375e36bf0830b412880cd1d7738d3e0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/0*s7q1b-KYuaBHWVmB"/></div><figcaption class="mx my gj gh gi mz na bd b be z dk translated">泄漏 ReLU 函数</figcaption></figure><p id="e983" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">通过对负输入值进行这种微小的修改，图形左侧的梯度变成了一个非零值。因此，我们在那个区域不会再遇到死亡的神经元。</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi no"><img src="../Images/35460d4401d46def055ec2c3e04756fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/0*tFbeyrg5XoN_Dkf-"/></div><figcaption class="mx my gj gh gi mz na bd b be z dk translated">泄漏 ReLU 的推导</figcaption></figure><p id="4566" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">7.参数 ReLU 函数</p><p id="d4c6" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">另一个旨在解决<strong class="jm io">将死的 ReLU 问题的函数。</strong>该函数提供函数负部分的斜率作为参数 a。<em class="ki">通过执行反向传播，学习到最合适的 a 值。</em></p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi np"><img src="../Images/86107127af39345bb84ebf81da1e723c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/0*O2DOD3FcQcVI_Tgg"/></div><figcaption class="mx my gj gh gi mz na bd b be z dk translated">参数 ReLU 函数:<strong class="bd kl"> f(x) = max(ax，x) </strong></figcaption></figure><p id="5296" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">当泄漏 ReLU 函数在解决死神经元问题上仍然失败，并且相关信息没有成功传递到下一层时，使用参数化 ReLU 函数。</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nq"><img src="../Images/6dd2c2dd3818df639a8510f549edecfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/0*-bVptF1AoQ4sEiYL"/></div></div><figcaption class="mx my gj gh gi mz na bd b be z dk translated">参数 ReLU 的导数</figcaption></figure><p id="5051" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">该功能的局限性在于，根据斜率参数<strong class="jm io"> <em class="ki"> a. </em> </strong>的值，对于不同的问题会有不同的表现</p><p id="51d6" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">8.指数线性单位</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/d12556661e79756a8cb0a8c7757fad8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*TjUhXg-0jF1ojRWW2jtN1A.png"/></div><figcaption class="mx my gj gh gi mz na bd b be z dk translated">ELU 函数</figcaption></figure><p id="0202" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">它通过在训练期间将平均激活推向零来减少偏差，ELU 代表了 ReLU 的一个很好的替代方案。ELU 的一个局限性是 ELU 不会将值集中在零上。</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/ea9e101be11418be06af1f8e10a2bdc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*nFC9ZEYsYJQuZ75PfawsUg.png"/></div><figcaption class="mx my gj gh gi mz na bd b be z dk translated">ELU 函数的导数</figcaption></figure><p id="cbfd" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">由于以下优势，ELU 是 ReLU 的强有力替代产品:</p><ul class=""><li id="04c8" class="lp lq in jm b jn jo jr js jv nt jz nu kd nv kh lu lv lw lx bi translated">ELU 慢慢变得平滑，直到其输出等于-α，而 RELU 急剧平滑。</li><li id="c7f0" class="lp lq in jm b jn ly jr lz jv ma jz mb kd mc kh lu lv lw lx bi translated">通过引入输入负值的对数曲线，避免了死 ReLU 问题。它有助于网络向正确的方向推动权重和偏好。</li></ul><p id="7557" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">9.Softmax 函数</p><p id="76e4" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">它是一个函数，将 K 个实数值的向量转换为 K 个实数值的向量，其和为 1。输入值可以是正数、负数、零或大于 1 的值，但是 softmax 会将它们转换为介于 0 和 1 之间的值，以便可以将它们解释为概率。</p><p id="dfed" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">如果其中一个输入很小或为负，softmax 会将其转换为小概率，如果输入很大，则会将其转换为大概率，但它将始终保持在 0 和 1 之间。在多类分类的情况下，它最常用作神经网络最后一层的激活函数。</p><p id="7155" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">它计算相对概率。类似于 sigmoid/logistic 激活函数，SoftMax 函数返回每个类的概率。</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/4006ef569b9ebfca4afd2fd11d2c7dc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/0*6JvEjNY-zIOTyHYu"/></div><figcaption class="mx my gj gh gi mz na bd b be z dk translated">此处指数-&gt;指数</figcaption></figure><p id="c931" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">softmax 输出小于 1，softmax 功能的输出总和为 1。因此，可以说 softmax 函数的输出是一个<strong class="jm io">概率分布</strong>。由于这一特性，Softmax 函数被认为是神经网络和算法(如多项式逻辑回归)中的激活函数。注意，对于二元逻辑回归，使用的激活函数是 sigmoid 函数。</p><p id="9e52" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">10.Swish 函数</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/e474e427be118e6f9ed3f92e12f27ac2.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*qs_bwzfvKmhLRtpBBSkefQ.png"/></div><figcaption class="mx my gj gh gi mz na bd b be z dk translated">Swish 函数</figcaption></figure><p id="631b" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">它是由<em class="ki">s 形激活函数</em>和输入函数组合提出的第一个复合<em class="ki">激活函数</em>之一，以实现一个<em class="ki">混合激活函数</em>。Swish 函数的性质包括<em class="ki">光滑，非单调，下面有界</em>，上限无界。</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/3eec3a46f6fd28d497f3afb1b925b097.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/0*82TXHzBAVnLU9WcK"/></div><figcaption class="mx my gj gh gi mz na bd b be z dk translated">Swish 函数的导数</figcaption></figure><p id="28d8" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">11.高斯误差线性单位</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/80a76f8df26536de04eed14716dc1ae5.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*kJym0AC7W29HgFrUuHoB9Q.png"/></div><figcaption class="mx my gj gh gi mz na bd b be z dk translated">GELU 函数</figcaption></figure><p id="7c51" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">高斯误差线性单元(GELU)激活函数与 BERT、ROBERTa、ALBERT 和其他顶级 NLP 模型兼容。这个激活函数是由 dropout、zoneout 和 ReLUs 的属性组合而成的。</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/d23ba005e307962a3f9f072f4e976f53.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/0*AO5pCL24C441tyg3"/></div><figcaption class="mx my gj gh gi mz na bd b be z dk translated">格律派生词</figcaption></figure><p id="d3cf" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">12.标度指数线性单位</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/db84ed118eba83cd0dff9f0960aca056.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*4sA6VLhHKqaLeK9T19iKuQ.png"/></div><figcaption class="mx my gj gh gi mz na bd b be z dk translated">SELU 函数</figcaption></figure><p id="5426" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">SELU 是在自归一化网络中定义的，并负责内部归一化，这意味着每一层都保留了前一层的均值和方差。SELU 通过调整均值和方差来实现这种标准化。</p><p id="c66a" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">SELU 有正值和负值来移动平均值，这对于 ReLU 激活功能是不可能的，因为它不能输出负值。</p><p id="7ac0" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">梯度可以用来调整方差。激活函数需要梯度大于 1 的区域来增加它。SELU 具有预定义的α值和λ值。</p><p id="c997" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">以下是 SELU 相对于 ReLU 的主要优势:</p><ul class=""><li id="0be2" class="lp lq in jm b jn jo jr js jv nt jz nu kd nv kh lu lv lw lx bi translated">内部归一化比外部归一化更快，这意味着网络收敛更快。</li><li id="8043" class="lp lq in jm b jn ly jr lz jv ma jz mb kd mc kh lu lv lw lx bi translated">SELU 是一个相对较新的激活函数，需要更多关于架构的论文，如 CNN 和 RNNs，在那里它被比较深入地研究。</li></ul><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/ca794deeed60a67f0e94373d3a3cb189.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/0*0aKMpUlGhNKd2EeH"/></div><figcaption class="mx my gj gh gi mz na bd b be z dk translated">SELU 的衍生物</figcaption></figure></div></div>    
</body>
</html>