<html>
<head>
<title>WordCount using PySpark and HDFS</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 PySpark 和 HDFS 的字数统计</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/wordcount-using-pyspark-and-hdfs-2567367beb63?source=collection_archive---------8-----------------------#2022-11-06">https://blog.devgenius.io/wordcount-using-pyspark-and-hdfs-2567367beb63?source=collection_archive---------8-----------------------#2022-11-06</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><figure class="gl gn jl jm jn jo gh gi paragraph-image"><div class="gh gi jk"><img src="../Images/48249648399526935f1459ec41df3a54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*CR2yYZE1oXOs_ecaSCuC7g.jpeg"/></div></figure><h2 id="bace" class="jr js in bd jt ju jv dn jw jx jy dp jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">介绍</h2><p id="1e0e" class="pw-post-body-paragraph kn ko in kp b kq kr ks kt ku kv kw kx ka ky kz la ke lb lc ld ki le lf lg lh ig bi translated">这些年来，由于<em class="li">社交媒体应用</em>、<em class="li">在线交易</em>和<em class="li">物联网</em>的<em class="li">开发</em>，生成的<em class="li">数据</em>的<em class="li">量</em>和生成的<em class="li">各种</em> <em class="li">格式</em>出现了急剧的<em class="li">增长。这种被称为“<strong class="kp io"><em class="li"/></strong>”的<em class="li"/>巨量<em class="li"/><em class="li">数据</em>使得<em class="li">传统数据库系统</em>几乎不可能在有限的时间内处理它，从而产生了<em class="li">分布式数据存储</em>和<em class="li">实时分布式处理</em>。</em></p><p id="99b8" class="pw-post-body-paragraph kn ko in kp b kq lj ks kt ku lk kw kx ka ll kz la ke lm lc ld ki ln lf lg lh ig bi translated"><strong class="kp io"> <em class="li"> Apache Hadoop </em> </strong>是一个<em class="li">开源，</em> <em class="li">分布式框架</em>用于<em class="li">存储</em>和<em class="li">处理</em>的<em class="li">大数据应用</em>和<strong class="kp io"> <em class="li"> Spark </em> </strong>是一个<em class="li">开源执行引擎，计算框架</em>用于<em class="li">分布式数据处理</em>。<strong class="kp io"> <em class="li"> Pyspark </em> </strong>是与<em class="li"> Apache Spark </em>合作的<em class="li"> python 库/API </em>。</p><figure class="lp lq lr ls gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi lo"><img src="../Images/a4fe0235a2ee9f05c14ef7d12cb89424.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*puFE4ZdwWMebPuRoKNmSfQ.png"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk translated">资料来源:Databricks</figcaption></figure><h2 id="60de" class="jr js in bd jt ju jv dn jw jx jy dp jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">履行</h2><p id="de3a" class="pw-post-body-paragraph kn ko in kp b kq kr ks kt ku kv kw kx ka ky kz la ke lb lc ld ki le lf lg lh ig bi translated">在本文中我们将了解如何使用<strong class="kp io"> <em class="li"> PySpark </em> </strong>执行一个简单的<em class="li">字数</em> <em class="li">程序</em>。我们将对其执行<em class="li">字数统计</em>的<em class="li">输入文件</em>将存储在<strong class="kp io"> <em class="li"> Hadoop 分布式文件系统(HDFS)上。</em>T79】</strong></p><p id="029c" class="pw-post-body-paragraph kn ko in kp b kq lj ks kt ku lk kw kx ka ll kz la ke lm lc ld ki ln lf lg lh ig bi translated">让我们对文本文件进行<em class="li">预览</em>，我们将在这些文本文件上运行我们的<em class="li">字数统计</em>程序。这些文本文件目前在我们的本地文件系统中。</p><p id="6657" class="pw-post-body-paragraph kn ko in kp b kq lj ks kt ku lk kw kx ka ll kz la ke lm lc ld ki ln lf lg lh ig bi translated">文件 1.txt</p><pre class="lp lq lr ls gt mb mc md me aw mf bi"><span id="ee31" class="jr js in mc b gy mg mh l mi mj">this is a sample input text file for wordcount program<br/>wordcount program is being implemented using pyspark<br/>text file will be stored on hdfs<br/>hdfs is a distributed file system</span></pre><p id="d68e" class="pw-post-body-paragraph kn ko in kp b kq lj ks kt ku lk kw kx ka ll kz la ke lm lc ld ki ln lf lg lh ig bi translated">文件 2.txt</p><pre class="lp lq lr ls gt mb mc md me aw mf bi"><span id="1397" class="jr js in mc b gy mg mh l mi mj">spark is an execution engine<br/>pyspark library of python is used for data processing<br/>pyspark is opensource</span></pre><figure class="lp lq lr ls gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi mk"><img src="../Images/9ec0a5bb8d1ae38d55e1183b04801f02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lHE2impow9HWrhUzUMQXNQ.png"/></div></div></figure><p id="b17c" class="pw-post-body-paragraph kn ko in kp b kq lj ks kt ku lk kw kx ka ll kz la ke lm lc ld ki ln lf lg lh ig bi translated">这里我们将在一个<em class="li">单节点集群</em>上运行<strong class="kp io"> <em class="li"> Hadoop </em> </strong>。上面的设置包括在<em class="li">虚拟箱</em>上设置<em class="li"> ubuntu 20.04 </em>和<em class="li">安装单节点 Hadoop 集群</em>。Hadoop 生态系统由<strong class="kp io"> <em class="li"> Hadoop 分布式文件系统(HDFS) </em> </strong>组成，它基本上是一个<em class="li">分布式文件系统</em>。</p><p id="c012" class="pw-post-body-paragraph kn ko in kp b kq lj ks kt ku lk kw kx ka ll kz la ke lm lc ld ki ln lf lg lh ig bi translated">格式化配置好的 HDFS 后，我们需要通过命令<em class="li"> start-dfs.sh </em>启动分布式文件系统，该命令启动<em class="li">NameNode</em>和<em class="li">datanode</em>，或者我们也可以通过执行以下命令一次启动所有<em class="li"> Hadoop 守护进程</em>:</p><pre class="lp lq lr ls gt mb mc md me aw mf bi"><span id="1e1b" class="jr js in mc b gy mg mh l mi mj">$ start-all.sh</span></pre><figure class="lp lq lr ls gt jo gh gi paragraph-image"><div class="gh gi jk"><img src="../Images/9aade4f0a0bd46043ed1ca019a882460.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*I3o1btDHM4Z-LCwaGXDJtQ.png"/></div></figure><p id="5ad3" class="pw-post-body-paragraph kn ko in kp b kq lj ks kt ku lk kw kx ka ll kz la ke lm lc ld ki ln lf lg lh ig bi translated">打开<em class="li"> hadoop 集群</em>，为了将输入文件从<em class="li">本地文件系统</em>转移到<em class="li"> Hadoop 文件系统</em>，我们执行命令:</p><pre class="lp lq lr ls gt mb mc md me aw mf bi"><span id="9096" class="jr js in mc b gy mg mh l mi mj">$ hdfs dfs -put &lt;path of file in local file system&gt; &lt;path in hdfs&gt;</span></pre><figure class="lp lq lr ls gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ml"><img src="../Images/5b507dacd6798c94d848799a6f652b4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rSdZXGCJAUW3c-Llg3jMjw.png"/></div></div></figure><p id="1c6d" class="pw-post-body-paragraph kn ko in kp b kq lj ks kt ku lk kw kx ka ll kz la ke lm lc ld ki ln lf lg lh ig bi translated">通过<em class="li">列出 HDFS </em>的文件，我们可以看到<em class="li">输入文件</em>已经<em class="li">成功插入<em class="li">/输入</em>目录<strong class="kp io"> <em class="li"> Hadoop 文件系统</em> </strong>。</em></p><pre class="lp lq lr ls gt mb mc md me aw mf bi"><span id="637e" class="jr js in mc b gy mg mh l mi mj">$ hdfs dfs -ls /input</span></pre><p id="13ea" class="pw-post-body-paragraph kn ko in kp b kq lj ks kt ku lk kw kx ka ll kz la ke lm lc ld ki ln lf lg lh ig bi translated">让我们看一下<strong class="kp io"> <em class="li"> pyspark </em> </strong>为<em class="li"> wordcount </em>程序逐行处理文件的代码。</p><pre class="lp lq lr ls gt mb mc md me aw mf bi"><span id="5a29" class="jr js in mc b gy mg mh l mi mj">spark = SparkSession.builder.appName('wordcount').getOrCreate()</span></pre><p id="533b" class="pw-post-body-paragraph kn ko in kp b kq lj ks kt ku lk kw kx ka ll kz la ke lm lc ld ki ln lf lg lh ig bi translated">作为<em class="li"> PySpark 应用程序</em>的<em class="li">入口点</em>的<strong class="kp io"> <em class="li"> SparkSession </em> </strong>是使用<em class="li">构建器设计模式</em>创建的，该模式又具有一个<strong class="kp io"><em class="li">spark context</em></strong>(spark 功能的入口点)，它代表了到<em class="li"> spark 集群</em>的<em class="li">连接</em>。</p><p id="97e5" class="pw-post-body-paragraph kn ko in kp b kq lj ks kt ku lk kw kx ka ll kz la ke lm lc ld ki ln lf lg lh ig bi translated">接下来我们使用<em class="li"> SparkContext 对象</em>的<em class="li"> textFile() </em>函数从<em class="li">数据源</em>中<em class="li">读取</em>数据<em class="li">。它可以是一个<em class="li">本地文件系统</em>、<em class="li"> HDFS </em>(就像在我们的例子中一样)、<em class="li">亚马逊 s3 </em>或任何<em class="li">数据存储</em>。</em></p><pre class="lp lq lr ls gt mb mc md me aw mf bi"><span id="952c" class="jr js in mc b gy mg mh l mi mj">rdd = spark.sparkContext.textFile("hdfs://localhost:9000/input")</span></pre><p id="233c" class="pw-post-body-paragraph kn ko in kp b kq lj ks kt ku lk kw kx ka ll kz la ke lm lc ld ki ln lf lg lh ig bi translated">从文本文件加载的数据将以<strong class="kp io"><em class="li">【RDD】(弹性分布式数据集)</em> </strong>的形式存在。因为我们需要每一行的所有单词，所以我们使用了一个<em class="li">平面映射转换</em>，它在一个 RDD 中按照空格分割每个记录，最后<em class="li">将</em>展平成一个<em class="li">单个列表</em>。</p><blockquote class="mm mn mo"><p id="0fb5" class="kn ko li kp b kq lj ks kt ku lk kw kx mp ll kz la mq lm lc ld mr ln lf lg lh ig bi translated">RDD 代表弹性分布式数据集，是跨集群中的节点分区的数据元素的分布式集合。它们用于执行内存中的计算。它们是不可变的，即一旦创建就不能改变，但是可以对它们执行不同的操作，例如转换和动作。</p></blockquote><pre class="lp lq lr ls gt mb mc md me aw mf bi"><span id="6dcf" class="jr js in mc b gy mg mh l mi mj">rdd = rdd.flatMap(lambda word:word.split())</span></pre><p id="00de" class="pw-post-body-paragraph kn ko in kp b kq lj ks kt ku lk kw kx ka ll kz la ke lm lc ld ki ln lf lg lh ig bi translated">接下来我们使用<em class="li">映射转换</em>，我们将<em class="li">每个单词</em>映射到它的<em class="li">出现处</em>。这导致了<strong class="kp io"> <em class="li">对的创建，其中<em class="li">键</em>是<em class="li">字</em>，而<em class="li">值</em>是<em class="li">出现(整数 1) </em>。</em></strong></p><pre class="lp lq lr ls gt mb mc md me aw mf bi"><span id="32f8" class="jr js in mc b gy mg mh l mi mj">rdd = rdd.map(lambda word:(word,1))</span></pre><p id="4ce9" class="pw-post-body-paragraph kn ko in kp b kq lj ks kt ku lk kw kx ka ll kz la ke lm lc ld ki ln lf lg lh ig bi translated">最后，我们使用<em class="li">关联归约函数</em>对<em class="li">值</em>进行到<em class="li">的</em> <em class="li">值</em>组合，在我们的例子中，关联归约函数对具有<em class="li">相同关键字</em>的<em class="li">值</em>应用<em class="li">求和函数</em>。</p><pre class="lp lq lr ls gt mb mc md me aw mf bi"><span id="fa79" class="jr js in mc b gy mg mh l mi mj">rdd = rdd.reduceByKey(lambda x,y:(x+y))</span></pre><p id="8a2c" class="pw-post-body-paragraph kn ko in kp b kq lj ks kt ku lk kw kx ka ll kz la ke lm lc ld ki ln lf lg lh ig bi translated">接下来，我们将<em class="li">结果 RDD </em>保存到<em class="li">目的地</em>。这里的目的地可以是一个<em class="li">本地文件系统</em>、<em class="li"> HDFS </em>(在我们的例子中是/word count out 目录)、<em class="li"> Amazon s3 </em>或任何<em class="li">数据存储。</em></p><pre class="lp lq lr ls gt mb mc md me aw mf bi"><span id="50d0" class="jr js in mc b gy mg mh l mi mj">rdd.saveAsTextFile("hdfs://localhost:9000/wordcountoutput")</span></pre><p id="c227" class="pw-post-body-paragraph kn ko in kp b kq lj ks kt ku lk kw kx ka ll kz la ke lm lc ld ki ln lf lg lh ig bi translated">接下来，我们使用命令停止<em class="li">火花会话</em></p><pre class="lp lq lr ls gt mb mc md me aw mf bi"><span id="87e7" class="jr js in mc b gy mg mh l mi mj">spark.stop()</span></pre><p id="c763" class="pw-post-body-paragraph kn ko in kp b kq lj ks kt ku lk kw kx ka ll kz la ke lm lc ld ki ln lf lg lh ig bi translated">包括以上所有步骤，让我们最后看一下我们的 wordcount.py</p><figure class="lp lq lr ls gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ms"><img src="../Images/c66061e5051d792668a6ede1ac67de86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6218-3x2BUW2T9F2MvEndw.png"/></div></div></figure><p id="951e" class="pw-post-body-paragraph kn ko in kp b kq lj ks kt ku lk kw kx ka ll kz la ke lm lc ld ki ln lf lg lh ig bi translated">为了<em class="li">执行</em>我们的<em class="li"> python 脚本</em>，我们运行命令</p><pre class="lp lq lr ls gt mb mc md me aw mf bi"><span id="3699" class="jr js in mc b gy mg mh l mi mj">$ spark-submit wordcount.py</span></pre><p id="e2cd" class="pw-post-body-paragraph kn ko in kp b kq lj ks kt ku lk kw kx ka ll kz la ke lm lc ld ki ln lf lg lh ig bi translated">当<em class="li">成功执行<em class="li">火花任务</em>的</em>后，我们可以在我们的<em class="li">输出目录</em>中找到<em class="li">结果</em>。我们可以观察到有 3 个<em class="li">输出文件</em>，基本上取决于映射器/缩减器任务的<em class="li">数量，而映射器/缩减器任务</em>又取决于<em class="li">RDD</em>的<em class="li">分区</em>数量。每个任务在单个分区上运行，如果没有明确指定，RDD 的分区数量取决于底层的<em class="li">默认并行级别</em>。</p><figure class="lp lq lr ls gt jo gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/d15977ff14951a430cd8f889687d69ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*0MsMMc8Giw2JJYaC-I5riQ.png"/></div></figure><figure class="lp lq lr ls gt jo gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/c509897f4b585805e9c3847171a10bc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4bgTMwYnslyexcgv-v8hgg.png"/></div></figure><p id="9ce9" class="pw-post-body-paragraph kn ko in kp b kq lj ks kt ku lk kw kx ka ll kz la ke lm lc ld ki ln lf lg lh ig bi translated">我们还可以通过在我们的<strong class="kp io"><em class="li">reduceByKey()</em></strong><em class="li">转换</em>中显式地提及<strong class="kp io"><em class="li">num partitions</em></strong>来配置输出，使其呈现在一个文件中。如果未指定，输出将使用<em class="li">默认并行度进行分区。</em></p><pre class="lp lq lr ls gt mb mc md me aw mf bi"><span id="f11b" class="jr js in mc b gy mg mh l mi mj">#rdd=rdd.reduceByKey(function,number of partitions)<br/>rdd=rdd.reduceByKey(lambda x,y:(x+y),1)</span></pre><figure class="lp lq lr ls gt jo gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/b37a062c0577603f3b2e666451abaca0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*0XqEKlMoGFtAh5F7OmGdhQ.png"/></div></figure><p id="b26d" class="pw-post-body-paragraph kn ko in kp b kq lj ks kt ku lk kw kx ka ll kz la ke lm lc ld ki ln lf lg lh ig bi translated">由于我们已经在 reduceByKey()转换中将 numPartitions 指定为 1，我们可以观察到<em class="li">最终输出</em>出现在<em class="li">单个文件</em>中。</p><h2 id="a905" class="jr js in bd jt ju jv dn jw jx jy dp jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">结论</h2><p id="6ee1" class="pw-post-body-paragraph kn ko in kp b kq kr ks kt ku kv kw kx ka ky kz la ke lb lc ld ki le lf lg lh ig bi translated"><strong class="kp io"> <em class="li"> Spark </em> </strong>还为我们提供了一个与<em class="li"> python </em>交互的<em class="li">shell</em>称为<strong class="kp io"> <em class="li"> Pyspark shell </em> </strong>可以直接从<em class="li">命令行</em>启动，与默认的<em class="li"> SparkContext </em>作为<em class="li"> sc </em>和<em class="li"> SparkSession </em>作为<em class="li">进行交互</em></p><figure class="lp lq lr ls gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi mw"><img src="../Images/02c269b8c944a5055b5e37a5a188462a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-rafo5YR_oVnsVue8dF-3w.png"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk translated">Pyspark 外壳</figcaption></figure><p id="34d7" class="pw-post-body-paragraph kn ko in kp b kq lj ks kt ku lk kw kx ka ll kz la ke lm lc ld ki ln lf lg lh ig bi translated">所以现在我们知道如何使用<em class="li"> PySpark 来统计独特单词的<em class="li">出现次数</em>。</em>这是一个简单的程序，供初学者了解<strong class="kp io"> <em class="li"> Spark </em> </strong>和<strong class="kp io"> <em class="li"> Hadoop </em> </strong>的基础知识，作为进入<strong class="kp io"> <em class="li">数据工程的<em class="li">踏脚石！！</em> </em></strong></p></div></div>    
</body>
</html>