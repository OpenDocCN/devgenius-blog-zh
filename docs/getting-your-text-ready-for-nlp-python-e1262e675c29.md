# ä¸º NLP å‡†å¤‡å¥½æ–‡æœ¬(Python)

> åŸæ–‡ï¼š<https://blog.devgenius.io/getting-your-text-ready-for-nlp-python-e1262e675c29?source=collection_archive---------20----------------------->

![](img/6d705cdaebd82ad798238f4bf8c340d6.png)

ä»¥ä¸‹æ˜¯ä¸º NLP å‡†å¤‡æ•°æ®æ—¶å¯ä»¥æ‰§è¡Œçš„ 7 ä¸ªæ­¥éª¤ã€‚æ²¡æœ‰å¿…è¦æ‰€æœ‰çš„ 7 ä»¶äº‹éƒ½å¿…é¡»åšï¼Œæˆ–è€…éƒ½ä¸ä½ çš„å¤§å±€ç›¸å…³ã€‚

1.  **æ¸…ç†æ•°æ®ã€‚**åˆ é™¤æ‰€æœ‰å¼‚å¸¸å’Œä¸å¯ç†è§£çš„æ•°æ®ã€‚ä½ å¯ä»¥è¿™æ ·å†™:

```
#fileContent = "<li>Who drank my coffee?ğŸ˜¡</li><li>I would say whoever ate your avocado toastğŸ˜œ</li>"import re
processedText = ""
with open(filename, â€œrbâ€) as f:
    rawData = f.readlines()
    for line in rawData:
    line = line.decode('ascii','ignore')
    processedText = re.sub("<.*?>","",line) #remove html tags#processedText = "Who drank my coffee? I would say whoever ate your avocado toast"
```

**2ã€‚å°†æ•°æ®æ ‡è®°åŒ–ã€‚**å°†æ•´ä¸ªæ–‡æœ¬åˆ†æˆä¸€ç³»åˆ—å¥å­ã€‚æ­¤å¤–ï¼Œæ¯ä¸ªå¥å­åº”è¯¥åˆ†æˆä¸€ä¸ªå•è¯æ•°ç»„ã€‚æ‚¨å¯ä»¥ä½¿ç”¨[è‡ªç„¶è¯­è¨€å·¥å…·åŒ…(NLTK)](https://www.nltk.org/) æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºå¤„ç† python ä¸­äººç±»å¯è¯»æ•°æ®è€Œæ„å»ºçš„å¹³å°ã€‚

```
import nltk.data
from nltk.tokenize import word_tokenize, sent_tokenize
sentences = sent_tokenize(processedText)
tokenizedData = [word_tokenize(sent) for sent in sentences]
#tokenizedData = [['Who', 'drank', 'my', 'coffee', '?'], ['I', "would", 'say', 'whoever', 'ate', 'your', 'avocado', 'toast']]
```

**3ã€‚åˆ é™¤åœç”¨è¯ã€‚**åœç”¨è¯å°±åƒæ•°æ®ä¸­çš„å™ªéŸ³ã€‚ä»–ä»¬æ­ªæ›²äº†ä½ çš„ç»“æœã€‚åˆ é™¤å®ƒä»¬æœ‰åŠ©äºå‡å°‘åˆ†å¸ƒåœ¨ä¸é‡è¦å…ƒç´ ä¸Šçš„æƒé‡ï¼Œå¦‚â€œtheâ€ã€â€œisâ€ã€â€œareâ€ç­‰å¯¹æœ¬è´¨æ²¡æœ‰è´¡çŒ®çš„è¯ã€‚

```
import nltk
nltk.download('stopwords') #download stopwords recognized by nltk
from nltk.corpus import stopwords
from string import punctuation
customStopWords = list(stopwords.words('english')+list(punctuation))
fiteredSentences = []
for sent in tokenizedData:
    fiteredSentences.append([word for word in sent if word not in customStopWords])
#filteredSentences = [['Who', 'drank', 'coffee'], ['I', 'would', 'say', 'whoever', 'ate', 'avocado', 'toast']]
```

**4ã€‚è¯ä¹‰æ¶ˆæ­§ã€‚éœ€è¦åŒºåˆ†æ‹¼å†™ç›¸åŒä½†æ„æ€ä¸åŒçš„å•è¯ã€‚è¿™å¯ä»¥ä½¿ç”¨ Lesk ç®—æ³•æ¥å®Œæˆã€‚**

```
nltk.download(â€˜wordnetâ€™) 
from nltk.corpus import wordnet as wn
print(lesk("The dog continued to bark.",'bark'))
# Synset('bark.v.03')
print([(ss,ss.definition()) for ss in wn.synsets('bark')])
# [(Synset('bark.n.01'), 'tough protective covering of the woody stems and roots of trees and other woody plants'), (Synset('bark.n.02'), 'a noise resembling the bark of a dog'), (Synset('bark.n.03'), 'a sailing ship with 3 (or more) masts'), (Synset('bark.n.04'), 'the sound made by a dog'), (Synset('bark.v.01'), 'speak in an unfriendly tone'), (Synset('bark.v.02'), 'cover with bark'), (Synset('bark.v.03'), 'remove the bark of a tree'), (Synset('bark.v.04'), 'make barking sounds'), (Synset('bark.v.05'), 'tan (a skin) with bark tannins')]
```

**5ã€‚å µå¡ã€‚å°†æ‰€æœ‰å•è¯ç¼©å‡åˆ°å®ƒä»¬çš„è¯å¹²ã€‚è¿™å°†æœ‰åŠ©äºå°†ä¸€ä¸ªå•è¯çš„æ‰€æœ‰å˜ä½“ç»“åˆåˆ°å®ƒçš„è¯æ ¹ï¼Œè€Œä¸æ˜¯å°†å®ƒä»¬è§†ä¸ºå•ç‹¬çš„å•è¯ã€‚ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨ NLTK çš„é›ªçƒèŒå¹²ä»ªã€‚**

```
from nltk.stem.snowball import SnowballStemmer
stemmer = SnowballStemmer("english")
print(stemmer.stem("running"))
#run
```

**6ã€‚ç»„åˆ**[**N-Grams**](https://en.wikipedia.org/wiki/N-gram)**ã€‚**æˆ‘ä»¬å¯ä»¥ä¸ºéœ€è¦å°†å…¶åˆ†ç±»ä¸º N å…ƒè¯­æ³•çš„å•è¯åºåˆ—çš„å‡ºç°æ¬¡æ•°è®¾ç½®ä¸€ä¸ªé˜ˆå€¼ã€‚ä¾‹å¦‚ï¼Œå¦‚æœâ€œNewâ€åè·Ÿâ€œYorkâ€åœ¨æ•°æ®ä¸­å‡ºç°çš„æ¬¡æ•°è¶…è¿‡ 6 æ¬¡ï¼Œåˆ™å¯ä»¥è®¤ä¸ºå®ƒæ˜¯ä¸€ä¸ªåŒå­—æ¯â€œNew York â€,æ‚¨å¯èƒ½éœ€è¦è€ƒè™‘å®ƒã€‚æ‚¨å¯ä»¥ç¼–å†™ç±»ä¼¼è¿™æ ·çš„ä¸œè¥¿æ¥è·å–æ•°æ®ä¸­æœ€å¸¸è§çš„äºŒå…ƒæ¨¡å‹ï¼Œå¹¶å†³å®šæ˜¯å¦æ„¿æ„å°†å®ƒä»¬ä½œä¸ºå•ä¸ªå®ä½“æ¥å¤„ç†ã€‚

```
import collections
from nltk.util import ngrams
tokenizedDataFlattened = sum(tokenizedData, [])
bigrams = ngrams(tokenizedDataFlattened, 2)
bigramFreq = collections.Counter(bigrams)
print(bigramFreq.most_common(10))# top 10 bigrams
```

7ã€‚è¯æ±‡åŒ–ã€‚èƒœè¿‡ç‚®æ³¥ã€‚å®ƒæŠŠæ„æ€ç›¸ä¼¼çš„å•è¯ç»„åˆåœ¨ä¸€èµ·ã€‚æ¯”å¦‚æŠŠå¥½çš„å‡å°‘åˆ°å¥½çš„ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡é¦–å…ˆè¿›è¡Œè¯æ€§æ ‡æ³¨å¹¶å°†å…¶ä¼ é€’ç»™ NLTK çš„ lemmatizer æ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚ä¸‹é¢çš„å‡½æ•°å°† treebank çš„æ ‡ç­¾è½¬æ¢ä¸º wordnet çš„æ ‡ç­¾çº¦å®šã€‚è®°ä½åˆ é™¤åœç”¨è¯ï¼Œå¦‚ä»‹è¯ã€è¿è¯ã€å† è¯ï¼Œå› ä¸ºå®ƒä»¬åœ¨ wordnet ä¸­æ²¡æœ‰å®šä¹‰ã€‚

```
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
nltk.download('averaged_perceptron_tagger') #for POS taggingdef getWordnetTag(treebankTag):
 if treebankTag.startswith(â€˜Jâ€™):
 return wordnet.ADJ
 elif treebankTag.startswith(â€˜Vâ€™):
 return wordnet.VERB
 elif treebankTag.startswith(â€˜Nâ€™):
 return wordnet.NOUN
 elif treebankTag.startswith(â€˜Râ€™):
 return wordnet.ADV
 else:
 return â€˜â€™#remember to tokenize the data and remove stop words. 
#tokenized = [â€˜rainingâ€™, â€˜catsâ€™, â€˜dogsâ€™]
posTagged = nltk.pos_tag(tokenized)
lemmatizer = WordNetLemmatizer() 
lemmatizedData = [lemmatizer.lemmatize(word,pos = get_wordnet_pos(tag)) for word,tag in posTagged]
print(lemmatizedData)
#lemmatizedData = ['rain', 'cat', 'dog']
```