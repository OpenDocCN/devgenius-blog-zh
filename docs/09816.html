<html>
<head>
<title>Spark Metadata : Commit Cleanup Issue in Structured Streaming</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Spark 元数据:结构化流中的提交清理问题</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/spark-metadata-commit-cleanup-issue-in-structured-streaming-c6f8d69baf38?source=collection_archive---------6-----------------------#2022-09-15">https://blog.devgenius.io/spark-metadata-commit-cleanup-issue-in-structured-streaming-c6f8d69baf38?source=collection_archive---------6-----------------------#2022-09-15</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><p id="521c" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">简介:</strong></p><p id="44d1" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">这个故事是为那些仍在使用 Spark 2.3 并在生产中运行结构化流作业的开发人员而写的。伙计们，小心点。你可能已经做了各种各样的优化和调整，但是仍然发现工作有问题。如果您正在处理大量数据(每小时几 TB 到几百 TB ),并且看到间歇性延迟，那么您的工作很可能面临元数据压缩问题。这里我来解释一下问题。</p><p id="a873" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">问题详情:</strong></p><p id="b446" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><a class="ae ki" href="https://issues.apache.org/jira/browse/SPARK-24295" rel="noopener ugc nofollow" target="_blank">https://issues.apache.org/jira/browse/SPARK-24295</a></p><p id="b8ff" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">检查点是在 Spark 中实现<strong class="jm io"><em class="kj"/></strong>语义的一种方式。流式作业将输入和输出批处理信息保存在检查点中，并按批处理提交文件。流式作业全天候运行，可能会运行数年，导致处理大量数据，文件大小会随着时间的推移而增长，最高可达数十 GB。<strong class="jm io">压缩文件存储流式作业从开始日期开始处理和生成的所有文件的详细信息。</strong>对，你没看错，<em class="kj">从申请开始</em>。</p><p id="d42b" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">提交文件压缩(_Spark_Metadata)在驱动程序上每 10 批执行一次(从 0 开始)。这是一个整体过程，根据压缩文件的大小，最多需要 10-15 分钟。它大大降低了结构化流作业的性能，因为当驱动程序压缩文件时，所有工作节点都不做任何事情，并且作业完全闲置，浪费了资源。有一个控制压缩间隔的 spark config:<strong class="jm io"><em class="kj">spark . SQL . streaming . filesink . log . compact interval</em></strong></p><p id="f3d3" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">默认情况下，此配置的值为 10，即每 10 个微批次后，提交文件压缩就会发生。它将最后 9 个批次的增量添加到前面的<strong class="jm io"> <em class="kj">中。压缩</em> </strong>文件并新建一个<strong class="jm io"> <em class="kj">。压缩</em>文件和</strong>文件。你可以根据你的需要改变它的频率。</p><p id="b3d7" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">解决方案:</strong></p><p id="2deb" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">您唯一的解决方案是编写一个代码来清理 _Spark_Metadata 目录中的元数据。压缩文件是 JSON 格式的，并且还跟踪文件的时间戳和其他信息。</p><p id="5aff" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">可以采取以下方法:</p><ul class=""><li id="0905" class="kk kl in jm b jn jo jr js jv km jz kn kd ko kh kp kq kr ks bi translated">启动 spark 作业，读取 _Spark_Metadata 的最新<code class="fe kt ku kv kw b">.comapct</code> JSON 文件</li><li id="1c76" class="kk kl in jm b jn kx jr ky jv kz jz la kd lb kh kp kq kr ks bi translated">根据保留期过滤掉旧文件的所有记录。</li><li id="947c" class="kk kl in jm b jn kx jr ky jv kz jz la kd lb kh kp kq kr ks bi translated">保存旧的压缩文件以备备份。</li><li id="4d52" class="kk kl in jm b jn kx jr ky jv kz jz la kd lb kh kp kq kr ks bi translated">将新的粉碎压缩文件移动到<em class="kj"> _Spark_Metadata </em>目录。保持相同的文件结构和文件名。</li><li id="c458" class="kk kl in jm b jn kx jr ky jv kz jz la kd lb kh kp kq kr ks bi translated">在我们更新元数据压缩文件时，请确保 spark 流作业没有运行。</li></ul><p id="c580" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">下面火花码接最新<strong class="jm io">。compact</strong>file from _ spark _ metadata dir，根据配置驱动的参数 retentionPeriod 清除元数据，并重写丢弃的。压缩同一路径下的文件。这已经在 ORC 文件和 HDFS 接收器的生产中进行了测试。如果您的触发时间较短(即 60 秒或更长)，您可以每月运行一次此应用程序作为辅助维护应用程序。或者，如果您的 triggertime 少于 60 秒，并且您的应用程序处理大量数据，那么最好每周进行一次清理。您可以将此代码添加到您的流媒体应用程序的开头，以便在重新启动时，首先进行清理。</p><pre class="lc ld le lf gt lg kw lh li aw lj bi"><span id="4c36" class="lk ll in kw b gy lm ln l lo lp">import org.apache.hadoop.fs._<br/>import org.apache.spark.sql._<br/>import org.apache.spark.sql.execution.streaming._<br/>import org.json4s._<br/>import org.json4s.jackson.JsonMethods._<br/><br/>object MetadataRemover extends App {<br/><br/>  val <em class="kj">spark </em>= SparkSession<br/>    .<em class="kj">builder<br/>    </em>.master(ApplicationConfig.<em class="kj">sparkMaster</em>)<br/>    .appName(ApplicationConfig.<em class="kj">sparkAppName</em>)<br/>    .getOrCreate()<br/>  <em class="kj">spark</em>.sparkContext.setLogLevel("ERROR")<br/><br/>  val <em class="kj">sc </em>= <em class="kj">spark</em>.sparkContext<br/>  <em class="kj">/**<br/>   * regex to find last compact file<br/>   */<br/>  </em>val <em class="kj">file_pattern </em>= """(hdfs://.*/_spark_metadata/\d+\.compact)""".r.unanchored<br/>  val <em class="kj">fs </em>= FileSystem.<em class="kj">get</em>(<em class="kj">sc</em>.hadoopConfiguration)<br/><br/>  <em class="kj">/**<br/>   * implicit hadoop RemoteIterator convertor<br/>   */<br/>  </em>implicit def convertToScalaIterator[T](underlying: RemoteIterator[T]): Iterator[T] = {<br/>    case class wrapper(underlying: RemoteIterator[T]) extends Iterator[T] {<br/>      override def hasNext: Boolean = underlying.hasNext<br/><br/>      override def next: T = underlying.next<br/>    }<br/>    wrapper(underlying)<br/>  }<br/><br/>  <em class="kj">/**<br/>   * delete file or folder recursively<br/>   */<br/>  </em>def removePath(dstPath: String, fs: FileSystem): Unit = {<br/>    val path = new Path(dstPath)<br/>    if (fs.exists(path)) {<br/>      <em class="kj">println</em>(s"deleting <strong class="kw io">$</strong>{dstPath}...")<br/>      fs.delete(path, true)<br/>    }<br/>  }<br/><br/>  <em class="kj">/**<br/>   * remove json entries older than </em>`<em class="kj">days</em>` <em class="kj">from compact file<br/>   * preserve </em>`<em class="kj">v1</em>` <em class="kj">at the head of the file<br/>   * re write the small file back to the original destination<br/>   */<br/>  </em>def compact(file: Path, days: Int) = {<br/>    val ttl = new java.util.Date().getTime - java.util.concurrent.TimeUnit.<em class="kj">DAYS</em>.toMillis(days)<br/>    val compacted_file = s"/tmp/<strong class="kw io">$</strong>{file.getName.toString}"<br/>    // If this path already exists : then it will be removed<br/>    <em class="kj">removePath</em>(compacted_file, <em class="kj">fs</em>)<br/>    val lines = <em class="kj">sc</em>.textFile(file.toString)<br/>    val reduced_lines = lines.mapPartitions({<br/>      p =&gt;<br/>        implicit val formats = DefaultFormats<br/>        p.collect({<br/>          case "v1" =&gt; "v1"<br/>          case x if {<br/>            parse(x).extract[SinkFileStatus].modificationTime &gt; ttl<br/>          } =&gt; x<br/>        })<br/>    }).coalesce(1)<br/>    <em class="kj">println</em>(s"removing <strong class="kw io">$</strong>{lines.count - reduced_lines.count} lines from <strong class="kw io">$</strong>{file.toString}...")<br/>    reduced_lines.saveAsTextFile(compacted_file)<br/>    FileUtil.<em class="kj">copy</em>(<em class="kj">fs</em>, new Path(compacted_file + "/part-00000"), <em class="kj">fs</em>, file, false, <em class="kj">sc</em>.hadoopConfiguration)<br/>    <em class="kj">removePath</em>(compacted_file, <em class="kj">fs</em>)<br/>  }<br/><br/>  <em class="kj">/**<br/>   * get last compacted files if exists:<br/>   */<br/>  </em>def getLastCompactFile(path: Path) = {<br/>    <em class="kj">fs</em>.listFiles(path, true).toList.sortBy(_.getModificationTime).reverse.collectFirst({<br/>      case x if (<em class="kj">file_pattern</em>.findFirstMatchIn(x.getPath.toString).isDefined) =&gt;<br/>        x.getPath<br/>    })<br/>  }<br/><br/>  val <em class="kj">landingDir </em>= ApplicationConfig.<em class="kj">landingPath<br/>  </em>val <em class="kj">retentionPeriod </em>= ApplicationConfig.<em class="kj">retentionPeriod<br/>  </em>val <em class="kj">metadataDir </em>= new Path(s"<strong class="kw io">$</strong><em class="kj">landingDir</em>/_spark_metadata")<br/>  <em class="kj">getLastCompactFile</em>(<em class="kj">metadataDir</em>).map(x =&gt; <em class="kj">compact</em>(x, <em class="kj">retentionPeriod</em>))<br/><br/>}</span></pre></div></div>    
</body>
</html>