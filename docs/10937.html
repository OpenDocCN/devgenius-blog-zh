<html>
<head>
<title>MLflow With Active Learning.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主动学习。</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/mlflow-with-active-learning-b804c49e978b?source=collection_archive---------6-----------------------#2022-12-08">https://blog.devgenius.io/mlflow-with-active-learning-b804c49e978b?source=collection_archive---------6-----------------------#2022-12-08</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><div class=""><h2 id="0d35" class="pw-subtitle-paragraph jk im in bd b jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb dk translated">重要提示。</h2></div><p id="9664" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">本文的目的不是太深入地解释什么是 ml flow/主动学习，而是了解如何实际使用它，因此本文期望人们对<strong class="ke io">ml flow/主动学习</strong>有所了解。</p><h2 id="9aed" class="ky kz in bd la lb lc dn ld le lf dp lg kl lh li lj kp lk ll lm kt ln lo lp lq bi translated">MLflow</h2><p id="77da" class="pw-post-body-paragraph kc kd in ke b kf lr jo kh ki ls jr kk kl lt kn ko kp lu kr ks kt lv kv kw kx ig bi translated"><a class="ae lw" href="https://www.mlflow.org/" rel="noopener ugc nofollow" target="_blank"> MLflow </a>是一个管理端到端机器学习生命周期的开源平台。它有以下主要组件:</p><ul class=""><li id="f7eb" class="lx ly in ke b kf kg ki kj kl lz kp ma kt mb kx mc md me mf bi translated"><strong class="ke io">跟踪</strong>:允许您跟踪实验，记录和比较参数和结果。</li><li id="958b" class="lx ly in ke b kf mg ki mh kl mi kp mj kt mk kx mc md me mf bi translated"><strong class="ke io"> Models </strong>:允许您从各种 ML 库中管理和部署模型到各种模型服务和推理平台。</li><li id="e0b7" class="lx ly in ke b kf mg ki mh kl mi kp mj kt mk kx mc md me mf bi translated"><strong class="ke io">项目</strong>:允许您将 ML 代码打包成可重用、可复制的形式，以便与其他数据科学家共享或转移到生产中。</li><li id="24ab" class="lx ly in ke b kf mg ki mh kl mi kp mj kt mk kx mc md me mf bi translated"><strong class="ke io">Model</strong><strong class="ke io">Registry</strong>:允许您集中管理模型的整个生命周期阶段转换的模型存储:从阶段转换到生产，具有版本控制和注释的能力。</li><li id="2613" class="lx ly in ke b kf mg ki mh kl mi kp mj kt mk kx mc md me mf bi translated"><strong class="ke io">模型</strong> <strong class="ke io">服务</strong>:允许您托管 MLflow 模型作为 REST 端点。</li></ul><h2 id="37c1" class="ky kz in bd la lb lc dn ld le lf dp lg kl lh li lj kp lk ll lm kt ln lo lp lq bi translated">主动学习。</h2><p id="0322" class="pw-post-body-paragraph kc kd in ke b kf lr jo kh ki ls jr kk kl lt kn ko kp lu kr ks kt lv kv kw kx ig bi translated">主动学习是机器学习的一种特殊情况，在这种情况下，学习算法可以交互式地询问用户(或一些其他信息源)以用期望的输出来标记新的数据点。有些情况下，未标记的数据很丰富，但手动标记的成本很高。在这种情况下，学习算法可以主动向用户/教师查询标签。这种类型的迭代监督学习被称为主动学习。由于学习者选择例子，学习一个概念的例子的数量通常比正常的监督学习所需的数量少得多。在本文中，我将介绍一个简单的使用 MLflow 进行主动学习的例子。</p><h2 id="196c" class="ky kz in bd la lb lc dn ld le lf dp lg kl lh li lj kp lk ll lm kt ln lo lp lq bi translated">实施细节。</h2><p id="1fd3" class="pw-post-body-paragraph kc kd in ke b kf lr jo kh ki ls jr kk kl lt kn ko kp lu kr ks kt lv kv kw kx ig bi translated">在这个实验中，我使用主动学习结合 HuggingFace 模型训练<strong class="ke io">情感分析任务</strong>，使用 MLFlow 进行<strong class="ke io">实验跟踪</strong>、<strong class="ke io">模型</strong>、<strong class="ke io">代码</strong> <strong class="ke io">版本化</strong>。我还通过创建不同的文件，如<strong class="ke io"> MLproject </strong>、<strong class="ke io"> conda.yaml </strong>，使用 MLFlow 展示了<strong class="ke io">代码可再现性</strong>。</p><p id="750e" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="ke io">数据集详细信息。</strong></p><ol class=""><li id="42d4" class="lx ly in ke b kf kg ki kj kl lz kp ma kt mb kx ml md me mf bi translated">我使用了取自 Kaggle 的情感分析 IMDB 电影评论数据集，该数据集可从<a class="ae lw" href="https://www.kaggle.com/code/lakshmi25npathi/sentiment-analysis-of-imdb-movie-reviews" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/code/Lakshmi 25 npathi/perspective-Analysis-of-IMDB-Movie-reviews</a>下载</li></ol><p id="a890" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="ke io">预处理。</strong></p><pre class="mm mn mo mp gt mq mr ms bn mt mu bi"><span id="ea06" class="mv kz in mr b be mw mx l my mz">from configs import CONFIG<br/>import pandas as pd<br/>import re <br/>import os <br/>import mlflow<br/>from tqdm import tqdm <br/>import pickle<br/>import sys<br/>import string <br/>from sklearn.model_selection import StratifiedKFold<br/><br/>class prepDataset:<br/>    def __init__(self) -&gt; None:<br/>        self.df = pd.read_csv(CONFIG.TRAIN_FILE)<br/>        self.index2word = {idx: word for idx, word in enumerate(set(self.df["sentiment"].values))}<br/>        self.word2index = {word: idx for idx, word in self.index2word.items()}<br/>        self.df["label"] = self.df["sentiment"].apply(lambda x: self.word2index[x])<br/>        arg1, arg2 = sys.argv[1], sys.argv[2]<br/>        self.dumpData(self.index2word,arg1.split("src/")[-1])<br/>        self.dumpData(self.word2index, arg2.split("src/")[-1])<br/>        self.threshold = 256<br/>        self.df["preprocessedREVIEW"]  = self.df["review"].apply(self.preprocessing)<br/>        <br/>        self.stratkFold = StratifiedKFold(n_splits=4)<br/>        self.splitData()<br/><br/><br/>    def splitData(self):<br/>        X = self.df["preprocessedREVIEW"]<br/>        Y = self.df["label"]<br/>        for idx, (train_idx, test_idx) in enumerate(self.stratkFold.split(X,Y)):<br/>            # let's create folder<br/>            print(f"--------------------------------------- FOLD_{idx} ---------------------------------------")<br/>            path = os.path.join(CONFIG.MULTIFOLD, f"fold_{idx}")<br/>            if not os.path.isdir(path):<br/>                os.mkdir(path)<br/>            # let's save the train and test data inside the corresponding fold <br/>            train = self.df.iloc[train_idx]<br/>            test  = self.df.iloc[test_idx]<br/>            train.to_csv(os.path.join(path, "train.csv"), index=False)<br/>            test.to_csv(os.path.join(path, "test.csv"), index=False)<br/><br/>    @staticmethod<br/>    def dumpData(dictionary, filename):<br/>        if not os.path.isdir(CONFIG.WORDINDEX):<br/>            os.mkdir(CONFIG.WORDINDEX)<br/>        path = os.path.join(CONFIG.WORDINDEX, filename)<br/>        pickle.dump(dictionary, open(path, "wb"))<br/>    <br/>    @staticmethod<br/>    def loadData(filename):<br/>        path = os.path.join(CONFIG.WORDINDEX, filename)<br/>        return pickle.load(open(path, "rb"))<br/><br/><br/>    def preprocessing(self,sentence):<br/>        sentence = sentence.translate(str.maketrans("","", string.punctuation)).lower()<br/>        #let's remove if their any links <br/>        sentence = re.sub(r"https?://\s+", "", sentence)<br/>        sentence = re.sub(r"\b\d+\b",  "", sentence)<br/>        sentence = re.sub(r" +"," ",sentence)<br/>        return sentence<br/><br/><br/>prepDataset()</span></pre><p id="e5c6" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="ke io"> prepDataset </strong>类负责预处理 IMDB 数据集，去除所有的<em class="na">链接</em>、<em class="na">数字、</em>、<em class="na">多个空格、标点</em>。它还创建了<em class="na"> 4 个折叠 CV 数据集</em>，该数据集存储在 4 个不同的折叠中，所有折叠用于训练模型，并考虑表现最佳的折叠。这个类包含两个静态方法<strong class="ke io"> </strong>，负责保存 pickle 文件，这些文件存储了与标签的键值对相关的信息。</p><p id="3381" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="ke io">主动学习类。</strong></p><pre class="mm mn mo mp gt mq mr ms bn mt mu bi"><span id="58fe" class="mv kz in mr b be mw mx l my mz">import random<br/>import torch<br/>import pandas as pd<br/>class ACLearning:<br/>    def __init__(self, dataset) -&gt; None:<br/>        self.unlabelledDataset = dataset<br/>        self.labelledDataset = None<br/><br/>    def randomSample(self, k = 200):<br/>        labelledIndexes= random.sample(range(len(self.unlabelledDataset)), k=200)<br/>        unlabelledIndexes = range(len(self.unlabelledDataset))<br/>        # let's remove the labelled Indexes from unlabelled Indexes<br/>        unlabelledIndexes = list(filter(lambda x: x not in labelledIndexes, unlabelledIndexes))<br/>        <br/>        self.labelledDataset = self.unlabelledDataset.iloc[labelledIndexes]<br/>        self.unlabelledDataset= self.unlabelledDataset.iloc[unlabelledIndexes]<br/><br/>        self.labelledDataset.reset_index(inplace=True, drop=True)<br/>        self.unlabelledDataset.reset_index(inplace=True, drop=True)<br/><br/>        # self.labelledDataset = torch.utils.data.Subset(self.unlabelledDataset, labelledIndexes)<br/>        # self.unlabelledDataset= torch.utils.data.Subset(self.unlabelledDataset, unlabelledIndexes)<br/>        # now we have the labelled dataset we can substract the labelled datset from unlabelled one<br/>    <br/>    def getlabelledDataset(self, labelledIndexes):<br/>        unlabelledIndexes = range(len(self.unlabelledDataset))<br/>        unlabelledIndexes = list(filter(lambda x: x not in labelledIndexes, unlabelledIndexes))<br/><br/>        self.labelledDataset = pd.concat([self.labelledDataset, self.unlabelledDataset.iloc[labelledIndexes]])<br/>        self.unlabelledDataset = self.unlabelledDataset.iloc[unlabelledIndexes]<br/><br/>        self.labelledDataset.reset_index(inplace=True, drop=True)<br/>        self.unlabelledDataset.reset_index(inplace=True, drop=True)<br/>        <br/>        # self.labelledDataset = torch.utils.data.ConcatDataset([self.labelledDataset, torch.utils.data.Subset(self.unlabelledDataset, labelledIndexes)])<br/>        # self.unlabelledDataset= torch.utils.data.Subset(self.unlabelledDataset, unlabelledIndexes)<br/><br/>    @property<br/>    def get_unlabelled_dataset(self):<br/>        return len(self.unlabelledDataset)<br/>    <br/>    @property<br/>    def get_labelled_dataset(self):<br/>        return 0 if self.labelledDataset is None else len(self.labelledDataset) </span></pre><p id="a936" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="ke io"> ACLearning </strong>类是主动学习的大脑，这个类背后的整个思想是管理<strong class="ke io">已标记和未标记的数据集。</strong>在这一类中最重要的两种方法是:</p><ol class=""><li id="0b6d" class="lx ly in ke b kf kg ki kj kl lz kp ma kt mb kx ml md me mf bi translated"><strong class="ke io"> randomSample : </strong>它负责最初创建一个带标签的数据集并将其从一个无标签的数据集中移除，它需要传递一个参数<strong class="ke io"> K </strong>，该参数告诉<strong class="ke io">随机算法</strong>要从无标签的数据集中抽取多少个样本。</li><li id="0a34" class="lx ly in ke b kf mg ki mh kl mi kp mj kt mk kx ml md me mf bi translated"><strong class="ke io"> getlabelledDataset: </strong>这个负责把<strong class="ke io">标签索引</strong>作为实参。标记为<strong class="ke io">的索引</strong>是使用主动学习的抽样策略计算的，我们将在本文后面看到。已标记的<strong class="ke io">索引</strong>随后用于移动与未标记数据集中的索引相对应的数据点，并将其与已标记数据集中的数据点相结合。</li></ol><p id="9f27" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="ke io">主动学习策略</strong></p><p id="4476" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">在这种情况下，我只展示了一种策略，即<strong class="ke io">熵采样、</strong>但是可以使用不同的策略，如多样性采样、不确定性采样或两者一起使用。我们可以简单地在下面显示的类中添加另一个策略。</p><pre class="mm mn mo mp gt mq mr ms bn mt mu bi"><span id="ac52" class="mv kz in mr b be mw mx l my mz">import torch<br/>import itertools<br/>from tqdm import tqdm<br/>from customDataset import dataset<br/>class strategies:<br/>    def __init__(self) -&gt; None:<br/>        pass<br/><br/>    def predict_proba(self, dataloader, rows):<br/>        self.model.eval()<br/>        batchIDs = []<br/>        data = torch.ones([rows, 2])<br/>        start = 0<br/>        counter =0 <br/>        with torch.no_grad():<br/>          for  batch_idx, element in tqdm(dataloader,position=0, leave=True):<br/>              out = self.model(**element)<br/>              pred = torch.softmax(out.logits, dim=-1)<br/>              end = start + element["input_ids"].shape[0]<br/>              data[start:end] = pred<br/>              start = end<br/>              batchIDs.append(batch_idx.tolist())<br/>              counter +=1<br/>        return data, batchIDs<br/><br/>    def entropySampling(self, ac_learning, model, accelerator, tokenizer):<br/>        self.model =model<br/>        unlabelledDataset = dataset(ac_learning.unlabelledDataset["preprocessedREVIEW"], ac_learning.unlabelledDataset["label"], tokenizer)<br/>        dataloader = torch.utils.data.DataLoader(unlabelledDataset, batch_size=32,shuffle=False)<br/>        self.model, dataloader = accelerator.prepare(self.model, dataloader)<br/><br/>        probs, batchIDs =  self.predict_proba(dataloader, len(unlabelledDataset))<br/>        log_prob = torch.log(probs)<br/>        batchIDs = list(itertools.chain(*batchIDs))<br/>        return (-probs * log_prob).sum(1), batchIDs<br/><br/><br/>    def entropySamplignDropout(self):<br/>        pass</span></pre><p id="1961" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">在这个类中，<strong class="ke io"> entropySampling </strong>方法接受输入(ac_learning 是<strong class="ke io"> ACLearning </strong>类的一个对象，<strong class="ke io">模型</strong>在带标签的数据集上训练，<strong class="ke io">加速器</strong>是 huggingFace 库，而 Tokenizer 我们将在接下来的 Tokenizer 中深入讨论)。<strong class="ke io"> entropySampling </strong>方法负责返回所有未标记数据集的概率及其批量 IDX。我们只根据概率从高到低挑选排名前 k 的指数。一旦顶部 k 索引被选择，它被传递到上面讨论的<strong class="ke io"> getlabelledDataset </strong>方法。</p><h2 id="7f12" class="ky kz in bd la lb lc dn ld le lf dp lg kl lh li lj kp lk ll lm kt ln lo lp lq bi translated"><strong class="ak">流量测井信息</strong></h2><p id="f062" class="pw-post-body-paragraph kc kd in ke b kf lr jo kh ki ls jr kk kl lt kn ko kp lu kr ks kt lv kv kw kx ig bi translated">MLflow 是一个 MLops 框架，有助于代码版本控制、实验跟踪、再现性、模型注册等。MLflow 最棒的地方在于它易于使用，并且可以很容易地与代码集成。</p><p id="c67e" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">MLflow 提供了记录信息的不同方式:</p><ol class=""><li id="ea51" class="lx ly in ke b kf kg ki kj kl lz kp ma kt mb kx ml md me mf bi translated"><strong class="ke io"> mlflow.log_params </strong> →这用于记录超参数。如果我们使用随机搜索、网格搜索或贝叶斯搜索，我们可以使用它将参数记录到 MLflow UI 服务器中，进行比较，以选择最佳参数。</li><li id="b9f1" class="lx ly in ke b kf mg ki mh kl mi kp mj kt mk kx ml md me mf bi translated"><strong class="ke io"> mlflow.log_metrics </strong> →在 mlflow 中，我们可以记录不同的指标，例如(f1_score、recall_score、precision_score 等)。</li><li id="2e34" class="lx ly in ke b kf mg ki mh kl mi kp mj kt mk kx ml md me mf bi translated"><strong class="ke io"> mlflow.log_artifacts </strong> →记录工件对于代码、数据和模型的版本控制很重要。MLflow 提供了记录这些信息的方法。我们也可以使用<strong class="ke io"> mlflow.log_artifact </strong>，只要我们需要传递文件而不是文件夹。</li><li id="5fe4" class="lx ly in ke b kf mg ki mh kl mi kp mj kt mk kx ml md me mf bi translated"><strong class="ke io"> mlflow.sklearn.model </strong> →一旦我们训练了模型，我们需要将模型信息记录到 mlflow 中，如果我们正在使用 sklearn 模型，我们可以使用 MLflow 提供的方式简单地记录。</li><li id="4e15" class="lx ly in ke b kf mg ki mh kl mi kp mj kt mk kx ml md me mf bi translated"><strong class="ke io">ml flow . py torch . autolog</strong>→这是用于在我们使用 PyTorch lightning ( <code class="fe nb nc nd mr b">pl.LightningModule</code>)的情况下进行自动记录。</li><li id="2435" class="lx ly in ke b kf mg ki mh kl mi kp mj kt mk kx ml md me mf bi translated"><strong class="ke io">ml flow . pytorch . log _ model</strong>→用于记录 py torch 的型号(nn。模块)。</li><li id="53b6" class="lx ly in ke b kf mg ki mh kl mi kp mj kt mk kx ml md me mf bi translated"><strong class="ke io">ml fow . py torch . load _ model</strong>→用于从不同的 URI (S3、azure、file 等)加载模型。</li></ol><p id="9cb7" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">我们使用<strong class="ke io"> MLflow </strong>记录的所有信息都可以在<strong class="ke io"> MLflow UI 中查看。</strong>如果我们使用的是本地机器，我们可以通过简单的命令来访问它:</p><blockquote class="ne nf ng"><p id="1074" class="kc kd na ke b kf kg jo kh ki kj jr kk nh km kn ko ni kq kr ks nj ku kv kw kx ig bi translated">mlflow ui</p></blockquote><p id="c16a" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">如果您使用<strong class="ke io"> google colab </strong>访问<strong class="ke io"> MLflow 的<strong class="ke io"> UI </strong>界面，</strong>您必须使用<strong class="ke io"> ngrok </strong>来托管本地 URL。使用下面的代码可以做到这一点:</p><pre class="mm mn mo mp gt mq mr ms bn mt mu bi"><span id="2a92" class="mv kz in mr b be mw mx l my mz">from pyngrok import ngrok<br/>import mlflow<br/><br/>get_ipython().system_raw("mlflow ui --port 5000 &amp;")<br/># Terminate open tunnels if exist<br/>ngrok.kill()<br/><br/># Setting the authtoken (optional)<br/># Get your authtoken from https://dashboard.ngrok.com/auth<br/>auth = getpass('Authentication Token:')<br/>ngrok.set_auth_token(auth)<br/><br/># Open an HTTPs tunnel on port 5000 for http://localhost:5000<br/>ngrok_tunnel = ngrok.connect(addr="5000", proto="http", bind_tls=True)<br/>print("MLflow Tracking UI:", ngrok_tunnel.public_url)</span></pre><p id="9570" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">传递身份验证令牌。</p><p id="91e4" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="ke io">使用 MLflow 进行模型注册。</strong></p><p id="6189" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">MLflow 模型注册组件是一个集中式模型存储、一组 API 和 UI，用于协作管理 MLflow 模型的整个生命周期。它提供了模型沿袭(MLflow 实验和运行产生了模型)、模型版本化、阶段转换(例如从阶段转换到生产)和注释。我们可以从 MLFlow 提供的用户界面或代码本身创建模型注册表，如 ml flow<a class="ae lw" href="https://www.mlflow.org/docs/latest/model-registry.html" rel="noopener ugc nofollow" target="_blank">https://www.mlflow.org/docs/latest/model-registry.html</a>中所述。</p><p id="8926" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="ke io">ml 项目描述。</strong></p><p id="0a6f" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">如果我们想要执行代码再现性，此文件非常重要。一旦创建了此 MLProject 文件和 Conda.yml 文件，我们就可以运行远程服务器中的代码，甚至无需下载代码，我们只需更改参数并执行 CLI <strong class="ke io"> MLflow run 命令</strong>。</p><pre class="mm mn mo mp gt mq mr ms bn mt mu bi"><span id="f2c4" class="mv kz in mr b be mw mx l nk mz">name: text-classification<br/>conda_env: conda.yaml<br/>entry_points:<br/>  preprocessing_classification:<br/>    parameters:<br/>      index2word: {type: str, default="index2word.pickle"}<br/>      word2index: {type: str, default="word2index.pickle"}<br/>    command: "python preprocessing.py {index2word} {word2index}"<br/>  training_classification:<br/>    parameters:<br/>      grad_accumulation: {type: int, default: 1}<br/>      budget: {type: int, default: 200}<br/>      trainFile: {type: str, default: "dataset/"}<br/>      testFile: {type: str, default: "dataset/"}<br/>      batch: {type: int, default: 4}<br/>      lr: {type: float, default:1e-5}</span></pre><ol class=""><li id="177e" class="lx ly in ke b kf kg ki kj kl lz kp ma kt mb kx ml md me mf bi translated"><strong class="ke io">名称</strong>:定义文件的名称，可以是任何名称。</li><li id="16cb" class="lx ly in ke b kf mg ki mh kl mi kp mj kt mk kx ml md me mf bi translated"><strong class="ke io"> conda_env </strong>:需要指定 conda.yaml 文件，该文件包含我们需要安装的所有库的相关信息。</li><li id="9f39" class="lx ly in ke b kf mg ki mh kl mi kp mj kt mk kx ml md me mf bi translated"><strong class="ke io">entry _ points</strong><em class="na"/>:entry point 是入口点，例如如果我们想要执行多个文件，如<em class="na"> modeltraining </em>、<em class="na">预处理</em>、<em class="na">度量、</em>等。我们需要有多个入口点，通过这些入口点<strong class="ke io"> mflow 运行</strong>命令，以了解它需要为特定文件运行什么命令。</li><li id="9f3d" class="lx ly in ke b kf mg ki mh kl mi kp mj kt mk kx ml md me mf bi translated"><strong class="ke io">参数</strong> <em class="na"> </em>:包含我们传递给训练/预处理/度量文件等的参数。</li><li id="0c4d" class="lx ly in ke b kf mg ki mh kl mi kp mj kt mk kx ml md me mf bi translated"><strong class="ke io">命令</strong>:该命令按照我们指定的参数执行文件。</li></ol><p id="cedf" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="ke io"> Conda.yaml 文件描述</strong></p><p id="b7da" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">该文件负责存储与所有依赖项相关的信息。</p><pre class="mm mn mo mp gt mq mr ms bn mt mu bi"><span id="dfc3" class="mv kz in mr b be mw mx l my mz">name: text_classification<br/>channels:<br/>  - conda-forge<br/>dependencies:<br/>  - python=3.7<br/>  - pip<br/>  - pip:<br/>    - mlflow==1.30.0<br/>    - scipy==1.7.3<br/>    - scikit-learn==1.0.2<br/>    - torch==1.12.1+cu113<br/>    - sentencepiece!=0.1.92<br/>    - transformers==4.16.2<br/>    - datasets&gt;=1.8.0<br/>    - seqeval==1.2.2<br/>    - accelerate</span></pre><ol class=""><li id="3c70" class="lx ly in ke b kf kg ki kj kl lz kp ma kt mb kx ml md me mf bi translated">conda.yaml 文件由文件的<strong class="ke io">名</strong>组成。</li><li id="f396" class="lx ly in ke b kf mg ki mh kl mi kp mj kt mk kx ml md me mf bi translated">然后我们指定<strong class="ke io">通道</strong>和<strong class="ke io">依赖关系</strong>，它们基本上定义了 python、pip 和不同重要包的版本。</li><li id="02f9" class="lx ly in ke b kf mg ki mh kl mi kp mj kt mk kx ml md me mf bi translated">最后一个是所有文件的<strong class="ke io"> pip 安装</strong>，在运行时安装。</li></ol><h2 id="f83e" class="ky kz in bd la lb lc dn ld le lf dp lg kl lh li lj kp lk ll lm kt ln lo lp lq bi translated"><strong class="ak">模型/标记器信息。</strong></h2><p id="710a" class="pw-post-body-paragraph kc kd in ke b kf lr jo kh ki ls jr kk kl lt kn ko kp lu kr ks kt lv kv kw kx ig bi translated">我正在使用 HuggingFace 模型，tokenizer 作为<strong class="ke io"> roberta-base </strong>到<strong class="ke io"> </strong>训练语义分析任务。HuggingFace 模型上没有添加新层。我已经使用 HuggingFace 的 Auto 类来加载模型和记号赋予器，它们是:</p><ol class=""><li id="7c9e" class="lx ly in ke b kf kg ki kj kl lz kp ma kt mb kx ml md me mf bi translated">自动 Tokenizer。</li><li id="b44b" class="lx ly in ke b kf mg ki mh kl mi kp mj kt mk kx ml md me mf bi translated">AutoModelForSequenceClassification。</li></ol><h2 id="ed2a" class="ky kz in bd la lb lc dn ld le lf dp lg kl lh li lj kp lk ll lm kt ln lo lp lq bi translated"><strong class="ak">使用 MLflow &amp;主动学习进行培训。</strong></h2><pre class="mm mn mo mp gt mq mr ms bn mt mu bi"><span id="e080" class="mv kz in mr b be mw mx l my mz">from configs import CONFIG<br/>import os <br/>import pandas as pd<br/>from tqdm import tqdm<br/>from customDataset import dataset<br/>from ActiveLearning import ACLearning<br/>import torch<br/>import pprint<br/>import mlflow <br/>import numpy as np<br/>import math<br/>from strategies import strategies<br/>from accelerate import Accelerator<br/>from torch.utils.data import DataLoader<br/>from sklearn.metrics import f1_score, precision_score, recall_score<br/>from transformers import AutoModelForSequenceClassification, AdamW, get_scheduler, AutoTokenizer<br/>import argparse<br/>def metric(pred, gt):<br/>    return f1_score(pred, gt, average="weighted", zero_division=1), precision_score(pred, gt,  average="weighted", zero_division=1), recall_score(pred, gt,  average="weighted", zero_division=1)<br/><br/>def main():<br/>  with mlflow.start_run():<br/>    parser = argparse.ArgumentParser(<br/>        description="Make text classification dataset")<br/>    parser.add_argument("--grad_accumulation", help="output_dir")<br/>    parser.add_argument("--budget", help="Enter budget")<br/>    parser.add_argument("--trainFile", help="Train path")<br/>    parser.add_argument("--testFile", help="evaluation path")<br/>    parser.add_argument("--batch", help="Enter Batch Size.")<br/>    parser.add_argument("--lr", help="learning rate")<br/>    <br/>    args = parser.parse_args()<br/><br/>    mlflow.log_params(vars(args))<br/>    # training on all folds<br/>    strat = strategies()<br/>    fold= args.trainFile.split("datasets/")[-1].split("/")[0]<br/>    mlflow.log_artifact(args.trainFile)<br/>    mlflow.log_artifact(args.testFile)<br/>    gradient_accumulation_steps = int(args.grad_accumulation)<br/>    accelerator =  Accelerator(gradient_accumulation_steps=gradient_accumulation_steps)<br/>    # folder_path = os.path.join(CONFIG.MULTIFOLD, f"fold_{i}")<br/>    # trainFile = os.path.join(folder_path, "train.csv")<br/>    # testFile  = os.path.join(folder_path, "test.csv")<br/><br/>    trainData = pd.read_csv(args.trainFile)[["preprocessedREVIEW","label"]]<br/>    testData  = pd.read_csv(args.testFile)[["preprocessedREVIEW", "label"]]<br/>    model = AutoModelForSequenceClassification.from_pretrained("roberta-base", num_labels =len(trainData["label"].unique().tolist()) )<br/>    tokenizer = AutoTokenizer.from_pretrained("roberta-base")<br/><br/>    ac_learning = ACLearning(trainData)<br/>    ac_learning.randomSample()<br/><br/>    # train and test dataset <br/>    traindataset = dataset(ac_learning.labelledDataset["preprocessedREVIEW"], ac_learning.labelledDataset["label"],tokenizer)<br/>    testdataset  = dataset(testData["preprocessedREVIEW"], testData["label"], tokenizer)<br/><br/>    trainloader = DataLoader(traindataset, batch_size=int(args.batch), shuffle=True)<br/>    testloader = DataLoader(testdataset, batch_size=int(args.batch), shuffle=False)<br/><br/>    #optimizer <br/><br/>    no_decay = ["bias", "LayerNorm.weight"]<br/>    optimizer_grouped_parameters = [<br/>        {<br/>            "params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],<br/>            "weight_decay": 0.0,<br/>        },<br/>        {<br/>            "params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],<br/>            "weight_decay": 0.0,<br/>        },<br/>    ]<br/>    num_train_epochs = 100<br/>    #num_update_steps_per_epoch = math.ceil(len(trainloader) / gradient_accumulation_steps)<br/>    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=float(args.lr))<br/>    max_train_steps = num_train_epochs * len(trainloader)<br/>    lr_scheduler = get_scheduler(<br/>        name="linear",<br/>        optimizer=optimizer,<br/>        num_warmup_steps=10,<br/>        num_training_steps=max_train_steps,<br/>    )<br/>    trainloader, testloader, model, optimizer, lr_scheduler = accelerator.prepare(trainloader, testloader, model, optimizer, lr_scheduler)<br/>    <br/>    for epc in range(num_train_epochs):<br/>        print(f"############## Labelled Dataset: {ac_learning.get_labelled_dataset} &amp;&amp; Unlabelled Dataset: {ac_learning.get_unlabelled_dataset} ##############")<br/>        model.train()<br/>        for data in tqdm(trainloader,position=0, leave=True):<br/>            with accelerator.accumulate(model):<br/>                optimizer.zero_grad()<br/>                batch_idx, inputs = data<br/>                output = model(**inputs)<br/>                loss = output.loss<br/>                accelerator.backward(loss)<br/>                optimizer.step()<br/>                lr_scheduler.step()<br/><br/>        # we can perform testing here<br/>        <br/>        model.eval()<br/>        f1_scores = []<br/>        precisions = []<br/>        recalls = []<br/>        for data in tqdm(testloader,position=0, leave=True):<br/>            batch_idx, inputs = data<br/>            output = model(**inputs)<br/>            logits = output.logits<br/>            probab = torch.softmax(logits, dim=-1)<br/>            pred = torch.argmax(probab, -1).tolist()<br/>            gt   = inputs["labels"].tolist()<br/>            f1Score, precision, recall = metric(pred, gt)<br/>            f1_scores.append(f1Score)<br/>            precisions.append(precision)<br/>            recalls.append(recall)<br/>        scores = {<br/>            "f1_score": np.mean(np.array(f1Score)),<br/>            "precision": np.mean(np.array(precisions)),<br/>            "recall": np.mean(np.array(recalls))<br/>        }<br/>        print("\n\n")<br/>        pprint.pprint(scores)<br/>        print("\n\n")<br/>        mlflow.log_metrics(scores) # logging the metrics <br/>        # perform the ActiveLearning pipeline <br/><br/>        probabilities, batchIds = strat.entropySampling(ac_learning, model, accelerator, tokenizer)<br/>        assert len(probabilities.tolist()) == len(batchIds)<br/>        probIndex = list(zip( probabilities.tolist(),batchIds))<br/>        ot = sorted(probIndex, key = lambda x: x[0], reverse=True)<br/>        indexes = [ind for score, ind in ot]<br/>        # now use these indxes to extract next sample from unlabelled dataset<br/>        ac_learning.getlabelledDataset(indexes[:int(args.budget)])<br/>        traindataset = dataset(ac_learning.labelledDataset["preprocessedREVIEW"], ac_learning.labelledDataset["label"], tokenizer)<br/>        trainloader = DataLoader(traindataset, batch_size=int(args.batch), shuffle=True) # create train loader again<br/>        trainloader = accelerator.prepare(trainloader)<br/>          # logging the model <br/>    mlflow.pytorch.log_model(model, f"model-{fold}")<br/>  mlflow.end_run()<br/>if __name__ == "__main__":<br/>    main()</span></pre><p id="8bdd" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">这是<strong class="ke io">主</strong>方法，结合了<strong class="ke io">主动学习、MLflow </strong>以及 HuggingFace 模型。如果我们仔细观察代码，我们会发现<strong class="ke io"> MLflow </strong>用于记录模型、度量、工件等信息。在每个时期使用主动学习来将更多未标记的数据添加到已标记的数据集中，并重新训练模型。为了运行上述<strong class="ke io">训练&amp;预处理</strong>文件，我们使用 MLflow <strong class="ke io"> CLI </strong>命令行指令。</p><p id="5e9b" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">下面单元格中的命令用于运行预处理/模型训练文件。</p><ol class=""><li id="3a87" class="lx ly in ke b kf kg ki kj kl lz kp ma kt mb kx ml md me mf bi translated">它需要传递<strong class="ke io">实验</strong>名称，该名称将在<strong class="ke io"> MLflow UI 服务器中创建一个新实验。</strong></li><li id="7b3d" class="lx ly in ke b kf mg ki mh kl mi kp mj kt mk kx ml md me mf bi translated">我们传递我们在上面描述的<strong class="ke io"> MLproject </strong>文件中定义的入口点名称。</li><li id="c576" class="lx ly in ke b kf mg ki mh kl mi kp mj kt mk kx ml md me mf bi translated"><strong class="ke io"> — env-manager </strong>需要通过我们代码所在的本地/conda/虚拟环境。</li><li id="7c41" class="lx ly in ke b kf mg ki mh kl mi kp mj kt mk kx ml md me mf bi translated"><strong class="ke io"> -P </strong>表示参数，确保参数与<strong class="ke io"> MLproject </strong>文件中定义的参数相同，并且顺序相同。</li></ol><pre class="mm mn mo mp gt mq mr ms bn mt mu bi"><span id="4be6" class="mv kz in mr b be mw mx l my mz"># This one is for running the Preprocessing file.<br/>%%shell <br/>mlflow run --experiment-name prepData -b local \<br/>--entry-point preprocessing_classification --env-manager local --run-name preprocessText \<br/>-P index2word="index2word.pickle" \<br/>-P word2index="word2index.pickle" \<br/>src<br/><br/><br/>%%shell<br/>for i in {0..4}<br/>do<br/>  mlflow run --experiment-name TrainingModel -b local \<br/>  --entry-point training_classification --env-manager local --run-name fold_$i \<br/>  -P grad_accumulation=1 \<br/>  -P budget=200 \<br/>  -P trainFile="datasets/fold_$i/train.csv" \<br/>  -P testFile="datasets/fold_$i/test.csv" \<br/>  -P batch=4 \<br/>  -P lr=1e-5 \<br/>  src<br/>done</span></pre><p id="be16" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><em class="na">访问 github 获取完整代码:</em><a class="ae lw" href="https://github.com/Anurich/Active-Learning-With-MLflow" rel="noopener ugc nofollow" target="_blank">https://github.com/Anurich/Active-Learning-With-MLflow</a></p><h1 id="f18e" class="nl kz in bd la nm nn no ld np nq nr lg jt ns ju lj jw nt jx lm jz nu ka lp nv bi translated">结论。</h1><p id="e458" class="pw-post-body-paragraph kc kd in ke b kf lr jo kh ki ls jr kk kl lt kn ko kp lu kr ks kt lv kv kw kx ig bi translated">在本文中，主要讨论如何将主动学习与 MLflow 结合起来，以及如何使用 MLflow 来记录信息。我们还研究了使用 MLflow 记录信息的不同方式，以及主动学习如何帮助我们从未标记的数据集中选择最佳样本，并将其与标记的数据集相结合。我们还讨论了如何创建 MLProject 和 Conda.yaml 文件来实现代码再现性。</p></div></div>    
</body>
</html>