<html>
<head>
<title>What’s new in Spark-Radiant 1.0.4 ?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Spark-Radiant 1.0.4 有什么新功能？</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/whats-new-in-spark-radiant-1-0-4-e8190c1370d3?source=collection_archive---------10-----------------------#2022-10-17">https://blog.devgenius.io/whats-new-in-spark-radiant-1-0-4-e8190c1370d3?source=collection_archive---------10-----------------------#2022-10-17</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><p id="3ac8" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><a class="ae ki" href="https://github.com/SaurabhChawla100/spark-radiant" rel="noopener ugc nofollow" target="_blank"> <strong class="jm io"> Spark-Radiant </strong> </a>是阿帕奇 Spark 性能和成本优化器。spark-Radiant 产品将有助于优化性能和成本，考虑催化剂优化规则、Spark 中增强的自动缩放、收集与 Spark 作业相关的重要指标、Spark 中的 BloomFilter 指数等。</p><p id="3a4f" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我在<a class="ae ki" href="https://saurabhchawla100.medium.com/spark-radiant-is-now-available-13914e35a61f" rel="noopener">之前的博客</a>中介绍了 Spark Radiant 这个项目。Spark-Radiant 1.0.4 现已推出，随时可用。Spark-Radiant 1.0.4 的依赖关系在<a class="ae ki" href="https://mvnrepository.com/artifact/io.github.saurabhchawla100/spark-radiant-sql/1.0.4" rel="noopener ugc nofollow" target="_blank"> maven central </a>中可用。在这篇博客中，我将讨论 Spark-Radiant 1.0.4 的可用性，以及作为该版本的一部分添加的新功能。</p><blockquote class="kj kk kl"><p id="232c" class="jk jl km jm b jn jo jp jq jr js jt ju kn jw jx jy ko ka kb kc kp ke kf kg kh ig bi translated"><strong class="jm io"> <em class="in">如何使用</em></strong><a class="ae ki" href="https://github.com/SaurabhChawla100/spark-radiant" rel="noopener ugc nofollow" target="_blank"><strong class="jm io"><em class="in">Spark-Radiant-1.0？</em> </strong> </a> <strong class="jm io"> <em class="in"> 4 用星火就业？</em> </strong></p></blockquote><p id="539f" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">对于 maven 项目，在 pom.xml 中使用下面的依赖关系</p><p id="549f" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><a class="ae ki" href="https://mvnrepository.com/artifact/io.github.saurabhchawla100/spark-radiant-sql/1.0.4" rel="noopener ugc nofollow" target="_blank"> <strong class="jm io">火花-辐射-sql </strong> </a></p><p id="2426" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><dependency><br/>&lt;groupId&gt;io . github . saurabhchawla 100&lt;/groupId&gt;<br/>&lt;artifactId&gt;spark-radiant-SQL&lt;/artifactId&gt;<br/>&lt;版本&gt;1 . 0 . 4&lt;/版本&gt;<br/>&lt;/依赖关系&gt;</dependency></p><p id="0135" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><a class="ae ki" href="https://mvnrepository.com/artifact/io.github.saurabhchawla100/spark-radiant-core/1.0.4" rel="noopener ugc nofollow" target="_blank"> <strong class="jm io">火花辐射核心</strong> </a></p><p id="1604" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><dependency><br/>&lt;groupId&gt;io . github . saurabhchawla 100&lt;/groupId&gt;<br/>&lt;artifactId&gt;spark-radiant-core&lt;/artifactId&gt;<br/>&lt;版本&gt;1 . 0 . 4&lt;/版本&gt;<br/>&lt;/依赖&gt;</dependency></p><blockquote class="kj kk kl"><p id="7da7" class="jk jl km jm b jn jo jp jq jr js jt ju kn jw jx jy ko ka kb kc kp ke kf kg kh ig bi translated"><strong class="jm io"> <em class="in">先决条件</em></strong><em class="in"><br/>a)Spark-3.0 . x 和 Spark 的更新版本支持 Spark-Radiant<br/>b)支持 Scala 版本 2.12 . x .<br/>c)Spark-1 . 0 . 4 提供 Scala、Pyspark、Java、spark-sql 支持</em></p></blockquote><h1 id="4a12" class="kq kr in bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">用火花辐射器运行火花作业</h1><p id="ed77" class="pw-post-body-paragraph jk jl in jm b jn lo jp jq jr lp jt ju jv lq jx jy jz lr kb kc kd ls kf kg kh ig bi translated">运行 spark 作业时，在运行时使用 maven central 发布的 spark-radiant-sql-1.0.4.jar、spark-radiant-core-1.0.4.jar</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="17a2" class="mc kr in ly b gy md me l mf mg">./bin/spark-shell --packages "io.github.saurabhchawla100:spark-radiant-sql:1.0.4,io.github.saurabhchawla100:spark-radiant-core:1.0.4"</span><span id="a3ed" class="mc kr in ly b gy mh me l mf mg">./bin/spark-submit <br/> --packages "io.github.saurabhchawla100:spark-radiant-sql:1.0.4,io.github.saurabhchawla100:spark-radiant-core:1.0.4"<br/> --class com.test.spark.examples.SparkTestDF /spark/examples/target/scala-2.12/jars/spark-test_2.12-3.2.0.jar</span></pre><h1 id="78ed" class="kq kr in bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">Spark-Radiant 1.0.4 发行说明</h1><p id="d0f1" class="pw-post-body-paragraph jk jl in jm b jn lo jp jq jr lp jt ju jv lq jx jy jz lr kb kc kd ls kf kg kh ig bi translated">a) <strong class="jm io">对 Spark 3.2 的支持:</strong> Spark-Radiant 1.0.4 拥有 Apache Spark 的默认版本 Spark-3.2.1。发布的 maven jar 依赖于 spark-3.2.1。用于搭配 spark-radiant 使用较低版本的 spark(3.1.x)。需要使用 spark 3.1.x 构建 spark-radiant。spark 3.1 的配置文件已经得到支持。</p><p id="80ee" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">使用下面的命令构建 spark-3.1.x</p><p id="8d97" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><code class="fe mi mj mk ly b">mvn clean install -DskipTests -Pspark31 / mvn clean package -DskipTests -Pspark31</code></p><p id="3890" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">b) <strong class="jm io">度量收集器的改进:</strong>spark-radiant 1 . 0 . 4 的度量收集器中增加了一些新功能和改进。</p><ol class=""><li id="7b49" class="ml mm in jm b jn jo jr js jv mn jz mo kd mp kh mq mr ms mt bi translated">添加对为执行者能力提供计算建议的支持。</li><li id="2a82" class="ml mm in jm b jn mu jr mv jv mw jz mx kd my kh mq mr ms mt bi translated">为阶段级别指标提供作业 ID 信息。</li><li id="3cca" class="ml mm in jm b jn mu jr mv jv mw jz mx kd my kh mq mr ms mt bi translated">用户还可以通过扩展 org . Apache . spark . Publish metrics 接口并覆盖 publishStageLevelMetrics 方法，由他们自己的自定义 Publisher 类发布指标。SamplePublishMetrics 类已经添加到项目中以供参考，在运行 spark 应用程序时，需要在此配置中提供自定义发布类名。<code class="fe mi mj mk ly b">--conf spark.radiant.metrics.publishClassName=com.spark.radiant.core.SamplePublishMetrics</code></li></ol><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="9746" class="mc kr in ly b gy md me l mf mg">*****Stage Info Metrics*****<br/>    ***** Stage Info Metrics Stage Id:0 *****<br/>    {<br/>    "Job Id":"0",<br/>    "Stage Id":"0",<br/>    "Final Stage Status":"succeeded",<br/>    "Number of Task":"10",<br/>    "Total Executors ran to complete all Task":"2",<br/>    "Stage Completion Time":"858 ms",<br/>    "Stage Completion Time Recommendation":"With 2x executors(4), time to complete stage 366 ms.<br/>     With 4x executors(8), time to complete stage 183 ms.",<br/>    "Average Task Completion Time":"139 ms"<br/>    "Number of Task Failed in this Stage":"0"<br/>    "Few Skew task info in Stage":"Skew task in not present in this stage"<br/>    "Few Failed task info in Stage":"Failed task in not present in this stage"<br/>    }</span></pre><p id="c622" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">c)<strong class="jm io">joinreueseexchangeoptimizer ule:</strong>joinreueseexchangeoptimizer ule 适用于相同表之间存在连接并且多次扫描表的情况。应用此规则后，文件扫描将进行一次。请参照星火计划执行</p><figure class="lt lu lv lw gt na gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi mz"><img src="../Images/fcf60b24c87e43fd6882a96c62528202.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*NDrUHV_BAbrWJJGH.png"/></div></div><figcaption class="nh ni gj gh gi nj nk bd b be z dk translated">图 1 JoinReuseExchangeOptimizeRule</figcaption></figure><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="24e5" class="mc kr in ly b gy md me l mf mg">This feature is enabled using `--conf spark.sql.optimize.join.reuse.exchange.rule=true`<br/>   There is need to add this conf for adding this rule in Sql Extension<br/>   `--conf spark.sql.extensions=com.spark.radiant.sql.api.SparkRadiantSqlExtension`<br/>  <br/>   spark.sql("""select * from<br/>   (select col_1, count(col_1) count from table1 where col_2 in ('value0', 'value09') group by col_1) a,<br/>   (select col_1, max(col_2) max from table1 where col_2 in ('value0', 'value1', 'value119') group by col_1) b where a.col_1=b.col_1""")```</span></pre><p id="7ba5" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">对于这个查询，JoinReuseExchangeOptimizeRule 比常规的 Spark Join 快 2 倍。</p><p id="4e7e" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">d) <strong class="jm io">改进</strong> <a class="ae ki" href="https://github.com/SaurabhChawla100/spark-radiant/blob/master/docs/SizeBasedJoinOrderingRerInSpark.md" rel="noopener ugc nofollow" target="_blank"> <strong class="jm io">基于尺寸的连接重新排序</strong> </a></p><ol class=""><li id="985e" class="ml mm in jm b jn jo jr js jv mn jz mo kd mp kh mq mr ms mt bi translated">在两个排序合并连接之间增加了对基于大小的连接重新排序的支持。较小的表排序合并联接在较大的表排序合并联接之前执行。</li><li id="cde0" class="ml mm in jm b jn mu jr mv jv mw jz mx kd my kh mq mr ms mt bi translated">spark . sql . support . SBO . smj—Config 添加对 SQL 查询的 SizeBasedJoinReOrdering 的支持，当所有连接都不是 BHJ，只有 smj 时。默认值为 false。这将与 spark-radiant-1.0.4 一起工作。</li><li id="2ae7" class="ml mm in jm b jn mu jr mv jv mw jz mx kd my kh mq mr ms mt bi translated">scala、pyspark、spark-sql、Java、R 使用 conf <code class="fe mi mj mk ly b">--conf spark.sql.extensions=com.spark.radiant.sql.api.SparkRadiantSqlExtension</code>提供基于大小的连接重排序支持。</li></ol><blockquote class="kj kk kl"><p id="0e89" class="jk jl km jm b jn jo jp jq jr js jt ju kn jw jx jy ko ka kb kc kp ke kf kg kh ig bi translated"><strong class="jm io">T16】结论:T18】</strong></p><p id="14f1" class="jk jl km jm b jn jo jp jq jr js jt ju kn jw jx jy ko ka kb kc kp ke kf kg kh ig bi translated">在这篇博客中，我讨论了如何使用<a class="ae ki" href="https://github.com/SaurabhChawla100/spark-radiant" rel="noopener ugc nofollow" target="_blank"><em class="in">Spark-Radiant</em></a><em class="in">1 . 0 . 4。Spark-Radiant 1.0.4 中添加的新功能，如对</em><strong class="jm io">joinreuseexchange optimizer、</strong><em class="in"/><strong class="jm io"><em class="in">SizeBasedJoinReOrdering</em></strong><em class="in">、</em> <strong class="jm io">指标收集器、</strong> <a class="ae ki" href="https://github.com/SaurabhChawla100/spark-radiant/blob/master/docs/dynamicFilterInSpark.md" rel="noopener ugc nofollow" target="_blank"> <strong class="jm io">动态过滤器</strong> </a> <strong class="jm io"> </strong> <em class="in">等的改进，将提供与性能和成本优化相关的好处。</em></p><p id="9b4e" class="jk jl km jm b jn jo jp jq jr js jt ju kn jw jx jy ko ka kb kc kp ke kf kg kh ig bi translated">在不久的将来，我们会推出新的相关博客。请继续关注此空间，了解更多信息！</p></blockquote></div></div>    
</body>
</html>