<html>
<head>
<title>Creating an Automated Data Processing Pipeline with Apache Airflow, Kubernetes, and R — Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Apache Airflow、Kubernetes 和 R 创建自动化数据处理管道—第 1 部分</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/creating-an-automated-data-processing-pipeline-with-apache-airflow-kubernetes-and-r-part-1-925f99b812e7?source=collection_archive---------8-----------------------#2022-02-02">https://blog.devgenius.io/creating-an-automated-data-processing-pipeline-with-apache-airflow-kubernetes-and-r-part-1-925f99b812e7?source=collection_archive---------8-----------------------#2022-02-02</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><figure class="gl gn jl jm jn jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi jk"><img src="../Images/b013a876b8e417f7973c52b11f767dbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*nsmTBKUPXlocbZq1"/></div></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">照片由<a class="ae jz" href="https://unsplash.com/@siniz?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Siniz Kim </a>在<a class="ae jz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="a1b3" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">在研究如何自动化数据处理管道时，Apache Airflow 作为编排管道片段的方法被多次提到。但是当我深入研究时，常见的实现似乎建议在与 Airflow 相同的实例上运行操作符，并且使用的操作符是有限的(用<code class="fe ky kz la lb b">PythonOperator</code>运行 python，用<code class="fe ky kz la lb b">BashOperator</code>运行 shell 脚本)。我们的分析团队主要使用 R，所以我需要某种设置来让我执行 R 脚本，但我也想将任务执行隔离到单独的 VM 实例或应用程序或项目。</p><p id="ba10" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">我最终发现了<code class="fe ky kz la lb b">KubernetesPodOperator</code>,但是从零开始建立一个完整的 Kubernetes 集群来使用 Airflow 似乎需要很多时间，特别是对于没有太多使用 Kubernetes 的人来说。Cloud Composer 是 Google 为 Airflow 创建的一个完全管理的 Kubernetes 集群，它为我解决了这个问题，但是将点连接起来以执行我们的 R 脚本仍然很困难。</p><p id="8da6" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">BBC 数据科学来拯救我们。他们关于气流和 R 的文章是我需要的最后一篇文章。虽然它利用了 AWS，但我能够替换 Cloud Composer 的一些部分，并使用<code class="fe ky kz la lb b">KubernetesPodOperator</code>来代替他们的 AWS 批处理。</p><h1 id="2227" class="lc ld in bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">概观</h1><p id="e762" class="pw-post-body-paragraph ka kb in kc b kd ma kf kg kh mb kj kk kl mc kn ko kp md kr ks kt me kv kw kx ig bi translated">虽然我们完全可以从头开始构建这一切，设置一个气流实例、一个调度器、一个队列、一个保存 DAG 信息的数据库和一个用于执行我们的任务的节点池，但我转向了 Google Cloud 提供的一个“一体化”解决方案:<a class="ae jz" href="https://cloud.google.com/composer" rel="noopener ugc nofollow" target="_blank"> Cloud Composer </a>。作为一个小团队的工程师，完全或部分管理的解决方案大大减少了我们的开发时间，让我们可以更专注于开发。Cloud Composer 非常适合这一点。</p><p id="3683" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">他们的解决方案包含了我上面列出的大部分内容。它是一个 Kubernetes 集群，由以下部分组成:一个 Flask web server for Airflow(使用 App Engine Flex)，一个将任务分配给工人的调度程序，一个保存 DAG 数据的数据库，一个用于调度任务的 Redis 队列，当然还有工人本身。一切都建立在一个“租户”项目中，这是一个独立的谷歌管理的项目，与您最初的谷歌云项目相关联。几乎所有服务都有日志。并且所有 Dag 和 Airflow 插件都进入云存储桶，使用<code class="fe ky kz la lb b">syncd</code>同步到 Airflow。</p><p id="56ce" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">为了简化 Dag、任务和插件的配置和开发，我创建了一个存储库，以一种干净的方式保存所有这些内容，并允许我们“部署”我们的文件(即，将所有内容上传到云存储的相应文件夹中，然后供 Airflow 使用)。</p><p id="4902" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">我们的 Dag 将主要使用<code class="fe ky kz la lb b">KubernetesPodOperator</code>将每个任务旋转到它自己的 pod 中，利用 docker 文件来构建它。entrypoint 将是一个小的“bootstrap”脚本，它将通过 Google Cloud 进行身份验证，下载任务必需的 shell 脚本，并执行它。如果没有错误，pod 将关闭，工作流将转到下一个任务，重复此过程。</p><p id="cba7" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">虽然这种旋转加速和旋转减速确实增加了工作流的执行时间，但它确保了最大的隔离性、可再现性，以及幂等性(如果任务是这样编写的话)。假设用于任务的任何数据输入保持不变，那么任务应该总是以相同的方式执行，并且输出应该总是相同的。每个任务都会生成日志，如果出现问题，我们会对单个任务进行故障排除。此外，任何内存、配置或环境错误都会在单个任务中出现，因此也很容易修复。</p></div><div class="ab cl mf mg hr mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ig ih ii ij ik"><h1 id="d1be" class="lc ld in bd le lf mm lh li lj mn ll lm ln mo lp lq lr mp lt lu lv mq lx ly lz bi translated">设置</h1><p id="86ff" class="pw-post-body-paragraph ka kb in kc b kd ma kf kg kh mb kj kk kl mc kn ko kp md kr ks kt me kv kw kx ig bi translated">这个项目的设置将在三个地方进行:存储库、您的本地环境和 Cloud Composer 环境。一旦这些都设置好了，我们就可以开始调整设置和配置了。</p><h2 id="bae6" class="mr ld in bd le ms mt dn li mu mv dp lm kl mw mx lq kp my mz lu kt na nb ly nc bi translated">仓库</h2><p id="7cb9" class="pw-post-body-paragraph ka kb in kc b kd ma kf kg kh mb kj kk kl mc kn ko kp md kr ks kt me kv kw kx ig bi translated">这里已经设置了一个供你分叉的空白库<a class="ae jz" href="https://github.com/rgutierrez-cotech/airflow-k8s-r-template" rel="noopener ugc nofollow" target="_blank">。自述文件应该包含您需要的所有内容，但是我将把一些描述复制到本文中。<code class="fe ky kz la lb b">common_files</code>、<code class="fe ky kz la lb b">dags</code>、<code class="fe ky kz la lb b">plugins</code>和<code class="fe ky kz la lb b">config</code>文件夹的内容被部署(用<code class="fe ky kz la lb b">gsutil rsync</code>上传)到云存储中，而其他的内容要么只存在于存储库中，要么只存储在本地。</a></p><p id="435b" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">文件夹的布局如下:</p><p id="1335" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io"> dags </strong></p><p id="6c3f" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">包含气流的 Dag 和任务代码。为了便于维护，我将任务文件分离到它们自己的模块中。有一个示例 DAG 和任务说明了如何将所有内容连接在一起。</p><p id="4cf3" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">任务 _ 文件</strong></p><p id="147e" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">这是这个项目的核心。任务在 pod 中执行所需的所有文件都将存在于此。这将需要包含一个外壳脚本，该脚本将由容器下载并运行。在 shell 脚本中，我们将执行特定任务所需的所有代码，因此包括任何相关的 R 脚本、python 脚本等。这里也是。</p><p id="f0ef" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">图片</strong></p><p id="b02f" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">包含我们将使用的任何容器图像的 docker 文件和相关文件</p><p id="99c6" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">部署</strong></p><p id="9b40" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">包含以前部署的 TAR 存档。该目录中的任何内容都不会在存储库中被跟踪；这些档案将只存在于本地。部署脚本将使用这个目录，在将文件复制到云存储之前，创建文件当前状态的 tar。这里任何已经存在的 TAR 也可以用于部署，作为一种“回滚”部署。如果您提供 TAR 的名称(不包括文件扩展名)，部署脚本将解压缩它并从该归档文件中部署文件。</p><p id="10b9" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">常用 _ 档案</strong></p><p id="f257" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">多个任务使用的任何文件都将存在于此。这些将是像对应表，人行横道，代码簿，数据元素字典等东西。基本上，任何不经常改变的，被多个任务使用的，以及你想跟踪改变的。此文件夹是可选的，仅当您的存储库是私有的时才推荐使用。</p><p id="5fab" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">配置</strong></p><p id="f3f5" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">包含每个版本的 Cloud Composer 环境的气流配置文件(<code class="fe ky kz la lb b">airflow.cfg</code>)。可用设置可在<a class="ae jz" href="https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html" rel="noopener ugc nofollow" target="_blank">气流配置页面</a>找到。本文稍后将提供更多信息。</p><p id="dbef" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">插件</strong></p><p id="210b" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">包含气流插件文件。访问<a class="ae jz" href="https://airflow.apache.org/docs/apache-airflow/stable/plugins.html" rel="noopener ugc nofollow" target="_blank">气流插件页面</a>查看如何创建插件。关于如何为 Airflow 编写插件的信息将在本文后面提供。</p><h2 id="b1c3" class="mr ld in bd le ms mt dn li mu mv dp lm kl mw mx lq kp my mz lu kt na nb ly nc bi translated">局部环境</h2><p id="29a9" class="pw-post-body-paragraph ka kb in kc b kd ma kf kg kh mb kj kk kl mc kn ko kp md kr ks kt me kv kw kx ig bi translated">要处理任务创建并与 Google Cloud 交互，您需要在机器上安装以下包/库:</p><pre class="nd ne nf ng gt nh lb ni nj aw nk bi"><span id="6a0a" class="mr ld in lb b gy nl nm l nn no">google-cloud-sdk<br/>git-lfs<br/>virtualbox<br/>kubectl<br/>docker-machine<br/>docker<br/>pyenv<br/>pyenv-virtualenv</span></pre><p id="d58e" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">其中三个是可选的。我正在使用<code class="fe ky kz la lb b">git-lfs</code>上传一个专有数据分析软件包的二进制文件，它需要被复制到我们的容器映像中，以便我们可以在我们的任务中使用它。<code class="fe ky kz la lb b">pyenv</code>和<code class="fe ky kz la lb b">pyenv-virtualenv</code>用于创建一个本地 python 环境，模仿 Airflow web 服务器的环境，以便在编写 Dag 和任务时帮助语法高亮、代码完成和林挺。</p><h2 id="bab5" class="mr ld in bd le ms mt dn li mu mv dp lm kl mw mx lq kp my mz lu kt na nb ly nc bi translated">云作曲家</h2><p id="7dfe" class="pw-post-body-paragraph ka kb in kc b kd ma kf kg kh mb kj kk kl mc kn ko kp md kr ks kt me kv kw kx ig bi translated">理想情况下，您应该创建至少两个 Cloud Composer 环境，一个生产环境和一个试运行/开发环境。我只使用了两个，并在我们的“登台”环境中完成了我所有的开发和测试，尽管拥有一个单独的开发环境和一个测试环境也是可行的。</p><p id="214e" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">创建 Cloud Composer 环境时，请确保您已经选择了希望成为环境“宿主”的项目。我们有单独的项目用于我们的产品化和生产代码，所以我从选择我们的产品化项目开始。</p><p id="0351" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">在 Google Cloud 控制台的导航菜单中，找到大数据部分，然后单击 Composer。在顶部，单击“创建”下拉菜单，然后选择“编写器 1”或“编写器 2”。在本教程中，我将使用 Composer 1。</p><p id="5bba" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">您使用的设置可能与我们的不同，但这些可以作为起点:</p><pre class="nd ne nf ng gt nh lb ni nj aw nk bi"><span id="e70d" class="mr ld in lb b gy nl nm l nn no">- Location: us-central1<br/>- Service account: default Compute Engine service account<br/>- Image version: composer-1.17.3-airflow-1.10.15<br/>- Python version: 3<br/>- Web server machine type: composer-n1-webserver-2 (2 vCPU, 1.6 GB memory)<br/>- Cloud SQL machine type: db-n1-standard-2 (2 vCPU, 7.5 GB memory)<br/>- Workers:<br/>    - Nodes: 3<br/>    - Disk size: 50gb<br/>    - Machine type: n1-standard-1<br/>- Number of schedulers: 1<br/>- GKE Cluster<br/>    - Zone: us-central1-c</span></pre><p id="8763" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">Composer 环境可能需要 15 分钟才能完全部署。这是再喝一杯咖啡或茶的好时机。</p><p id="29be" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">一旦部署完毕，我们还需要设置一些东西。</p><p id="8485" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">部署脚本</strong></p><p id="2261" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">一旦部署完成，确保将云存储 bucket 名称复制到部署脚本的<code class="fe ky kz la lb b">bucket</code>变量中。</p><p id="c613" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">秘密</strong></p><p id="a450" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">对于创建 Cloud Composer 环境时使用的任何服务帐户，您都需要创建一个密钥。该密钥将用于在您的任务工人 pod 中使用谷歌云服务，如<code class="fe ky kz la lb b">gcloud</code>和<code class="fe ky kz la lb b">gsutil</code>。为此，我们将利用 Kubernetes 的秘密功能。</p><p id="0c40" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">在云控制台中，转到 IAM &amp; Admin &gt;服务帐户。点按服务帐户上的三个点，然后选择“管理密钥”。添加一个新的 JSON 键，并将该文件复制到您机器上的项目根文件夹中。不要在存储库中跟踪它。</p><p id="09cf" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">接下来，我们需要将它添加为 Kubernetes 秘密。在您的计算机上，打开一个终端选项卡或窗口，导航到您的项目根目录，并运行以下命令，填写参数。您可以通过在控制台中转至您的 Cloud Composer 环境并单击“环境详细信息”选项卡来找到所需的信息。</p><pre class="nd ne nf ng gt nh lb ni nj aw nk bi"><span id="b3b7" class="mr ld in lb b gy nl nm l nn no">$ gcloud container clusters get-credentials CLUSTER_ID \<br/>--zone ZONE \<br/>--project PROJECT</span></pre><p id="21d6" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">该命令为我们提供了到集群的连接凭据。</p><p id="e18b" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">现在添加我们之前下载的密钥文件作为秘密:</p><pre class="nd ne nf ng gt nh lb ni nj aw nk bi"><span id="b821" class="mr ld in lb b gy nl nm l nn no">$ kubectl create secret generic KEY_NAME \<br/>-- from-file REMOTE_KEY_NAME.json=./LOCAL_KEY_NAME.json</span></pre><p id="3d84" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">远程密钥名称和本地密钥名称可以相同。</p><p id="d567" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">您可以通过进入控制台中的 Kubernetes Engine &gt; Configuration 来验证这个秘密的存在。</p><p id="d5be" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">环境变量</strong></p><p id="de45" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">我在 Dag 中使用了一些环境变量，现在让我们添加它们。从云控制台，转到您的 Cloud Composer 环境并打开 Airflow UI。在气流中，转到管理&gt;变量。</p><p id="4acf" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">我们将在这里创建三个变量:</p><pre class="nd ne nf ng gt nh lb ni nj aw nk bi"><span id="9cb9" class="mr ld in lb b gy nl nm l nn no">- cloud_storage_bucket: BUCKET<br/>- project_id: PROJECT<br/>- valid_instantiators: airflow\r\nuser 1\r\nuser 2</span></pre><p id="3181" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">填写为 Cloud Composer 创建的云存储空间的名称，并填写运行 Cloud Composer 的项目 id。</p><p id="9fdb" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><code class="fe ky kz la lb b">valid_instantiators</code>是一个换行符分隔的“实例化器”列表，它将触发 Dag。我添加了这一点，以便当我们手动触发 DAG 时，我们可以在任务中访问的<code class="fe ky kz la lb b">conf</code>变量中指定一个实例化器，然后根据有效实例化器的列表对其进行检查。创建自定义运行 ID 时将使用指定的实例化器，并且此自定义运行 ID 将用作 DAG 运行的云存储中的输出文件夹。当我在后面谈到示例 DAG/task 时，这将更有意义。</p><p id="59b7" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">气流配置</strong></p><p id="19e4" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">您可以通过在<code class="fe ky kz la lb b">config</code>文件夹的相关文件夹中包含一个<code class="fe ky kz la lb b">airflow.cfg</code>文件来定义自定义气流配置。我建议等到您的 Cloud Composer 环境完全实例化后，<em class="np">然后</em>复制。cfg 文件保存到这个存储库中的相关文件夹中(<code class="fe ky kz la lb b">config/staging</code>用于您的暂存环境，<code class="fe ky kz la lb b">config/production</code>用于您的生产环境)。</p><p id="5a24" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">您可以添加/修改的设置可以在<a class="ae jz" href="https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html" rel="noopener ugc nofollow" target="_blank">气流配置页面</a>上找到。</p><p id="9a7c" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">气流插件</strong></p><p id="a928" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">如果你想为 Airflow 编写插件，把它们放在<code class="fe ky kz la lb b">plugins</code>文件夹中。创建插件时，遵循<a class="ae jz" href="https://airflow.apache.org/docs/apache-airflow/stable/plugins.html" rel="noopener ugc nofollow" target="_blank">气流插件页面</a>上的说明和示例。</p><p id="ab88" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">所有 Cloud Composer 和 Airflow 版本都支持插件，但 UI/web 服务器插件<strong class="kc io">在 Cloud Composer 1 和 Airflow 2 中</strong>不可用。</p><p id="2be6" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io"> <em class="np">重要:</em> </strong>如果您正在使用 Cloud Composer 1 环境和 Airflow 1，并且正在编写一个 UI/web 服务器插件(将在 Airflow UI 中加载/使用)，<strong class="kc io"> <em class="np">您需要禁用 DAG 序列化，以便它能够工作</em> </strong> <em class="np">。</em>基于<a class="ae jz" href="https://cloud.google.com/composer/docs/concepts/versioning/composer-versioning-overview" rel="noopener ugc nofollow" target="_blank">该页面</a>，DAG 序列化禁用 UI 插件，<strong class="kc io"> <em class="np">，默认开启</em> </strong> <em class="np">。</em>为此，您需要禁用 airflow.cfg 文件中的两个核心配置设置:<code class="fe ky kz la lb b">core/store_serialized_dags</code>和<code class="fe ky kz la lb b">core/store_dag_code</code>。访问<a class="ae jz" href="https://cloud.google.com/composer/docs/dag-serialization#disable" rel="noopener ugc nofollow" target="_blank">本页</a>获取更多信息。</p><p id="6a60" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">这让我头疼了好几天，我偶然发现了解决方法。我把它贴在这里是为了让你不那么头疼。</p><h2 id="f086" class="mr ld in bd le ms mt dn li mu mv dp lm kl mw mx lq kp my mz lu kt na nb ly nc bi translated">气流/云合成器提示和技巧</h2><p id="efde" class="pw-post-body-paragraph ka kb in kc b kd ma kf kg kh mb kj kk kl mc kn ko kp md kr ks kt me kv kw kx ig bi translated">以下是在此回购中使用 Cloud Composer 和资产的一些(仅一个)分类提示和技巧。</p><p id="305d" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">刷新 Cloud Composer 环境/气流</strong></p><p id="b896" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">如果您想要重新部署您的 Cloud Composer 环境，可以通过添加一个“伪”环境变量并修改其值来实现。</p><p id="1ef3" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">导航到 Google Cloud 控制台中的 Cloud Composer 部分，然后单击您的环境。然后转到环境变量选项卡。现在添加一个假的键和值并保存。或者，如果您已经有一个伪变量，只需编辑值并保存即可。这将重新部署 Cloud Composer 环境。这可能需要 15 分钟才能完成。</p><p id="7a8b" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">如果您对 Airflow 配置文件进行了更改，或者正在调试 DAG/插件加载错误，您可能需要重新启动 Airflow web 服务器以查看任何更改是否生效。要重新启动 Airflow web 服务器(仅限 Cloud Composer 1)，您可以从 shell 中运行以下命令，并填入参数:</p><pre class="nd ne nf ng gt nh lb ni nj aw nk bi"><span id="6a8b" class="mr ld in lb b gy nl nm l nn no">$ gcloud composer environments restart-web-server ENVIRONMENT — location=LOCATION</span></pre><p id="db2f" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">这可能需要 15 分钟才能完成。</p></div><div class="ab cl mf mg hr mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ig ih ii ij ik"><h1 id="2d5f" class="lc ld in bd le lf mm lh li lj mn ll lm ln mo lp lq lr mp lt lu lv mq lx ly lz bi translated">总结</h1><p id="f077" class="pw-post-body-paragraph ka kb in kc b kd ma kf kg kh mb kj kk kl mc kn ko kp md kr ks kt me kv kw kx ig bi translated">这是数据处理管道教程的第一部分。我们已经设置了 Cloud Composer 环境，设置了用于创建 Dag 和任务的本地环境，并派生了模板 repo。</p><p id="1898" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">在下一部分的<a class="ae jz" href="https://robert-a-gutierrez.medium.com/creating-an-automated-data-processing-pipeline-with-apache-airflow-kubernetes-and-r-part-2-2e95c2e9ae5e" rel="noopener">中，我们将创建自己的 DAG 和任务，上传到 Airflow，并运行它！</a></p></div></div>    
</body>
</html>