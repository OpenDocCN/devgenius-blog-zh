<html>
<head>
<title>Basic Machine Learning on PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark 上的基本机器学习</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/basic-machine-learning-on-pyspark-f6264bdd9012?source=collection_archive---------15-----------------------#2022-08-19">https://blog.devgenius.io/basic-machine-learning-on-pyspark-f6264bdd9012?source=collection_archive---------15-----------------------#2022-08-19</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><p id="bf36" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">银行流失分类</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/096eef9af9600e77f441aa78eef40ba6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A_xKKGc5Tgjon9N4TYyxrw.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">马库斯·温克勒在<a class="ae ky" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="4967" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi kz translated"><span class="l la lb lc bm ld le lf lg lh di">如果</span>你有处理数据的经验，你可能以前用过<a class="ae ky" href="https://pandas.pydata.org/docs/index.html" rel="noopener ugc nofollow" target="_blank">熊猫</a>库。我们主要使用熊猫进行数据处理。这是一个非常强大的库，我们可以用它进行各种各样的数据预处理，比如处理缺失值、过滤和特征提取。问题是，如果我们处理的大数据的大小无法容纳在一台机器的内存中，我们就不能再使用“正常”的熊猫了，我们需要能够处理大数据的工具。最流行的大数据工具之一是<a class="ae ky" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Spark </a>。Apache Spark 是用 Scala 编写的，但是有了 PySpark，我们可以使用 Python API 编写 Spark 应用程序。此外，在 Apache Spark 3.2 上，他们已经提供了<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html" rel="noopener ugc nofollow" target="_blank"> Pandas API </a>，使 Pandas 用户能够毫无问题地使用 Spark。</p><h2 id="8cd8" class="li lj in bd lk ll lm dn ln lo lp dp lq jv lr ls lt jz lu lv lw kd lx ly lz ma bi translated">介绍</h2><p id="179c" class="pw-post-body-paragraph jk jl in jm b jn mb jp jq jr mc jt ju jv md jx jy jz me kb kc kd mf kf kg kh ig bi translated">在本文中，我们将尝试使用 PySpark 创建一个分类模型。我不会在 Spark 上使用 Pandas API，而是使用 PySpark API 来显示两个 API 之间的相似性。将要使用的数据集来自<a class="ae ky" href="https://www.kaggle.com/datasets/parisanahmadi/bank-data-churn-classification" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>。我还在<a class="ae ky" href="https://github.com/triesonyk/churn-classification-pyspark" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上传了完整的 python 笔记本代码</p><h2 id="14e7" class="li lj in bd lk ll lm dn ln lo lp dp lq jv lr ls lt jz lu lv lw kd lx ly lz ma bi translated">导入库</h2><p id="6d5b" class="pw-post-body-paragraph jk jl in jm b jn mb jp jq jr mc jt ju jv md jx jy jz me kb kc kd mf kf kg kh ig bi translated">在 PySpark 中开始制作机器学习模型，我们要做的第一件事就是导入我们需要的库。常用的库:</p><ul class=""><li id="f53c" class="mg mh in jm b jn jo jr js jv mi jz mj kd mk kh ml mm mn mo bi translated">SparkSession:无论我们使用 Scala、Java、Python 还是 r，启动任何 Spark 功能都需要它。</li><li id="85e4" class="mg mh in jm b jn mp jr mq jv mr jz ms kd mt kh ml mm mn mo bi translated">StructField、StructType 和任何常见的数据类型，如 StringType、Integer</li><li id="a66c" class="mg mh in jm b jn mp jr mq jv mr jz ms kd mt kh ml mm mn mo bi translated">常见函数，如 count、countDistinct、avg、mean 等。</li><li id="0c95" class="mg mh in jm b jn mp jr mq jv mr jz ms kd mt kh ml mm mn mo bi translated">管道来为我们想要预测的新数据集创建机器学习管道</li><li id="2134" class="mg mh in jm b jn mp jr mq jv mr jz ms kd mt kh ml mm mn mo bi translated">我们想要使用的算法，在这种情况下，我们想要建立一个分类模型，因此我将导入 DecisionTreeClassifier、RandomForestClassifier、GBTClassifier 和 LogisticRegression</li><li id="cbd9" class="mg mh in jm b jn mp jr mq jv mr jz ms kd mt kh ml mm mn mo bi translated">矢量和矢量装配器将用于装配特征</li><li id="14a9" class="mg mh in jm b jn mp jr mq jv mr jz ms kd mt kh ml mm mn mo bi translated">用于数据预处理的 StringIndexer、OneHotEncoder、Imputer、StandardScaler</li><li id="1bd5" class="mg mh in jm b jn mp jr mq jv mr jz ms kd mt kh ml mm mn mo bi translated">评估库，用于评估我们创建的模型</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><h2 id="d952" class="li lj in bd lk ll lm dn ln lo lp dp lq jv lr ls lt jz lu lv lw kd lx ly lz ma bi translated">Spark 会话和加载数据集</h2><p id="f861" class="pw-post-body-paragraph jk jl in jm b jn mb jp jq jr mc jt ju jv md jx jy jz me kb kc kd mf kf kg kh ig bi translated">每当我们想要使用任何种类的 spark 功能时，我们都必须执行 Spark 会话。appName 可以是我们想要的任何东西，我将使用<code class="fe mw mx my mz b">Churn </code>作为 appName。我们要做的下一件事是加载数据集，我将在这个项目中使用一个普通的 CSV 数据集。<code class="fe mw mx my mz b">inferSchema=True </code>意味着我们让 Spark 自动推断每个列的类型，而<code class="fe mw mx my mz b">header=True</code>意味着我们将使用 CSV 的第一行作为列名。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><h2 id="8329" class="li lj in bd lk ll lm dn ln lo lp dp lq jv lr ls lt jz lu lv lw kd lx ly lz ma bi translated">数据理解</h2><p id="c519" class="pw-post-body-paragraph jk jl in jm b jn mb jp jq jr mc jt ju jv md jx jy jz me kb kc kd mf kf kg kh ig bi translated">在制作模型之前，我们必须先了解数据。使用<code class="fe mw mx my mz b">printSchema()</code>我们可以看到每一列的名称和类型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/9fb012a41f93613b2f271bdc0781d325.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DIx11peO4TNMnpkz0cIlAw.png"/></div></div></figure><p id="8122" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">为了显示前 20 行，我们可以使用<code class="fe mw mx my mz b">show()</code>。不像熊猫，每次要展示数据框都要用<code class="fe mw mx my mz b">show()</code>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/7423a4318049f9ffa9a2b626e04d38ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9jRGBKtkz-_xGcUekykJsg.png"/></div></div></figure><p id="d53a" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">如果加上<code class="fe mw mx my mz b">vertical=True</code>参数，我们也可以垂直看到数据。当数据有很多列时，我通常使用这个参数，这样更容易查看所有的值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/72e00a5c1a2be7455b2175a0191a33b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pOl227Tg5gxZZhifYaJHjQ.png"/></div></div></figure><p id="2e52" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">要查看每一列的统计数据，我们可以使用<code class="fe mw mx my mz b">describe().show()</code>。我添加了参数，因为数据框太宽了，如果我们水平地看数据框，将会有一些行被分成两行，这使得数据框看起来很乱。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/af2d74e7510f428e1c1d620357e22bdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KpeMFE3NsrY_01AccfbkAw.png"/></div></div></figure><h2 id="336e" class="li lj in bd lk ll lm dn ln lo lp dp lq jv lr ls lt jz lu lv lw kd lx ly lz ma bi translated">缺少值</h2><p id="9876" class="pw-post-body-paragraph jk jl in jm b jn mb jp jq jr mc jt ju jv md jx jy jz me kb kc kd mf kf kg kh ig bi translated">要检查我们是否有丢失的值，我们可以使用下面的代码。因为我们没有丢失的值，所以我们不必做任何事情，但是如果您有丢失的值，您可以使用<code class="fe mw mx my mz b">DataFrame.na.fill({‘col_name’:any_value})</code>来填充丢失的值。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/c67dead51eecb9639b51dc36cf1159a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YjkZQ4h14W2ypb4iakatIg.png"/></div></div></figure><h2 id="900d" class="li lj in bd lk ll lm dn ln lo lp dp lq jv lr ls lt jz lu lv lw kd lx ly lz ma bi translated">字符串索引器</h2><p id="a0fd" class="pw-post-body-paragraph jk jl in jm b jn mb jp jq jr mc jt ju jv md jx jy jz me kb kc kd mf kf kg kh ig bi translated">一些列值仍然是字符串数据类型，所以在将它们放入模型之前，我们必须先将它们转换成数字。我们可以用<code class="fe mw mx my mz b">StringIndexer()</code>把字符串变成数字。这类似于标签编码。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/cefa5ee20ad65db9ce53f30e59ab3e3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*zVa8NMKK8cSv2T63mROLkQ.png"/></div></figure><h2 id="f97c" class="li lj in bd lk ll lm dn ln lo lp dp lq jv lr ls lt jz lu lv lw kd lx ly lz ma bi translated">OneHot 编码器</h2><p id="0abe" class="pw-post-body-paragraph jk jl in jm b jn mb jp jq jr mc jt ju jv md jx jy jz me kb kc kd mf kf kg kh ig bi translated">地理不是一个顺序特征，这就是为什么我们必须一次性编码它。这和 Scikitlearn 的 OneHotEncoder 有点不同，在 PySpark 上它会创建一个带有二进制向量值的列。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/627bd3dcf1f9df1a9008f319fa217a0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:494/format:webp/1*aTIb1xVAWLvFRGUNhtz54w.png"/></div></figure><h2 id="d735" class="li lj in bd lk ll lm dn ln lo lp dp lq jv lr ls lt jz lu lv lw kd lx ly lz ma bi translated">汇编数据</h2><p id="8a6f" class="pw-post-body-paragraph jk jl in jm b jn mb jp jq jr mc jt ju jv md jx jy jz me kb kc kd mf kf kg kh ig bi translated">当我们使用 PySpark 机器学习库时，他们希望输入是一种特定的格式，这就是为什么我们必须在拟合数据之前先组装数据。幸运的是，他们还提供了一个模块，可以将数据组装成正确的格式。为了更容易地显示数据框，我将创建另一个仅包含要素和标注的数据框。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/5f736639bedcc093d1fc79b7e38f64de.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/1*tuEuVc1sBT2Ngs_gPbQR0A.png"/></div></figure><h2 id="c399" class="li lj in bd lk ll lm dn ln lo lp dp lq jv lr ls lt jz lu lv lw kd lx ly lz ma bi translated">列车测试分离</h2><p id="761c" class="pw-post-body-paragraph jk jl in jm b jn mb jp jq jr mc jt ju jv md jx jy jz me kb kc kd mf kf kg kh ig bi translated">为了确保我们的模型不会过度拟合，让我们将数据分成训练和测试数据集。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><h2 id="0482" class="li lj in bd lk ll lm dn ln lo lp dp lq jv lr ls lt jz lu lv lw kd lx ly lz ma bi translated">使标准化</h2><p id="a4ab" class="pw-post-body-paragraph jk jl in jm b jn mb jp jq jr mc jt ju jv md jx jy jz me kb kc kd mf kf kg kh ig bi translated">一些算法对距离很敏感，所以让我们标准化这些特征，以确保我们创建了一个好的模型。我们在截图中看不出区别，因为列宽太小，但是<code class="fe mw mx my mz b">featuresScaled</code>上的数据已经标准化</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/933f15bc7f54b587598e769047f37a18.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*-YrAC6KRpAVzQfBKO1Yd5w.png"/></div></figure><h2 id="3b2e" class="li lj in bd lk ll lm dn ln lo lp dp lq jv lr ls lt jz lu lv lw kd lx ly lz ma bi translated">模型拟合</h2><p id="4bb6" class="pw-post-body-paragraph jk jl in jm b jn mb jp jq jr mc jt ju jv md jx jy jz me kb kc kd mf kf kg kh ig bi translated">下一步是拟合模型。我们将尝试使用 4 种不同的算法创建 4 个模型，并检查哪一个具有更好的度量评估。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><h2 id="3226" class="li lj in bd lk ll lm dn ln lo lp dp lq jv lr ls lt jz lu lv lw kd lx ly lz ma bi translated">模型评估</h2><p id="fbde" class="pw-post-body-paragraph jk jl in jm b jn mb jp jq jr mc jt ju jv md jx jy jz me kb kc kd mf kf kg kh ig bi translated">我们将使用准确性和 AUC-ROC 来评估模型。正如我们在下面看到的，梯度提升树分类器具有最好的准确性，而决策树具有最好的 AUC-ROC。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/e563b6193d3720fb04c54642d9a7a7a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*KLfrc5Zz5AXvSdM1ogaicA.png"/></div></figure><h2 id="7306" class="li lj in bd lk ll lm dn ln lo lp dp lq jv lr ls lt jz lu lv lw kd lx ly lz ma bi translated">特征重要性</h2><p id="fc42" class="pw-post-body-paragraph jk jl in jm b jn mb jp jq jr mc jt ju jv md jx jy jz me kb kc kd mf kf kg kh ig bi translated">对于商业智能或数据分析师来说，机器学习的重要性不是模型本身，而是特征重要性。特征重要性可以作为商业推荐的基础。下面是如何在 PySpark 中展示该特性的重要性。我们可以看到所有的模型都认同<code class="fe mw mx my mz b">Age </code>是客户流失的最大因素。根据这些信息，他们可以创建基于年龄的营销来降低流失率。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/533d1c14b3690ea8ab21e3725622ca99.png" data-original-src="https://miro.medium.com/v2/resize:fit:334/format:webp/1*NqEbdWVzRoSTEc0hYmjQrA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">基于决策树模型的特征重要性</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/d76e17ccc9084fea32ed06669b199d08.png" data-original-src="https://miro.medium.com/v2/resize:fit:330/format:webp/1*TcwfXJVPQyblN5PcMth2dw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">基于随机森林模型的特征重要性</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/a7a4465c2c28deb2e8b949fa2dad6145.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/1*lF7y7sN-wmtWPQYRL4oNaw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">基于梯度增强树模型的特征重要性</figcaption></figure><h2 id="63f5" class="li lj in bd lk ll lm dn ln lo lp dp lq jv lr ls lt jz lu lv lw kd lx ly lz ma bi translated">结论</h2><p id="3ebd" class="pw-post-body-paragraph jk jl in jm b jn mb jp jq jr mc jt ju jv md jx jy jz me kb kc kd mf kf kg kh ig bi translated">正如我们所看到的，PySpark 很容易学，并且和熊猫有很多相似之处。目前，<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/faq.html" rel="noopener ugc nofollow" target="_blank">Spark 上的 Pandas API 还没有正式支持结构化流</a>，所以我希望这篇文章能在你想在 Spark 上使用结构化流的时候给你一点帮助。如果你对完整的代码感兴趣，你可以访问我的<a class="ae ky" href="https://github.com/triesonyk/churn-classification-pyspark" rel="noopener ugc nofollow" target="_blank"> GitHub Repo </a>。</p></div></div>    
</body>
</html>