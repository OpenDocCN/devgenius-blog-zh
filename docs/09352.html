<html>
<head>
<title>Data processing with Spark: schema evolution</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Spark 数据处理:模式进化</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/data-processing-with-spark-schema-evolution-4d6032e3737c?source=collection_archive---------1-----------------------#2022-08-16">https://blog.devgenius.io/data-processing-with-spark-schema-evolution-4d6032e3737c?source=collection_archive---------1-----------------------#2022-08-16</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><p id="b536" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">在我之前的文章中，我已经讲述了如何设置 Spark 以及如何使用它进行数据处理和使用 T2 数据目录 T3。我们从短暂的默认 Spark 数据目录开始，并从那里开始配置持久的 Hive metastore。</p><p id="3d9c" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">那么这个每个人都在谈论的模式是什么，为什么我们会关心它呢？(如果你来自于一个繁重的数据库背景，schema 就是表结构。)它是数据的结构，主要包含关于数据外观的信息(元数据):字段/列名、字段/列类型、字段/列描述。我在这里有意使用结构，因为即使数据是半结构化的，通过定义一个模式，我们倾向于结构化它，以便更好地理解它。</p><h1 id="f98d" class="kj kk in bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">概述</h1><p id="7ce7" class="pw-post-body-paragraph jk jl in jm b jn lh jp jq jr li jt ju jv lj jx jy jz lk kb kc kd ll kf kg kh ig bi translated">让我们回顾一下使用案例。每天我都从网站上下载一个 xls 文件。该文件包含记录我的太阳能电池板产生的能量的一系列事件。我首先创建一个由 Hive 管理的表，它包含两列:</p><ul class=""><li id="8ae1" class="lm ln in jm b jn jo jr js jv lo jz lp kd lq kh lr ls lt lu bi translated">时间=事件发生的时间</li><li id="eb7e" class="lm ln in jm b jn lv jr lw jv lx jz ly kd lz kh lr ls lt lu bi translated">生产的千瓦时=直到该时间当天生产的千瓦时数</li></ul><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="da87" class="mj kk in mf b gy mk ml l mm mn">spark_session.sql("describe table raw_electricity_data").show()<br/>+------------+---------+-------+<br/>|    col_name|data_type|comment|<br/>+------------+---------+-------+<br/>|        time|timestamp|   null|<br/>|produced_kwh|   double|   null|<br/>+------------+---------+-------+</span><span id="a271" class="mj kk in mf b gy mo ml l mm mn">spark_session.sql("select * from raw_electricity_data").show()<br/>+-------------------+------------+<br/>|               time|produced_kwh|<br/>+-------------------+------------+<br/>|2022-06-29 06:48:44|         0.0|<br/>|2022-06-29 07:33:44|        1.12|<br/>|2022-06-29 08:18:44|        2.27|<br/>...</span></pre><p id="0653" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">根据原始数据，我创建了一个每日汇总表，以显示每天的发电量:</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="2f5d" class="mj kk in mf b gy mk ml l mm mn">spark_session.sql("describe table daily_electricity_agg").show()<br/>+--------------+---------+-------+<br/>|      col_name|data_type|comment|<br/>+--------------+---------+-------+<br/>|reporting_date|     date|   null|<br/>|  produced_kwh|   double|   null|<br/>+--------------+---------+-------+<br/></span><span id="f149" class="mj kk in mf b gy mo ml l mm mn">spark_session.sql("select * from daily_electricity_agg").show()<br/>+--------------+------------+<br/>|reporting_date|produced_kwh|<br/>+--------------+------------+<br/>|    2022-07-05|        3.45|<br/>|    2022-07-02|        6.61|<br/>|    2022-07-03|        9.01|<br/>|    2022-07-04|       12.12|<br/>|    2022-06-29|       11.06|<br/>|    2022-07-01|        5.54|<br/>|    2022-06-30|        7.92|<br/>+--------------+------------+</span></pre><h1 id="d83e" class="kj kk in bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">图式进化</h1><p id="eaf1" class="pw-post-body-paragraph jk jl in jm b jn lh jp jq jr li jt ju jv lj jx jy jz lk kb kc kd ll kf kg kh ig bi translated">随着时间的推移，数据的模式可能会发生变化:添加新的字段/列或删除现有的字段/列、更改数据类型或空特征都是需要模式更改的事件。因此，模式演变与以下因素密切相关:</p><ul class=""><li id="fc63" class="lm ln in jm b jn jo jr js jv lo jz lp kd lq kh lr ls lt lu bi translated">数据目录</li><li id="b946" class="lm ln in jm b jn lv jr lw jv lx jz ly kd lz kh lr ls lt lu bi translated">存储数据的文件格式</li></ul><p id="eaa5" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">另一个需要考虑的因素是我们应用什么样的变化。我们谈论向后和不向后兼容(中断)的变更。根据文件格式，我们可以在记录中间添加列，将字符串列类型更改为 int，或者从记录中间删除列。在本文中，我将展示仅向后兼容的变更。大多数情况下，重大更改需要重新创建模式和重新加载数据。</p><p id="8a1c" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">用例:我想在每日汇总表中添加我接收数据的小时数，基本上是一天的最大时间戳和一天的最小时间戳之间的小时差。该列应添加到文件的末尾。</p><figure class="ma mb mc md gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi mp"><img src="../Images/89fdae33ec2ca567c07da7bfba0eca66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zGq1R0e3Qi4IRSd7.png"/></div></div></figure><h1 id="6dee" class="kj kk in bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">非持久数据目录</h1><p id="1657" class="pw-post-body-paragraph jk jl in jm b jn lh jp jq jr li jt ju jv lj jx jy jz lk kb kc kd ll kf kg kh ig bi translated">首先，我创建一个 spark 会话，一个原始数据的临时视图，并实例化两个数据帧，其中包含我的请求的计算:</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="b8c0" class="mj kk in mf b gy mk ml l mm mn">from data_processing_spark_schema import get_some_df<br/>from pyspark.sql import SparkSession<br/>spark_session = SparkSession.builder.getOrCreate()<br/>input_df = get_some_df(spark_session)<br/>input_df.createTempView('raw_electricity_data')</span><span id="4b16" class="mj kk in mf b gy mo ml l mm mn">initial_agg_df = spark_session.sql(<br/>        "select to_date(time, 'yyyy-MM-dd') as reporting_date, \<br/>            max(produced_kwh) produced_kwh \<br/>        from raw_electricity_data group by 1"<br/>    )</span><span id="7125" class="mj kk in mf b gy mo ml l mm mn">altered_agg_df = spark_session.sql(<br/>    "select reporting_date, \<br/>        produced_kwh, \<br/>        (max_unix_timestamp - min_unix_timestamp)/60/60 as no_active_hours \<br/>    from (select to_date(time, 'yyyy-MM-dd') as reporting_date, \<br/>        max(produced_kwh) produced_kwh, \<br/>        min(unix_timestamp(time)) as min_unix_timestamp, \<br/>        max(unix_timestamp(time)) as max_unix_timestamp \<br/>    from raw_electricity_data group by 1) src"<br/>)</span></pre><h2 id="866c" class="mj kk in bd kl mx my dn kp mz na dp kt jv nb nc kx jz nd ne lb kd nf ng lf nh bi translated">覆盖模式</h2><p id="84c2" class="pw-post-body-paragraph jk jl in jm b jn lh jp jq jr li jt ju jv lj jx jy jz lk kb kc kd ll kf kg kh ig bi translated">在这种模式下，模式演化与加载过程无关，因为我们完全替换了数据，并且没有存储任何元数据定义来考虑。新数据简单地覆盖了以前的数据，新模式的读取将自动进行。模式演变与我们数据的用户相关，他们需要意识到模式已经改变。</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="9bf4" class="mj kk in mf b gy mk ml l mm mn">data_location = '/app/output_data/default_spark/temp/daily_electricity_agg'</span><span id="f389" class="mj kk in mf b gy mo ml l mm mn"># the initial load<br/>initial_agg_df.write.mode('overwrite').parquet(path=data_location)<br/>spark_session.read.parquet(data_location).show()<br/>+--------------+------------+<br/>|reporting_date|produced_kwh|<br/>+--------------+------------+<br/>|    2022-06-29|        6.61|<br/>|    2022-07-01|        7.99|<br/>|    2022-07-02|        5.57|<br/>|    2022-06-30|       14.94|<br/>|    2022-07-05|       13.72|<br/>|    2022-07-03|        9.99|<br/>|    2022-07-04|         9.9|<br/>+--------------+------------+</span><span id="2df4" class="mj kk in mf b gy mo ml l mm mn"># the altered load</span><span id="bd6b" class="mj kk in mf b gy mo ml l mm mn">altered_agg_df.write.mode('overwrite').parquet(path=data_location)<br/>spark_session.read.parquet(data_location).show()<br/>+--------------+------------+---------------+<br/>|reporting_date|produced_kwh|no_active_hours|<br/>+--------------+------------+---------------+<br/>|    2022-06-29|        6.61|            4.5|<br/>|    2022-07-01|        7.99|           5.25|<br/>|    2022-07-02|        5.57|           3.75|<br/>|    2022-06-30|       14.94|           9.75|<br/>|    2022-07-05|       13.72|            9.0|<br/>|    2022-07-03|        9.99|           6.75|<br/>|    2022-07-04|         9.9|           6.75|<br/>+--------------+------------+---------------+</span></pre><h2 id="215b" class="mj kk in bd kl mx my dn kp mz na dp kt jv nb nc kx jz nd ne lb kd nf ng lf nh bi translated">附加方式</h2><p id="1cf1" class="pw-post-body-paragraph jk jl in jm b jn lh jp jq jr li jt ju jv lj jx jy jz lk kb kc kd ll kf kg kh ig bi translated">在 append 模式下，Spark 在默认情况下不应用模式演化，它完全依赖于 Parquet 处理模式演化的方式。</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="109c" class="mj kk in mf b gy mk ml l mm mn">data_location = '/app/output_data/default_spark/temp_append/daily_electricity_agg'</span><span id="ec06" class="mj kk in mf b gy mo ml l mm mn">initial_agg_df.write.mode('append').parquet(path=data_location)</span><span id="c348" class="mj kk in mf b gy mo ml l mm mn">spark_session.read.parquet(data_location).show()<br/>+--------------+------------+<br/>|reporting_date|produced_kwh|<br/>+--------------+------------+<br/>|    2022-06-29|        6.61|<br/>|    2022-07-01|        7.99|<br/>|    2022-07-02|        5.57|<br/>|    2022-06-30|       14.94|<br/>|    2022-07-05|       13.72|<br/>|    2022-07-03|        9.99|<br/>|    2022-07-04|         9.9|<br/>+--------------+------------+<br/></span><span id="1d72" class="mj kk in mf b gy mo ml l mm mn">altered_agg_df.write.mode('append').parquet(path=data_location)<br/>spark_session.read.parquet(data_location).show()<br/>+--------------+------------+<br/>|reporting_date|produced_kwh|<br/>+--------------+------------+<br/>|    2022-06-29|        6.61|<br/>|    2022-07-01|        7.99|<br/>|    2022-07-02|        5.57|<br/>|    2022-06-30|       14.94|<br/>|    2022-07-05|       13.72|<br/>|    2022-07-03|        9.99|<br/>|    2022-07-04|         9.9|<br/>|    2022-06-29|        6.61|<br/>|    2022-07-01|        7.99|<br/>|    2022-07-02|        5.57|<br/>|    2022-06-30|       14.94|<br/>|    2022-07-05|       13.72|<br/>|    2022-07-03|        9.99|<br/>|    2022-07-04|         9.9|<br/>+--------------+------------+<br/></span><span id="c7c7" class="mj kk in mf b gy mo ml l mm mn">spark_session.read.option("mergeSchema", "true").parquet(data_location).show()<br/>+--------------+------------+---------------+<br/>|reporting_date|produced_kwh|no_active_hours|<br/>+--------------+------------+---------------+<br/>|    2022-06-29|        6.61|            4.5|<br/>|    2022-07-01|        7.99|           5.25|<br/>|    2022-07-02|        5.57|           3.75|<br/>|    2022-06-30|       14.94|           9.75|<br/>|    2022-07-05|       13.72|            9.0|<br/>|    2022-07-03|        9.99|           6.75|<br/>|    2022-07-04|         9.9|           6.75|<br/>|    2022-06-29|        6.61|           null|<br/>|    2022-07-01|        7.99|           null|<br/>|    2022-07-02|        5.57|           null|<br/>|    2022-06-30|       14.94|           null|<br/>|    2022-07-05|       13.72|           null|<br/>|    2022-07-03|        9.99|           null|<br/>|    2022-07-04|         9.9|           null|<br/>+--------------+------------+---------------+</span></pre><p id="08db" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我们在上面看到的是，如果不启用合并模式，Spark 将不会显示新字段。</p><h1 id="b735" class="kj kk in bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">蜂巢数据目录</h1><p id="b786" class="pw-post-body-paragraph jk jl in jm b jn lh jp jq jr li jt ju jv lj jx jy jz lk kb kc kd ll kf kg kh ig bi translated">加载数据可能会容易一点，但是当我们在没有数据目录的情况下工作时，检索数据会更加困难。我们需要知道数据的位置，有时需要知道数据的格式，了解模式或其他可能发生的变化。这就是为什么在托管数据系统中，数据目录是任何数据活动的核心。</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="b014" class="mj kk in mf b gy mk ml l mm mn">spark_session = SparkSession.builder.appName("My Spark ETL Session").config(<br/>    'spark.hive.metastore.uris', 'thrift://localhost:9083'<br/>    ).enableHiveSupport().getOrCreate()</span></pre><h1 id="35e6" class="kj kk in bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">覆盖模式</h1><p id="c53a" class="pw-post-body-paragraph jk jl in jm b jn lh jp jq jr li jt ju jv lj jx jy jz lk kb kc kd ll kf kg kh ig bi translated">在这种模式下，模式演化非常简单。数据只是在存储区域中被刷新，但是我们需要注意 Hive 中的表定义，因为我们不能覆盖现有的表:</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="2350" class="mj kk in mf b gy mk ml l mm mn">initial_agg_df.write.saveAsTable("daily_electricity_agg")<br/>---------------------------------------------------------------------------<br/>AnalysisException                         Traceback (most recent call last)<br/>AnalysisException: Table `daily_electrivity_agg` already exists.</span></pre><p id="dfc4" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">在数据完全刷新之前，可以删除并重新创建该表。请注意，使用这种方法，数据用户无法使用数据，因为在重新创建表时，表并不存在:</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="d8c6" class="mj kk in mf b gy mk ml l mm mn">spark_session.sql("drop table daily_electricity_agg")</span><span id="5382" class="mj kk in mf b gy mo ml l mm mn">initial_agg_df.write.saveAsTable("daily_electricity_agg")</span><span id="6b2c" class="mj kk in mf b gy mo ml l mm mn">spark_session.sql("describe table daily_electricity_agg").show()<br/>+---------------+---------+-------+<br/>|       col_name|data_type|comment|<br/>+---------------+---------+-------+<br/>| reporting_date|     date|   null|<br/>|   produced_kwh|   double|   null|<br/>|no_active_hours|   double|   null|<br/>+---------------+---------+-------+<br/></span><span id="9c74" class="mj kk in mf b gy mo ml l mm mn">spark_session.sql("select * from daily_electricity_agg").show()<br/>+--------------+------------+---------------+<br/>|reporting_date|produced_kwh|no_active_hours|<br/>+--------------+------------+---------------+<br/>|    2022-07-05|        3.45|           2.25|<br/>|    2022-07-02|        6.61|            4.5|<br/>|    2022-07-03|        9.01|            6.0|<br/>|    2022-07-04|       12.12|           8.25|<br/>|    2022-06-29|       11.06|            7.5|<br/>|    2022-07-01|        5.54|           3.75|<br/>|    2022-06-30|        7.92|           5.25|<br/>+--------------+------------+---------------+</span></pre><h1 id="abb1" class="kj kk in bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">附加方式</h1><p id="622f" class="pw-post-body-paragraph jk jl in jm b jn lh jp jq jr li jt ju jv lj jx jy jz lk kb kc kd ll kf kg kh ig bi translated">当我们总是添加数据时，有两种方法来处理模式演变。我们可以利用数据定义语言来改变表格，或者利用视图来反映变化。</p><h2 id="eee8" class="mj kk in bd kl mx my dn kp mz na dp kt jv nb nc kx jz nd ne lb kd nf ng lf nh bi translated">更改表定义</h2><p id="80fb" class="pw-post-body-paragraph jk jl in jm b jn lh jp jq jr li jt ju jv lj jx jy jz lk kb kc kd ll kf kg kh ig bi translated">就像在任何其他数据管理系统中一样，我们也可以在 Hive 中修改表:</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="f0a9" class="mj kk in mf b gy mk ml l mm mn">spark_session.sql("alter table daily_electricity_agg add column no_active_hours decimal(4,2)")</span><span id="df52" class="mj kk in mf b gy mo ml l mm mn">spark_session.sql("describe table daily_electricity_agg").show()<br/>+---------------+------------+-------+<br/>|       col_name|   data_type|comment|<br/>+---------------+------------+-------+<br/>| reporting_date|        date|   null|<br/>|   produced_kwh|      double|   null|<br/>|no_active_hours|decimal(4,2)|   null|<br/>+---------------+------------+-------+</span><span id="7956" class="mj kk in mf b gy mo ml l mm mn">spark_session.sql("select * from daily_electricity_agg").show()<br/>+--------------+------------+---------------+                                   <br/>|reporting_date|produced_kwh|no_active_hours|<br/>+--------------+------------+---------------+<br/>|    2022-07-05|       10.31|           null|<br/>|    2022-07-04|        6.83|           null|<br/>|    2022-07-01|        4.51|           null|<br/>|    2022-07-02|        5.68|           null|<br/>|    2022-06-30|       12.18|           null|<br/>|    2022-06-29|        7.89|           null|<br/>|    2022-07-03|       14.78|           null|<br/>+--------------+------------+---------------+</span></pre><p id="89b5" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">现在我们已经添加了字段，我们可以追加新数据:</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="2c42" class="mj kk in mf b gy mk ml l mm mn">altered_agg_df.write.mode("append").saveAsTable("daily_electricity_agg")<br/>                                                                                <br/>spark_session.sql("select * from daily_electricity_agg").show()<br/>+--------------+------------+---------------+<br/>|reporting_date|produced_kwh|no_active_hours|<br/>+--------------+------------+---------------+<br/>|    2022-07-05|       10.31|           6.75|<br/>|    2022-07-04|        6.83|           4.50|<br/>|    2022-07-01|        4.51|           3.00|<br/>|    2022-07-02|        5.68|           3.75|<br/>|    2022-06-30|       12.18|           8.25|<br/>|    2022-06-29|        7.89|           5.25|<br/>|    2022-07-03|       14.78|           9.75|<br/>|    2022-07-05|       10.31|           null|<br/>|    2022-07-04|        6.83|           null|<br/>|    2022-07-01|        4.51|           null|<br/>|    2022-07-02|        5.68|           null|<br/>|    2022-06-30|       12.18|           null|<br/>|    2022-06-29|        7.89|           null|<br/>|    2022-07-03|       14.78|           null|<br/>+--------------+------------+---------------+</span></pre><h2 id="a1db" class="mj kk in bd kl mx my dn kp mz na dp kt jv nb nc kx jz nd ne lb kd nf ng lf nh bi translated">视图方法</h2><p id="3d67" class="pw-post-body-paragraph jk jl in jm b jn lh jp jq jr li jt ju jv lj jx jy jz lk kb kc kd ll kf kg kh ig bi translated">我们可以不添加新列，而是将数据加载到一个新表中，并创建一个视图来显示两个表中的数据:</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="fcf3" class="mj kk in mf b gy mk ml l mm mn">spark_session.sql("drop table daily_electricity_agg")<br/>initial_agg_df.write.saveAsTable("daily_electricity_agg")<br/>altered_agg_df.write.saveAsTable("altered_daily_electricity_agg")</span><span id="a609" class="mj kk in mf b gy mo ml l mm mn">spark_session.sql("create or replace view daily_electricity_agg_v as \<br/>    select reporting_date, produced_kwh, null as no_active_hours from daily_electricity_agg \<br/>    union all \<br/>    select reporting_date, produced_kwh, no_active_hours from altered_daily_electricity_agg")</span><span id="0150" class="mj kk in mf b gy mo ml l mm mn">spark_session.sql("select * from daily_electricity_agg_v").show()<br/>+--------------+------------+---------------+<br/>|reporting_date|produced_kwh|no_active_hours|<br/>+--------------+------------+---------------+<br/>|    2022-07-05|       10.31|           null|<br/>|    2022-07-04|        6.83|           null|<br/>|    2022-07-01|        4.51|           null|<br/>|    2022-07-02|        5.68|           null|<br/>|    2022-06-30|       12.18|           null|<br/>|    2022-06-29|        7.89|           null|<br/>|    2022-07-03|       14.78|           null|<br/>|    2022-07-05|       10.31|           6.75|<br/>|    2022-07-04|        6.83|            4.5|<br/>|    2022-07-01|        4.51|            3.0|<br/>|    2022-07-02|        5.68|           3.75|<br/>|    2022-06-30|       12.18|           8.25|<br/>|    2022-06-29|        7.89|           5.25|<br/>|    2022-07-03|       14.78|           9.75|<br/>+--------------+------------+---------------+</span><span id="eacc" class="mj kk in mf b gy mo ml l mm mn">spark_session.sql("describe table  daily_electricity_agg_v").show()<br/>+---------------+---------+-------+<br/>|       col_name|data_type|comment|<br/>+---------------+---------+-------+<br/>| reporting_date|     date|   null|<br/>|   produced_kwh|   double|   null|<br/>|no_active_hours|   double|   null|<br/>+---------------+---------+-------+</span></pre><p id="3082" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我发现视图方法更容易开发，但随着时间的推移，我认为它更难维护，因为它会生成大量的对象。</p><h1 id="c1e2" class="kj kk in bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">结论</h1><p id="2a48" class="pw-post-body-paragraph jk jl in jm b jn lh jp jq jr li jt ju jv lj jx jy jz lk kb kc kd ll kf kg kh ig bi translated">管理模式演变增加了数据管理系统的复杂性。不管加载模式如何，模式演变对数据用户都很重要。每当模式发生变化时，都需要通知用户，以便对模式变化的影响采取措施。</p><p id="c5ec" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我们看到，无论何时处于覆盖模式，我们都不需要在加载时间注意<em class="ni">模式是如何变化的。</em></p><p id="9c0b" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">当我们处于追加模式时，我们可以通过使用 Hive 来改变模式。默认情况下，Spark 不会合并模式。通过使用 Hive，我们可以创建健壮的数据管道，并且可以在写入时生成错误，这样我们就不会影响数据用户:</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="b009" class="mj kk in mf b gy mk ml l mm mn">spark_session.sql("drop table daily_electricity_agg")<br/>initial_agg_df.write.saveAsTable("daily_electricity_agg")<br/>                                                                                <br/>spark_session.sql("alter table daily_electricity_agg add column no_active_hours decimal(4,2)")</span><span id="77f5" class="mj kk in mf b gy mo ml l mm mn">from pyspark.sql.functions import lit</span><span id="3bde" class="mj kk in mf b gy mo ml l mm mn">initial_agg_df = initial_agg_df.withColumn('no_active_hours', lit('string'))</span><span id="9994" class="mj kk in mf b gy mo ml l mm mn">initial_agg_df.write.mode("append").saveAsTable("daily_electricity_agg")<br/>---------------------------------------------------------------------------<br/>AnalysisException: Cannot write incompatible data to table 'daily_electricity_agg':<br/>- Cannot safely cast 'no_active_hours': string to decimal(4,2)</span></pre><p id="3f57" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">Spark 系列的下一款是什么？我们如何操作数据(更新/删除)以及如何在 Spark 中启用 ACID。</p><h1 id="c2af" class="kj kk in bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">文档:</h1><ol class=""><li id="38bf" class="lm ln in jm b jn lh jr li jv nj jz nk kd nl kh nm ls lt lu bi translated"><a class="ae ki" href="https://en.wikipedia.org/wiki/Semi-structured_data" rel="noopener ugc nofollow" target="_blank">半结构化数据</a></li><li id="d56f" class="lm ln in jm b jn lv jr lw jv lx jz ly kd lz kh nm ls lt lu bi translated"><a class="ae ki" href="https://spark.apache.org/docs/latest/sql-ref-datatypes.html" rel="noopener ugc nofollow" target="_blank">火花数据类型</a></li><li id="59d9" class="lm ln in jm b jn lv jr lw jv lx jz ly kd lz kh nm ls lt lu bi translated"><a class="ae ki" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL" rel="noopener ugc nofollow" target="_blank"> HIVE DDL 文档</a></li><li id="6946" class="lm ln in jm b jn lv jr lw jv lx jz ly kd lz kh nm ls lt lu bi translated"><a class="ae ki" href="https://cwiki.apache.org/confluence/display/hive/languagemanual+types" rel="noopener ugc nofollow" target="_blank">蜂巢数据类型</a></li><li id="7483" class="lm ln in jm b jn lv jr lw jv lx jz ly kd lz kh nm ls lt lu bi translated"><a class="ae ki" href="https://github.com/acirtep/ginlong-data-processing-spark" rel="noopener ugc nofollow" target="_blank"> GitHub 代码模拟上面的</a></li></ol><p id="44d6" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">文章首发@ own your data . ai。</p></div></div>    
</body>
</html>