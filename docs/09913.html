<html>
<head>
<title>Data processing with Spark: ACID compliance</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ä½¿ç”¨ Spark è¿›è¡Œæ•°æ®å¤„ç†:é…¸æ€§åˆè§„æ€§</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://blog.devgenius.io/data-processing-with-spark-acid-compliance-be89464073d9?source=collection_archive---------5-----------------------#2022-09-22">https://blog.devgenius.io/data-processing-with-spark-acid-compliance-be89464073d9?source=collection_archive---------5-----------------------#2022-09-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="c49e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">åœ¨ä¹‹å‰çš„ Spark æ–‡ç« ä¸­ï¼Œæˆ‘è®²è¿°äº†å¦‚ä½•<a class="ae kl" href="https://ownyourdata.ai/wp/data-processing-with-spark-intro/" rel="noopener ugc nofollow" target="_blank">è®¾ç½® Spark </a>ï¼Œå¦‚ä½•è®¾ç½®<a class="ae kl" href="https://ownyourdata.ai/wp/data-processing-with-spark-data-catalog/" rel="noopener ugc nofollow" target="_blank"> Hive æ•°æ®ç›®å½•</a>ä»¥åŠå¦‚ä½•å¤„ç†<a class="ae kl" href="https://ownyourdata.ai/wp/data-processing-with-spark-schema-evolution/" rel="noopener ugc nofollow" target="_blank">æ¨¡å¼æ¼”åŒ–</a>ã€‚ç°åœ¨æ˜¯æ—¶å€™ç»å† ACID å’Œæˆªè‡³ 2022 å¹´ 9 æœˆ Spark æœ‰å“ªäº›é€‰æ‹©:é˜¿å¸•å¥‡èƒ¡è¿ªï¼Œä¸‰è§’æ´²æ¹–å’Œé˜¿å¸•å¥‡å†°å±±ã€‚</p><h1 id="4bb8" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">é…¸</h1><p id="329b" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">åœ¨<em class="lp">ç®¡ç†çš„</em>ç¯å¢ƒä¸­æ•°æ®å¤„ç†çš„ 4 ä¸ªç‰¹å¾:</p><ol class=""><li id="c385" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated">åŸå­æ•°</li><li id="4325" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">ä¸€è‡´æ€§</li><li id="de11" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">éš”ç¦»</li><li id="f30a" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">æŒä¹…æ€§</li></ol><p id="8de8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">å³ä½¿ä» Spark çš„æ–‡çŒ®ä¸­ï¼Œä¹ŸæŒ‡å‡ºä¸å°Šé‡åŸå­æ•°ï¼Œè¿™å½±å“äº†é…¸çš„å…¶ä»–ç‰¹æ€§ã€‚é˜…è¯»æ–‡æ¡£åŒºä¸­é“¾æ¥çš„ç¬¬äºŒç¯‡æ–‡ç« ï¼Œå®ƒå¾ˆå¥½åœ°è§£é‡Šäº†è¿™ç§æƒ…å†µã€‚</p><p id="2f4c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">ä»€ä¹ˆæ—¶å€™åº”è¯¥å…³å¿ƒé…¸ï¼Ÿåœ¨æˆ‘çœ‹æ¥ï¼Œæ•°æ®å¤„ç†çš„æ‰€æœ‰æ­¥éª¤éƒ½ä¸éœ€è¦ ACIDã€‚åœ¨ç”¨æˆ·å±‚ï¼Œéœ€è¦ç¡®ä¿æ•°æ®ç®¡ç†ã€åˆè§„æ€§ä»¥åŠå¹¶å‘å’Œä¸€è‡´çš„è®¿é—®ï¼ŒACID æ˜¯åœ£æ¯ã€‚ä½†æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä¸´æ—¶/æš‚å­˜åŒºï¼ŒACID å°±ä¸é‚£ä¹ˆé‡è¦äº†ï¼Œå› ä¸ºå®ƒä¸ä¼šå½±å“æµç¨‹/ç”¨æˆ·ã€‚</p><p id="3644" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">å½“æˆ‘ä»¬è°ˆåˆ°è¦†ç›–æ¨¡å¼æ—¶ï¼ŒSpark ä¸­çš„ ACID è¢«ç ´åäº†ï¼Œå› ä¸º Spark å°†é¦–å…ˆåˆ é™¤æ•°æ®ï¼Œç„¶åå†å†™å…¥æ•°æ®ã€‚å› æ­¤ï¼Œæœ¬æ–‡å°†åªå…³æ³¨è¿™ç§ä¿å­˜æ¨¡å¼ã€‚</p><h1 id="47cd" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">æ¦‚å¿µéªŒè¯è®¾ç½®</h1><p id="6520" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">é¦–å…ˆï¼Œæˆ‘éœ€è¦ä¸€äº›è™šæ‹Ÿæ•°æ®ï¼Œè¿™äº›æ•°æ®æ˜¯æˆ‘éšæœºç”Ÿæˆçš„ï¼Œå¹¶ç”¨ pandas ä¿å­˜åˆ°ç£ç›˜ä¸Š:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="02fc" class="mn kn iq mj b gy mo mp l mq mr">def get_some_data(): </span><span id="b10a" class="mn kn iq mj b gy ms mp l mq mr">    start_date = date(2021, 1, 1)</span><span id="4340" class="mn kn iq mj b gy ms mp l mq mr">    end_date = date(2022, 6, 30)<br/>    record_list = []</span><span id="eebd" class="mn kn iq mj b gy ms mp l mq mr">    while start_date &lt;= end_date:<br/>        start_datetime = datetime(<br/>            start_date.year, start_date.month, start_date.day, <br/>            random.randrange(6, 8), random.randrange(0, 59), random.randrange(0, 59)<br/>        )<br/>        start_kwh = 0.0<br/>        record_list.append([start_datetime, start_kwh])<br/>        for i in range(1, random.randrange(2, 15)):<br/>            record_list.append(<br/>                [<br/>                    start_datetime+timedelta(minutes=i*45), <br/>                    round(i*random.uniform(1.10, 1.15), 2)<br/>                ]<br/>            )<br/>        start_date = start_date + timedelta(1)<br/>    <br/>    df = pd.DataFrame(record_list)<br/>    df.columns = ['time', 'produced_kwh']<br/>    df.to_csv('/app/input_data/acid_example_data.txt', sep='\t', index=False)</span></pre><p id="9643" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">ä¸ºäº†æ­£ç¡®å±•ç¤º Spark ä¸Šçš„å†™æ“ä½œï¼Œæˆ‘å°†æŒ‰æŠ¥å‘Šæœˆä»½å¯¹æ•°æ®è¿›è¡Œåˆ†åŒºï¼Œå¹¶å¼€å‘äº†ä¸¤ç§æ–¹æ³•:1 ç”¨äºå¿«ä¹æµï¼Œ1 ç”¨äºä¸­æ–­ Spark:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="1e01" class="mn kn iq mj b gy mo mp l mq mr">def get_input_df(spark_session):<br/>    return spark_session.read.option(<br/>        'delimiter', '\t').option(<br/>        'header', 'true').csv(<br/>        'input_data/acid_example_data.txt'<br/>    )<br/></span><span id="23a5" class="mn kn iq mj b gy ms mp l mq mr">def get_happy_flow(df_input):<br/>    return df_input.withColumn(<br/>        "time", to_timestamp("time")).withColumn(<br/>        "produced_kwh", col("produced_kwh").cast("decimal(18,2)")).withColumn(<br/>        "reporting_date", to_date(col("time"),"yyyy-MM-dd")).withColumn(<br/>        "reporting_month", date_format(col("reporting_date"),"yyyyMM"))<br/></span><span id="0234" class="mn kn iq mj b gy ms mp l mq mr">def get_error_flow(df_input):<br/>    return df_input.withColumn(<br/>        "produced_kwh", col("produced_kwh").cast("decimal(18,2)")).withColumn(<br/>        "reporting_date", to_date(col("time"),"yyyy-MM-dd")).withColumn(<br/>        "reporting_month", date_format(col("reporting_date"),"yyyyMM"))</span><span id="142f" class="mn kn iq mj b gy ms mp l mq mr">def get_data(spark_session, flow_type):<br/>    df_input = get_input_df(spark_session)<br/>    if flow_type == 'error':<br/>        df_input = get_error_flow(df_input)<br/>    else:<br/>        df_input = get_happy_flow(df_input)<br/>    <br/>    return df_input</span></pre><p id="7574" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">è®°ä½:Spark æ˜¯æƒ°æ€§çš„ï¼Œæ‰€ä»¥ get_error_flow åœ¨åˆ é™¤åä¼šå¤±è´¥ï¼Œå› ä¸ºæ—¶é—´æˆ³ä¸åŒ¹é…ã€‚</p><h1 id="919a" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">é»˜è®¤ç«èŠ±è®¾ç½®</h1><p id="8cfd" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">å¦‚ä½•è·å¾—é»˜è®¤çš„ spark ä¼šè¯:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="5ef3" class="mn kn iq mj b gy mo mp l mq mr">def get_default_spark_session():<br/>    spark_session = SparkSession.builder.appName('default').config(<br/>        'spark.ui.enabled', 'false').getOrCreate()<br/>    spark_session.sparkContext.setLogLevel('ERROR')<br/>    return spark_session</span></pre><p id="9622" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">åŠ è½½æ•°æ®:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="1bc4" class="mn kn iq mj b gy mo mp l mq mr">def load_default_spark(flow_type='happy'):<br/>    spark_session = get_default_spark_session()<br/>    df_input = get_data(spark_session, flow_type)</span><span id="0031" class="mn kn iq mj b gy ms mp l mq mr">    df_input.write.\<br/>        partitionBy("reporting_month").\<br/>        mode('overwrite').\<br/>        format('parquet').\<br/>        save(path)</span></pre><p id="6d08" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">è¯»å–æ•°æ®:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="7756" class="mn kn iq mj b gy mo mp l mq mr">def read_data():<br/>    spark_session = get_default_spark_session()<br/>    <br/>    number_of_records = spark_session.read.\<br/>        format('parquet').\<br/>        load(path).\<br/>        count()</span><span id="c60e" class="mn kn iq mj b gy ms mp l mq mr">    print(f"Number of records: {number_of_records}")</span><span id="7ef8" class="mn kn iq mj b gy ms mp l mq mr">    spark_session.read.\<br/>        format('parquet').\<br/>        load(path).\<br/>        filter("reporting_date = '2022-03-01'").\<br/>        show()</span></pre><p id="d8bd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">é‚£ä¹ˆå¿«ä¹æµä¸Šå‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="6d96" class="mn kn iq mj b gy mo mp l mq mr">In [1]: from acid.data_processing_acid_default_spark import load_default_spark, read_data</span><span id="fcf9" class="mn kn iq mj b gy ms mp l mq mr">In [2]: load_default_spark('happy')<br/>                                                                             <br/>In [3]: read_data()<br/>Number of records: 4481<br/>+-------------------+------------+--------------+---------------+<br/>|               time|produced_kwh|reporting_date|reporting_month|<br/>+-------------------+------------+--------------+---------------+<br/>|2022-03-01 07:25:20|        0.00|    2022-03-01|         202203|<br/>|2022-03-01 08:10:20|        1.11|    2022-03-01|         202203|<br/>+-------------------+------------+--------------+---------------+</span></pre><p id="81a4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">å…³äºé”™è¯¯çš„é‚£ä¸€ä¸ªå‘¢ï¼Ÿ</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="d02c" class="mn kn iq mj b gy mo mp l mq mr">In [4]: load_default_spark('error')<br/>22/09/22 09:20:18 ERROR Executor: Exception in task 0.0 in stage 10.0 (TID 14)<br/>org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading to Spark &gt;= 3.0: Fail to parse '2021-01-01 07:54:21' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.</span><span id="ac06" class="mn kn iq mj b gy ms mp l mq mr">In [5]: read_data()<br/>AnalysisException: Unable to infer schema for Parquet. It must be specified manually.</span></pre><p id="1fbe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">ä¸Šè¿°é”™è¯¯æ˜¯ç”±äºè·¯å¾„ä¸­ä¸å†æœ‰æ•°æ®è¿™ä¸€äº‹å®é€ æˆçš„ï¼Œå› ä¸º Spark é¦–å…ˆåˆ é™¤äº†æ•°æ®ï¼Œç„¶åè¯•å›¾å†™å…¥æ–°æ•°æ®ã€‚å› æ­¤ï¼ŒåŸå­æ€§ç‰¹å¾æ²¡æœ‰å¾—åˆ°å°Šé‡ï¼Œæˆ‘ä»¬ä¸¢å¤±äº†æ•°æ®å’Œä¸€è‡´æ€§ã€‚</p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><p id="2e59" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">åœ¨æˆ‘å†™è¿™ç¯‡æ–‡ç« çš„æ—¶å€™ï¼Œå›´ç»•ç€ data lakehouse å’Œ Spark æ˜¯å¦æ¯”å…¶ä»–æ•°æ®åº“æŠ€æœ¯æ›´å¥½è¿˜æœ‰äº‰è®®ã€‚æˆ‘æ‰¿è®¤æˆ‘æ›´å–œæ¬¢æ•°æ®åº“ï¼Œå› ä¸ºåœ¨ Spark ç”¨ null æ›¿æ¢æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå®ƒä»¬ä¼šåœ¨ write æ—¶å¤±è´¥ã€‚ä½†æˆ‘ç¡®å®è®¤ä¸ºå®ƒä»¬å¯ä»¥ç»“åˆèµ·æ¥åˆ›å»ºä¸€ä¸ªå¼ºå¤§çš„æ•°æ®å¹³å°ï¼Œä»¥å¤‡ä¸æ—¶ä¹‹éœ€ï¼</p><figure class="me mf mg mh gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi na"><img src="../Images/17ee57c769048b208e41234c9f9f34ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*o2cnr4c03ONlT2N3.jpg"/></div></div></figure><p id="9439" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">å¯ç”¨ ACID for Spark çš„å®ç”¨ç¨‹åºè¿˜é™„å¸¦äº†å…¶ä»–åŠŸèƒ½:æ’å…¥ã€åˆ é™¤å’Œæ—¶é—´æ—…è¡Œã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å°†ä»‹ç»æ¯ä¸ªå®ç”¨ç¨‹åºæ‰€éœ€çš„è®¾ç½®ï¼Œä½†é‡ç‚¹æ˜¯æµ‹è¯•åŸå­æ€§ã€‚</p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><h1 id="8760" class="km kn iq bd ko kp ni kr ks kt nj kv kw kx nk kz la lb nl ld le lf nm lh li lj bi translated">é˜¿å¸•å¥‡èƒ¡è¿ª</h1><p id="46fd" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">å¦‚ä½•ä¸èƒ¡è¿ªäº§ç”Ÿç«èŠ±ï¼Ÿæˆ‘ä»¬éœ€è¦ç¡®ä¿åœ¨æˆ‘ä»¬çš„ä¼šè¯ä¸­æ‹¥æœ‰ jar ä¾èµ–æ€§ä»¥åŠèƒ¡è¿ªç‰¹å®šçš„é…ç½®:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="108d" class="mn kn iq mj b gy mo mp l mq mr">def get_hudi_spark_session():<br/>    spark_session = SparkSession.builder.appName('hudi').config(<br/>        'spark.ui.enabled', 'false').config(<br/>        'spark.jars.packages', 'org.apache.hudi:hudi-spark3.3-bundle_2.12:0.12.0').config(<br/>        'spark.serializer', 'org.apache.spark.serializer.KryoSerializer').config(<br/>        'spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.hudi.catalog.HoodieCatalog').config(<br/>        'spark.sql.extensions', 'org.apache.spark.sql.hudi.HoodieSparkSessionExtension').getOrCreate()<br/>    spark_session.sparkContext.setLogLevel('ERROR')<br/>    return spark_session</span></pre><p id="a46b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">åŠ è½½æ•°æ®:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="a2ab" class="mn kn iq mj b gy mo mp l mq mr">def load_hudi_spark(flow_type='happy'):<br/>    spark_session = get_hudi_spark_session()<br/>    df_input = get_data(spark_session, flow_type)<br/>    <br/>    hudi_options = {<br/>        'hoodie.table.name': 'acid_hudi_spark',<br/>        'hoodie.datasource.write.recordkey.field': 'time',<br/>        'hoodie.datasource.write.partitionpath.field': 'reporting_month',<br/>        'hoodie.datasource.write.table.name': 'acid_hudi_spark',<br/>        'hoodie.datasource.write.operation': 'insert_overwrite',<br/>        'hoodie.datasource.write.precombine.field': 'time',<br/>        'hoodie.upsert.shuffle.parallelism': 2,<br/>        'hoodie.insert.shuffle.parallelism': 2,<br/>        'hoodie.datasource.write.hive_style_partitioning': 'true'<br/>    }</span><span id="33db" class="mn kn iq mj b gy ms mp l mq mr">    df_input.write. \<br/>        options(**hudi_options). \<br/>        mode("append"). \<br/>        format("hudi"). \<br/>        save(path)</span></pre><p id="658a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">è¯·æ³¨æ„ï¼Œèƒ¡è¿ªéœ€è¦å¤„ç†å¤§é‡çš„é…ç½®:</p><ol class=""><li id="439e" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated">è®°å½•é”®ï¼Œè¿™æ˜¯ä¸»é”®</li><li id="2b1e" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">åˆ†åŒºè·¯å¾„ï¼Œè¦åˆ†åŒºçš„å­—æ®µ</li><li id="4434" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">å†™æ“ä½œï¼Œå¯ä»¥æ˜¯ upsertã€bulk_insert ç­‰</li><li id="dc78" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">é¢„ç»„åˆå­—æ®µï¼Œå¯¹äºæ—¶é—´æ—…è¡Œå’Œé‡å¤æ•°æ®åˆ é™¤éå¸¸é‡è¦</li><li id="17f5" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">hive_style_partitioningï¼Œå·²å¯ç”¨ï¼Œä»¥ä¾¿ä¸ºåˆ†åŒºè‡ªåŠ¨å‘ç°åˆ›å»ºæ–‡ä»¶å¤¹ç»“æ„</li></ol><p id="1e0e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">æ³¨æ„èƒ¡è¿ªæœ‰å®ƒè‡ªå·±çš„å†™æ“ä½œæ¨¡å¼ã€‚é€šè¿‡å°† Spark æ¨¡å¼è®¾ç½®ä¸º appendï¼Œå°†èƒ¡è¿ªæ¨¡å¼è®¾ç½®ä¸º insert_overwriteï¼Œæˆ‘ä»¬ç¡®ä¿äº†åŸå­æ€§ã€‚</p><p id="527e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">è¯»å–æ•°æ®:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="4e38" class="mn kn iq mj b gy mo mp l mq mr">def read_data():<br/>    spark_session = get_hudi_spark_session()</span><span id="7033" class="mn kn iq mj b gy ms mp l mq mr">    number_of_records = spark_session.read.\<br/>        format('hudi').\<br/>        load(path).\<br/>        count()</span><span id="b277" class="mn kn iq mj b gy ms mp l mq mr">    print(f"Number of records: {number_of_records}")</span><span id="d70d" class="mn kn iq mj b gy ms mp l mq mr">    spark_session.read.\<br/>        format('hudi').\<br/>        load(path).\<br/>        filter("reporting_date = '2022-03-01'").\<br/>        show()</span></pre><p id="1fd7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">å¿«ä¹ä¹‹æµ:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="8ba7" class="mn kn iq mj b gy mo mp l mq mr">In [1]: from acid.data_processing_acid_hudi import load_hudi_spark, read_data</span><span id="2b83" class="mn kn iq mj b gy ms mp l mq mr">In [2]: load_hudi_spark('happy')<br/>                                                          <br/>In [3]: read_data()<br/>Number of records: 4481<br/>+-------------------+--------------------+------------------+----------------------+--------------------+-------------------+------------+--------------+---------------+<br/>|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|               time|produced_kwh|reporting_date|reporting_month|<br/>+-------------------+--------------------+------------------+----------------------+--------------------+-------------------+------------+--------------+---------------+<br/>|  20220922115641825|20220922115641825...|  1646119520000000|  reporting_month=2...|bb571e5b-6d19-473...|2022-03-01 07:25:20|        0.00|    2022-03-01|         202203|<br/>|  20220922115641825|20220922115641825...|  1646122220000000|  reporting_month=2...|bb571e5b-6d19-473...|2022-03-01 08:10:20|        1.11|    2022-03-01|         202203|<br/>+-------------------+--------------------+------------------+----------------------+--------------------+-------------------+------------+--------------+---------------+</span></pre><p id="e17a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">æˆ‘ä»¬çœ‹åˆ°èƒ¡è¿ªå‘æ•°æ®ä¸­æ·»åŠ äº†ä¸€äº›å®¡è®¡åˆ—ï¼Œè¿™äº›åˆ—åœ¨å†…éƒ¨ç”¨äºæ•°æ®æ“ä½œã€‚</p><p id="db3f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">é”™è¯¯æµä¸Šå‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="a16c" class="mn kn iq mj b gy mo mp l mq mr">In [4]: load_hudi_spark('error')<br/>22/09/22 12:01:57 ERROR Executor: Exception in task 0.0 in stage 50.0 (TID 212)<br/>org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading to Spark &gt;= 3.0: Fail to parse '2021-01-01 07:54:21' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.</span><span id="1d1a" class="mn kn iq mj b gy ms mp l mq mr">In [5]: read_data()<br/>Number of records: 4481<br/>+-------------------+--------------------+------------------+----------------------+--------------------+-------------------+------------+--------------+---------------+<br/>|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|               time|produced_kwh|reporting_date|reporting_month|<br/>+-------------------+--------------------+------------------+----------------------+--------------------+-------------------+------------+--------------+---------------+<br/>|  20220922115641825|20220922115641825...|  1646119520000000|  reporting_month=2...|bb571e5b-6d19-473...|2022-03-01 07:25:20|        0.00|    2022-03-01|         202203|<br/>|  20220922115641825|20220922115641825...|  1646122220000000|  reporting_month=2...|bb571e5b-6d19-473...|2022-03-01 08:10:20|        1.11|    2022-03-01|         202203|<br/>+-------------------+--------------------+------------------+----------------------+--------------------+-------------------+------------+--------------+---------------+</span></pre><p id="0770" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">ğŸ‰æˆåŠŸï¼æˆ‘ä»¬æœ‰åŸå­æ€§ï¼Œæˆ‘ä»¬çš„æ•°æ®ä¸ä¼šå› ä¸ºå¤„ç†ç®¡é“ä¸­çš„ä¸€äº›é”™è¯¯è€Œæ¶ˆå¤±ï¼</p><h1 id="1491" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">ä¸‰è§’æ´²æ¹–</h1><p id="648b" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">ä¸èƒ¡è¿ªä¸€æ ·ï¼ŒDelta Lake åœ¨ spark ä¼šè¯ä¸­éœ€è¦ä¸€ä¸ªé¢å¤–çš„ä¾èµ–é¡¹:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="dd81" class="mn kn iq mj b gy mo mp l mq mr">def get_delta_spark_session():<br/>    spark_session = SparkSession.builder.appName('delta').config(<br/>        'spark.ui.enabled', 'false').config(<br/>        'spark.jars.packages', 'io.delta:delta-core_2.12:2.1.0').config(<br/>        'spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension').config(<br/>        'spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog').getOrCreate()<br/>    spark_session.sparkContext.setLogLevel('ERROR')<br/>    return spark_session</span></pre><p id="8250" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">åŠ è½½æ•°æ®:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="1b2d" class="mn kn iq mj b gy mo mp l mq mr">def load_delta_spark(flow_type='happy'):<br/>    spark_session = get_delta_spark_session()<br/>    df_input = get_data(spark_session, flow_type)</span><span id="f5fd" class="mn kn iq mj b gy ms mp l mq mr">    df_input.write.\<br/>        partitionBy("reporting_month").\<br/>        mode("overwrite").\<br/>        format("delta"). \<br/>        save(path)</span></pre><p id="ab04" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">è¯»å–æ•°æ®:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="aeb8" class="mn kn iq mj b gy mo mp l mq mr">def read_data():<br/>    spark_session = get_delta_spark_session()</span><span id="f02b" class="mn kn iq mj b gy ms mp l mq mr">    number_of_records = spark_session.read.\<br/>        format('delta').\<br/>        load(path).\<br/>        count()</span><span id="88b8" class="mn kn iq mj b gy ms mp l mq mr">    print(f"Number of records: {number_of_records}")</span><span id="e5ca" class="mn kn iq mj b gy ms mp l mq mr">    spark_session.read.\<br/>        format('delta').\<br/>        load(path).\<br/>        filter("reporting_date = '2022-03-01'").\<br/>        show()</span></pre><p id="5d29" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">è¿™å¾ˆç®€å•ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬è¿è¡Œå¿«ä¹æµç¨‹:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="ae01" class="mn kn iq mj b gy mo mp l mq mr">In [1]: from acid.data_processing_acid_delta import load_delta_spark, read_data</span><span id="e2b7" class="mn kn iq mj b gy ms mp l mq mr">In [2]: load_delta_spark('happy')<br/>                                                  <br/>In [3]: read_data()<br/>Number of records: 4481<br/>+-------------------+------------+--------------+---------------+<br/>|               time|produced_kwh|reporting_date|reporting_month|<br/>+-------------------+------------+--------------+---------------+<br/>|2022-03-01 07:25:20|        0.00|    2022-03-01|         202203|<br/>|2022-03-01 08:10:20|        1.11|    2022-03-01|         202203|<br/>+-------------------+------------+--------------+---------------+</span></pre><p id="eb97" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">é”™è¯¯çš„é‚£ä¸ªå‘¢ï¼Ÿ</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="5255" class="mn kn iq mj b gy mo mp l mq mr">In [4]: load_delta_spark('error')<br/>...<br/>AnalysisException: Failed to merge fields 'time' and 'time'. Failed to merge incompatible data types TimestampType and StringType</span><span id="36f4" class="mn kn iq mj b gy ms mp l mq mr">In [5]: read_data()<br/>Number of records: 4481<br/>+-------------------+------------+--------------+---------------+<br/>|               time|produced_kwh|reporting_date|reporting_month|<br/>+-------------------+------------+--------------+---------------+<br/>|2022-03-01 07:25:20|        0.00|    2022-03-01|         202203|<br/>|2022-03-01 08:10:20|        1.11|    2022-03-01|         202203|<br/>+-------------------+------------+--------------+---------------+</span></pre><p id="01bd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">éå¸¸ç®€å•ï¼</p><h1 id="626c" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">é˜¿å¸•å¥‡å†°å±±</h1><p id="b2bd" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">å¦‚æœè¯´èƒ¡è¿ªå’Œå¾·å°”å¡”ä¸éœ€è¦æ•°æ®ç›®å½•(ä½†å®ƒä»¬å¯ä»¥é›†æˆåœ¨ä¸€èµ·)ï¼Œé‚£ä¹ˆå†°å±±å°±éœ€è¦ã€‚åŒæ ·å¯¹äº Icebergï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿å°†æ­£ç¡®çš„ä¾èµ–é¡¹æ·»åŠ åˆ°æˆ‘ä»¬çš„ä¼šè¯ä¸­:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="5757" class="mn kn iq mj b gy mo mp l mq mr">def get_iceberg_spark_session():<br/>    spark_session = SparkSession.builder.appName('iceberg').config(<br/>        'spark.ui.enabled', 'false').config(<br/>        'spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:0.14.1').config(<br/>        'spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions').config(<br/>        'spark.sql.catalog.spark_catalog', 'org.apache.iceberg.spark.SparkSessionCatalog').config(<br/>        'spark.sql.catalog.spark_catalog.type', 'hive').config(<br/>        'spark.sql.catalog.local', 'org.apache.iceberg.spark.SparkCatalog').getOrCreate()<br/>    spark_session.sparkContext.setLogLevel('ERROR')<br/>    return spark_session</span></pre><p id="000a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">æˆ‘ä»¬è¿˜éœ€è¦åˆ›å»ºè¡¨ï¼Œç„¶åæ‰èƒ½å‘å…¶ä¸­å†™å…¥æ•°æ®:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="79a0" class="mn kn iq mj b gy mo mp l mq mr">def create_table():<br/>    spark_session = get_iceberg_spark_session()</span><span id="bbf4" class="mn kn iq mj b gy ms mp l mq mr">    spark_session.sql(<br/>        "CREATE or REPLACE TABLE iceberg_acid_example (  \<br/>            time timestamp,  \<br/>            produced_kwh decimal(18,2),  \<br/>            reporting_date date, \<br/>            reporting_month string ) \<br/>        USING iceberg \<br/>        PARTITIONED BY (reporting_month) \<br/>        LOCATION '/app/output_data/acid_example/iceberg_spark'"<br/>    )</span></pre><p id="273f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">åŠ è½½æ•°æ®:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="9320" class="mn kn iq mj b gy mo mp l mq mr">def load_iceberg_spark(flow_type='happy'):<br/>    spark_session = get_iceberg_spark_session()<br/>    df_input = get_data(spark_session, flow_type)</span><span id="5519" class="mn kn iq mj b gy ms mp l mq mr">    df_input.write.\<br/>        mode("overwrite").\<br/>        format("iceberg"). \<br/>        save('iceberg_acid_example')</span></pre><p id="8b53" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">è¯»å–æ•°æ®:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="5c13" class="mn kn iq mj b gy mo mp l mq mr">def read_data():<br/>    spark_session = get_iceberg_spark_session()</span><span id="0696" class="mn kn iq mj b gy ms mp l mq mr">    number_of_records = spark_session.read.\<br/>        format('iceberg').\<br/>        table('iceberg_acid_example').\<br/>        count()</span><span id="504f" class="mn kn iq mj b gy ms mp l mq mr">    print(f"Number of records: {number_of_records}")</span><span id="f06e" class="mn kn iq mj b gy ms mp l mq mr">    spark_session.read.\<br/>        format('iceberg').\<br/>        table('iceberg_acid_example').\<br/>        filter("reporting_date = '2022-03-01'").\<br/>        show()</span></pre><p id="13d3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">ç°åœ¨è®©æˆ‘ä»¬ğŸš€ï¼š</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="3762" class="mn kn iq mj b gy mo mp l mq mr">In [1]: from acid.data_processing_acid_iceberg import create_table, load_iceberg_spark, read_data</span><span id="80b9" class="mn kn iq mj b gy ms mp l mq mr">In [2]: create_table()</span><span id="4741" class="mn kn iq mj b gy ms mp l mq mr">In [3]: load_iceberg_spark('happy')<br/>                                                                                <br/>In [4]: read_data()<br/>Number of records: 4481<br/>+-------------------+------------+--------------+---------------+<br/>|               time|produced_kwh|reporting_date|reporting_month|<br/>+-------------------+------------+--------------+---------------+<br/>|2022-03-01 07:25:20|        0.00|    2022-03-01|         202203|<br/>|2022-03-01 08:10:20|        1.11|    2022-03-01|         202203|<br/>+-------------------+------------+--------------+---------------+</span></pre><p id="d8aa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">è€Œé”™è¯¯æµå‘¢ï¼Ÿ</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="58c9" class="mn kn iq mj b gy mo mp l mq mr">In [5]: load_iceberg_spark('error')<br/>...<br/>AnalysisException: Cannot write incompatible data to table 'spark_catalog.default.iceberg_acid_example':<br/>- Cannot safely cast 'time': string to timestamp</span><span id="fab1" class="mn kn iq mj b gy ms mp l mq mr">In [6]: read_data()<br/>Number of records: 4481<br/>+-------------------+------------+--------------+---------------+<br/>|               time|produced_kwh|reporting_date|reporting_month|<br/>+-------------------+------------+--------------+---------------+<br/>|2022-03-01 07:25:20|        0.00|    2022-03-01|         202203|<br/>|2022-03-01 08:10:20|        1.11|    2022-03-01|         202203|<br/>+-------------------+------------+--------------+---------------+</span></pre><h1 id="aaaf" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">ç»“è®º</h1><p id="1805" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">è™½ç„¶ Spark ä¸èƒ½ç¡®ä¿ ACID åˆè§„æ€§ï¼Œä½†æˆ‘ä»¬å¯ä»¥åˆ©ç”¨é˜¿å¸•å¥‡èƒ¡è¿ªã€é˜¿å¸•å¥‡å†°å±±æˆ–å¾·å°”å¡”æ¹–æ¥å®ç°è¿™ä¸€ç‚¹ã€‚è¿™ 3 ä¸ªå·¥å…·å¸¦æ¥çš„ä¸ä»…ä»…æ˜¯ ACID:åŸºäºæ•°æ®æ¹–å­˜å‚¨è§£å†³æ–¹æ¡ˆçš„å®Œæ•´æ•°æ®ä»“åº“æŠ€æœ¯ã€‚</p><p id="69df" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">é‚£ä¹ˆï¼Œåœ¨è¿™ä¸ªæ¦‚å¿µéªŒè¯ä¸­ï¼Œå®ƒä»¬ä¹‹é—´çš„åŸºæœ¬åŒºåˆ«æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿ</p><figure class="me mf mg mh gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nn"><img src="../Images/3d6e55b84133bff2f6116f82f6c67df3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*kYVoDa3to-q5Kid7.png"/></div></div></figure><p id="c2cf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">æœ‰è¶£çš„æ˜¯ï¼Œæ ¹æ®æˆ‘ä½¿ç”¨çš„æ•°æ®ï¼Œèƒ¡è¿ªå°†åˆå§‹å­˜å‚¨å¢åŠ äº†å¤§çº¦ 8 å€ã€‚å­˜å‚¨è§„æ¨¡çš„å¢é•¿æ˜¯æœ‰æ„ä¹‰çš„ï¼Œå› ä¸ºèƒ¡è¿ªå°†å®¡è®¡ä¿¡æ¯é™„åŠ åˆ°æ¯ä¸ªè®°å½•ä¸­ã€‚</p><p id="c56e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">ä»ä»–ä»¬ä¸‰ä¸ªä¸­ï¼Œä¸‰è§’æ´²æ¹–æ˜¯æœ€å®¹æ˜“å»ºç«‹çš„ã€‚æœ€éš¾çš„æ˜¯ Apache Icebergï¼Œå› ä¸ºå®ƒä¾èµ–äº Hive(ä»¥åŠä¸æ”¯æŒ Java 11 çš„ Hadoop)ã€‚åŒæ ·ï¼Œæœ‰è¶£çš„æ˜¯è§‚å¯Ÿåˆ° Apache Iceberg é™„å¸¦äº†å¯¹ Hadoop çš„ä¾èµ–ï¼Œæ®ä¼ é—»ï¼Œè¿™ç§ä¾èµ–å³å°†ç»“æŸã€‚</p><p id="0d58" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Apache èƒ¡è¿ªå¸¦æ¥äº†å¦ä¸€ä¸ªæŒ‘æˆ˜:ä¸æ˜¯è®¾ç½®ï¼Œè€Œæ˜¯åœ¨ç¼–å†™ä»£ç æ—¶éœ€è¦è€ƒè™‘ç‰©ç†æ•°æ®æ¨¡å‹(ä¸»é”®ã€åˆ†åŒºç­‰)ã€‚æˆ‘è®¤ä¸ºè¿™æ˜¯ç§¯æçš„ï¼Œå› ä¸ºå®ƒä¿ƒä½¿æ‚¨åœ¨éƒ¨ç½²åˆ°äº§å“ä¹‹å‰è€ƒè™‘æ‚¨çš„æ•°æ®æ¨¡å¼ã€‚</p><p id="8c91" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">å“ªä¸ªæœ€å¥½ï¼Ÿè¿™ä¹ˆå°çš„ POC éƒ½éš¾è¯´ï¼Œä½ è¯´å‘¢ï¼Ÿ</p><p id="0d9a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">è¯·éšæ„æŸ¥çœ‹ Github repoï¼Œå¹¶è‡ªè¡Œè¯•ç”¨ï¼</p><h1 id="0261" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">è¯æ˜æ–‡ä»¶</h1><ol class=""><li id="318d" class="lq lr iq jp b jq lk ju ll jy no kc np kg nq kk lv lw lx ly bi translated"><a class="ae kl" href="https://en.wikipedia.org/wiki/ACID" rel="noopener ugc nofollow" target="_blank"> ACID </a></li><li id="c4c7" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated"><a class="ae kl" href="https://blog.knoldus.com/spark-acid-compliant-or-not/" rel="noopener ugc nofollow" target="_blank">ç«èŠ±:æ˜¯å¦ç¬¦åˆ ACID æ ‡å‡†</a></li><li id="abeb" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated"><a class="ae kl" href="https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes" rel="noopener ugc nofollow" target="_blank">ç«èŠ±ä¿å­˜æ¨¡å¼</a></li><li id="0025" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated"><a class="ae kl" href="https://www.databricks.com/p/ebook/building-the-data-lakehouse" rel="noopener ugc nofollow" target="_blank">å»ºè®¾æ•°æ®æ¹–åº“</a></li><li id="bf69" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated"><a class="ae kl" href="https://hudi.apache.org/" rel="noopener ugc nofollow" target="_blank">é˜¿å¸•å¥‡èƒ¡è¿ª</a></li><li id="6830" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated"><a class="ae kl" href="https://delta.io/learn/getting-started" rel="noopener ugc nofollow" target="_blank">ä¸‰è§’æ´²æ¹–</a></li><li id="926a" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated"><a class="ae kl" href="https://iceberg.apache.org/docs/latest/getting-started/" rel="noopener ugc nofollow" target="_blank">é˜¿å¸•å¥‡å†°å±±</a></li><li id="1891" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated"><a class="ae kl" href="https://www.databricks.com/p/ebook/building-the-data-lakehouse?utm_medium=paid+search&amp;utm_source=google&amp;utm_campaign=17159505560&amp;utm_adgroup=137123059355&amp;utm_content=ebook&amp;utm_offer=building-the-data-lakehouse&amp;utm_ad=596310483100&amp;utm_term=data%20lakehouse%20databricks&amp;gclid=Cj0KCQjwj7CZBhDHARIsAPPWv3d2fVr8yAoMM5gbCsd89dsgttYJwAKcYJf_BYT_SvUVSOZoQlRKORgaAozVEALw_wcB" rel="noopener ugc nofollow" target="_blank"> Data Lakehouse ç”µå­ä¹¦</a></li><li id="93e5" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated"><a class="ae kl" href="https://github.com/acirtep/ginlong-data-processing-spark" rel="noopener ugc nofollow" target="_blank"> Github å›è´­</a></li></ol><p id="ca2b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">æ–‡ç« æœ€åˆå‘è¡¨äº@ <a class="ae kl" href="https://ownyourdata.ai/wp/data-processing-with-spark-acid-compliance/" rel="noopener ugc nofollow" target="_blank"> ownyourdata.ai </a>ã€‚</p></div></div>    
</body>
</html>