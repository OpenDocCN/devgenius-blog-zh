<html>
<head>
<title>Data processing with Spark: ACID compliance</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Spark 进行数据处理:酸性合规性</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/data-processing-with-spark-acid-compliance-be89464073d9?source=collection_archive---------5-----------------------#2022-09-22">https://blog.devgenius.io/data-processing-with-spark-acid-compliance-be89464073d9?source=collection_archive---------5-----------------------#2022-09-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="c49e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在之前的 Spark 文章中，我讲述了如何<a class="ae kl" href="https://ownyourdata.ai/wp/data-processing-with-spark-intro/" rel="noopener ugc nofollow" target="_blank">设置 Spark </a>，如何设置<a class="ae kl" href="https://ownyourdata.ai/wp/data-processing-with-spark-data-catalog/" rel="noopener ugc nofollow" target="_blank"> Hive 数据目录</a>以及如何处理<a class="ae kl" href="https://ownyourdata.ai/wp/data-processing-with-spark-schema-evolution/" rel="noopener ugc nofollow" target="_blank">模式演化</a>。现在是时候经历 ACID 和截至 2022 年 9 月 Spark 有哪些选择:阿帕奇胡迪，三角洲湖和阿帕奇冰山。</p><h1 id="4bb8" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">酸</h1><p id="329b" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">在<em class="lp">管理的</em>环境中数据处理的 4 个特征:</p><ol class=""><li id="c385" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated">原子数</li><li id="4325" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">一致性</li><li id="de11" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">隔离</li><li id="f30a" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">持久性</li></ol><p id="8de8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">即使从 Spark 的文献中，也指出不尊重原子数，这影响了酸的其他特性。阅读文档区中链接的第二篇文章，它很好地解释了这种情况。</p><p id="2f4c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">什么时候应该关心酸？在我看来，数据处理的所有步骤都不需要 ACID。在用户层，需要确保数据管理、合规性以及并发和一致的访问，ACID 是圣杯。但是，如果我们的目标是临时/暂存区，ACID 就不那么重要了，因为它不会影响流程/用户。</p><p id="3644" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当我们谈到覆盖模式时，Spark 中的 ACID 被破坏了，因为 Spark 将首先删除数据，然后再写入数据。因此，本文将只关注这种保存模式。</p><h1 id="47cd" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">概念验证设置</h1><p id="6520" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">首先，我需要一些虚拟数据，这些数据是我随机生成的，并用 pandas 保存到磁盘上:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="02fc" class="mn kn iq mj b gy mo mp l mq mr">def get_some_data(): </span><span id="b10a" class="mn kn iq mj b gy ms mp l mq mr">    start_date = date(2021, 1, 1)</span><span id="4340" class="mn kn iq mj b gy ms mp l mq mr">    end_date = date(2022, 6, 30)<br/>    record_list = []</span><span id="eebd" class="mn kn iq mj b gy ms mp l mq mr">    while start_date &lt;= end_date:<br/>        start_datetime = datetime(<br/>            start_date.year, start_date.month, start_date.day, <br/>            random.randrange(6, 8), random.randrange(0, 59), random.randrange(0, 59)<br/>        )<br/>        start_kwh = 0.0<br/>        record_list.append([start_datetime, start_kwh])<br/>        for i in range(1, random.randrange(2, 15)):<br/>            record_list.append(<br/>                [<br/>                    start_datetime+timedelta(minutes=i*45), <br/>                    round(i*random.uniform(1.10, 1.15), 2)<br/>                ]<br/>            )<br/>        start_date = start_date + timedelta(1)<br/>    <br/>    df = pd.DataFrame(record_list)<br/>    df.columns = ['time', 'produced_kwh']<br/>    df.to_csv('/app/input_data/acid_example_data.txt', sep='\t', index=False)</span></pre><p id="9643" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了正确展示 Spark 上的写操作，我将按报告月份对数据进行分区，并开发了两种方法:1 用于快乐流，1 用于中断 Spark:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="1e01" class="mn kn iq mj b gy mo mp l mq mr">def get_input_df(spark_session):<br/>    return spark_session.read.option(<br/>        'delimiter', '\t').option(<br/>        'header', 'true').csv(<br/>        'input_data/acid_example_data.txt'<br/>    )<br/></span><span id="23a5" class="mn kn iq mj b gy ms mp l mq mr">def get_happy_flow(df_input):<br/>    return df_input.withColumn(<br/>        "time", to_timestamp("time")).withColumn(<br/>        "produced_kwh", col("produced_kwh").cast("decimal(18,2)")).withColumn(<br/>        "reporting_date", to_date(col("time"),"yyyy-MM-dd")).withColumn(<br/>        "reporting_month", date_format(col("reporting_date"),"yyyyMM"))<br/></span><span id="0234" class="mn kn iq mj b gy ms mp l mq mr">def get_error_flow(df_input):<br/>    return df_input.withColumn(<br/>        "produced_kwh", col("produced_kwh").cast("decimal(18,2)")).withColumn(<br/>        "reporting_date", to_date(col("time"),"yyyy-MM-dd")).withColumn(<br/>        "reporting_month", date_format(col("reporting_date"),"yyyyMM"))</span><span id="142f" class="mn kn iq mj b gy ms mp l mq mr">def get_data(spark_session, flow_type):<br/>    df_input = get_input_df(spark_session)<br/>    if flow_type == 'error':<br/>        df_input = get_error_flow(df_input)<br/>    else:<br/>        df_input = get_happy_flow(df_input)<br/>    <br/>    return df_input</span></pre><p id="7574" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">记住:Spark 是惰性的，所以 get_error_flow 在删除后会失败，因为时间戳不匹配。</p><h1 id="919a" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">默认火花设置</h1><p id="8cfd" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">如何获得默认的 spark 会话:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="5ef3" class="mn kn iq mj b gy mo mp l mq mr">def get_default_spark_session():<br/>    spark_session = SparkSession.builder.appName('default').config(<br/>        'spark.ui.enabled', 'false').getOrCreate()<br/>    spark_session.sparkContext.setLogLevel('ERROR')<br/>    return spark_session</span></pre><p id="9622" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">加载数据:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="1bc4" class="mn kn iq mj b gy mo mp l mq mr">def load_default_spark(flow_type='happy'):<br/>    spark_session = get_default_spark_session()<br/>    df_input = get_data(spark_session, flow_type)</span><span id="0031" class="mn kn iq mj b gy ms mp l mq mr">    df_input.write.\<br/>        partitionBy("reporting_month").\<br/>        mode('overwrite').\<br/>        format('parquet').\<br/>        save(path)</span></pre><p id="6d08" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">读取数据:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="7756" class="mn kn iq mj b gy mo mp l mq mr">def read_data():<br/>    spark_session = get_default_spark_session()<br/>    <br/>    number_of_records = spark_session.read.\<br/>        format('parquet').\<br/>        load(path).\<br/>        count()</span><span id="c60e" class="mn kn iq mj b gy ms mp l mq mr">    print(f"Number of records: {number_of_records}")</span><span id="7ef8" class="mn kn iq mj b gy ms mp l mq mr">    spark_session.read.\<br/>        format('parquet').\<br/>        load(path).\<br/>        filter("reporting_date = '2022-03-01'").\<br/>        show()</span></pre><p id="d8bd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">那么快乐流上发生了什么？</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="6d96" class="mn kn iq mj b gy mo mp l mq mr">In [1]: from acid.data_processing_acid_default_spark import load_default_spark, read_data</span><span id="fcf9" class="mn kn iq mj b gy ms mp l mq mr">In [2]: load_default_spark('happy')<br/>                                                                             <br/>In [3]: read_data()<br/>Number of records: 4481<br/>+-------------------+------------+--------------+---------------+<br/>|               time|produced_kwh|reporting_date|reporting_month|<br/>+-------------------+------------+--------------+---------------+<br/>|2022-03-01 07:25:20|        0.00|    2022-03-01|         202203|<br/>|2022-03-01 08:10:20|        1.11|    2022-03-01|         202203|<br/>+-------------------+------------+--------------+---------------+</span></pre><p id="81a4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">关于错误的那一个呢？</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="d02c" class="mn kn iq mj b gy mo mp l mq mr">In [4]: load_default_spark('error')<br/>22/09/22 09:20:18 ERROR Executor: Exception in task 0.0 in stage 10.0 (TID 14)<br/>org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading to Spark &gt;= 3.0: Fail to parse '2021-01-01 07:54:21' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.</span><span id="ac06" class="mn kn iq mj b gy ms mp l mq mr">In [5]: read_data()<br/>AnalysisException: Unable to infer schema for Parquet. It must be specified manually.</span></pre><p id="1fbe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上述错误是由于路径中不再有数据这一事实造成的，因为 Spark 首先删除了数据，然后试图写入新数据。因此，原子性特征没有得到尊重，我们丢失了数据和一致性。</p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><p id="2e59" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我写这篇文章的时候，围绕着 data lakehouse 和 Spark 是否比其他数据库技术更好还有争议。我承认我更喜欢数据库，因为在 Spark 用 null 替换数据的情况下，它们会在 write 时失败。但我确实认为它们可以结合起来创建一个强大的数据平台，以备不时之需！</p><figure class="me mf mg mh gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi na"><img src="../Images/17ee57c769048b208e41234c9f9f34ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*o2cnr4c03ONlT2N3.jpg"/></div></div></figure><p id="9439" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">启用 ACID for Spark 的实用程序还附带了其他功能:插入、删除和时间旅行。在本文中，我将介绍每个实用程序所需的设置，但重点是测试原子性。</p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><h1 id="8760" class="km kn iq bd ko kp ni kr ks kt nj kv kw kx nk kz la lb nl ld le lf nm lh li lj bi translated">阿帕奇胡迪</h1><p id="46fd" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">如何与胡迪产生火花？我们需要确保在我们的会话中拥有 jar 依赖性以及胡迪特定的配置:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="108d" class="mn kn iq mj b gy mo mp l mq mr">def get_hudi_spark_session():<br/>    spark_session = SparkSession.builder.appName('hudi').config(<br/>        'spark.ui.enabled', 'false').config(<br/>        'spark.jars.packages', 'org.apache.hudi:hudi-spark3.3-bundle_2.12:0.12.0').config(<br/>        'spark.serializer', 'org.apache.spark.serializer.KryoSerializer').config(<br/>        'spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.hudi.catalog.HoodieCatalog').config(<br/>        'spark.sql.extensions', 'org.apache.spark.sql.hudi.HoodieSparkSessionExtension').getOrCreate()<br/>    spark_session.sparkContext.setLogLevel('ERROR')<br/>    return spark_session</span></pre><p id="a46b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">加载数据:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="a2ab" class="mn kn iq mj b gy mo mp l mq mr">def load_hudi_spark(flow_type='happy'):<br/>    spark_session = get_hudi_spark_session()<br/>    df_input = get_data(spark_session, flow_type)<br/>    <br/>    hudi_options = {<br/>        'hoodie.table.name': 'acid_hudi_spark',<br/>        'hoodie.datasource.write.recordkey.field': 'time',<br/>        'hoodie.datasource.write.partitionpath.field': 'reporting_month',<br/>        'hoodie.datasource.write.table.name': 'acid_hudi_spark',<br/>        'hoodie.datasource.write.operation': 'insert_overwrite',<br/>        'hoodie.datasource.write.precombine.field': 'time',<br/>        'hoodie.upsert.shuffle.parallelism': 2,<br/>        'hoodie.insert.shuffle.parallelism': 2,<br/>        'hoodie.datasource.write.hive_style_partitioning': 'true'<br/>    }</span><span id="33db" class="mn kn iq mj b gy ms mp l mq mr">    df_input.write. \<br/>        options(**hudi_options). \<br/>        mode("append"). \<br/>        format("hudi"). \<br/>        save(path)</span></pre><p id="658a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意，胡迪需要处理大量的配置:</p><ol class=""><li id="439e" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated">记录键，这是主键</li><li id="2b1e" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">分区路径，要分区的字段</li><li id="4434" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">写操作，可以是 upsert、bulk_insert 等</li><li id="dc78" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">预组合字段，对于时间旅行和重复数据删除非常重要</li><li id="17f5" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">hive_style_partitioning，已启用，以便为分区自动发现创建文件夹结构</li></ol><p id="1e0e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意胡迪有它自己的写操作模式。通过将 Spark 模式设置为 append，将胡迪模式设置为 insert_overwrite，我们确保了原子性。</p><p id="527e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">读取数据:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="4e38" class="mn kn iq mj b gy mo mp l mq mr">def read_data():<br/>    spark_session = get_hudi_spark_session()</span><span id="7033" class="mn kn iq mj b gy ms mp l mq mr">    number_of_records = spark_session.read.\<br/>        format('hudi').\<br/>        load(path).\<br/>        count()</span><span id="b277" class="mn kn iq mj b gy ms mp l mq mr">    print(f"Number of records: {number_of_records}")</span><span id="d70d" class="mn kn iq mj b gy ms mp l mq mr">    spark_session.read.\<br/>        format('hudi').\<br/>        load(path).\<br/>        filter("reporting_date = '2022-03-01'").\<br/>        show()</span></pre><p id="1fd7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">快乐之流:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="8ba7" class="mn kn iq mj b gy mo mp l mq mr">In [1]: from acid.data_processing_acid_hudi import load_hudi_spark, read_data</span><span id="2b83" class="mn kn iq mj b gy ms mp l mq mr">In [2]: load_hudi_spark('happy')<br/>                                                          <br/>In [3]: read_data()<br/>Number of records: 4481<br/>+-------------------+--------------------+------------------+----------------------+--------------------+-------------------+------------+--------------+---------------+<br/>|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|               time|produced_kwh|reporting_date|reporting_month|<br/>+-------------------+--------------------+------------------+----------------------+--------------------+-------------------+------------+--------------+---------------+<br/>|  20220922115641825|20220922115641825...|  1646119520000000|  reporting_month=2...|bb571e5b-6d19-473...|2022-03-01 07:25:20|        0.00|    2022-03-01|         202203|<br/>|  20220922115641825|20220922115641825...|  1646122220000000|  reporting_month=2...|bb571e5b-6d19-473...|2022-03-01 08:10:20|        1.11|    2022-03-01|         202203|<br/>+-------------------+--------------------+------------------+----------------------+--------------------+-------------------+------------+--------------+---------------+</span></pre><p id="e17a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们看到胡迪向数据中添加了一些审计列，这些列在内部用于数据操作。</p><p id="db3f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">错误流上发生了什么？</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="a16c" class="mn kn iq mj b gy mo mp l mq mr">In [4]: load_hudi_spark('error')<br/>22/09/22 12:01:57 ERROR Executor: Exception in task 0.0 in stage 50.0 (TID 212)<br/>org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading to Spark &gt;= 3.0: Fail to parse '2021-01-01 07:54:21' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.</span><span id="1d1a" class="mn kn iq mj b gy ms mp l mq mr">In [5]: read_data()<br/>Number of records: 4481<br/>+-------------------+--------------------+------------------+----------------------+--------------------+-------------------+------------+--------------+---------------+<br/>|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|               time|produced_kwh|reporting_date|reporting_month|<br/>+-------------------+--------------------+------------------+----------------------+--------------------+-------------------+------------+--------------+---------------+<br/>|  20220922115641825|20220922115641825...|  1646119520000000|  reporting_month=2...|bb571e5b-6d19-473...|2022-03-01 07:25:20|        0.00|    2022-03-01|         202203|<br/>|  20220922115641825|20220922115641825...|  1646122220000000|  reporting_month=2...|bb571e5b-6d19-473...|2022-03-01 08:10:20|        1.11|    2022-03-01|         202203|<br/>+-------------------+--------------------+------------------+----------------------+--------------------+-------------------+------------+--------------+---------------+</span></pre><p id="0770" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">🎉成功！我们有原子性，我们的数据不会因为处理管道中的一些错误而消失！</p><h1 id="1491" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">三角洲湖</h1><p id="648b" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">与胡迪一样，Delta Lake 在 spark 会话中需要一个额外的依赖项:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="dd81" class="mn kn iq mj b gy mo mp l mq mr">def get_delta_spark_session():<br/>    spark_session = SparkSession.builder.appName('delta').config(<br/>        'spark.ui.enabled', 'false').config(<br/>        'spark.jars.packages', 'io.delta:delta-core_2.12:2.1.0').config(<br/>        'spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension').config(<br/>        'spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog').getOrCreate()<br/>    spark_session.sparkContext.setLogLevel('ERROR')<br/>    return spark_session</span></pre><p id="8250" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">加载数据:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="1b2d" class="mn kn iq mj b gy mo mp l mq mr">def load_delta_spark(flow_type='happy'):<br/>    spark_session = get_delta_spark_session()<br/>    df_input = get_data(spark_session, flow_type)</span><span id="f5fd" class="mn kn iq mj b gy ms mp l mq mr">    df_input.write.\<br/>        partitionBy("reporting_month").\<br/>        mode("overwrite").\<br/>        format("delta"). \<br/>        save(path)</span></pre><p id="ab04" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">读取数据:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="aeb8" class="mn kn iq mj b gy mo mp l mq mr">def read_data():<br/>    spark_session = get_delta_spark_session()</span><span id="f02b" class="mn kn iq mj b gy ms mp l mq mr">    number_of_records = spark_session.read.\<br/>        format('delta').\<br/>        load(path).\<br/>        count()</span><span id="88b8" class="mn kn iq mj b gy ms mp l mq mr">    print(f"Number of records: {number_of_records}")</span><span id="e5ca" class="mn kn iq mj b gy ms mp l mq mr">    spark_session.read.\<br/>        format('delta').\<br/>        load(path).\<br/>        filter("reporting_date = '2022-03-01'").\<br/>        show()</span></pre><p id="5d29" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这很简单，所以让我们运行快乐流程:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="ae01" class="mn kn iq mj b gy mo mp l mq mr">In [1]: from acid.data_processing_acid_delta import load_delta_spark, read_data</span><span id="e2b7" class="mn kn iq mj b gy ms mp l mq mr">In [2]: load_delta_spark('happy')<br/>                                                  <br/>In [3]: read_data()<br/>Number of records: 4481<br/>+-------------------+------------+--------------+---------------+<br/>|               time|produced_kwh|reporting_date|reporting_month|<br/>+-------------------+------------+--------------+---------------+<br/>|2022-03-01 07:25:20|        0.00|    2022-03-01|         202203|<br/>|2022-03-01 08:10:20|        1.11|    2022-03-01|         202203|<br/>+-------------------+------------+--------------+---------------+</span></pre><p id="eb97" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">错误的那个呢？</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="5255" class="mn kn iq mj b gy mo mp l mq mr">In [4]: load_delta_spark('error')<br/>...<br/>AnalysisException: Failed to merge fields 'time' and 'time'. Failed to merge incompatible data types TimestampType and StringType</span><span id="36f4" class="mn kn iq mj b gy ms mp l mq mr">In [5]: read_data()<br/>Number of records: 4481<br/>+-------------------+------------+--------------+---------------+<br/>|               time|produced_kwh|reporting_date|reporting_month|<br/>+-------------------+------------+--------------+---------------+<br/>|2022-03-01 07:25:20|        0.00|    2022-03-01|         202203|<br/>|2022-03-01 08:10:20|        1.11|    2022-03-01|         202203|<br/>+-------------------+------------+--------------+---------------+</span></pre><p id="01bd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">非常简单！</p><h1 id="626c" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">阿帕奇冰山</h1><p id="b2bd" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">如果说胡迪和德尔塔不需要数据目录(但它们可以集成在一起)，那么冰山就需要。同样对于 Iceberg，我们需要确保将正确的依赖项添加到我们的会话中:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="5757" class="mn kn iq mj b gy mo mp l mq mr">def get_iceberg_spark_session():<br/>    spark_session = SparkSession.builder.appName('iceberg').config(<br/>        'spark.ui.enabled', 'false').config(<br/>        'spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:0.14.1').config(<br/>        'spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions').config(<br/>        'spark.sql.catalog.spark_catalog', 'org.apache.iceberg.spark.SparkSessionCatalog').config(<br/>        'spark.sql.catalog.spark_catalog.type', 'hive').config(<br/>        'spark.sql.catalog.local', 'org.apache.iceberg.spark.SparkCatalog').getOrCreate()<br/>    spark_session.sparkContext.setLogLevel('ERROR')<br/>    return spark_session</span></pre><p id="000a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们还需要创建表，然后才能向其中写入数据:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="79a0" class="mn kn iq mj b gy mo mp l mq mr">def create_table():<br/>    spark_session = get_iceberg_spark_session()</span><span id="bbf4" class="mn kn iq mj b gy ms mp l mq mr">    spark_session.sql(<br/>        "CREATE or REPLACE TABLE iceberg_acid_example (  \<br/>            time timestamp,  \<br/>            produced_kwh decimal(18,2),  \<br/>            reporting_date date, \<br/>            reporting_month string ) \<br/>        USING iceberg \<br/>        PARTITIONED BY (reporting_month) \<br/>        LOCATION '/app/output_data/acid_example/iceberg_spark'"<br/>    )</span></pre><p id="273f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">加载数据:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="9320" class="mn kn iq mj b gy mo mp l mq mr">def load_iceberg_spark(flow_type='happy'):<br/>    spark_session = get_iceberg_spark_session()<br/>    df_input = get_data(spark_session, flow_type)</span><span id="5519" class="mn kn iq mj b gy ms mp l mq mr">    df_input.write.\<br/>        mode("overwrite").\<br/>        format("iceberg"). \<br/>        save('iceberg_acid_example')</span></pre><p id="8b53" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">读取数据:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="5c13" class="mn kn iq mj b gy mo mp l mq mr">def read_data():<br/>    spark_session = get_iceberg_spark_session()</span><span id="0696" class="mn kn iq mj b gy ms mp l mq mr">    number_of_records = spark_session.read.\<br/>        format('iceberg').\<br/>        table('iceberg_acid_example').\<br/>        count()</span><span id="504f" class="mn kn iq mj b gy ms mp l mq mr">    print(f"Number of records: {number_of_records}")</span><span id="f06e" class="mn kn iq mj b gy ms mp l mq mr">    spark_session.read.\<br/>        format('iceberg').\<br/>        table('iceberg_acid_example').\<br/>        filter("reporting_date = '2022-03-01'").\<br/>        show()</span></pre><p id="13d3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在让我们🚀：</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="3762" class="mn kn iq mj b gy mo mp l mq mr">In [1]: from acid.data_processing_acid_iceberg import create_table, load_iceberg_spark, read_data</span><span id="80b9" class="mn kn iq mj b gy ms mp l mq mr">In [2]: create_table()</span><span id="4741" class="mn kn iq mj b gy ms mp l mq mr">In [3]: load_iceberg_spark('happy')<br/>                                                                                <br/>In [4]: read_data()<br/>Number of records: 4481<br/>+-------------------+------------+--------------+---------------+<br/>|               time|produced_kwh|reporting_date|reporting_month|<br/>+-------------------+------------+--------------+---------------+<br/>|2022-03-01 07:25:20|        0.00|    2022-03-01|         202203|<br/>|2022-03-01 08:10:20|        1.11|    2022-03-01|         202203|<br/>+-------------------+------------+--------------+---------------+</span></pre><p id="d8aa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">而错误流呢？</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="58c9" class="mn kn iq mj b gy mo mp l mq mr">In [5]: load_iceberg_spark('error')<br/>...<br/>AnalysisException: Cannot write incompatible data to table 'spark_catalog.default.iceberg_acid_example':<br/>- Cannot safely cast 'time': string to timestamp</span><span id="fab1" class="mn kn iq mj b gy ms mp l mq mr">In [6]: read_data()<br/>Number of records: 4481<br/>+-------------------+------------+--------------+---------------+<br/>|               time|produced_kwh|reporting_date|reporting_month|<br/>+-------------------+------------+--------------+---------------+<br/>|2022-03-01 07:25:20|        0.00|    2022-03-01|         202203|<br/>|2022-03-01 08:10:20|        1.11|    2022-03-01|         202203|<br/>+-------------------+------------+--------------+---------------+</span></pre><h1 id="aaaf" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">结论</h1><p id="1805" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">虽然 Spark 不能确保 ACID 合规性，但我们可以利用阿帕奇胡迪、阿帕奇冰山或德尔塔湖来实现这一点。这 3 个工具带来的不仅仅是 ACID:基于数据湖存储解决方案的完整数据仓库技术。</p><p id="69df" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">那么，在这个概念验证中，它们之间的基本区别是什么呢？</p><figure class="me mf mg mh gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nn"><img src="../Images/3d6e55b84133bff2f6116f82f6c67df3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*kYVoDa3to-q5Kid7.png"/></div></div></figure><p id="c2cf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有趣的是，根据我使用的数据，胡迪将初始存储增加了大约 8 倍。存储规模的增长是有意义的，因为胡迪将审计信息附加到每个记录中。</p><p id="c56e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从他们三个中，三角洲湖是最容易建立的。最难的是 Apache Iceberg，因为它依赖于 Hive(以及不支持 Java 11 的 Hadoop)。同样，有趣的是观察到 Apache Iceberg 附带了对 Hadoop 的依赖，据传闻，这种依赖即将结束。</p><p id="0d58" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Apache 胡迪带来了另一个挑战:不是设置，而是在编写代码时需要考虑物理数据模型(主键、分区等)。我认为这是积极的，因为它促使您在部署到产品之前考虑您的数据模式。</p><p id="8c91" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">哪个最好？这么小的 POC 都难说，你说呢？</p><p id="0d9a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请随意查看 Github repo，并自行试用！</p><h1 id="0261" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">证明文件</h1><ol class=""><li id="318d" class="lq lr iq jp b jq lk ju ll jy no kc np kg nq kk lv lw lx ly bi translated"><a class="ae kl" href="https://en.wikipedia.org/wiki/ACID" rel="noopener ugc nofollow" target="_blank"> ACID </a></li><li id="c4c7" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated"><a class="ae kl" href="https://blog.knoldus.com/spark-acid-compliant-or-not/" rel="noopener ugc nofollow" target="_blank">火花:是否符合 ACID 标准</a></li><li id="abeb" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated"><a class="ae kl" href="https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes" rel="noopener ugc nofollow" target="_blank">火花保存模式</a></li><li id="0025" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated"><a class="ae kl" href="https://www.databricks.com/p/ebook/building-the-data-lakehouse" rel="noopener ugc nofollow" target="_blank">建设数据湖库</a></li><li id="bf69" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated"><a class="ae kl" href="https://hudi.apache.org/" rel="noopener ugc nofollow" target="_blank">阿帕奇胡迪</a></li><li id="6830" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated"><a class="ae kl" href="https://delta.io/learn/getting-started" rel="noopener ugc nofollow" target="_blank">三角洲湖</a></li><li id="926a" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated"><a class="ae kl" href="https://iceberg.apache.org/docs/latest/getting-started/" rel="noopener ugc nofollow" target="_blank">阿帕奇冰山</a></li><li id="1891" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated"><a class="ae kl" href="https://www.databricks.com/p/ebook/building-the-data-lakehouse?utm_medium=paid+search&amp;utm_source=google&amp;utm_campaign=17159505560&amp;utm_adgroup=137123059355&amp;utm_content=ebook&amp;utm_offer=building-the-data-lakehouse&amp;utm_ad=596310483100&amp;utm_term=data%20lakehouse%20databricks&amp;gclid=Cj0KCQjwj7CZBhDHARIsAPPWv3d2fVr8yAoMM5gbCsd89dsgttYJwAKcYJf_BYT_SvUVSOZoQlRKORgaAozVEALw_wcB" rel="noopener ugc nofollow" target="_blank"> Data Lakehouse 电子书</a></li><li id="93e5" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated"><a class="ae kl" href="https://github.com/acirtep/ginlong-data-processing-spark" rel="noopener ugc nofollow" target="_blank"> Github 回购</a></li></ol><p id="ca2b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">文章最初发表于@ <a class="ae kl" href="https://ownyourdata.ai/wp/data-processing-with-spark-acid-compliance/" rel="noopener ugc nofollow" target="_blank"> ownyourdata.ai </a>。</p></div></div>    
</body>
</html>