<html>
<head>
<title>Getting your text ready for NLP (Python)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为 NLP 准备好文本(Python)</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/getting-your-text-ready-for-nlp-python-e1262e675c29?source=collection_archive---------20-----------------------#2020-06-06">https://blog.devgenius.io/getting-your-text-ready-for-nlp-python-e1262e675c29?source=collection_archive---------20-----------------------#2020-06-06</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><figure class="gl gn jl jm jn jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi jk"><img src="../Images/6d705cdaebd82ad798238f4bf8c340d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R7q_XuufiCYtYj4N4bmv_w.jpeg"/></div></div></figure><p id="2802" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">以下是为 NLP 准备数据时可以执行的 7 个步骤。没有必要所有的 7 件事都必须做，或者都与你的大局相关。</p><ol class=""><li id="76d7" class="kt ku in jx b jy jz kc kd kg kv kk kw ko kx ks ky kz la lb bi translated"><strong class="jx io">清理数据。</strong>删除所有异常和不可理解的数据。你可以这样写:</li></ol><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="fae7" class="ll lm in lh b gy ln lo l lp lq">#fileContent = "&lt;li&gt;Who drank my coffee?😡&lt;/li&gt;&lt;li&gt;I would say whoever ate your avocado toast😜&lt;/li&gt;"</span><span id="81ac" class="ll lm in lh b gy lr lo l lp lq">import re<br/>processedText = ""<br/>with open(filename, “rb”) as f:<br/>    rawData = f.readlines()<br/>    for line in rawData:<br/>    line = line.decode('ascii','ignore')<br/>    processedText = re.sub("&lt;.*?&gt;","",line) #remove html tags</span><span id="d9d4" class="ll lm in lh b gy lr lo l lp lq">#processedText = "Who drank my coffee? I would say whoever ate your avocado toast"</span></pre><p id="3090" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated"><strong class="jx io"> 2。将数据标记化。</strong>将整个文本分成一系列句子。此外，每个句子应该分成一个单词数组。您可以使用<a class="ae ls" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank">自然语言工具包(NLTK) </a>来实现这一点，这是一个专门为处理 python 中人类可读数据而构建的平台。</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="ec2b" class="ll lm in lh b gy ln lo l lp lq">import nltk.data<br/>from nltk.tokenize import word_tokenize, sent_tokenize<br/>sentences = sent_tokenize(processedText)<br/>tokenizedData = [word_tokenize(sent) for sent in sentences]<br/>#tokenizedData = [['Who', 'drank', 'my', 'coffee', '?'], ['I', "would", 'say', 'whoever', 'ate', 'your', 'avocado', 'toast']]</span></pre><p id="0219" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated"><strong class="jx io"> 3。删除停用词。</strong>停用词就像数据中的噪音。他们歪曲了你的结果。删除它们有助于减少分布在不重要元素上的权重，如“the”、“is”、“are”等对本质没有贡献的词。</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="dc5b" class="ll lm in lh b gy ln lo l lp lq">import nltk<br/>nltk.download('stopwords') #download stopwords recognized by nltk<br/>from nltk.corpus import stopwords<br/>from string import punctuation<br/>customStopWords = list(stopwords.words('english')+list(punctuation))<br/>fiteredSentences = []<br/>for sent in tokenizedData:<br/>    fiteredSentences.append([word for word in sent if word not in customStopWords])<br/>#filteredSentences = [['Who', 'drank', 'coffee'], ['I', 'would', 'say', 'whoever', 'ate', 'avocado', 'toast']]</span></pre><p id="5ce2" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated"><strong class="jx io"> 4。词义消歧。需要区分拼写相同但意思不同的单词。这可以使用 Lesk 算法来完成。</strong></p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="a928" class="ll lm in lh b gy ln lo l lp lq">nltk.download(‘wordnet’) <br/>from nltk.corpus import wordnet as wn<br/>print(lesk("The dog continued to bark.",'bark'))<br/># Synset('bark.v.03')<br/>print([(ss,ss.definition()) for ss in wn.synsets('bark')])<br/># [(Synset('bark.n.01'), 'tough protective covering of the woody stems and roots of trees and other woody plants'), (Synset('bark.n.02'), 'a noise resembling the bark of a dog'), (Synset('bark.n.03'), 'a sailing ship with 3 (or more) masts'), (Synset('bark.n.04'), 'the sound made by a dog'), (Synset('bark.v.01'), 'speak in an unfriendly tone'), (Synset('bark.v.02'), 'cover with bark'), (Synset('bark.v.03'), 'remove the bark of a tree'), (Synset('bark.v.04'), 'make barking sounds'), (Synset('bark.v.05'), 'tan (a skin) with bark tannins')]</span></pre><p id="a7ff" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated"><strong class="jx io"> 5。堵塞。将所有单词缩减到它们的词干。这将有助于将一个单词的所有变体结合到它的词根，而不是将它们视为单独的单词。一种方法是使用 NLTK 的雪球茎干仪。</strong></p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="0dd0" class="ll lm in lh b gy ln lo l lp lq">from nltk.stem.snowball import SnowballStemmer<br/>stemmer = SnowballStemmer("english")<br/>print(stemmer.stem("running"))<br/>#run</span></pre><p id="1ec4" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated"><strong class="jx io"> 6。组合</strong><a class="ae ls" href="https://en.wikipedia.org/wiki/N-gram" rel="noopener ugc nofollow" target="_blank"><strong class="jx io">N-Grams</strong></a><strong class="jx io">。</strong>我们可以为需要将其分类为 N 元语法的单词序列的出现次数设置一个阈值。例如，如果“New”后跟“York”在数据中出现的次数超过 6 次，则可以认为它是一个双字母“New York ”,您可能需要考虑它。您可以编写类似这样的东西来获取数据中最常见的二元模型，并决定是否愿意将它们作为单个实体来处理。</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="a653" class="ll lm in lh b gy ln lo l lp lq">import collections<br/>from nltk.util import ngrams<br/>tokenizedDataFlattened = sum(tokenizedData, [])<br/>bigrams = ngrams(tokenizedDataFlattened, 2)<br/>bigramFreq = collections.Counter(bigrams)<br/>print(bigramFreq.most_common(10))# top 10 bigrams</span></pre><p id="ad2a" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">7。词汇化。胜过炮泥。它把意思相似的单词组合在一起。比如把好的减少到好的。我们可以通过首先进行词性标注并将其传递给 NLTK 的 lemmatizer 来做到这一点。下面的函数将 treebank 的标签转换为 wordnet 的标签约定。记住删除停用词，如介词、连词、冠词，因为它们在 wordnet 中没有定义。</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="99fd" class="ll lm in lh b gy ln lo l lp lq">import nltk<br/>from nltk.stem import WordNetLemmatizer<br/>from nltk.corpus import wordnet<br/>nltk.download('averaged_perceptron_tagger') #for POS tagging</span><span id="cd97" class="ll lm in lh b gy lr lo l lp lq">def getWordnetTag(treebankTag):<br/> if treebankTag.startswith(‘J’):<br/> return wordnet.ADJ<br/> elif treebankTag.startswith(‘V’):<br/> return wordnet.VERB<br/> elif treebankTag.startswith(‘N’):<br/> return wordnet.NOUN<br/> elif treebankTag.startswith(‘R’):<br/> return wordnet.ADV<br/> else:<br/> return ‘’</span><span id="743a" class="ll lm in lh b gy lr lo l lp lq">#remember to tokenize the data and remove stop words. <br/>#tokenized = [‘raining’, ‘cats’, ‘dogs’]<br/>posTagged = nltk.pos_tag(tokenized)<br/>lemmatizer = WordNetLemmatizer() <br/>lemmatizedData = [lemmatizer.lemmatize(word,pos = get_wordnet_pos(tag)) for word,tag in posTagged]<br/>print(lemmatizedData)<br/>#lemmatizedData = ['rain', 'cat', 'dog']</span></pre></div></div>    
</body>
</html>