<html>
<head>
<title>Getting your text ready for NLP (Python)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ä¸º NLP å‡†å¤‡å¥½æ–‡æœ¬(Python)</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://blog.devgenius.io/getting-your-text-ready-for-nlp-python-e1262e675c29?source=collection_archive---------20-----------------------#2020-06-06">https://blog.devgenius.io/getting-your-text-ready-for-nlp-python-e1262e675c29?source=collection_archive---------20-----------------------#2020-06-06</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><figure class="gl gn jl jm jn jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi jk"><img src="../Images/6d705cdaebd82ad798238f4bf8c340d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R7q_XuufiCYtYj4N4bmv_w.jpeg"/></div></div></figure><p id="2802" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">ä»¥ä¸‹æ˜¯ä¸º NLP å‡†å¤‡æ•°æ®æ—¶å¯ä»¥æ‰§è¡Œçš„ 7 ä¸ªæ­¥éª¤ã€‚æ²¡æœ‰å¿…è¦æ‰€æœ‰çš„ 7 ä»¶äº‹éƒ½å¿…é¡»åšï¼Œæˆ–è€…éƒ½ä¸ä½ çš„å¤§å±€ç›¸å…³ã€‚</p><ol class=""><li id="76d7" class="kt ku in jx b jy jz kc kd kg kv kk kw ko kx ks ky kz la lb bi translated"><strong class="jx io">æ¸…ç†æ•°æ®ã€‚</strong>åˆ é™¤æ‰€æœ‰å¼‚å¸¸å’Œä¸å¯ç†è§£çš„æ•°æ®ã€‚ä½ å¯ä»¥è¿™æ ·å†™:</li></ol><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="fae7" class="ll lm in lh b gy ln lo l lp lq">#fileContent = "&lt;li&gt;Who drank my coffee?ğŸ˜¡&lt;/li&gt;&lt;li&gt;I would say whoever ate your avocado toastğŸ˜œ&lt;/li&gt;"</span><span id="81ac" class="ll lm in lh b gy lr lo l lp lq">import re<br/>processedText = ""<br/>with open(filename, â€œrbâ€) as f:<br/>    rawData = f.readlines()<br/>    for line in rawData:<br/>    line = line.decode('ascii','ignore')<br/>    processedText = re.sub("&lt;.*?&gt;","",line) #remove html tags</span><span id="d9d4" class="ll lm in lh b gy lr lo l lp lq">#processedText = "Who drank my coffee? I would say whoever ate your avocado toast"</span></pre><p id="3090" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated"><strong class="jx io"> 2ã€‚å°†æ•°æ®æ ‡è®°åŒ–ã€‚</strong>å°†æ•´ä¸ªæ–‡æœ¬åˆ†æˆä¸€ç³»åˆ—å¥å­ã€‚æ­¤å¤–ï¼Œæ¯ä¸ªå¥å­åº”è¯¥åˆ†æˆä¸€ä¸ªå•è¯æ•°ç»„ã€‚æ‚¨å¯ä»¥ä½¿ç”¨<a class="ae ls" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank">è‡ªç„¶è¯­è¨€å·¥å…·åŒ…(NLTK) </a>æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºå¤„ç† python ä¸­äººç±»å¯è¯»æ•°æ®è€Œæ„å»ºçš„å¹³å°ã€‚</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="ec2b" class="ll lm in lh b gy ln lo l lp lq">import nltk.data<br/>from nltk.tokenize import word_tokenize, sent_tokenize<br/>sentences = sent_tokenize(processedText)<br/>tokenizedData = [word_tokenize(sent) for sent in sentences]<br/>#tokenizedData = [['Who', 'drank', 'my', 'coffee', '?'], ['I', "would", 'say', 'whoever', 'ate', 'your', 'avocado', 'toast']]</span></pre><p id="0219" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated"><strong class="jx io"> 3ã€‚åˆ é™¤åœç”¨è¯ã€‚</strong>åœç”¨è¯å°±åƒæ•°æ®ä¸­çš„å™ªéŸ³ã€‚ä»–ä»¬æ­ªæ›²äº†ä½ çš„ç»“æœã€‚åˆ é™¤å®ƒä»¬æœ‰åŠ©äºå‡å°‘åˆ†å¸ƒåœ¨ä¸é‡è¦å…ƒç´ ä¸Šçš„æƒé‡ï¼Œå¦‚â€œtheâ€ã€â€œisâ€ã€â€œareâ€ç­‰å¯¹æœ¬è´¨æ²¡æœ‰è´¡çŒ®çš„è¯ã€‚</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="dc5b" class="ll lm in lh b gy ln lo l lp lq">import nltk<br/>nltk.download('stopwords') #download stopwords recognized by nltk<br/>from nltk.corpus import stopwords<br/>from string import punctuation<br/>customStopWords = list(stopwords.words('english')+list(punctuation))<br/>fiteredSentences = []<br/>for sent in tokenizedData:<br/>    fiteredSentences.append([word for word in sent if word not in customStopWords])<br/>#filteredSentences = [['Who', 'drank', 'coffee'], ['I', 'would', 'say', 'whoever', 'ate', 'avocado', 'toast']]</span></pre><p id="5ce2" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated"><strong class="jx io"> 4ã€‚è¯ä¹‰æ¶ˆæ­§ã€‚éœ€è¦åŒºåˆ†æ‹¼å†™ç›¸åŒä½†æ„æ€ä¸åŒçš„å•è¯ã€‚è¿™å¯ä»¥ä½¿ç”¨ Lesk ç®—æ³•æ¥å®Œæˆã€‚</strong></p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="a928" class="ll lm in lh b gy ln lo l lp lq">nltk.download(â€˜wordnetâ€™) <br/>from nltk.corpus import wordnet as wn<br/>print(lesk("The dog continued to bark.",'bark'))<br/># Synset('bark.v.03')<br/>print([(ss,ss.definition()) for ss in wn.synsets('bark')])<br/># [(Synset('bark.n.01'), 'tough protective covering of the woody stems and roots of trees and other woody plants'), (Synset('bark.n.02'), 'a noise resembling the bark of a dog'), (Synset('bark.n.03'), 'a sailing ship with 3 (or more) masts'), (Synset('bark.n.04'), 'the sound made by a dog'), (Synset('bark.v.01'), 'speak in an unfriendly tone'), (Synset('bark.v.02'), 'cover with bark'), (Synset('bark.v.03'), 'remove the bark of a tree'), (Synset('bark.v.04'), 'make barking sounds'), (Synset('bark.v.05'), 'tan (a skin) with bark tannins')]</span></pre><p id="a7ff" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated"><strong class="jx io"> 5ã€‚å µå¡ã€‚å°†æ‰€æœ‰å•è¯ç¼©å‡åˆ°å®ƒä»¬çš„è¯å¹²ã€‚è¿™å°†æœ‰åŠ©äºå°†ä¸€ä¸ªå•è¯çš„æ‰€æœ‰å˜ä½“ç»“åˆåˆ°å®ƒçš„è¯æ ¹ï¼Œè€Œä¸æ˜¯å°†å®ƒä»¬è§†ä¸ºå•ç‹¬çš„å•è¯ã€‚ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨ NLTK çš„é›ªçƒèŒå¹²ä»ªã€‚</strong></p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="0dd0" class="ll lm in lh b gy ln lo l lp lq">from nltk.stem.snowball import SnowballStemmer<br/>stemmer = SnowballStemmer("english")<br/>print(stemmer.stem("running"))<br/>#run</span></pre><p id="1ec4" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated"><strong class="jx io"> 6ã€‚ç»„åˆ</strong><a class="ae ls" href="https://en.wikipedia.org/wiki/N-gram" rel="noopener ugc nofollow" target="_blank"><strong class="jx io">N-Grams</strong></a><strong class="jx io">ã€‚</strong>æˆ‘ä»¬å¯ä»¥ä¸ºéœ€è¦å°†å…¶åˆ†ç±»ä¸º N å…ƒè¯­æ³•çš„å•è¯åºåˆ—çš„å‡ºç°æ¬¡æ•°è®¾ç½®ä¸€ä¸ªé˜ˆå€¼ã€‚ä¾‹å¦‚ï¼Œå¦‚æœâ€œNewâ€åè·Ÿâ€œYorkâ€åœ¨æ•°æ®ä¸­å‡ºç°çš„æ¬¡æ•°è¶…è¿‡ 6 æ¬¡ï¼Œåˆ™å¯ä»¥è®¤ä¸ºå®ƒæ˜¯ä¸€ä¸ªåŒå­—æ¯â€œNew York â€,æ‚¨å¯èƒ½éœ€è¦è€ƒè™‘å®ƒã€‚æ‚¨å¯ä»¥ç¼–å†™ç±»ä¼¼è¿™æ ·çš„ä¸œè¥¿æ¥è·å–æ•°æ®ä¸­æœ€å¸¸è§çš„äºŒå…ƒæ¨¡å‹ï¼Œå¹¶å†³å®šæ˜¯å¦æ„¿æ„å°†å®ƒä»¬ä½œä¸ºå•ä¸ªå®ä½“æ¥å¤„ç†ã€‚</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="a653" class="ll lm in lh b gy ln lo l lp lq">import collections<br/>from nltk.util import ngrams<br/>tokenizedDataFlattened = sum(tokenizedData, [])<br/>bigrams = ngrams(tokenizedDataFlattened, 2)<br/>bigramFreq = collections.Counter(bigrams)<br/>print(bigramFreq.most_common(10))# top 10 bigrams</span></pre><p id="ad2a" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">7ã€‚è¯æ±‡åŒ–ã€‚èƒœè¿‡ç‚®æ³¥ã€‚å®ƒæŠŠæ„æ€ç›¸ä¼¼çš„å•è¯ç»„åˆåœ¨ä¸€èµ·ã€‚æ¯”å¦‚æŠŠå¥½çš„å‡å°‘åˆ°å¥½çš„ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡é¦–å…ˆè¿›è¡Œè¯æ€§æ ‡æ³¨å¹¶å°†å…¶ä¼ é€’ç»™ NLTK çš„ lemmatizer æ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚ä¸‹é¢çš„å‡½æ•°å°† treebank çš„æ ‡ç­¾è½¬æ¢ä¸º wordnet çš„æ ‡ç­¾çº¦å®šã€‚è®°ä½åˆ é™¤åœç”¨è¯ï¼Œå¦‚ä»‹è¯ã€è¿è¯ã€å† è¯ï¼Œå› ä¸ºå®ƒä»¬åœ¨ wordnet ä¸­æ²¡æœ‰å®šä¹‰ã€‚</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="99fd" class="ll lm in lh b gy ln lo l lp lq">import nltk<br/>from nltk.stem import WordNetLemmatizer<br/>from nltk.corpus import wordnet<br/>nltk.download('averaged_perceptron_tagger') #for POS tagging</span><span id="cd97" class="ll lm in lh b gy lr lo l lp lq">def getWordnetTag(treebankTag):<br/> if treebankTag.startswith(â€˜Jâ€™):<br/> return wordnet.ADJ<br/> elif treebankTag.startswith(â€˜Vâ€™):<br/> return wordnet.VERB<br/> elif treebankTag.startswith(â€˜Nâ€™):<br/> return wordnet.NOUN<br/> elif treebankTag.startswith(â€˜Râ€™):<br/> return wordnet.ADV<br/> else:<br/> return â€˜â€™</span><span id="743a" class="ll lm in lh b gy lr lo l lp lq">#remember to tokenize the data and remove stop words. <br/>#tokenized = [â€˜rainingâ€™, â€˜catsâ€™, â€˜dogsâ€™]<br/>posTagged = nltk.pos_tag(tokenized)<br/>lemmatizer = WordNetLemmatizer() <br/>lemmatizedData = [lemmatizer.lemmatize(word,pos = get_wordnet_pos(tag)) for word,tag in posTagged]<br/>print(lemmatizedData)<br/>#lemmatizedData = ['rain', 'cat', 'dog']</span></pre></div></div>    
</body>
</html>