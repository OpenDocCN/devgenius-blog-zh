<html>
<head>
<title>Why PySpark and How can we run in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么要用 PySpark，我们如何在 Python 中运行</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/why-pyspark-and-how-can-we-run-in-python-b9e2ddc4d48e?source=collection_archive---------7-----------------------#2022-03-04">https://blog.devgenius.io/why-pyspark-and-how-can-we-run-in-python-b9e2ddc4d48e?source=collection_archive---------7-----------------------#2022-03-04</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><p id="23ea" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">Python 中 PySpark 的基本代码</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/37a5c9ef9b1165ff79cc02878886aa1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ZGUImwZaW9ExiGq5"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@jstrippa?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">詹姆斯·哈里逊</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="9310" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">如果你是初学者，并试图选择 PySpark，并有困难开始，那么这个博客希望你一些麻烦，我忍受了。</p><h1 id="bdb1" class="kz la in bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">为什么要看这个博客？</h1><p id="59f1" class="pw-post-body-paragraph jk jl in jm b jn lx jp jq jr ly jt ju jv lz jx jy jz ma kb kc kd mb kf kg kh ig bi translated">在这篇博客中，我会尽量让事情变得简单，目标是让你尽可能快地编写 Spark 代码。如果你已经很好地掌握了某个特定的主题，你可以随意地跳来跳去。</p><p id="3682" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我将带您使用 Jupyter notebook 在本地机器上运行 Spark 的工作版本。</p><h1 id="f006" class="kz la in bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">你会学到什么</h1><ul class=""><li id="d6e2" class="mc md in jm b jn lx jr ly jv me jz mf kd mg kh mh mi mj mk bi translated">让 PySpark 在您的计算机上运行的相对简单的方法</li><li id="b930" class="mc md in jm b jn ml jr mm jv mn jz mo kd mp kh mh mi mj mk bi translated">我们将熟悉 PySpark 的基本功能，尤其是数据操作部分。在这里，我们将学习如何加载数据、浏览数据、处理缺失值、执行过滤操作以及对不同的数据组应用聚合。</li><li id="8935" class="mc md in jm b jn ml jr mm jv mn jz mo kd mp kh mh mi mj mk bi translated">在 PySpark 中构建和评估基本线性回归模型</li></ul><p id="373c" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">让我们开始吧:</p><h1 id="ab1b" class="kz la in bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">什么是火花</h1><p id="3797" class="pw-post-body-paragraph jk jl in jm b jn lx jp jq jr ly jt ju jv lz jx jy jz ma kb kc kd mb kf kg kh ig bi translated">Apache Spark 是一个快速而强大的框架，它提供了一个 API 来对弹性数据集执行大规模分布式处理，简而言之就是<strong class="jm io">分布式数据/分布式计算，</strong>当数据集变得太大或新数据流太快时，单台计算机可能难以处理。这就是分布式计算出现的地方，它支持各种语言，如 Scala、Python、Java 和 r。</p><p id="5445" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">在大数据时代，PySpark 被 Python 用户广泛用于对大型数据集执行操作。</p><h2 id="19ef" class="mq la in bd lb mr ms dn lf mt mu dp lj jv mv mw ln jz mx my lr kd mz na lv nb bi translated">PySpark 安装和设置</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/ac0850965ca3c3043cfee26162005fa8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FULQO_m0VobO8rrN"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@sigmund?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">西格蒙德</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><h2 id="e9da" class="mq la in bd lb mr ms dn lf mt mu dp lj jv mv mw ln jz mx my lr kd mz na lv nb bi translated">1.安装 Java</h2><p id="0c93" class="pw-post-body-paragraph jk jl in jm b jn lx jp jq jr ly jt ju jv lz jx jy jz ma kb kc kd mb kf kg kh ig bi translated">在开始使用 spark 之前，您需要确保您已经安装了 java(版本应该至少是 java8 或以上)。去<a class="ae ky" href="https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" rel="noopener ugc nofollow" target="_blank"> Java 的官方</a>下载网站，接受 Oracle 许可，下载 Java JDK 8，哪个适合你的系统。</p><p id="15be" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">一旦安装了 Java，我们需要在高级系统设置中设置环境变量</p><p id="10c1" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">2.下载并安装 Spark</p><p id="d213" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">进入<a class="ae ky" href="https://spark.apache.org/downloads.html" rel="noopener ugc nofollow" target="_blank"> Spark 主页</a>，下载。tgz 文件</p><p id="a707" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">将文件解压到您选择的目录中(7z 可以打开 tgz ),并在安装后像我们对 Java 一样设置环境变量。</p><h2 id="4287" class="mq la in bd lb mr ms dn lf mt mu dp lj jv mv mw ln jz mx my lr kd mz na lv nb bi translated">3.火花:更多的东西(winutils)</h2><ul class=""><li id="f5b1" class="mc md in jm b jn lx jr ly jv me jz mf kd mg kh mh mi mj mk bi translated">从这里下载 winutils.exe:<a class="ae ky" href="https://github.com/steveloughran/winutils" rel="noopener ugc nofollow" target="_blank">https://github.com/steveloughran/winutils</a></li></ul><h2 id="fb12" class="mq la in bd lb mr ms dn lf mt mu dp lj jv mv mw ln jz mx my lr kd mz na lv nb bi translated">4.安装 Anaconda 框架</h2><p id="7d61" class="pw-post-body-paragraph jk jl in jm b jn lx jp jq jr ly jt ju jv lz jx jy jz ma kb kc kd mb kf kg kh ig bi translated">我们需要在我们的系统中安装<a class="ae ky" href="https://www.anaconda.com/products/individual" rel="noopener ugc nofollow" target="_blank"> Anaconda 框架</a>。我们也可以在 Google colab 上运行 PySpark 代码。</p><p id="0e17" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">下面是安装指南链接:<a class="ae ky" href="https://inblog.in/Install-Spark-PySpark-to-run-in-Jupyter-Notebook-on-Windows-p2eZ2qQPmO" rel="noopener ugc nofollow" target="_blank">链接</a></p><h1 id="7753" class="kz la in bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">PySpark 中的编码</h1><p id="3ab9" class="pw-post-body-paragraph jk jl in jm b jn lx jp jq jr ly jt ju jv lz jx jy jz ma kb kc kd mb kf kg kh ig bi translated">让我们从在 PySpark 中导入数据集开始，你可以从这个<a class="ae ky" href="https://github.com/techpathak/PY-Spark-data" rel="noopener ugc nofollow" target="_blank"> <strong class="jm io">链接</strong> </a>访问数据集。一旦你下载了数据集，我们需要导入必要的库，导入后，我们将需要创建一个 Spark 会话。一旦创建了 Spark 会话，就可以访问 Spark web 用户界面(Web UI)。</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="66f0" class="mq la in ne b gy ni nj l nk nl">`# importing findspark <br/>import findspark<br/>findspark.init()  # init the spark</span><span id="a46f" class="mq la in ne b gy nm nj l nk nl">import pyspark<br/>findspark.find()</span><span id="a265" class="mq la in ne b gy nm nj l nk nl">from pyspark.sql import SparkSession <br/>#The entry point to programming Spark with the Dataset and DataFrame API</span></pre><p id="6af8" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">现在，我正在使用下面定义的“教程”创建应用程序名称，该名称将作为应用程序名称显示在 Web UI 的右上角。在这篇文章中，我们不会使用 Web UI，但是，如果你有兴趣了解更多，请查看官方文档。</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="0b36" class="mq la in ne b gy ni nj l nk nl">from pyspark.sql import SparkSession <br/>#The entry point to programming Spark with the Dataset and DataFrame API</span><span id="e79b" class="mq la in ne b gy nm nj l nk nl">spark = SparkSession.builder.appName('tutorial').getOrCreate()<br/>spark</span><span id="6783" class="mq la in ne b gy nm nj l nk nl"># To create a SparkSession, use the following builder pattern appName(name)[source]<br/># Sets a name for the application, which will be shown in the Spark web UI.<br/># If no application name is set, a randomly generated name will be used<br/># getOrCreate()[source] Gets an existing SparkSession or, if there is no existing one, creates a new one based on the options set in this builder.<br/># builder<!-- --> — gives access to <a class="ae ky" href="https://spark.apache.org/docs/2.2.0/api/java/index.html?org/apache/spark/sql/SparkSession.Builder.html" rel="noopener ugc nofollow" target="_blank">Builder API </a>which is used to configure the session</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/41db835586fe4085c1921cb402cef004.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*z_x72j-mGt1RX-dNlnGbOg.png"/></div></figure><p id="9e39" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">将数据集加载到 PySpark。</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="1ffe" class="mq la in ne b gy ni nj l nk nl">df = spark.read.json('people.json') # similarly we can read other flat files like (.csv, excel,etc..)<br/>df.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/cc962cd4ebbe1163d173fab754b4246a.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*BoSXKQhWANJmakfZxv3YVQ.png"/></div></figure><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="f4af" class="mq la in ne b gy ni nj l nk nl">df.printSchema()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/00666b7f240bb1d2ad6ee5b92aaf6490.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*6gRKukamnoHmT1NyV9LTtA.png"/></div></figure><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="f44e" class="mq la in ne b gy ni nj l nk nl">df.columns</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/63f0685d15f7de6a1f0d19ae28bd4c74.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*XPd9foecJwMRtoPhdCquGA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">输出</figcaption></figure><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="e5cd" class="mq la in ne b gy ni nj l nk nl">df.describe().show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/4110c680ece365cc4239f8f70bcafeac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*6Bp9OY5JzOsjwdCiUqbNxQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">输出</figcaption></figure><h1 id="dd04" class="kz la in bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">PySpark 结构类型&amp;结构字段</h1><p id="80bb" class="pw-post-body-paragraph jk jl in jm b jn lx jp jq jr ly jt ju jv lz jx jy jz ma kb kc kd mb kf kg kh ig bi translated">PySpark StructType &amp; StructField 类用于以编程方式指定数据帧的模式，并创建复杂的列，如嵌套结构、数组和映射列。<a class="ae ky" href="https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/types/StructType.scala" rel="noopener ugc nofollow" target="_blank"> StructType </a>是<a class="ae ky" href="https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/types/StructField.scala" rel="noopener ugc nofollow" target="_blank"> StructField 的</a>的集合，它定义了列名、列数据类型、用于指定该字段是否可为空的布尔值以及元数据。</p><p id="da6c" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">通过运行以下代码来导入结构类型和结构字段，以更改变量的数据类型:</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="6eec" class="mq la in ne b gy ni nj l nk nl">from pyspark.sql.types import (StructField,StringType,IntegerType,StructType)</span><span id="fdb2" class="mq la in ne b gy nm nj l nk nl">data_schema = [StructField('age',IntegerType(),True),<br/>               StructField('name',StringType(),True)]<br/>final_Struct = StructType(fields=data_schema)<br/>df.printSchema()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/e0fc9326d508b2abd3cf3eaa58003416.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*iR6mUUoypz3WX48cDYmL2A.png"/></div></figure><p id="820f" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">年龄的数据类型现在转换为整数，名称转换为字符串。</p><p id="e7dc" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">选择变量:</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="e389" class="mq la in ne b gy ni nj l nk nl">df.select('age').show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/0c0bb21f92cb0be329792aac24ff6602.png" data-original-src="https://miro.medium.com/v2/resize:fit:378/format:webp/1*IbKr3ht8vNLeuehu9-_oDg.png"/></div></figure><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="f197" class="mq la in ne b gy ni nj l nk nl">df.head(3)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/a5f8046ae6f44393eac745f38ebfc228.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*lPqik9Kj3Zu-DsimBHjHsw.png"/></div></figure><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="f0e2" class="mq la in ne b gy ni nj l nk nl"># Extract second row of this dataset<br/>df.head(3)[2]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/56b1bc3bce2b94ef51063f284648e5a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*9ElIBqUuXS93ybKFgxezhA.png"/></div></figure><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="7632" class="mq la in ne b gy ni nj l nk nl">Extract Top 2 row</span><span id="5cff" class="mq la in ne b gy nm nj l nk nl">df.select(['age','name']).show(2)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/b10f0bcb29ba0df72f91fb76827365e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*9l9GNZk3n5j_P7aym7lWKw.png"/></div></div></figure><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="031f" class="mq la in ne b gy ni nj l nk nl">Extract all rows</span><span id="8bc9" class="mq la in ne b gy nm nj l nk nl">df.select(['age','name']).show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/34c02bc7df3a1f02e2dedaccae31eee5.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/format:webp/1*-RAQdaoZm-etMXIRK9ugbA.png"/></div></figure><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="8037" class="mq la in ne b gy ni nj l nk nl"># create new columns <br/># Approach - 1 : using withColumn function<br/>df.withColumn("double_2", df.age * 2).show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/426d09d87eaf5964b054201f5bcf46d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*eISj0svQD1pE3FYJktgvqg.png"/></div></figure><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="7361" class="mq la in ne b gy ni nj l nk nl">Approach - 2 : using select with alias function.<br/>df.select("*", (df.age * 3).alias("new_age")).show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/734e35fc511580f3f6ea69818d2832d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*b3DTBP0f4D6JxDLalAjylQ.png"/></div></figure><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="3714" class="mq la in ne b gy ni nj l nk nl"># Approach - 3 : Using as clause in SQL statement.<br/>df.createTempView("temp1")<br/>spark.sql("select *, age * 2 as new_age from temp").show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/544ddb939508d0fd22024c3446db8eca.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*lb5lrc7hvZwC4j5hOhVKXQ.png"/></div></figure><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="66a4" class="mq la in ne b gy ni nj l nk nl"># Drop any row that contains missing data<br/>df.na.drop().show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/006b8f33aa6f3dc9445c88a39cf88d29.png" data-original-src="https://miro.medium.com/v2/resize:fit:356/format:webp/1*odlldmOaXn3SQq3Izih8uw.png"/></div></figure><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="843f" class="mq la in ne b gy ni nj l nk nl">df.na.fill(0).show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/4f034c3497b3e09650303d0abc32211a.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/1*KFh1s-iwEpw6oHUI8Kde8g.png"/></div></figure><h1 id="a1aa" class="kz la in bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">py spark . SQL . data frame . createorreplacetenview</h1><p id="51b8" class="pw-post-body-paragraph jk jl in jm b jn lx jp jq jr ly jt ju jv lz jx jy jz ma kb kc kd mb kf kg kh ig bi translated">用数据框创建或替换本地临时视图，我们可以运行 SQL 查询来提取信息。</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="1da4" class="mq la in ne b gy ni nj l nk nl">df.createOrReplaceTempView('temp1')temp1</span><span id="214b" class="mq la in ne b gy nm nj l nk nl">#if view is already created then it will replace and create new view with name temp1</span><span id="cc17" class="mq la in ne b gy nm nj l nk nl">spark.sql('select * from temp1').show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/c157dd78ffae34d417504caacdbfa781.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*-AHIxCrS9DGOUK73vfLE-A.png"/></div></figure><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="9d83" class="mq la in ne b gy ni nj l nk nl"># Filtering the variables using SQL</span><span id="7fbc" class="mq la in ne b gy nm nj l nk nl">spark.sql('select * from temp1 where age&lt;20').show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/c2403c806f750385693cc262f8bc212e.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*ILTfj6lUpKO_wjD-t_8Q7Q.png"/></div></figure><h1 id="2da5" class="kz la in bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">从头开始创建数据框架:</h1><p id="fa27" class="pw-post-body-paragraph jk jl in jm b jn lx jp jq jr ly jt ju jv lz jx jy jz ma kb kc kd mb kf kg kh ig bi translated">我们可以使用<code class="fe of og oh ne b">toDF()</code>和<code class="fe of og oh ne b">createDataFrame()</code>方法手动创建 PySpark 数据帧。</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="ba30" class="mq la in ne b gy ni nj l nk nl">columns = ["language","users_count"]<br/>data = [("Java", "20000"), ("Python", "100000"), ("Scala", "3000")]</span><span id="3e85" class="mq la in ne b gy nm nj l nk nl">rdd = spark.sparkContext.parallelize(data)</span><span id="f40b" class="mq la in ne b gy nm nj l nk nl">dfFromRDD_1 = rdd.toDF()</span><span id="0559" class="mq la in ne b gy nm nj l nk nl">dfFromRDD_1.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/f8e30aa74887158d32c4dea2a3c6cf63.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/format:webp/1*Q55b05cxVdJFUxGwJYT28w.png"/></div></figure><p id="de62" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">如果我们想为数据帧提供列名:</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="35ab" class="mq la in ne b gy ni nj l nk nl">dfFromRDD = spark.createDataFrame(rdd).toDF(*columns)</span><span id="8538" class="mq la in ne b gy nm nj l nk nl">dfFromRDD.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/15b79a3eca5038404765336589e35f09.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*wfWp76DEHYGwKaadDJ4fjw.png"/></div></div></figure><h1 id="a1a1" class="kz la in bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">结论</h1><p id="7a9b" class="pw-post-body-paragraph jk jl in jm b jn lx jp jq jr ly jt ju jv lz jx jy jz ma kb kc kd mb kf kg kh ig bi translated">在本教程中，我们介绍了如何在本地系统中安装 Py-spark，以及如何在 Py-Spark 中执行基本操作，如读取平面文件，如何更改现有数据帧中任何变量的数据类型，如何在某些条件下选择不同的变量，我们还从头开始创建了视图和数据帧。</p><p id="9b67" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">希望您觉得这些代码有用，并了解了一点 PySpark 语法。在下一篇教程中，我将讨论 Py-Spark 和 MLlib 的一些其他功能。</p><p id="98d6" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我希望这篇文章能帮助你并节省大量的时间。如果你有任何建议，请告诉我。</p><p id="0bb6" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">快乐编码。</p><p id="0bf0" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io"><em class="ok">Prabhat Pathak</em></strong><em class="ok">(</em><a class="ae ky" href="https://www.linkedin.com/in/prabhat-pathak-029b6466/" rel="noopener ugc nofollow" target="_blank"><em class="ok">Linkedin 简介</em> </a> <em class="ok">)是数据科学顾问。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7a0fea165e405d53bb24bf4be0c13e5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jSmswrt-x9rJ1kns"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@bhushan07?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Bhushan Sadani </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure></div></div>    
</body>
</html>