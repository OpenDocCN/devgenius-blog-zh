<html>
<head>
<title>Introduction to Machine Learning: Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习导论:逻辑回归</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/introduction-to-machine-learning-logistic-regression-ab19c49247c0?source=collection_archive---------21-----------------------#2022-08-29">https://blog.devgenius.io/introduction-to-machine-learning-logistic-regression-ab19c49247c0?source=collection_archive---------21-----------------------#2022-08-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="5fa9" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">机器学习</h2><div class=""/><div class=""><h2 id="25f1" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">平！这又是一封垃圾邮件吗？还是合法的？你的垃圾邮件过滤器如何辨别？</h2></div><p id="19a5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">平！这又是一封垃圾邮件吗？还是合法的？你的垃圾邮件过滤器如何辨别？也许它使用了一种简单的机器学习技术。在这篇文章中，我们将了解它是什么，以及我们如何创建一个 Python 逻辑回归程序。</p><p id="cece" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们将涵盖:</p><ul class=""><li id="5b68" class="lk ll iq kq b kr ks ku kv kx lm lb ln lf lo lj lp lq lr ls bi translated">机器学习:什么是逻辑回归？</li><li id="72e4" class="lk ll iq kq b kr lt ku lu kx lv lb lw lf lx lj lp lq lr ls bi translated">随机数据集上的 Python 逻辑回归</li><li id="c7aa" class="lk ll iq kq b kr lt ku lu kx lv lb lw lf lx lj lp lq lr ls bi translated">用 Python 实现 Iris 数据集逻辑回归</li></ul><h1 id="e216" class="ly lz iq bd ma mb mc md me mf mg mh mi kf mj kg mk ki ml kj mm kl mn km mo mp bi translated">机器学习:什么是逻辑回归？</h1><p id="658f" class="pw-post-body-paragraph ko kp iq kq b kr mq ka kt ku mr kd kw kx ms kz la lb mt ld le lf mu lh li lj ij bi translated">与<a class="ae mv" href="https://pythonalgos.com/introduction-to-machine-learning-linear-regression/" rel="noopener ugc nofollow" target="_blank">线性回归</a>不同，逻辑回归用于分类，而不是沿着连续范围进行预测。逻辑回归的秘诀是一个“激活函数”,它对独立变量进行评分，如果所得分数低于阈值，则返回一个<code class="fe mw mx my mz b">0</code>,如果所得分数高于阈值，则返回<code class="fe mw mx my mz b">1</code>。它可以用于各种二进制分类问题，如预测患者是否患有癌症或电子邮件是否是垃圾邮件。</p><p id="78f6" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">乙状结肠激活函数是</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi na"><img src="../Images/3b8257e906a4fd334500bb6bae2578e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:70/0*pQgSnlnoCIjTY0j_"/></div></figure><p id="61fe" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">看起来像</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/71f85e17c1f24c95bca7cef422590f20.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/0*kYC-AHeOQGdgLi_S"/></div></figure><p id="c9f0" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在本模块中，我们将回顾一个简单的逻辑回归模型，并使用来自<a class="ae mv" href="https://scikit-learn.org/stable/index.html" rel="noopener ugc nofollow" target="_blank"> sklearn </a>的 Iris 数据集进行多项式逻辑回归和训练/测试分割。<br/>我们将从导入我们需要的库开始。这些是我们在上一篇文章<a class="ae mv" href="https://pythonalgos.com/2021/11/04/introduction-to-machine-learning-linear-regression/" rel="noopener ugc nofollow" target="_blank">机器学习简介:线性回归</a>中安装的相同的库。</p><pre class="nb nc nd ne gt nj mz nk nl aw nm bi"><span id="bfbd" class="nn lz iq mz b gy no np l nq nr">imports for logistic regression with sklearn</span><span id="7cf0" class="nn lz iq mz b gy ns np l nq nr">import numpy as np<br/>import math<br/>from sklearn.linear_model import LogisticRegression<br/>import matplotlib.pyplot as plt<br/>import random</span></pre><h1 id="a48c" class="ly lz iq bd ma mb mc md me mf mg mh mi kf mj kg mk ki ml kj mm kl mn km mo mp bi translated">随机数据集上的 Python 逻辑回归</h1><p id="8e66" class="pw-post-body-paragraph ko kp iq kq b kr mq ka kt ku mr kd kw kx ms kz la lb mt ld le lf mu lh li lj ij bi translated">让我们创建一个测试数据集。我们将基于上面的图像创建一个数据集，记住，基于阈值<code class="fe mw mx my mz b">(0.5)</code>，所有负数(和<code class="fe mw mx my mz b">0</code>)将被分类为<code class="fe mw mx my mz b">0</code>，正数将被分类为<code class="fe mw mx my mz b">1</code>。</p><pre class="nb nc nd ne gt nj mz nk nl aw nm bi"><span id="d03c" class="nn lz iq mz b gy no np l nq nr">create data points</span><span id="46d4" class="nn lz iq mz b gy ns np l nq nr">x = np.array([-3, -2, -1, 0, 1, 2, 3]).reshape(-1, 1)<br/>y = np.array([0, 0, 0, 0, 1, 1, 1])</span></pre><p id="7246" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，我们所要做的就是从 sklearn 库中创建我们的逻辑回归模型，并运行一个测试来看看它是如何工作的。</p><pre class="nb nc nd ne gt nj mz nk nl aw nm bi"><span id="d377" class="nn lz iq mz b gy no np l nq nr">using sklearn's logisticregression function</span><span id="20e9" class="nn lz iq mz b gy ns np l nq nr"># fit the model<br/>model = LogisticRegression().fit(x, y)</span><span id="439b" class="nn lz iq mz b gy ns np l nq nr"># test data<br/>new_x = np.array([-13, -0.5, 1, 0.3, -5, 11, 12]).reshape(-1, 1)<br/>new_y = np.array([0, 0, 1, 1, 0, 1, 1])</span><span id="6250" class="nn lz iq mz b gy ns np l nq nr"># predict<br/>model.predict(new_x)</span><span id="5ea4" class="nn lz iq mz b gy ns np l nq nr"># expected output<br/>array([0, 0, 1, 0, 0, 1, 1])</span><span id="e546" class="nn lz iq mz b gy ns np l nq nr"># score the model<br/>model.score(new_x, new_y)</span><span id="4136" class="nn lz iq mz b gy ns np l nq nr"># expected output (this is equal to 6/7 points classified correctly)<br/>0.8571428571428571</span></pre><p id="ca34" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们的模型在这个数据集上做得不太好，但这不是模型的错，我们只是给了它<code class="fe mw mx my mz b">7</code>个点来处理。它对<code class="fe mw mx my mz b">6/7</code>进行了正确的分类，我们可以看到在<code class="fe mw mx my mz b">0</code>周围有一个误差范围，因为<code class="fe mw mx my mz b">0.3</code>被错误地分类了。下面，我们将用蓝色标出预期点，用红色标出预测点，以及<a class="ae mv" href="https://en.wikipedia.org/wiki/Softmax_function" rel="noopener ugc nofollow" target="_blank"> softmax 函数</a>。</p><pre class="nb nc nd ne gt nj mz nk nl aw nm bi"><span id="ea91" class="nn lz iq mz b gy no np l nq nr">scatter plot the results with matplotlib</span><span id="ff83" class="nn lz iq mz b gy ns np l nq nr">plt.scatter(new_x, new_y, color="blue", alpha=0.5)<br/>plt.scatter(new_x, model.predict(new_x), color="red", alpha=0.5)<br/>logx = np.linspace(-13, 12, 100)<br/>logy = 1/(1+math.e**-logx)<br/>plt.plot(logx, logy)</span></pre><p id="ccaa" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">预期的输出是:(这是我在上面演示逻辑曲线时使用的同一张图片)</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/d7a1066e5564bf9f9e01dfd9a6f383d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/0*V5PcuBvhIS53x-uH"/></div></figure><h1 id="cde7" class="ly lz iq bd ma mb mc md me mf mg mh mi kf mj kg mk ki ml kj mm kl mn km mo mp bi translated">用 Python 实现 Iris 数据集逻辑回归</h1><p id="293c" class="pw-post-body-paragraph ko kp iq kq b kr mq ka kt ku mr kd kw kx ms kz la lb mt ld le lf mu lh li lj ij bi translated">在我们的下一个例子中，我们将从 sklearn 的 Iris 数据集导入数据，对其进行逻辑回归。在我们加载数据集之后，我们需要在对其进行回归之前检查数据的外观。让我们加载<code class="fe mw mx my mz b">y</code>值，以及第一个和最后一个<code class="fe mw mx my mz b">10</code> <code class="fe mw mx my mz b">X</code>值。</p><pre class="nb nc nd ne gt nj mz nk nl aw nm bi"><span id="dfe2" class="nn lz iq mz b gy no np l nq nr">load the iris dataset from sklearn</span><span id="f0d6" class="nn lz iq mz b gy ns np l nq nr">from sklearn.datasets import load_iris<br/>X, y = load_iris(return_X_y = True)<br/>print(y)<br/>print(X[0:10])<br/>print("...")<br/>print(X[-10:])</span><span id="0ee4" class="nn lz iq mz b gy ns np l nq nr"># expected output<br/>[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0<br/> 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1<br/> 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2<br/> 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2<br/> 2 2]<br/>[[5.1 3.5 1.4 0.2]<br/> [4.9 3.  1.4 0.2]<br/> [4.7 3.2 1.3 0.2]<br/> [4.6 3.1 1.5 0.2]<br/> [5.  3.6 1.4 0.2]<br/> [5.4 3.9 1.7 0.4]<br/> [4.6 3.4 1.4 0.3]<br/> [5.  3.4 1.5 0.2]<br/> [4.4 2.9 1.4 0.2]<br/> [4.9 3.1 1.5 0.1]]<br/>...<br/>[[6.7 3.1 5.6 2.4]<br/> [6.9 3.1 5.1 2.3]<br/> [5.8 2.7 5.1 1.9]<br/> [6.8 3.2 5.9 2.3]<br/> [6.7 3.3 5.7 2.5]<br/> [6.7 3.  5.2 2.3]<br/> [6.3 2.5 5.  1.9]<br/> [6.5 3.  5.2 2. ]<br/> [6.2 3.4 5.4 2.3]<br/> [5.9 3.  5.1 1.8]]</span></pre><p id="5107" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们看到我们的 y 值是离散的，所以我们可以对它们进行逻辑回归。我们还可以看到，我们有<code class="fe mw mx my mz b">3</code>类，而不是通常的<code class="fe mw mx my mz b">2</code>类，逻辑回归就是用来做这个的。幸运的是，sklearn 的逻辑回归软件包提供了进行多类逻辑回归的选项。随着我们的训练测试分割，我们还将把<code class="fe mw mx my mz b">X</code>缩放到标准缩放器，以确保模型收敛。标准定标器的平均值为<code class="fe mw mx my mz b">0</code>，标准偏差为<code class="fe mw mx my mz b">1</code>。</p><pre class="nb nc nd ne gt nj mz nk nl aw nm bi"><span id="b300" class="nn lz iq mz b gy no np l nq nr">train_test_split for logisticregression</span><span id="cc01" class="nn lz iq mz b gy ns np l nq nr"># imports for preprocessing<br/>from sklearn import preprocessing<br/>from sklearn.model_selection import train_test_split</span><span id="9b61" class="nn lz iq mz b gy ns np l nq nr"># create normalized x values<br/>xscaler = preprocessing.StandardScaler().fit(X)<br/>xscaled = xscaler.transform(X)</span><span id="2b0e" class="nn lz iq mz b gy ns np l nq nr"># create training/test datasets<br/>x_train, x_test, y_train, y_test = train_test_split(xscaled, y, test_size=0.2, random_state=1)</span><span id="c245" class="nn lz iq mz b gy ns np l nq nr"># create model<br/>model = LogisticRegression(multi_class="multinomial", random_state=1).fit(x_train, y_train)</span><span id="3e85" class="nn lz iq mz b gy ns np l nq nr"># test model<br/>model.score(x_test, y_test)</span><span id="5b99" class="nn lz iq mz b gy ns np l nq nr"># expected output<br/>0.9666666666666667</span><span id="acd7" class="nn lz iq mz b gy ns np l nq nr"># let's take a look at the predictions<br/>print(y_test)<br/>print(model.predict(x_test))</span><span id="3975" class="nn lz iq mz b gy ns np l nq nr"># expected output<br/>[0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2]<br/>[0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 2 0 2 1 0 0 1 2]</span></pre><p id="7ac3" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">很好，我们的模特做得很好。在<code class="fe mw mx my mz b">30</code>预测中，它只错过了<code class="fe mw mx my mz b">1</code>。不幸的是，由于我们的<code class="fe mw mx my mz b">X</code>变量具有<code class="fe mw mx my mz b">4</code>维度，我们将无法绘制它。</p><h1 id="b28e" class="ly lz iq bd ma mb mc md me mf mg mh mi kf mj kg mk ki ml kj mm kl mn km mo mp bi translated">进一步阅读</h1><ul class=""><li id="3647" class="lk ll iq kq b kr mq ku mr kx nt lb nu lf nv lj lp lq lr ls bi translated"><a class="ae mv" href="https://pythonalgos.com/long-short-term-memory-lstm-in-keras/" rel="noopener ugc nofollow" target="_blank">长短期记忆(LSTM)在 Keras </a></li><li id="0cce" class="lk ll iq kq b kr lt ku lu kx lv lb lw lf lx lj lp lq lr ls bi translated"><a class="ae mv" href="https://pythonalgos.com/the-best-way-to-do-named-entity-recognition-ner/" rel="noopener ugc nofollow" target="_blank">识别命名实体的最佳方式</a></li><li id="8138" class="lk ll iq kq b kr lt ku lu kx lv lb lw lf lx lj lp lq lr ls bi translated">建立你自己的人工智能文本摘要器</li></ul><p id="1f46" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如果你喜欢这篇文章，请在 Twitter 上分享！为了无限制地访问媒体文章，今天就注册成为<a class="ae mv" href="https://www.medium.com/@ytang07/membership" rel="noopener">媒体会员</a>！别忘了关注我，<a class="ae mv" href="https://www.medium.com/@ytang07" rel="noopener">唐</a>，获取更多关于增长、技术等方面的文章！</p></div></div>    
</body>
</html>