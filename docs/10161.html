<html>
<head>
<title>Introduction to PyTorch Audio Data via TorchAudio</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过 Torch Audio 介绍 PyTorch 音频数据</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/introduction-to-pytorch-audio-data-via-torchaudio-41dbea208d28?source=collection_archive---------5-----------------------#2022-10-11">https://blog.devgenius.io/introduction-to-pytorch-audio-data-via-torchaudio-41dbea208d28?source=collection_archive---------5-----------------------#2022-10-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="128b" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">自然语言处理</h2><div class=""/><div class=""><h2 id="ce11" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">如何使用 TorchAudio 处理音频数据</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/ad28f7e03be8c475a7e0457a240e54c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uNWeLpPX6NrpY7NA4C-vOw.png"/></div></div></figure><p id="474e" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">PyTorch 是 Python 中领先的机器学习框架之一。最近，PyTorch 发布了他们处理音频数据的框架的更新版本，<a class="ae lw" href="https://github.com/pytorch/audio" rel="noopener ugc nofollow" target="_blank"> <strong class="lc ja"> TorchAudio </strong> </a>。TorchAudio 支持的不仅仅是使用音频数据进行机器学习。它还支持将音频数据用于机器学习模型所需的数据转换、扩充和特征提取。</p><p id="df77" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在本帖中，我们将讨论:</p><ul class=""><li id="9654" class="lx ly iq lc b ld le lg lh lj lz ln ma lr mb lv mc md me mf bi translated"><strong class="lc ja">为音频数据增强设置 py torch torch Audio</strong></li><li id="2b45" class="lx ly iq lc b ld mg lg mh lj mi ln mj lr mk lv mc md me mf bi translated"><strong class="lc ja">使用 PyTorch TorchAudio 添加音频数据增强效果</strong></li><li id="b3ab" class="lx ly iq lc b ld mg lg mh lj mi ln mj lr mk lv mc md me mf bi translated"><strong class="lc ja">在 TorchAudio 中使用音效</strong></li><li id="8428" class="lx ly iq lc b ld mg lg mh lj mi ln mj lr mk lv mc md me mf bi translated"><strong class="lc ja">添加背景噪音</strong></li><li id="8898" class="lx ly iq lc b ld mg lg mh lj mi ln mj lr mk lv mc md me mf bi translated"><strong class="lc ja">添加房间混响</strong></li><li id="fb8f" class="lx ly iq lc b ld mg lg mh lj mi ln mj lr mk lv mc md me mf bi translated"><strong class="lc ja">使用 TorchAudio 对音频数据进行高级重采样</strong></li><li id="6a94" class="lx ly iq lc b ld mg lg mh lj mi ln mj lr mk lv mc md me mf bi translated"><strong class="lc ja">用 PyTorch TorchAudio 提取音频特征</strong></li><li id="d280" class="lx ly iq lc b ld mg lg mh lj mi ln mj lr mk lv mc md me mf bi translated"><strong class="lc ja">总之</strong></li></ul><h1 id="8452" class="ml mm iq bd mn mo mp mq mr ms mt mu mv kf mw kg mx ki my kj mz kl na km nb nc bi translated">为音频数据增强设置 PyTorch TorchAudio</h1><p id="04f3" class="pw-post-body-paragraph la lb iq lc b ld nd ka lf lg ne kd li lj nf ll lm ln ng lp lq lr nh lt lu lv ij bi translated">在撰写本文时，<code class="fe ni nj nk nl b">torchaudio</code>在版本<code class="fe ni nj nk nl b">0.11.0</code>上，并且只适用于 Python 版本 3.6 到 3.9。对于这个例子，我们将使用 Python 3.9。在开始之前，我们还需要安装一些库。我们需要的第一个库是 PyTorch 的<code class="fe ni nj nk nl b">torch</code>和<code class="fe ni nj nk nl b">torchaudio</code>。我们将使用<code class="fe ni nj nk nl b">matplotlib</code>来绘制我们的视觉表示，<code class="fe ni nj nk nl b">requests</code>来获取数据，<code class="fe ni nj nk nl b">librosa</code>来对光谱图进行更多的视觉操作。</p><p id="a728" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">首先，我们将把所有这些安装到一个新的虚拟环境中。<a class="ae lw" href="https://developers.deepgram.com/blog/2022/02/python-virtual-environments/" rel="noopener ugc nofollow" target="_blank"> <strong class="lc ja">启动虚拟环境</strong> </a>运行<code class="fe ni nj nk nl b">python3 -m venv &lt;new environment name&gt;</code>。然后运行<code class="fe ni nj nk nl b">pip install torch torchaudio matplotlib requests librosa</code>，让<code class="fe ni nj nk nl b">pip</code>安装本教程所需的所有库。</p><h1 id="a084" class="ml mm iq bd mn mo mp mq mr ms mt mu mv kf mw kg mx ki my kj mz kl na km nb nc bi translated">使用 PyTorch TorchAudio 添加音频数据增强效果</h1><p id="0d31" class="pw-post-body-paragraph la lb iq lc b ld nd ka lf lg ne kd li lj nf ll lm ln ng lp lq lr nh lt lu lv ij bi translated">最近，我们讲述了<a class="ae lw" href="https://developers.deepgram.com/blog/2022/06/best-python-audio-manipulation-tools/" rel="noopener ugc nofollow" target="_blank"> <strong class="lc ja">如何在 Python </strong> </a>中操作音频数据的基础知识。在这一节中，我们将介绍如何将音效选项传递给 TorchAudio 的基础知识。然后，我们将详细介绍如何在不同的声音级别添加背景噪音，以及如何添加房间混响。</p><p id="4e74" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在我们开始之前，我们必须设置一些东西。这段代码完全是辅助代码，你可以<a class="ae lw" href="https://developers.deepgram.com/blog/2022/06/pytorch-intro-with-torchaudio/#using-sound-effects-in-torchaudio" rel="noopener ugc nofollow" target="_blank"> <strong class="lc ja">跳过</strong> </a>。如果您想继续对所提供的数据进行测试，那么理解这些代码会很有帮助。</p><p id="183b" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在下面的代码块中，我们首先导入我们需要的所有库。然后，我们定义存储音频数据的 URL 和存储音频的本地路径。接下来，我们获取数据并定义一些辅助函数。</p><p id="6c06" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">对于这个例子，我们将定义函数来获取噪声、语音和混响样本。我们还将定义函数来绘制我们正在处理的声音的波形、声谱图和<code class="fe ni nj nk nl b">numpy</code>表示。</p><pre class="kp kq kr ks gt nm nl nn no aw np bi"><span id="9da8" class="nq mm iq nl b gy nr ns l nt nu">import math<br/>import os</span><span id="f8ad" class="nq mm iq nl b gy nv ns l nt nu">import matplotlib.pyplot as plt<br/>import requests<br/>import torchaudio<br/>import torch</span><span id="d803" class="nq mm iq nl b gy nv ns l nt nu">_SAMPLE_DIR = "_assets"<br/>SAMPLE_WAV_URL = "https://pytorch-tutorial-assets.s3.amazonaws.com/steam-train-whistle-daniel_simon.wav"<br/>SAMPLE_WAV_PATH = os.path.join(_SAMPLE_DIR, "steam.wav")</span><span id="163e" class="nq mm iq nl b gy nv ns l nt nu">SAMPLE_RIR_URL = "https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/distant-16k/room-response/rm1/impulse/Lab41-SRI-VOiCES-rm1-impulse-mc01-stu-clo.wav"  # noqa: E501<br/>SAMPLE_RIR_PATH = os.path.join(_SAMPLE_DIR, "rir.wav")</span><span id="553b" class="nq mm iq nl b gy nv ns l nt nu">SAMPLE_WAV_SPEECH_URL = "https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav"  # noqa: E501<br/>SAMPLE_WAV_SPEECH_PATH = os.path.join(_SAMPLE_DIR, "speech.wav")</span><span id="b09d" class="nq mm iq nl b gy nv ns l nt nu">SAMPLE_NOISE_URL = "https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/distant-16k/distractors/rm1/babb/Lab41-SRI-VOiCES-rm1-babb-mc01-stu-clo.wav"  # noqa: E501<br/>SAMPLE_NOISE_PATH = os.path.join(_SAMPLE_DIR, "bg.wav")</span><span id="e608" class="nq mm iq nl b gy nv ns l nt nu">os.makedirs(_SAMPLE_DIR, exist_ok=True)</span><span id="4b27" class="nq mm iq nl b gy nv ns l nt nu">def _fetch_data():<br/>   uri = [(SAMPLE_WAV_URL, SAMPLE_WAV_PATH),<br/>           (SAMPLE_RIR_URL, SAMPLE_RIR_PATH),<br/>           (SAMPLE_WAV_SPEECH_URL, SAMPLE_WAV_SPEECH_PATH),<br/>           (SAMPLE_NOISE_URL, SAMPLE_NOISE_PATH),]<br/>   for url, path in uri:<br/>       with open(path, "wb") as file_:<br/>           file_.write(requests.get(url).content)</span><span id="cb98" class="nq mm iq nl b gy nv ns l nt nu">_fetch_data()</span><span id="5590" class="nq mm iq nl b gy nv ns l nt nu">def _get_sample(path, resample=None):<br/>   effects = [["remix","1"]]<br/>   if resample:<br/>       effects.extend([<br/>           ["lowpass", f"{resample // 2}"],<br/>           ["rate", f"{resample}"]<br/>       ])<br/>   return torchaudio.sox_effects.apply_effects_file(path, effects=effects)</span><span id="2f0f" class="nq mm iq nl b gy nv ns l nt nu">def get_sample(*, resample=None):<br/>   return _get_sample(SAMPLE_WAV_PATH, resample=resample)</span><span id="38af" class="nq mm iq nl b gy nv ns l nt nu">def get_speech_sample(*, resample=None):<br/>   return _get_sample(SAMPLE_WAV_SPEECH_PATH, resample=resample)</span><span id="a6b2" class="nq mm iq nl b gy nv ns l nt nu">def plot_waveform(waveform, sample_rate, title="Waveform", xlim=None, ylim=None):<br/>   waveform = waveform.numpy()<br/>   num_channels, num_frames = waveform.shape<br/>   time_axis = torch.arange(0, num_frames) / sample_rate</span><span id="963b" class="nq mm iq nl b gy nv ns l nt nu">   figure, axes = plt.subplots(num_channels, 1)<br/>   if num_channels == 1:<br/>       axes = [axes]<br/>   for c in range(num_channels):<br/>       axes[c].plot(time_axis, waveform[c], linewidth=1)<br/>       axes[c].grid(True)<br/>       if num_channels &gt; 1:<br/>           axes[c].set_ylabel(f"Channel {c+1}")<br/>       if xlim:<br/>           axes[c].set_xlim(xlim)<br/>       if ylim:<br/>           axes[c].set_ylim(ylim)<br/>   figure.suptitle(title)<br/>   plt.show(block=False)</span><span id="3e7c" class="nq mm iq nl b gy nv ns l nt nu">def print_stats(waveform, sample_rate=None, src=None):<br/>   if src:<br/>       print("-"*10)<br/>       print(f"Source: {src}")<br/>       print("-"*10)<br/>   if sample_rate:<br/>       print(f"Sample Rate: {sample_rate}")<br/>   print("Dtype:", waveform.dtype)<br/>   print(f" - Max:     {waveform.max().item():6.3f}")<br/>   print(f" - Min:     {waveform.min().item():6.3f}")<br/>   print(f" - Mean:    {waveform.mean().item():6.3f}")<br/>   print(f" - Std Dev: {waveform.std().item():6.3f}")<br/>   print()<br/>   print(waveform)<br/>   print()</span><span id="7c8d" class="nq mm iq nl b gy nv ns l nt nu">def plot_specgram(waveform, sample_rate, title="Spectrogram", xlim=None):<br/>   waveform = waveform.numpy()<br/>   num_channels, num_frames = waveform.shape<br/>   figure, axes = plt.subplots(num_channels, 1)<br/>   if num_channels == 1:<br/>       axes = [axes]<br/>   for c in range(num_channels):<br/>       axes[c].specgram(waveform[c], Fs=sample_rate)<br/>       if num_channels &gt; 1:<br/>           axes[c].set_ylabel(f"Channel {c+1}")<br/>       if xlim:<br/>           axes[c].set_xlim(xlim)<br/>   figure.suptitle(title)<br/>   plt.show(block=False)</span><span id="1be3" class="nq mm iq nl b gy nv ns l nt nu">def get_rir_sample(*, resample=None, processed=False):<br/>   rir_raw, sample_rate = _get_sample(SAMPLE_RIR_PATH, resample=resample)<br/>   if not processed:<br/>       return rir_raw, sample_rate<br/>   rir = rir_raw[:, int(sample_rate*1.01) : int(sample_rate * 1.3)]<br/>   rir = rir / torch.norm(rir, p=2)<br/>   rir = torch.flip(rir, [1])<br/>   return rir, sample_rate</span><span id="9cad" class="nq mm iq nl b gy nv ns l nt nu">def get_noise_sample(*, resample=None):<br/>   return _get_sample(SAMPLE_NOISE_PATH, resample=resample)</span></pre><h1 id="7031" class="ml mm iq bd mn mo mp mq mr ms mt mu mv kf mw kg mx ki my kj mz kl na km nb nc bi translated">在 Torchaudio 中使用声音效果</h1><p id="9e49" class="pw-post-body-paragraph la lb iq lc b ld nd ka lf lg ne kd li lj nf ll lm ln ng lp lq lr nh lt lu lv ij bi translated">现在我们已经设置好了一切，让我们来看看如何使用 PyTorch 的<code class="fe ni nj nk nl b">torchaudio</code>库来添加音效。我们将向来自<code class="fe ni nj nk nl b">torchaudio</code>的<code class="fe ni nj nk nl b">sox_effects.apply_effects_tensor</code>函数传递一个字符串列表(<code class="fe ni nj nk nl b">List[List[Str]])</code>对象)。</p><p id="8056" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">列表列表中的每个内部列表都包含一组定义效果的字符串。序列中的第一个字符串表示效果，接下来的条目表示如何应用该效果的参数。在下面的例子中，我们展示了如何添加一个低通滤波器，提高速度，并添加一些混响。有关可用音效选项的完整列表，请查看 sox 文档<a class="ae lw" href="http://sox.sourceforge.net/sox.html" rel="noopener ugc nofollow" target="_blank"><strong class="lc ja"/></a>。注意:这个函数返回两个返回值，波形和新的采样率。</p><pre class="kp kq kr ks gt nm nl nn no aw np bi"><span id="b80d" class="nq mm iq nl b gy nr ns l nt nu"># Load the data<br/>waveform1, sample_rate1 = get_sample(resample=16000)</span><span id="a2a3" class="nq mm iq nl b gy nv ns l nt nu"># Define effects<br/>effects = [<br/>   ["lowpass", "-1", "300"],  # apply single-pole lowpass filter<br/>   ["speed", "0.8"],  # reduce the speed<br/>   # This only changes sample rate, so it is necessary to<br/>   # add `rate` effect with original sample rate after this.<br/>   ["rate", f"{sample_rate1}"],<br/>   ["reverb", "-w"],  # Reverbration gives some dramatic feeling<br/>]<br/># Apply effects<br/>waveform2, sample_rate2 = torchaudio.sox_effects.apply_effects_tensor(waveform1, sample_rate1, effects)<br/>print_stats(waveform1, sample_rate=sample_rate1, src="Original")<br/>print_stats(waveform2, sample_rate=sample_rate2, src="Effects Applied")<br/>plot_waveform(waveform1, sample_rate1, title="Original", xlim=(-0.1, 3.2))<br/>plot_specgram(waveform1, sample_rate1, title="Original", xlim=(0, 3.04))<br/>plot_waveform(waveform2, sample_rate2, title="Effects Applied", xlim=(-0.1, 3.2))<br/>plot_specgram(waveform2, sample_rate2, title="Effects Applied", xlim=(0, 3.04))</span></pre><p id="313e" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">绘制波形和光谱图的打印输出如下。请注意，添加混响需要一个多声道波形来产生这种效果。从效果上可以看出波形和声谱图的区别。降低速度会拉长声音。添加滤波器会压缩部分声音(在声谱图中可见)。最后，混响增加了噪声，我们可以看到它主要反映在波形的“更细”或更安静的部分。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nw"><img src="../Images/e0f025224af43802d1cc301ce061e905.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iB_EqmvvmvmTWGOorldlyA.png"/></div></div></figure><p id="7f6a" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><em class="nx">上图:原始波形和声谱图 TorchAudio 添加的效果</em></p><h1 id="8c11" class="ml mm iq bd mn mo mp mq mr ms mt mu mv kf mw kg mx ki my kj mz kl na km nb nc bi translated">添加背景噪音</h1><p id="e208" class="pw-post-body-paragraph la lb iq lc b ld nd ka lf lg ne kd li lj nf ll lm ln ng lp lq lr nh lt lu lv ij bi translated">现在我们知道了如何使用<code class="fe ni nj nk nl b">torchaudio</code>给音频添加效果，让我们深入一些更具体的用例。如果您的模型需要能够在有背景噪音的情况下检测音频，那么在训练数据中添加一些背景噪音是一个好主意。</p><p id="2649" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在下面的例子中，我们将从声明一个采样速率开始(8000 是一个非常典型的速率)。接下来，我们将调用我们的助手函数来获取语音和背景噪声，并对噪声进行整形。之后，我们将使用<code class="fe ni nj nk nl b">norm</code>函数将语音和文本规范化到<a class="ae lw" href="https://pytorch.org/docs/stable/generated/torch.norm.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lc ja">二阶</strong> </a>。接下来，我们将定义一个分贝列表，我们希望在演讲中播放背景噪音，并在每个级别创建一个“背景噪音”版本。</p><pre class="kp kq kr ks gt nm nl nn no aw np bi"><span id="8487" class="nq mm iq nl b gy nr ns l nt nu">sample_rate = 8000<br/>speech, _ = get_speech_sample(resample=sample_rate)<br/>noise, _ = get_noise_sample(resample=sample_rate)<br/>noise = noise[:, : speech.shape[1]]</span><span id="5345" class="nq mm iq nl b gy nv ns l nt nu">speech_power = speech.norm(p=2)<br/>noise_power = noise.norm(p=2)</span><span id="0ea5" class="nq mm iq nl b gy nv ns l nt nu">snr_dbs = [20, 10, 3]<br/>noisy_speeches = []<br/>for snr_db in snr_dbs:<br/>   snr = math.exp(snr_db / 10)<br/>   scale = snr * noise_power / speech_power<br/>   noisy_speeches.append((scale * speech + noise) / 2)</span><span id="b135" class="nq mm iq nl b gy nv ns l nt nu">plot_waveform(noise, sample_rate, title="Background noise")<br/>plot_specgram(noise, sample_rate, title="Background noise")</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ny"><img src="../Images/52ef1c32f038d2429c324a6f582fbdf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XKEXYu8kSBigaOFq3uonZQ.png"/></div></div></figure><p id="0387" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">以上图片显示了背景噪音的波形和声谱图。我们已经在上面的代码中创建了所有的噪声语音音频数据剪辑。下面的代码将它们全部打印出来，这样我们就可以看到不同音频级别的数据是什么样子。请注意，20 dB <code class="fe ni nj nk nl b">snr</code>意味着信号(语音)与噪声(背景噪声)的比率为 20dB，而不是播放的噪声为 20dB。</p><pre class="kp kq kr ks gt nm nl nn no aw np bi"><span id="12fc" class="nq mm iq nl b gy nr ns l nt nu"># background noise at certain levels<br/>snr_db20, noisy_speech20 = snr_dbs[0], noisy_speeches[0]<br/>plot_waveform(noisy_speech20, sample_rate, title=f"SNR: {snr_db20} [dB]")<br/>plot_specgram(noisy_speech20, sample_rate, title=f"SNR: {snr_db20} [dB]")</span><span id="569d" class="nq mm iq nl b gy nv ns l nt nu">snr_db10, noisy_speech10 = snr_dbs[1], noisy_speeches[1]<br/>plot_waveform(noisy_speech10, sample_rate, title=f"SNR: {snr_db10} [dB]")<br/>plot_specgram(noisy_speech10, sample_rate, title=f"SNR: {snr_db10} [dB]")</span><span id="554a" class="nq mm iq nl b gy nv ns l nt nu">snr_db3, noisy_speech3 = snr_dbs[2], noisy_speeches[2]<br/>plot_waveform(noisy_speech3, sample_rate, title=f"SNR: {snr_db3} [dB]")<br/>plot_specgram(noisy_speech3, sample_rate, title=f"SNR: {snr_db3} [dB]")</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nz"><img src="../Images/e230a3c1b0b431d1daea0fb7b1eb090d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Eothb3ovjtfBe_UwFCBHMw.png"/></div></div></figure><p id="2ff1" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><em class="nx">上图:通过 PyTorch TorchAudio 添加了 20 和 10 dB SNR 的背景噪声可视化效果</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oa"><img src="../Images/f45da9356d941d5f2202a2946f1bca8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4oan5NzPFRlFFf213EgBSw.png"/></div></div></figure><p id="1c3b" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><em class="nx">上图:添加背景噪声的 3 dB 信噪比波形和频谱图</em></p><h1 id="d8a6" class="ml mm iq bd mn mo mp mq mr ms mt mu mv kf mw kg mx ki my kj mz kl na km nb nc bi translated">添加房间混响</h1><p id="65d1" class="pw-post-body-paragraph la lb iq lc b ld nd ka lf lg ne kd li lj nf ll lm ln ng lp lq lr nh lt lu lv ij bi translated">到目前为止，我们已经应用了不同噪声水平的音频效果和背景噪声。让我们也来看看如何添加一个混响。给音频剪辑添加混响会给人一种音频是在回声室内录制的感觉。您可以这样做，让它看起来像是您在电脑上做的演示实际上是在剧院里给观众做的。</p><p id="de7b" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">要添加房间混响，我们首先要使用上面的一个函数(<code class="fe ni nj nk nl b">get_rir_sample</code>)在线请求音频。我们先看一下波形，然后再对其进行剪辑，以获得声音的“混响”，将其归一化，然后翻转声音，使混响正常工作。</p><pre class="kp kq kr ks gt nm nl nn no aw np bi"><span id="32fc" class="nq mm iq nl b gy nr ns l nt nu">sample_rate = 8000</span><span id="1a26" class="nq mm iq nl b gy nv ns l nt nu">rir_raw, _ = get_rir_sample(resample=sample_rate)</span><span id="4afb" class="nq mm iq nl b gy nv ns l nt nu">plot_waveform(rir_raw, sample_rate, title="Room Impulse Response (raw)", ylim=None)<br/>plot_specgram(rir_raw, sample_rate, title="Room Impulse Response (raw)")</span><span id="bb65" class="nq mm iq nl b gy nv ns l nt nu">rir = rir_raw[:, int(sample_rate * 1.01) : int(sample_rate * 1.3)]<br/>rir = rir / torch.norm(rir, p=2)<br/>rir = torch.flip(rir, [1])</span><span id="fc96" class="nq mm iq nl b gy nv ns l nt nu">print_stats(rir)<br/>plot_waveform(rir, sample_rate, title="Room Impulse Response", ylim=None)<br/>plot_specgram(rir_raw, sample_rate, title="Room Impulse Response (raw)")</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ob"><img src="../Images/6fe90a1b99b5a22a17bc2ece32c5725a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R6r2yaieiG295UTQs8M-HA.png"/></div></div></figure><p id="fff2" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><em class="nx">上图:PyTorch TorchAudio 的原始和增强混响声音可视化效果</em></p><p id="6743" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">一旦我们对声音进行了标准化和翻转，我们就可以用它来增强现有的音频。我们将首先使用 PyTorch 创建一个使用语音和增强声音的“填充”。然后，我们将使用 PyTorch 对声音进行一维卷积。</p><pre class="kp kq kr ks gt nm nl nn no aw np bi"><span id="5050" class="nq mm iq nl b gy nr ns l nt nu">speech, _ = get_speech_sample(resample=sample_rate)</span><span id="0007" class="nq mm iq nl b gy nv ns l nt nu">speech_ = torch.nn.functional.pad(speech, (rir.shape[1] - 1, 0))<br/>augmented = torch.nn.functional.conv1d(speech_[None, ...], rir[None, ...])[0]</span><span id="781f" class="nq mm iq nl b gy nv ns l nt nu">plot_waveform(speech, sample_rate, title="Original", ylim=None)<br/>plot_specgram(speech, sample_rate, title="Original")<br/>play_audio(speech, sample_rate)</span><span id="278c" class="nq mm iq nl b gy nv ns l nt nu">plot_waveform(augmented, sample_rate, title="RIR Applied", ylim=None)<br/>plot_specgram(augmented, sample_rate, title="RIR Applied")<br/>play_audio(augmented, sample_rate)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oc"><img src="../Images/8a4ac63c9278e3460cdf43eef1dd2ed3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uwV12X5oms-nDmji0XuPPg.png"/></div></div></figure><p id="d854" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><em class="nx">上图:TorchAudio 应用混响后的音频可视化效果</em></p><p id="250e" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">从上面的打印输出中我们可以看到，添加房间混响会将类似回声的声音添加到波形中。我们还可以看到，声谱图的清晰度低于接近话筒的清晰声音。</p><h1 id="e3ec" class="ml mm iq bd mn mo mp mq mr ms mt mu mv kf mw kg mx ki my kj mz kl na km nb nc bi translated">使用 TorchAudio 对音频数据进行高级重采样</h1><p id="23c7" class="pw-post-body-paragraph la lb iq lc b ld nd ka lf lg ne kd li lj nf ll lm ln ng lp lq lr nh lt lu lv ij bi translated">我们简要地提到了在使用<code class="fe ni nj nk nl b">pydub</code>和<code class="fe ni nj nk nl b">sklearn</code>库之前如何重新采样数据。TorchAudio 还可以让您使用多种方法轻松地对音频数据进行重新采样。在本节中，我们将讨论如何使用低通滤波器、滚降滤波器和窗口滤波器对数据进行重采样。</p><p id="f9d2" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">正如我们在上面所做的，在我们真正开始对数据进行重采样之前，我们需要设置一些辅助函数。这些设置功能中的许多功能与上述功能相同。这里需要注意的是<code class="fe ni nj nk nl b">get_sine_sweep</code>，我们将使用它来代替现有的音频文件。所有其他功能，如获取刻度和反向对数频率，都是为了绘制数据。</p><pre class="kp kq kr ks gt nm nl nn no aw np bi"><span id="e8fe" class="nq mm iq nl b gy nr ns l nt nu">import math<br/>import torch</span><span id="9eb7" class="nq mm iq nl b gy nv ns l nt nu">import matplotlib.pyplot as plt<br/>from IPython.display import Audio, display<br/></span><span id="f573" class="nq mm iq nl b gy nv ns l nt nu">DEFAULT_OFFSET = 201<br/>SWEEP_MAX_SAMPLE_RATE = 48000<br/>DEFAULT_LOWPASS_FILTER_WIDTH = 6<br/>DEFAULT_ROLLOFF = 0.99<br/>DEFAULT_RESAMPLING_METHOD = "sinc_interpolation"</span><span id="14cd" class="nq mm iq nl b gy nv ns l nt nu">def _get_log_freq(sample_rate, max_sweep_rate, offset):<br/>   """Get freqs evenly spaced out in log-scale, between [0, max_sweep_rate // 2]</span><span id="e061" class="nq mm iq nl b gy nv ns l nt nu">   offset is used to avoid negative infinity `log(offset + x)`.</span><span id="898e" class="nq mm iq nl b gy nv ns l nt nu">   """<br/>   start, stop = math.log(offset), math.log(offset + max_sweep_rate // 2)<br/>   return torch.exp(torch.linspace(start, stop, sample_rate, dtype=torch.double)) - offset</span><span id="bec8" class="nq mm iq nl b gy nv ns l nt nu">def _get_inverse_log_freq(freq, sample_rate, offset):<br/>   """Find the time where the given frequency is given by _get_log_freq"""<br/>   half = sample_rate // 2<br/>   return sample_rate * (math.log(1 + freq / offset) / math.log(1 + half / offset))<br/></span><span id="4a96" class="nq mm iq nl b gy nv ns l nt nu">def _get_freq_ticks(sample_rate, offset, f_max):<br/>   # Given the original sample rate used for generating the sweep,<br/>   # find the x-axis value where the log-scale major frequency values fall in<br/>   time, freq = [], []<br/>   for exp in range(2, 5):<br/>       for v in range(1, 10):<br/>           f = v * 10 ** exp<br/>           if f &lt; sample_rate // 2:<br/>               t = _get_inverse_log_freq(f, sample_rate, offset) / sample_rate<br/>               time.append(t)<br/>               freq.append(f)<br/>   t_max = _get_inverse_log_freq(f_max, sample_rate, offset) / sample_rate<br/>   time.append(t_max)<br/>   freq.append(f_max)<br/>   return time, freq</span><span id="4e07" class="nq mm iq nl b gy nv ns l nt nu">def get_sine_sweep(sample_rate, offset=DEFAULT_OFFSET):<br/>   max_sweep_rate = sample_rate<br/>   freq = _get_log_freq(sample_rate, max_sweep_rate, offset)<br/>   delta = 2 * math.pi * freq / sample_rate<br/>   cummulative = torch.cumsum(delta, dim=0)<br/>   signal = torch.sin(cummulative).unsqueeze(dim=0)<br/>   return signal</span><span id="99fa" class="nq mm iq nl b gy nv ns l nt nu">def plot_sweep(<br/>   waveform,<br/>   sample_rate,<br/>   title,<br/>   max_sweep_rate=SWEEP_MAX_SAMPLE_RATE,<br/>   offset=DEFAULT_OFFSET,<br/>):<br/>   x_ticks = [100, 500, 1000, 5000, 10000, 20000, max_sweep_rate // 2]<br/>   y_ticks = [1000, 5000, 10000, 20000, sample_rate // 2]</span><span id="3855" class="nq mm iq nl b gy nv ns l nt nu">   time, freq = _get_freq_ticks(max_sweep_rate, offset, sample_rate // 2)<br/>   freq_x = [f if f in x_ticks and f &lt;= max_sweep_rate // 2 else None for f in freq]<br/>   freq_y = [f for f in freq if f &gt;= 1000 and f in y_ticks and f &lt;= sample_rate // 2]</span><span id="c5aa" class="nq mm iq nl b gy nv ns l nt nu">   figure, axis = plt.subplots(1, 1)<br/>   axis.specgram(waveform[0].numpy(), Fs=sample_rate)<br/>   plt.xticks(time, freq_x)<br/>   plt.yticks(freq_y, freq_y)<br/>   axis.set_xlabel("Original Signal Frequency (Hz, log scale)")<br/>   axis.set_ylabel("Waveform Frequency (Hz)")<br/>   axis.xaxis.grid(True, alpha=0.67)<br/>   axis.yaxis.grid(True, alpha=0.67)<br/>   figure.suptitle(f"{title} (sample rate: {sample_rate} Hz)")<br/>   plt.show(block=True)</span><span id="3068" class="nq mm iq nl b gy nv ns l nt nu">def plot_specgram(waveform, sample_rate, title="Spectrogram", xlim=None):<br/>   waveform = waveform.numpy()</span><span id="ad42" class="nq mm iq nl b gy nv ns l nt nu">   num_channels, num_frames = waveform.shape</span><span id="e5e7" class="nq mm iq nl b gy nv ns l nt nu">   figure, axes = plt.subplots(num_channels, 1)<br/>   if num_channels == 1:<br/>       axes = [axes]<br/>   for c in range(num_channels):<br/>       axes[c].specgram(waveform[c], Fs=sample_rate)<br/>       if num_channels &gt; 1:<br/>           axes[c].set_ylabel(f"Channel {c+1}")<br/>       if xlim:<br/>           axes[c].set_xlim(xlim)<br/>   figure.suptitle(title)<br/>   plt.show(block=False)</span></pre><p id="25df" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我把两个<code class="fe ni nj nk nl b">torchaudio</code>导入放在这里是为了澄清这是我们将用来提取函数的<code class="fe ni nj nk nl b">T</code>和<code class="fe ni nj nk nl b">F</code>字母(相对于 true 和 false！).我们将声明一个采样速率和一个重采样速率，它们是什么并不重要，你可以随意改变它们。</p><p id="6fc7" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我们要做的第一件事是使用<code class="fe ni nj nk nl b">get_sine_sweep</code>函数创建一个波形。然后，我们将不传递任何参数进行重采样。接下来，我们来看看当我们使用低通滤波器宽度参数时，扫描是什么样子的。为此，我们需要功能性的<code class="fe ni nj nk nl b">torchaudio</code>包。</p><p id="c3f4" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">从技术上来说，频率是无限的，所以低通滤波器可以滤除低于某一频率的声音。低通滤波器的宽度决定了该滤波器的窗口大小。Torchaudio 的默认值是 6，所以我们的第一次和第二次重采样是相同的。这里的值越大，噪波越“尖锐”。</p><pre class="kp kq kr ks gt nm nl nn no aw np bi"><span id="0a29" class="nq mm iq nl b gy nr ns l nt nu">import torchaudio.functional as F<br/>import torchaudio.transforms as T<br/>sample_rate = 48000<br/>resample_rate = 32000</span><span id="c5fc" class="nq mm iq nl b gy nv ns l nt nu">waveform = get_sine_sweep(sample_rate)<br/>plot_sweep(waveform, sample_rate, title="Original Waveform")</span><span id="0f1e" class="nq mm iq nl b gy nv ns l nt nu">print("basic resampling")<br/>resampler = T.Resample(sample_rate, resample_rate, dtype=waveform.dtype)<br/>resampled_waveform = resampler(waveform)<br/>plot_sweep(resampled_waveform, resample_rate, title="Resampled Waveform")</span><span id="0ff0" class="nq mm iq nl b gy nv ns l nt nu">print("lowpass resampling")<br/>lp6_resampled_waveform = F.resample(waveform, sample_rate, resample_rate, lowpass_filter_width=6)<br/>plot_sweep(resampled_waveform, resample_rate, title="lowpass_filter_width=6")</span><span id="f51e" class="nq mm iq nl b gy nv ns l nt nu">lp128_resampled_waveform = F.resample(waveform, sample_rate, resample_rate, lowpass_filter_width=128)<br/>plot_sweep(resampled_waveform, resample_rate, title="lowpass_filter_width=128")</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi od"><img src="../Images/66d57dec2f494c356482951d97186647.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dMe7aewQcJmaZqOHNihpqw.png"/></div></div></figure><p id="38fe" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><em class="nx">上图:TorchAudio 的基本和低通滤波器示例频谱图</em></p><p id="43dc" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">过滤器不是我们唯一可以用于重采样的东西。在下面的示例代码中，我们将同时使用默认的汉恩窗口和 Kaiser 窗口。两个窗口都用作自动过滤的方式。使用滚降进行重采样可以达到同样的目的。在我们的示例中，我们将采用 0.99 和 0.8 的滚降。衰减表示音频的衰减比例。</p><pre class="kp kq kr ks gt nm nl nn no aw np bi"><span id="46ea" class="nq mm iq nl b gy nr ns l nt nu">print("using a window to resample")<br/>hann_window_resampled_waveform = F.resample(waveform, sample_rate, resample_rate, resampling_method="sinc_interpolation")<br/>plot_sweep(resampled_waveform, resample_rate, title="Hann Window Default")</span><span id="7bfc" class="nq mm iq nl b gy nv ns l nt nu">kaiser_window_resampled_waveform = F.resample(waveform, sample_rate, resample_rate, resampling_method="kaiser_window")<br/>plot_sweep(resampled_waveform, resample_rate, title="Kaiser Window Default")</span><span id="7554" class="nq mm iq nl b gy nv ns l nt nu">print("user rollof to determine window")<br/>rolloff_resampled_waveform = F.resample(waveform, sample_rate, resample_rate, rolloff=0.99)<br/>plot_sweep(resampled_waveform, resample_rate, title="rolloff=0.99")</span><span id="4cc8" class="nq mm iq nl b gy nv ns l nt nu">rolloff_resampled_waveform = F.resample(waveform, sample_rate, resample_rate, rolloff=0.8)<br/>plot_sweep(resampled_waveform, resample_rate, title="rolloff=0.8")</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi od"><img src="../Images/bc2c219555bcbe4dab843d676f41bb0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*avoEhFFXlgPLWLHffkkd8w.png"/></div></div></figure><p id="c0aa" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><em class="nx">上图:TorchAudio 的窗口和滚降参数重采样可视化</em></p><h1 id="1261" class="ml mm iq bd mn mo mp mq mr ms mt mu mv kf mw kg mx ki my kj mz kl na km nb nc bi translated">基于 PyTorch TorchAudio 的音频特征提取</h1><p id="cccb" class="pw-post-body-paragraph la lb iq lc b ld nd ka lf lg ne kd li lj nf ll lm ln ng lp lq lr nh lt lu lv ij bi translated">到目前为止，我们已经了解了如何以多种方式使用<code class="fe ni nj nk nl b">torchaudio</code>来处理我们的音频数据。现在我们来看看如何用<code class="fe ni nj nk nl b">torchaudio</code>做特征提取。正如我们在上面两个部分中所做的那样，我们将从设置开始。</p><p id="39a1" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我们的设置函数将包括获取数据的函数，以及像上面的“效果”部分那样可视化数据的函数。我们还添加了一些用于 Mel 缩放桶的函数。我们将使用<a class="ae lw" href="https://en.wikipedia.org/wiki/Mel_scale" rel="noopener ugc nofollow" target="_blank"> <strong class="lc ja">梅尔标度</strong> </a>桶来制作梅尔频率倒谱系数(MFCC)，这些系数代表音频音色。</p><pre class="kp kq kr ks gt nm nl nn no aw np bi"><span id="ac1d" class="nq mm iq nl b gy nr ns l nt nu">import os</span><span id="63aa" class="nq mm iq nl b gy nv ns l nt nu">import torch<br/>import torchaudio<br/>import torchaudio.functional as F<br/>import torchaudio.transforms as T<br/>import librosa<br/>import matplotlib.pyplot as plt<br/>import requests<br/></span><span id="23d3" class="nq mm iq nl b gy nv ns l nt nu">_SAMPLE_DIR = "_assets"</span><span id="3ed0" class="nq mm iq nl b gy nv ns l nt nu">SAMPLE_WAV_SPEECH_URL = "https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav"  # noqa: E501<br/>SAMPLE_WAV_SPEECH_PATH = os.path.join(_SAMPLE_DIR, "speech.wav")</span><span id="642c" class="nq mm iq nl b gy nv ns l nt nu">os.makedirs(_SAMPLE_DIR, exist_ok=True)<br/></span><span id="64d5" class="nq mm iq nl b gy nv ns l nt nu">def _fetch_data():<br/>   uri = [<br/>       (SAMPLE_WAV_SPEECH_URL, SAMPLE_WAV_SPEECH_PATH),<br/>   ]<br/>   for url, path in uri:<br/>       with open(path, "wb") as file_:<br/>           file_.write(requests.get(url).content)<br/></span><span id="a7e1" class="nq mm iq nl b gy nv ns l nt nu">_fetch_data()<br/></span><span id="fa60" class="nq mm iq nl b gy nv ns l nt nu">def _get_sample(path, resample=None):<br/>   effects = [["remix", "1"]]<br/>   if resample:<br/>       effects.extend(<br/>           [<br/>               ["lowpass", f"{resample // 2}"],<br/>               ["rate", f"{resample}"],<br/>           ]<br/>       )<br/>   return torchaudio.sox_effects.apply_effects_file(path, effects=effects)<br/></span><span id="559d" class="nq mm iq nl b gy nv ns l nt nu">def get_speech_sample(*, resample=None):<br/>   return _get_sample(SAMPLE_WAV_SPEECH_PATH, resample=resample)<br/></span><span id="9db1" class="nq mm iq nl b gy nv ns l nt nu">def plot_spectrogram(spec, title=None, ylabel="freq_bin", aspect="auto", xmax=None):<br/>   fig, axs = plt.subplots(1, 1)<br/>   axs.set_title(title or "Spectrogram (db)")<br/>   axs.set_ylabel(ylabel)<br/>   axs.set_xlabel("frame")<br/>   im = axs.imshow(librosa.power_to_db(spec), origin="lower", aspect=aspect)<br/>   if xmax:<br/>       axs.set_xlim((0, xmax))<br/>   fig.colorbar(im, ax=axs)<br/>   plt.show(block=False)<br/></span><span id="cf20" class="nq mm iq nl b gy nv ns l nt nu">def plot_waveform(waveform, sample_rate, title="Waveform", xlim=None, ylim=None):<br/>   waveform = waveform.numpy()</span><span id="1b81" class="nq mm iq nl b gy nv ns l nt nu">   num_channels, num_frames = waveform.shape<br/>   time_axis = torch.arange(0, num_frames) / sample_rate</span><span id="5e1b" class="nq mm iq nl b gy nv ns l nt nu">   figure, axes = plt.subplots(num_channels, 1)<br/>   if num_channels == 1:<br/>       axes = [axes]<br/>   for c in range(num_channels):<br/>       axes[c].plot(time_axis, waveform[c], linewidth=1)<br/>       axes[c].grid(True)<br/>       if num_channels &gt; 1:<br/>           axes[c].set_ylabel(f"Channel {c+1}")<br/>       if xlim:<br/>           axes[c].set_xlim(xlim)<br/>       if ylim:<br/>           axes[c].set_ylim(ylim)<br/>   figure.suptitle(title)<br/>   plt.show(block=False)<br/></span><span id="137b" class="nq mm iq nl b gy nv ns l nt nu">def plot_mel_fbank(fbank, title=None):<br/>   fig, axs = plt.subplots(1, 1)<br/>   axs.set_title(title or "Filter bank")<br/>   axs.imshow(fbank, aspect="auto")<br/>   axs.set_ylabel("frequency bin")<br/>   axs.set_xlabel("mel bin")<br/>   plt.show(block=False)</span></pre><p id="a45f" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我们在这里要做的第一件事是绘制声谱图，并将其反转。将波形转换成声谱图，然后再转换回来。为什么将波形转换成声谱图对特征提取有用？这种表示有助于提取频谱特征，如频率、音色、密度、滚降等。</p><p id="0d7b" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在创建光谱图和反转光谱图之前，我们将定义一些常数。首先，我们要定义<code class="fe ni nj nk nl b">n_fft</code>，快速傅立叶变换的大小，然后是窗口长度(窗口的大小)和跳跃长度(短时傅立叶变换之间的距离)。然后，我们将调用<code class="fe ni nj nk nl b">torchaudio</code>将我们的波形转换成声谱图。为了将声谱图转换回波形，我们将使用来自<code class="fe ni nj nk nl b">torchaudio</code>的<code class="fe ni nj nk nl b">GriffinLim</code>函数，使用与上面相同的参数将波形转换成声谱图。</p><pre class="kp kq kr ks gt nm nl nn no aw np bi"><span id="3d07" class="nq mm iq nl b gy nr ns l nt nu"># plot spectrogram<br/>waveform, sample_rate = get_speech_sample()</span><span id="6e80" class="nq mm iq nl b gy nv ns l nt nu">n_fft = 1024<br/>win_length = None<br/>hop_length = 512</span><span id="6f2d" class="nq mm iq nl b gy nv ns l nt nu"># create spectrogram<br/>torch.random.manual_seed(0)<br/>plot_waveform(waveform, sample_rate, title="Original")</span><span id="b77f" class="nq mm iq nl b gy nv ns l nt nu">spec = T.Spectrogram(<br/>   n_fft=n_fft,<br/>   win_length=win_length,<br/>   hop_length=hop_length,<br/>)(waveform)<br/>plot_spectrogram(spec[0], title="torchaudio spec")</span><span id="88d6" class="nq mm iq nl b gy nv ns l nt nu"># reverse spectrogram to waveform with griffinlim<br/>griffin_lim = T.GriffinLim(<br/>   n_fft=n_fft,<br/>   win_length=win_length,<br/>   hop_length=hop_length,<br/>)<br/>waveform = griffin_lim(spec)<br/>plot_waveform(waveform, sample_rate, title="Reconstructed")</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oe"><img src="../Images/cf9b11c2d9a2411e3787b2f4fa6c5719.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CT9AUlW9pfrhB4Z7DPmz5w.png"/></div></div></figure><p id="eab3" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><em class="nx">上图:在 PyTorch 中创建和反转光谱图</em></p><p id="f823" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">让我们来看看我们可以用频谱特征做的更有趣的事情之一，<a class="ae lw" href="https://en.wikipedia.org/wiki/Mel-frequency_cepstrum" rel="noopener ugc nofollow" target="_blank"> <strong class="lc ja">梅尔频率倒谱</strong> </a>。梅尔频率频谱系数(MFCC)代表音频的音色。在我们开始获取这些特征系数之前，我们将定义一些 mel 滤波器组(256)和一个新的采样速率。</p><p id="fdb2" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我们为 MFCC 需要的第一件事是得到梅尔滤波器组。一旦我们得到了 mel 滤波器组，我们就可以用它来得到 mel 谱图。现在，我们准备好得到系数了。首先，我们需要定义我们想要多少系数，然后我们将使用梅尔滤波器组和梅尔频谱图来创建一个 MFCC 图。这是我们的 mel 谱图在减少到我们上面指定的系数数量时的样子。</p><pre class="kp kq kr ks gt nm nl nn no aw np bi"><span id="855e" class="nq mm iq nl b gy nr ns l nt nu"># mel spectrogram<br/># mel scale waveforms<br/># mel scale bins<br/>n_mels = 256<br/>sample_rate = 6000</span><span id="cbe9" class="nq mm iq nl b gy nv ns l nt nu">mel_filters = F.melscale_fbanks(<br/>   int(n_fft // 2 + 1),<br/>   n_mels=n_mels,<br/>   f_min=0.0,<br/>   f_max=sample_rate / 2.0,<br/>   sample_rate=sample_rate,<br/>   norm="slaney",<br/>)<br/>plot_mel_fbank(mel_filters, "Mel Filter Bank - torchaudio")</span><span id="d7a5" class="nq mm iq nl b gy nv ns l nt nu"># mel spectrogram<br/>mel_spectrogram = T.MelSpectrogram(<br/>   sample_rate=sample_rate,<br/>   n_fft=n_fft,<br/>   win_length=win_length,<br/>   hop_length=hop_length,<br/>   center=True,<br/>   pad_mode="reflect",<br/>   power=2.0,<br/>   norm="slaney",<br/>   onesided=True,<br/>   n_mels=n_mels,<br/>   mel_scale="htk",<br/>)</span><span id="0a97" class="nq mm iq nl b gy nv ns l nt nu">melspec = mel_spectrogram(waveform)<br/>plot_spectrogram(melspec[0], title="MelSpectrogram - torchaudio", ylabel="mel freq")</span><span id="ab1a" class="nq mm iq nl b gy nv ns l nt nu">n_mfcc = 256</span><span id="cd4c" class="nq mm iq nl b gy nv ns l nt nu">mfcc_transform = T.MFCC(<br/>   sample_rate=sample_rate,<br/>   n_mfcc=n_mfcc,<br/>   melkwargs={<br/>       "n_fft": n_fft,<br/>       "n_mels": n_mels,<br/>       "hop_length": hop_length,<br/>       "mel_scale": "htk",<br/>   },<br/>)</span><span id="f0cb" class="nq mm iq nl b gy nv ns l nt nu">mfcc = mfcc_transform(waveform)</span><span id="b860" class="nq mm iq nl b gy nv ns l nt nu">plot_spectrogram(mfcc[0])</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oe"><img src="../Images/31cc92542ae5b96e0ca931157cf3a50d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NEi3a2l3kBMAR9ZetXJ5rA.png"/></div></div></figure><p id="fb5d" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><em class="nx">上图:用 PyTorch TorchAudio 提取音频数据的 MFCC 特征</em></p><h1 id="9bc0" class="ml mm iq bd mn mo mp mq mr ms mt mu mv kf mw kg mx ki my kj mz kl na km nb nc bi translated">概括起来</h1><p id="d20b" class="pw-post-body-paragraph la lb iq lc b ld nd ka lf lg ne kd li lj nf ll lm ln ng lp lq lr nh lt lu lv ij bi translated">在这篇史诗般的文章中，我们介绍了如何使用 PyTorch 的<code class="fe ni nj nk nl b">torchaudio</code>库的基础知识。我们看到，我们可以使用<code class="fe ni nj nk nl b">torchaudio</code>来做详细和复杂的音频操作。我们讨论的具体例子是添加音效、背景噪音和房间混响。</p><p id="5fa5" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">TorchAudio 还提供了其他音频处理方法，如高级重采样。在我们的重采样示例中，我们展示了如何使用 TorchAudio 的<code class="fe ni nj nk nl b">functional</code>和<code class="fe ni nj nk nl b">transform</code>库中的多个函数和参数，使用不同的过滤器进行重采样。我们使用低通滤波器、滚降滤波器和窗口滤波器。</p><p id="277a" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">最后，我们介绍了如何使用 TorchAudio 进行特征提取。我们展示了如何创建频谱图以获得频谱特征，使用 Griffin-Lim 公式反转频谱图，以及如何创建和使用梅尔尺度箱以获得梅尔频率倒谱系数(MFCC)特征。</p><h1 id="42d6" class="ml mm iq bd mn mo mp mq mr ms mt mu mv kf mw kg mx ki my kj mz kl na km nb nc bi translated">进一步阅读</h1><ul class=""><li id="e3e9" class="lx ly iq lc b ld nd lg ne lj of ln og lr oh lv mc md me mf bi translated"><a class="ae lw" href="https://pythonalgos.com/dijkstras-algorithm-in-5-steps-with-python/" rel="noopener ugc nofollow" target="_blank">Python 中的 Dijkstra 算法</a></li><li id="42e0" class="lx ly iq lc b ld mg lg mh lj mi ln mj lr mk lv mc md me mf bi translated">建立你自己的人工智能文本摘要器</li><li id="7188" class="lx ly iq lc b ld mg lg mh lj mi ln mj lr mk lv mc md me mf bi translated"><a class="ae lw" href="https://pythonalgos.com/the-best-way-to-do-named-entity-recognition-ner/" rel="noopener ugc nofollow" target="_blank">在 Python 中进行命名实体识别(NER)的最佳方式</a></li></ul><p id="b428" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">如果你喜欢这篇文章，请在 Twitter 上分享！为了无限制地访问媒体文章，今天就注册成为<a class="ae lw" href="https://www.medium.com/@ytang07/membership" rel="noopener">媒体会员</a>！别忘了关注我，<a class="ae lw" href="https://www.medium.com/@ytang07" rel="noopener">唐</a>，获取更多关于增长、技术等方面的文章！</p></div></div>    
</body>
</html>