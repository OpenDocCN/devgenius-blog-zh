<html>
<head>
<title>Deep Learning : In gradient descent style! — Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习:以梯度下降的方式！—第一部分</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/deep-learning-in-gradient-decent-style-part-1-ed747e7cc2a3?source=collection_archive---------0-----------------------#2018-03-18">https://blog.devgenius.io/deep-learning-in-gradient-decent-style-part-1-ed747e7cc2a3?source=collection_archive---------0-----------------------#2018-03-18</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><p id="aff4" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated"><strong class="jn io"> <em class="kj">老实说，我花了一些时间来理解“深度学习”的概念，但正如他们所说的，不同的学习速率、大量的干净数据和足够的EPOCS将导致更好的准确性。我想说，现在我的准确率已经接近“y-hat”了，但仍然需要更多的学习。但是到目前为止这段旅程真的很棒！</em>T3】</strong></p><p id="1940" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated">让我们先从<strong class="jn io">几个</strong>关键定义开始，这样我们在开始学习神经网络之前就有了一些基本的理解，然后再去理解更大的概念。有各种各样的关键字，当我们到达那个点时，让我们处理它们。</p><p id="49ce" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated"><strong class="jn io">线性代数</strong>:在神经网络的环境中，线性代数运算描述了信息从一层流向另一层时的转换。线性代数是一个数学工具箱，用于处理包含信息/数据的向量和矩阵。现代计算机具有(图形处理单元)GPU，该GPU被设计成以最佳速度并行进行矢量和矩阵运算。用于深度学习的线性代数在这篇文章中有很好的描述。</p><p id="429d" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated"><strong class="jn io">线性方程</strong>:线性方程是一次多项式。例:<em class="kj"> y = x </em>或<em class="kj"> y = 2x是一次多项式</em>。当你用这些等式在图上画出这些点时，它将形成一条直线。如果你增加更多的维度，它将形成<em class="kj">平面</em>或<em class="kj">超平面</em>，但它们的形状将始终是直的和<em class="kj">线性的</em>，没有任何曲线。要获得多维线性平面的直观感受，请看下面的视频。</p><figure class="kl km kn ko gt kp"><div class="bz fp l di"><div class="kq kr l"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk translated">多维对象</figcaption></figure><p id="4d29" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated"><strong class="jn io">非线性方程</strong>:高次多项式(<em class="kj">多于一个</em>)在图形上绘制点时会形成曲线，例如:<em class="kj"> y = x </em>或<em class="kj"> sin </em>或<em class="kj">余弦</em>等。类似地，多维非线性平面看起来会像下面这样:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi kw"><img src="../Images/6d4aebe57d0a3f8549d4186ff30dc11f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*7030GXGlVD-u9VyqVJdTyw.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk translated">非线性方程的3D视图</figcaption></figure><p id="c7c2" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated"><strong class="jn io">矢量</strong>:在线性代数中，矢量是点<em class="kj"> a </em>到<em class="kj"> b </em>的运动，既保持方向又保持大小。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi kz"><img src="../Images/5e9798179fc493a2568aaea6425f5ee2.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*mX2xBh1OPA7xLSVfbD67Aw.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk translated">矢量</figcaption></figure><p id="fd3e" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated">向量[2，3]表示右边的2个点和向上的3个点。同样，直线上的所有点也可以通过以下等式获得:</p><pre class="kl km kn ko gt la lb lc ld aw le bi"><span id="86cc" class="lf lg in lb b gy lh li l lj lk"><em class="kj">                              </em><strong class="lb io"><em class="kj">y = mx + c</em></strong></span><span id="c64c" class="lf lg in lb b gy ll li l lj lk">where:</span><span id="7f2b" class="lf lg in lb b gy ll li l lj lk"><em class="kj">m = slope<br/>c = intersection</em></span></pre><p id="d5c1" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated"><strong class="jn io">矩阵</strong>:一维以上的向量称为矩阵，即2D、3D等。顺便说一句，深度学习是向量或矩阵的不可知论者，这意味着数据可以是任何形式:1D，2D，3D，…格式。</p><p id="8249" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated"><strong class="jn io"> Tensor/Numpy </strong>:它们是向量和矩阵的数学概括，因此被称为多维数组。张量的几个例子:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi lm"><img src="../Images/3340283447f1ca623acb5b0d2e1c7dc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bMTuzr7ScRNvJVzjR-vD0Q.png"/></div></div></figure><p id="3990" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated">下面的视频将更详细地解释它们:</p><figure class="kl km kn ko gt kp"><div class="bz fp l di"><div class="lr kr l"/></div></figure><p id="ab1f" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated"><strong class="jn io">线性回归</strong>:线性回归试图用观测数据的线性方程(<strong class="jn io"> <em class="kj"> y = mx + c </em> </strong>)来模拟两个变量之间的关系。一个变量被认为是解释变量<strong class="jn io"> <em class="kj"> (x) </em> </strong>，另一个被认为是因变量<strong class="jn io"> <em class="kj"> (y) </em> </strong>。例如，如果您必须预测您所在地区的房价，其中价格是解释变量，而“房屋大小”是因变量。换句话说，线性回归就是将一条直线拟合到由<strong class="jn io"><em class="kj">(x)</em></strong><strong class="jn io"><em class="kj">(y)</em></strong>创建的一组点上。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi ls"><img src="../Images/5b783efe2c44a1e3418722b0aa160f6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yq7ML21TR1Z53wD1TtlzSQ.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk translated">使用最小二乘法的线性回归示例</figcaption></figure><p id="4158" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated">为了对上面的例子有更好的直觉，让我们看下图:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi lt"><img src="../Images/955ce3e1cad89e094d217054802fcca1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S7u40KuqHB7dnt7YPBrTjA.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk translated">最小二乘法</figcaption></figure><p id="c197" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated">直觉上，如果你能想到将它向上或向下移动一点，将会改变所有方块各自的面积(即<strong class="jn io"> a、b、c、d </strong>)。这个想法是，当所有的平方和最小时，就定义了直线的斜率，或者说是最佳拟合直线。</p><p id="8f95" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated">(面积<strong class="jn io"> a </strong> +面积<strong class="jn io"> b </strong> +面积<strong class="jn io"> c </strong> +面积<strong class="jn io"> d </strong>)应该是最小的，将是直线的最佳拟合斜率。</p><p id="0ed3" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated"><strong class="jn io">微积分<em class="kj">(在几张幻灯片中)</em> </strong>:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi lu"><img src="../Images/1f5ac9c26a4a8a14fd6694479bc443ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vbb2hvjRjmBLVP1D23h0gw.png"/></div></div></figure><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi lv"><img src="../Images/701347e236f898bb3a7d89f02c12aff7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fq3tqV3vI63FrERjZlzZfA.png"/></div></div></figure><p id="e50c" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated"><strong class="jn io">特征图/特征矩阵/过滤器/权重/内核:</strong>首先，所有这些术语的意思都是一样的。这些是保存在矩阵中的训练过程中学习到的重量。它们更像一个容器，在训练时保持重量的状态。一旦系统被训练得非常精确，这些权重就被用于预测。</p><p id="b54c" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated"><strong class="jn io">感受野:</strong>神经网络中被滤波器覆盖的小区域称为感受野。根据定义，这种连接的空间范围是一个超参数，称为神经元的<strong class="jn io">感受野</strong>(相当于滤波器大小)。换句话说，过滤器叠加的数据部分是感受野。</p><p id="8740" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated"><strong class="jn io">挤压</strong>:将输入限制为一个有限值。比方说，忽略长宽比，将图像压缩到(宽:224，高:224)大小。</p><p id="f826" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated"><strong class="jn io">激活</strong>:“激活是一个数”，其背后的思想暗示，“<strong class="jn io"> <em class="kj">这个</em> </strong>特征是在<strong class="jn io"> <em class="kj">这个</em> </strong>位置并与<strong class="jn io"> <em class="kj">这个</em> </strong>置信”。训练的权重只不过是激活。比方说，我有一个大小为3x3的过滤器，因此过滤器中的所有9个权重都是激活，它们都代表一个非常小但特定的属性。</p><p id="eb07" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated"><strong class="jn io">激活函数</strong>:它们帮助神经网络捕捉特征，不仅能够学习线性函数，也能够学习非线性函数。换句话说，它们在系统中引入了非线性。非线性在多层网络中很重要，否则，如果没有非线性，系统将给出线性输出。有各种类型的激活:Sigmoid和Tanh，ReLU，leaky ReLU，softmax，…下面的<a class="ae kk" href="https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6" rel="noopener" target="_blank">文章</a>给出了很好的想法。</p><p id="835b" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated"><strong class="jn io">训练数据:</strong>我们用来训练系统的数据，这是网络看到并开始学习的数据。</p><p id="501e" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated">验证数据:作为培训的一部分，我们也验证数据及其结果的准确性。这是相同的数据，通常数据集的一部分图像被搁置起来，稍后用于验证。</p><p id="d1d8" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated"><strong class="jn io">测试数据:</strong>这些图像以前从未被系统看到过，我们对系统进行真正的测试，因为在验证过程中，验证的端口被涂抹，它成为训练数据的一部分，这就是为什么验证结果可能很高，但必须用系统以前从未见过的图像进行真正的测试。</p><p id="99b9" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated"><strong class="jn io">退出</strong>:顾名思义，在训练中随机退出一些早先学习的激活。这是确保神经网络不会过度拟合的方法之一。</p><p id="6c30" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated"><strong class="jn io">历元:</strong>历元是对给定数据集的完整遍历。也就是说，在一个时期结束时，神经网络将已经暴露给数据集内的每个记录一次。</p><p id="8d1c" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated">假设我们的数据集中有100个样本，我们的小批量是10。这意味着10个小批量将覆盖数据集中的所有数据样本。10个小批量完成一遍所用的时间称为1个时期。</p><p id="0e2b" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated"><strong class="jn io">批量/小批量</strong>:批量是在一个时间点通过网络的样本组。这也被称为“批量”或“小批量”。例如:一个历元传递1000幅图像，一个批次的大小是100幅图像，那么我们将需要10个批次来传递数据集中的所有图像。批量大小是一个超级参数。</p><blockquote class="lw"><p id="e4f2" class="lx ly in bd lz ma mb mc md me mf ki dk translated">批量越大=训练越快</p></blockquote><p id="646d" class="pw-post-body-paragraph jl jm in jn b jo mg jq jr js mh ju jv jw mi jy jz ka mj kc kd ke mk kg kh ki ig bi translated">现在，问题是，如果我的机器支持32批次大小，我应该使用这个数字吗？答案是肯定和否定的，确保所有的硬件模块都支持它，如GPU，内存，互连总线。这不仅仅取决于机器能够并行处理多少数据。Keras文件提到以下内容:</p><blockquote class="lw"><p id="5ac5" class="lx ly in bd lz ma mb mc md me mf ki dk translated">一个<strong class="ak">批次</strong>通常比单个输入更接近输入数据的分布。批量越大，近似值越好；然而，批处理将花费更长的时间来处理，并且仍然只导致一次更新，这也是事实。对于推断(评估/预测)，建议在不超出内存的情况下选择尽可能大的批处理大小(因为较大的批处理通常会导致更快的评估/预测)。</p></blockquote><figure class="mm mn mo mp mq kp gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi ml"><img src="../Images/44df1b394aa6bc29a77dd83a30dbbdd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sKyLxWyhJKYkPIDtMqjIcA.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk translated">小批量</figcaption></figure><p id="76a0" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated"><strong class="jn io">特征空间:</strong>如前所述，可以有“n”维矩阵，这些数据可以用“n”维表示，这样的特征空间称为特征空间。当从机器学习模型中提取特征时，我们基于它们的属性对它们进行聚类，这些属性可以很多(很多真的可以是'<em class="kj">很多</em>'，理论上没有限制)。</p><p id="4000" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated"><strong class="jn io">数据拟合</strong>:这是关于你的激活在训练过程中如何正确设置的，因为一个系统应该是良好的通用系统(<em class="kj">良好的准确性</em>)。这就像是过度适应与欠适应以及达到最小局部最优之间的细微界限。如果将数据集适当地分为训练和验证是重要的，则在向前传递和向后传递中的所有数据之后设置权重。一个系统通常应该是一个好的通用器。</p><p id="dc73" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated"><strong class="jn io">模型评估</strong>:在高层次上，我们可以考虑以下类别之一的模型:</p><ol class=""><li id="14a7" class="mr ms in jn b jo jp js jt jw mt ka mu ke mv ki mw mx my mz bi translated"><strong class="jn io">过拟合</strong>:这意味着训练损失远低于验证损失。这样的数据被称为具有高偏倚。</li><li id="12af" class="mr ms in jn b jo na js nb jw nc ka nd ke ne ki mw mx my mz bi translated"><strong class="jn io">欠拟合</strong>:此处训练和验证损失都很高，被视为高方差。</li><li id="e7f9" class="mr ms in jn b jo na js nb jw nc ka nd ke ne ki mw mx my mz bi translated"><strong class="jn io">良好拟合</strong>:训练和验证误差都较低，但略高于训练误差。在即将达到过度拟合之前训练模型总是一个好主意。</li></ol><p id="295c" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated"><strong class="jn io">偏差-方差权衡:</strong>让我们考虑以下损失/误差条件:</p><pre class="kl km kn ko gt la lb lc ld aw le bi"><span id="8b64" class="lf lg in lb b gy lh li l lj lk">+------------+-----------------+----------------------+<br/>| Train loss | Validation loss |         Remark       |<br/>+------------+-----------------+----------------------+<br/>| 2%         | 12%             | High variance        |<br/>| 15%        | 17%             | High bias            |<br/>| 14%        | 32%             | High bias &amp; variance |<br/>| 0.1%       | 0.5%            | Low bias &amp; variance  |<br/>+------------+-----------------+----------------------+</span></pre><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi nf"><img src="../Images/843e40c36c9dbf8113c9b7638ac8d698.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ldHQjwqj-w61RsXctONujQ.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk translated">来源:Jermey的Fastai笔记本</figcaption></figure><p id="675c" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated"><strong class="jn io">图</strong>:这是我们模型的结构，或者说是我们计划训练的模型的架构。它由一个接一个的不同层组成，它们本质上执行以下三种操作:卷积、池化、全连接层等。</p><p id="f980" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated"><strong class="jn io">冻结图形</strong>:训练后，我们的模型学习到大量与图形/模型中所有这些节点相对应的小权重。将图形和相应的权重保存在外部缓冲区意味着冻结图形。这将在以后用于重新加载图表或进行推理。</p><blockquote class="lw"><p id="1aa3" class="lx ly in bd lz ma mb mc md me mf ki dk translated"><em class="jk">他们就像蜂箱里装满了甘露。</em></p></blockquote><p id="0f72" class="pw-post-body-paragraph jl jm in jn b jo mg jq jr js mh ju jv jw mi jy jz ka mj kc kd ke mk kg kh ki ig bi translated"><strong class="jn io"> BatchNormalization </strong>:非常重要的一点是，在输入DNN之前，输入数据的预处理也必须对数据进行归一化处理。同样，我们也将DNN层之间的数据标准化，这称为BatchNorm。这是它们进入下一个神经网络层之前的数据处理。它在相同的尺度上转换数据，并避免任何不稳定性。这有两个部分:</p><ol class=""><li id="8029" class="mr ms in jn b jo jp js jt jw mt ka mu ke mv ki mw mx my mz bi translated">归一化:对于DNN，通常我们在从<em class="kj"> 0到1 </em>的范围内缩小数据。让我们假设我有<em class="kj"> data_values = [1，200，100000，500] </em>。这是一个相当大的值范围，我们需要将它们缩小到0到1的范围，中间有很多浮点。这一点很重要，否则网络将变得不稳定，数据点的范围很广，训练将花费很长时间，也不能保证网络将正确收敛。</li><li id="a512" class="mr ms in jn b jo na js nb jw nc ka nd ke ne ki mw mx my mz bi translated">标准化:这实际上是计算Z分数，并使数据集的均值为“零”，标准差为“一”。</li></ol><pre class="kl km kn ko gt la lb lc ld aw le bi"><span id="d74b" class="lf lg in lb b gy lh li l lj lk">                                  x — m<br/>                              Z = -----<br/>                                    S<br/>Where: <br/>x = data point<br/>m = Mean of dataset<br/>S = Standard deviation</span></pre><p id="eb86" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated">关于tensorflow batch-norm的大文章<a class="ae kk" href="http://ruishu.io/2016/12/27/batchnorm/" rel="noopener ugc nofollow" target="_blank">和</a>。</p><p id="9c8a" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated"><strong class="jn io">量化</strong>:在归一化过程中，数字被转换为浮点，为了在内存中存储单精度浮点格式(IEEE-754)，我们需要32位内存。量化是将浮点内存存储从32位减少到8位的过程(事实上，就我所知，还可以进一步减少)。这也影响准确性，但不是很明显，因此这是一个权衡。</p><p id="4db8" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated"><strong class="jn io">最后一层激活&amp;损失函数:</strong></p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi ng"><img src="../Images/9ce895d2c6297f0743097100a80f0f34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZpC5YbLpHAIW-iNslsU43w.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk translated">最后一层激活和损失函数</figcaption></figure><p id="2604" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated"><strong class="jn io">均方误差(MSE)与二元交叉熵(BCE)/对数损失:</strong></p><p id="75d7" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated">MSE损失函数给出连续值，因此用于回归问题。而BCE给出离散值，即0类或1类，因此用于逻辑回归问题。如果你看下面的图表，你会知道梯度值增加非常快，并惩罚模型权重。这就是我们在输出中得到离散值的方式。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/1c8f78e6f22ba6801e0f2504773811fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*6-Io7Ff_jCzHV0CFIy1n4g.png"/></div></figure><p id="4046" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated"><em class="kj">有许多重要的遗漏项，但这需要对神经网络有所了解，因此将其保留为</em> <a class="ae kk" href="https://medium.com/@tomdeore/deep-learning-in-gradient-descent-style-part-2-e159e2cf8a99" rel="noopener"> <em class="kj">第二部分</em> </a> <em class="kj">。</em></p><p id="d62f" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated">在下一篇文章中，将涵盖更大的神经网络概念。</p><p id="1da2" class="pw-post-body-paragraph jl jm in jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ig bi translated"><strong class="jn io">可以在|</strong><a class="ae kk" href="https://www.linkedin.com/in/mdeore/" rel="noopener ugc nofollow" target="_blank"><strong class="jn io">LinkedIn</strong></a><strong class="jn io">|</strong><a class="ae kk" href="https://tomdeore.wixsite.com/epoch" rel="noopener ugc nofollow" target="_blank"><strong class="jn io">网站</strong></a><strong class="jn io">|</strong><a class="ae kk" href="https://github.com/milinddeore" rel="noopener ugc nofollow" target="_blank"><strong class="jn io">Github</strong></a><strong class="jn io">|</strong></p></div></div>    
</body>
</html>