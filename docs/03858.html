<html>
<head>
<title>Automating Hadoop Cluster Deployment on AWS using Ansible</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Ansible 在 AWS 上自动化 Hadoop 集群部署</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/automate-hadoop-cluster-deployment-on-top-of-aws-using-ansible-194b623b9103?source=collection_archive---------2-----------------------#2020-12-26">https://blog.devgenius.io/automate-hadoop-cluster-deployment-on-top-of-aws-using-ansible-194b623b9103?source=collection_archive---------2-----------------------#2020-12-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="cf3e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">本教程将带您完成使用 Ansible 在 AWS 上部署 Hadoop 集群的过程</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/3e53e59e5b9cb2bcccc6e50045dd4e8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d-S0uLimWA6ILqjyUvJZ5Q.png"/></div></div></figure><p id="668b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">嘿大家好，</p><p id="5025" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在本文中，我将向您展示一个有趣的<strong class="kt ir">自动化</strong>，其中我们将<strong class="kt ir">在 AWS Cloud (EC2) </strong>之上设置 Hadoop 集群(HDFS)，并且我们将使用一个名为<a class="ae ln" href="https://www.ansible.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="kt ir"> Ansible </strong> </a> <strong class="kt ir"> </strong>的工具来做所有事情，这是最好的<strong class="kt ir"> DevOps </strong> <strong class="kt ir">自动化工具之一。</strong>本文中最棒的是，我们将使用 Ansible 的<strong class="kt ir">角色</strong>和<strong class="kt ir">动态库存</strong>概念，使整个过程更加<strong class="kt ir">动态。</strong></p><p id="98a6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我希望你对<strong class="kt ir"> AWS </strong>有些熟悉，也许你已经启动了<strong class="kt ir"> EC2 实例</strong>，或者类似的。并且也有一些关于<strong class="kt ir"> Ansible </strong>和<strong class="kt ir"> Hadoop 的基础知识。你可以很容易地在互联网上找到关于这些技术的信息，在这篇文章中，我们的主要目的是整合所有这些技术。</strong></p><p id="2e27" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">让我们了解一下我们实际上要做些什么来设置这种自动化。</strong></p><p id="0fa8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi lo translated"><span class="l lp lq lr bm ls lt lu lv lw di">一个</span>一个<strong class="kt ir">根大！！！</strong></p><p id="7e5b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在现实世界中，我们通常处理<strong class="kt ir"> BigData </strong>并且这个 BigData 的大小在<strong class="kt ir"> Peta 或 EB 或更多。</strong>为了处理这些大数据，我们使用了像<strong class="kt ir"> Hadoop 这样的技术。</strong>但是为了存储如此庞大的数据，我们需要越来越多的存储设备或资源<strong class="kt ir">这将增加我们的成本</strong>。但是我们可以使用云存储来存储这些大数据，因为云提供商提供几乎无限的存储，如<strong class="kt ir"> AWS S3、GCP 存储</strong>，从维护、可用性、可靠性等方面来看，这将是一个更便宜、更好的选择。</p><p id="a4bf" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">因此，我们将启动<strong class="kt ir"> EC2 </strong>实例来设置我们的 Hadoop 集群，即<strong class="kt ir"> NameNode </strong>和<strong class="kt ir">datanode。</strong>然后，我们将创建 EBS 卷并将其与 DataNodes 连接起来，以存储 HDFS 集群处理的大数据。为了设置所有这些东西，我们将使用<strong class="kt ir"> Ansible </strong>来自动化我们的工作。</p><p id="337e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">那么让我们动手实践吧！</p><p id="8e2b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><a class="ae ln" href="https://www.linkedin.com/pulse/cicd-pipeline-deploying-webserver-docker-using-ansible-ajay-pathak/" rel="noopener ugc nofollow" target="_blank">要了解 Ansible 的安装请参考我之前的文章</a></p><h1 id="9d95" class="lx ly iq bd lz ma mb mc md me mf mg mh jw mi jx mj jz mk ka ml kc mm kd mn mo bi translated">设置 AWS 可行的动态库存</h1><p id="d94e" class="pw-post-body-paragraph kr ks iq kt b ku mp jr kw kx mq ju kz la mr lc ld le ms lg lh li mt lk ll lm ij bi translated">现在，首先我们必须使用 Ansible 启动 EC2 实例。但在此之前，我们必须在 ansible 中设置更多的东西，以便我们能够使用 Ansible 在 AWS 上进行<strong class="kt ir">供应。</strong></p><p id="8a12" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">首先，我们必须设置一个可响应的<strong class="kt ir">动态清单</strong>，这样它就能够获取所有 EC2 实例，然后能够配置这些实例。</p><p id="9861" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><em class="mu"> Dynamic inventory 是一个 ansible 插件，它对 AWS 进行 API 调用，以在运行时获取实例信息。它为您提供了动态的 ec2 实例细节来管理 AWS 基础设施。</em></p><p id="6013" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">要在 ansible 中设置动态清单，最常用的方法是使用预先创建的 EC2 python 文件。但是代替它，我们将使用一个 ansible 动态清单插件，即<strong class="kt ir"> aws_ec2。</strong></p><blockquote class="mv mw mx"><p id="9bc5" class="kr ks mu kt b ku kv jr kw kx ky ju kz my lb lc ld mz lf lg lh na lj lk ll lm ij bi translated">因此<strong class="kt ir"> aws_ec2 插件</strong>是管理 AWS EC2 Linux 实例的好方法，而不必维护标准的本地清单。作为 AWS EC2 实例的代码，这将允许更容易的 Linux 自动化、配置管理和基础设施。</p></blockquote><p id="b914" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了使用 Ansible for AWS，我们必须安装<strong class="kt ir"> boto、boto3、</strong>和<strong class="kt ir"> botocore </strong> Python 库</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="670d" class="ng ly iq nc b gy nh ni l nj nk">pip3 install boto boto3</span></pre><p id="e7a5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">要启用<strong class="kt ir"> aws_ec2 </strong>插件，我们必须将以下语句添加到<strong class="kt ir"> ansible.cfg </strong>文件中:</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="9ab6" class="ng ly iq nc b gy nh ni l nj nk">enable_plugins = aws_ec2</span></pre><p id="d050" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">所以我们的 ansible.cfg 文件看起来像这样</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/de99313836400e309d4ec108c245d15a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hQhP3ksHiwkJNgJaw6eUeQ.png"/></div></div><figcaption class="nm nn gj gh gi no np bd b be z dk translated">ansible.cfg 文件示例</figcaption></figure><p id="4e2f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在上图中，你可以看到我的目录路径是<strong class="kt ir"> ansible_plugins，</strong>所以这是一种管理所有 ansible 插件的便捷方式，就像如果我们有多个 AWS 帐户或不同的云帐户，那么我们可以将所有相关的插件放在一个目录中。</p><p id="3b75" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在，在这之后，我们必须为 aws_ec2 插件创建一个<strong class="kt ir">配置文件，</strong>这里我已经创建了<strong class="kt ir"> aws_ec2.yaml </strong>文件，如下所示</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="d72a" class="ng ly iq nc b gy nh ni l nj nk">---<br/>plugin: aws_ec2<br/>aws_access_key: <!-- -->&lt;YOUR-AWS-ACCESS-KEY-HERE&gt;<br/>aws_secret_key: <!-- -->&lt;YOUR-AWS-SECRET-KEY-HERE&gt;</span><span id="fd6f" class="ng ly iq nc b gy nq ni l nj nk">regions:<br/>  - ap-south-1</span><span id="5c32" class="ng ly iq nc b gy nq ni l nj nk">strict: False<br/>keyed_groups:<br/>  - key: tags<br/>    prefix: tag<br/>  - key: placement.region<br/>    prefix: aws_region</span></pre><p id="066d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这是一个简单的配置文件，您可以根据需要修改它。所以在上面的文件中，注意<strong class="kt ir"> keyed_groups </strong>关键字，在它下面我们基本上写了我们想要如何根据标签、区域、实例类型等对实例进行分组。这里我使用了<strong class="kt ir">标签</strong>来对我的 HDFS 集群 NameNode 和 DataNodes 实例进行分组。</p><p id="1d3c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">通过如下列出所有 EC2 实例来测试动态清单</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="00c0" class="ng ly iq nc b gy nh ni l nj nk">ansible-inventory --list</span></pre><h1 id="6393" class="lx ly iq bd lz ma mb mc md me mf mg mh jw mi jx mj jz mk ka ml kc mm kd mn mo bi translated">创建可变角色</h1><blockquote class="mv mw mx"><p id="4422" class="kr ks mu kt b ku kv jr kw kx ky ju kz my lb lc ld mz lf lg lh na lj lk ll lm ij bi translated">在 Ansible 中，<strong class="kt ir">角色</strong>基本上是一种将我们写在剧本中的所有东西分组的方法，或者说它是将剧本分成多个文件的主要机制。所以角色是变量、任务、文件、模板和模块的集合。这简化了复杂行动手册的编写，并且使它们更容易重用。</p></blockquote><p id="addc" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在 ansible 中创建一个角色目录结构，我们可以这样使用<strong class="kt ir"> ansible-galaxy </strong>命令</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="9d8a" class="ng ly iq nc b gy nh ni l nj nk">ansible-galaxy init &lt;role name&gt;</span></pre><p id="b8eb" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">或者，如果我们的组有限，我们也可以手动创建此结构。</p><p id="7624" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在，我们将创建 3 个角色，分别是:</p><ul class=""><li id="42a7" class="nr ns iq kt b ku kv kx ky la nt le nu li nv lm nw nx ny nz bi translated">用于启动 EC2 实例来设置 HDFS 集群</li><li id="c1c7" class="nr ns iq kt b ku oa kx ob la oc le od li oe lm nw nx ny nz bi translated">将实例配置为 Hadoop Master (NameNode)</li><li id="cbe3" class="nr ns iq kt b ku oa kx ob la oc le od li oe lm nw nx ny nz bi translated">将实例配置为 Hadoop 从节点(DataNodes)</li></ul><h1 id="82bc" class="lx ly iq bd lz ma mb mc md me mf mg mh jw mi jx mj jz mk ka ml kc mm kd mn mo bi translated">创建角色以启动 EC2 实例</h1><p id="b071" class="pw-post-body-paragraph kr ks iq kt b ku mp jr kw kx mq ju kz la mr lc ld le ms lg lh li mt lk ll lm ij bi translated">在这个角色中，我们将创建一个<strong class="kt ir">任务</strong>文件和一个<strong class="kt ir">变量</strong>文件。</p><p id="29cc" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">下面是任务文件，其中包含 Ansible 将要执行的所有任务</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/c64fae62b9d65b103b96686284e2d805.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f8upI_3d-5AiDY-VgKKxjQ.png"/></div></div></figure><p id="352f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在上面的文件中，你可以看到我使用了变量和一些用户定义的变量，比如实例和标签的数量，我们将使用这些变量来启动具有单个角色的 namenode 和 datanodes 实例。请注意，我使用了一个名为<strong class="kt ir"> meta </strong>的模块，它会在我们启动某个实例时自动刷新我们的动态清单。</p><p id="99ed" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">该角色的变量文件</strong></p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="dbda" class="ng ly iq nc b gy nh ni l nj nk">---<br/>region: "ap-south-1"<br/>ami_id: "ami-0a9d27a9f4f5c0efc"<br/>instance_type: "t2.micro"<br/>subnet_id: "subnet-2e8ee562"<br/>sg_id: "sg-007e984dff1d14721"<br/>key: "taskoskey"</span></pre><p id="a453" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我附加到所有实例的安全组只允许 SSH 和 HDFS 服务在 9001 端口上运行。</p><h1 id="3245" class="lx ly iq bd lz ma mb mc md me mf mg mh jw mi jx mj jz mk ka ml kc mm kd mn mo bi translated">创建角色以配置 HDFS 集群</h1><p id="794d" class="pw-post-body-paragraph kr ks iq kt b ku mp jr kw kx mq ju kz la mr lc ld le ms lg lh li mt lk ll lm ij bi translated"><em class="mu">要使用 ansible 配置 HDFS 集群，您需要具备一些基本知识，如</em> <strong class="kt ir"> <em class="mu">如何手动设置 HDFS 集群</em> </strong> <em class="mu">。但是如果你是 Hadoop 新手，不知道如何设置 HDFS，那么你可以看看我的</em> <a class="ae ln" href="https://www.linkedin.com/pulse/setting-up-hadoop-cluster-hdfs-locally-ajay-pathak" rel="noopener ugc nofollow" target="_blank"> <em class="mu">这篇文章，它是在本地设置 HDFS 的分步指南。</em>T19】</a></p><p id="89a2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">让我们看看如何用 Ansible 设置 HDFS</p><h2 id="7f68" class="ng ly iq bd lz of og dn md oh oi dp mh la oj ok mj le ol om ml li on oo mn op bi translated">正在为 NameNode 创建角色</h2><p id="4749" class="pw-post-body-paragraph kr ks iq kt b ku mp jr kw kx mq ju kz la mr lc ld le ms lg lh li mt lk ll lm ij bi translated">在这个角色中，我们主要有 3 个目录，即<strong class="kt ir">任务、模板、</strong>和<strong class="kt ir">var。</strong></p><p id="fd80" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这里是<strong class="kt ir">任务</strong>文件，它包含了我们希望 Ansible 执行的所有任务，以便配置 NameNode，它通常会执行这些任务</p><ul class=""><li id="a4ce" class="nr ns iq kt b ku kv kx ky la nt le nu li nv lm nw nx ny nz bi translated">安装 Hadoop 和 java</li><li id="0a6b" class="nr ns iq kt b ku oa kx ob la oc le od li oe lm nw nx ny nz bi translated">将实例配置为 NameNode 并启动其服务</li><li id="53e5" class="nr ns iq kt b ku oa kx ob la oc le od li oe lm nw nx ny nz bi translated">安装 EBS 卷及其 NameNode 数据目录</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="8f59" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在在<strong class="kt ir"> templates </strong>目录下有两个 Hadoop 文件，即<strong class="kt ir"> hdfs-site.xml </strong>和<strong class="kt ir"> core-site.xml </strong>，ansible 会将它们复制到 NameNode 的<strong class="kt ir"> /etc/hadoop </strong>目录下，并用它们的值替换一些变量。而这些文件也被称为<strong class="kt ir"> jinja 模板。</strong>所以这些文件中的内容是</p><p id="eac6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> hdfs-site.xml 文件</strong></p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="54d7" class="ng ly iq nc b gy nh ni l nj nk">&lt;?xml version="1.0"?&gt;<br/>&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span><span id="aa91" class="ng ly iq nc b gy nq ni l nj nk">&lt;!-- Put site-specific property overrides in this file. --&gt;</span><span id="ce66" class="ng ly iq nc b gy nq ni l nj nk">&lt;configuration&gt;</span><span id="906d" class="ng ly iq nc b gy nq ni l nj nk">&lt;property&gt;<br/>&lt;name&gt;dfs.{{val}}.dir&lt;/name&gt;<br/>&lt;value&gt;{{hdfs_dir}}&lt;/value&gt;<br/>&lt;/property&gt;</span><span id="148c" class="ng ly iq nc b gy nq ni l nj nk">&lt;/configuration&gt;</span></pre><p id="b66e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> core-site.xml 文件</strong></p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="9be7" class="ng ly iq nc b gy nh ni l nj nk">&lt;?xml version="1.0"?&gt;<br/>&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span><span id="88ef" class="ng ly iq nc b gy nq ni l nj nk">&lt;!-- Put site-specific property overrides in this file. --&gt;</span><span id="4d61" class="ng ly iq nc b gy nq ni l nj nk">&lt;configuration&gt;</span><span id="8dec" class="ng ly iq nc b gy nq ni l nj nk">&lt;property&gt;<br/>&lt;name&gt;fs.default.name&lt;/name&gt;<br/>&lt;value&gt;hdfs://{{master_ip}}:{{hdfs_port}}&lt;/value&gt;<br/>&lt;/property&gt;</span><span id="fcbc" class="ng ly iq nc b gy nq ni l nj nk">&lt;/configuration&gt;</span></pre><p id="f36e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这是这个角色的变量</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="88b7" class="ng ly iq nc b gy nh ni l nj nk">master_ip: "0.0.0.0"<br/>val: "name"<br/>hdfs_dir: "/master_hdfs"<br/>hadoop_dir: "/etc/hadoop"<br/>hdfs_port: 9001</span></pre><h2 id="d4c3" class="ng ly iq bd lz of og dn md oh oi dp mh la oj ok mj le ol om ml li on oo mn op bi translated">为 DataNodes 创建角色</h2><p id="417f" class="pw-post-body-paragraph kr ks iq kt b ku mp jr kw kx mq ju kz la mr lc ld le ms lg lh li mt lk ll lm ij bi translated"><em class="mu">配置 DataNode 的角色结构与 NameNode 角色相同，在任务文件和变量文件中有一些小的变化。</em></p><p id="6989" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">由于在 Hadoop 中我们不格式化 DataNode，因此我们必须删除执行格式化的任务，并且我们使用不同的命令来启动 DataNode 服务，因此我们也必须删除此任务。因此，通常我们只需从 NameNode 任务文件中删除 2 个任务，只添加下面的任务，其他任务保持不变。</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="15f3" class="ng ly iq nc b gy nh ni l nj nk">- name: Starting Hadoop DataNode services<br/>  command: "hadoop-daemon.sh start datanode"<br/>  ignore_errors: true</span></pre><p id="6b49" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这个角色的变量文件是</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="36bc" class="ng ly iq nc b gy nh ni l nj nk">master_ip: "{{ groups.tag_Hadoop_namenode[0] }}"<br/>val: "data"<br/>hdfs_dir: "/slave_hdfs"<br/>hadoop_dir: "/etc/hadoop"<br/>hdfs_port: 9001</span></pre><p id="55dc" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在已经创建了所有需要的角色，动态清单也准备好了。现在，我们只需创建一个最终的可行剧本，运行所有这些角色，并在 AWS 上设置我们的 HDFS 集群。</p><p id="478f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这是剧本</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="3527" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在我们只需要像这样运行这个剧本</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="e27c" class="ng ly iq nc b gy nh ni l nj nk">ansible-playbook setup.yml</span></pre><p id="281d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">仅仅这一条命令就可以为我们做一切事情，从启动 EC2 实例到将它们配置为 NameNode 和 DataNodes。</p><p id="931f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">该命令的输出</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi os"><img src="../Images/9ad1f37a723fea756298218b0e5f1c35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hjj0EV5WoKf77P50dwAnDA.png"/></div></div><figcaption class="nm nn gj gh gi no np bd b be z dk translated">产出 1</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ot"><img src="../Images/fb60fba8cc92dbedbbfb259724485359.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nVd8zoNOQijQVWoYtFckeA.png"/></div></div><figcaption class="nm nn gj gh gi no np bd b be z dk translated">产出 2</figcaption></figure><p id="0649" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在我们的 HDFS 集群已经准备好了，但是最好交叉检查所有的 DataNodes 是否都与 NameNode 连接。为此，我们必须对任何 DataNode 或 NameNode 执行 SSH，然后运行这个命令</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="b5a6" class="ng ly iq nc b gy nh ni l nj nk">hadoop dfsadmin -report</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ou"><img src="../Images/9064a608b95375db26aeb7edd6943b2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7b5c3_bIc4epk4LpDUgQ6Q.jpeg"/></div></div></figure><p id="e35a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">在上图中，您可以看到所有 3 个 DataNodes 都成功连接到 NameNode，并将其存储贡献给与它们连接的 EBS 卷所在的群集。</strong></p><p id="59d2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在，我们的 Hadoop 集群已经成功部署在 AWS 上，我们现在可以使用它来处理和存储大数据，并且我们的所有数据都存储在安全且灵活的 AWS EBS 卷上。ansible 帮助我们实现这种自动化，并设置完整的 HDFS 集群及其高级功能，即动态库存和角色，使整个流程保持动态。</p><blockquote class="mv mw mx"><p id="90f8" class="kr ks mu kt b ku kv jr kw kx ky ju kz my lb lc ld mz lf lg lh na lj lk ll lm ij bi translated">但是，在这方面，我们还有一些工作要做，就像你注意到的那样，我们已经使用 Ansible 为集群提供 EC2 实例，但是现在我们使用不同的工具在云上提供资源，例如 Terraform，虽然我们可以使用 Ansible 进行提供，但是 Ansible 实际上是用于配置管理的，Terraform 是比它更好的选择。</p><p id="4c95" class="kr ks mu kt b ku kv jr kw kx ky ju kz my lb lc ld mz lf lg lh na lj lk ll lm ij bi translated"><strong class="kt ir">因此，在本文的第 2 部分，我将向您展示如何集成 Terraform 和 Ansible 来完成相同的任务，并使 Ansible 剧本更加动态。</strong></p></blockquote><p id="4151" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我已经在我的<a class="ae ln" href="https://github.com/Ajaypathak372/ansible-hadoop-aws.git" rel="noopener ugc nofollow" target="_blank"> GitHub repo </a>上上传了所有的文件和剧本。</p><p id="cd5c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">要了解 Ansible 的安装，点击<a class="ae ln" href="https://www.linkedin.com/pulse/cicd-pipeline-deploying-webserver-docker-using-ansible-ajay-pathak/" rel="noopener ugc nofollow" target="_blank"> <strong class="kt ir">这里</strong> </a></p><p id="0533" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">要知道如何一步一步手动设置 HDFS，点击<a class="ae ln" href="https://www.linkedin.com/pulse/setting-up-hadoop-cluster-hdfs-locally-ajay-pathak" rel="noopener ugc nofollow" target="_blank"> <strong class="kt ir">这里</strong> </a></p><p id="f335" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">感谢阅读！！！</strong></p><p id="c554" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如果你喜欢我的作品，请继续关注我的作品。</p><p id="2299" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在<a class="ae ln" href="http://www.linkedin.com/in/ajay-pathak372" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上与我联系。</p></div></div>    
</body>
</html>