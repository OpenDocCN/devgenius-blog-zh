<html>
<head>
<title>Basic ETL using Pyspark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Pyspark 的基本 ETL</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/basic-etl-using-pyspark-ed08b7e53cf4?source=collection_archive---------1-----------------------#2022-09-02">https://blog.devgenius.io/basic-etl-using-pyspark-ed08b7e53cf4?source=collection_archive---------1-----------------------#2022-09-02</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><p id="0668" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">在本文中，我们将使用 PySpark 执行 ETL 操作。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/d0f1d4ba0b808643300514bd6d53dc99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*g1fVfiHVhUpXCXnwi5zqGQ.png"/></div></figure><p id="4ddd" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我们使用两种类型的源代码，MySQL 作为数据库，CSV 文件作为文件系统。摘录 2。变形 3。加载。</p><blockquote class="kq kr ks"><p id="943d" class="jk jl kt jm b jn jo jp jq jr js jt ju ku jw jx jy kv ka kb kc kw ke kf kg kh ig bi translated">我们总共有 3 个数据源——两个表 CITY、COUNTRY 和一个 csv 文件 COUNTRY_LANGUAGE.csv</p></blockquote><p id="4140" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我们将创建 4 个 python 文件。</p><p id="d378" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io"> <em class="kt"> job.py </em> </strong> —逐步执行 ETL 操作(提取转换和加载)<br/><strong class="jm io"><em class="kt">constant . py</em></strong>—存储所有常量细节，如列、spark 实例和与连接相关的列。<br/><strong class="jm io"><em class="kt">Extract . py</em></strong>—通过创建 dataframe 提取源数据。<br/><strong class="jm io"><em class="kt">transform . py</em></strong>—所有的转换逻辑都在这里。我们将使用提取的数据帧来完成所有的转换。<br/><strong class="jm io"><em class="kt">load . py</em></strong>—将清理后的数据存储到数据库或文件系统中。</p><p id="e5b4" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">首先，我们将提取所有数据并创建一个 spark 数据帧，然后我们将对这些数据帧进行转换。</p><p id="4bdc" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我们先在<strong class="jm io"> <em class="kt"> constant.py </em> </strong>下创建函数<strong class="jm io"> <em class="kt"> spark_inst() </em> </strong>来发起 SparkSession。每当我们需要一个 Spark 实例时，我们可以调用<strong class="jm io"> <em class="kt"> spark_inst() </em> </strong>函数。</p><pre class="kj kk kl km gt kx ky kz bn la lb bi"><span id="e549" class="lc ld in ky b be le lf l lg lh">from pyspark.sql import SparkSession<br/><br/>#  Using this function as spark instance<br/>def spark_inst():<br/>    return SparkSession.builder.master("local[*]")\<br/>           .appName('Spark')\<br/>           .getOrCreate()</span></pre><p id="2afe" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">现在，我们将使用 extract.py 文件提取源数据，在该文件中我们创建了 extract 函数。在此之下，我们将为不同的数据源创建数据框架。</p><p id="d6bd" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我们正在从不同的源读取数据，以确定我们正在使用哪个源，我们必须创建一个<strong class="jm io"> <em class="kt">类型</em> </strong>参数来标识源，即数据库或文件系统，并基于此创建数据帧。MySQL 的“JDBC”和文件系统的“CSV”。</p><pre class="kj kk kl km gt kx ky kz bn la lb bi"><span id="0d47" class="lc ld in ky b be le lf l lg lh">#extract.py<br/><br/>from pyspark.sql import SparkSession<br/><br/><br/>def extract(spark: SparkSession, type: str, source: str):<br/><br/>    # Read data from mysql database<br/>    if type=="JDBC":<br/>       output_df =     spark.read.format("JDBC").options(url='jdbc:mysql://localhost/world',dbtable=source,driver='com.mysql.cj.jdbc.Driver',user='root',password='root').load()<br/>       return output_df<br/><br/>    if type=="CSV":<br/>    # read data from filesystem<br/>       output_df = spark.read.format("CSV").options(header=True,inferSchema=True).load(source)<br/>       return output_df</span></pre><p id="82bb" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">下面是创建的数据框架的模式和列。</p><p id="8466" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">CITY<br/>|—ID:integer(nullable = true)<br/>|—Name:string(nullable = true)<br/>|—country code:string(nullable = true)<br/>|—District:string(nullable = true)<br/>|—Population:integer(nullable = true)</p><p id="4770" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">国家<br/> | —代码:string (nullable = true) <br/> | —名称:string (nullable = true) <br/> | —洲:string (nullable = true) <br/> | —地区:string (nullable = true) <br/> | —表面积:decimal(10，2)(nullable = true)<br/>|—indepiear:integer(nullable = true)<br/>|—人口:integer (nullable = true) <br/> | —寿命:decimate(3 <br/>|—local name:string(nullable = true)<br/>|—government form:string(nullable = true)<br/>|—head of state:string(nullable = true)<br/>|—Capital:integer(nullable = true)<br/>|—code 2:string(nullable = true)</p><p id="0c4a" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">COUNTRY _ LANGUAGE<br/>|—COUNTRY code:string(nullable = true)<br/>|—LANGUAGE:string(nullable = true)<br/>|—iso official:string(nullable = true)<br/>|—Percentage:double(nullable = true)</p><p id="9ee2" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">现在我们必须转换这些数据，在此之前我们需要列细节，来声明那些我们使用的<strong class="jm io"> <em class="kt"> constant.py </em> </strong>文件。</p><p id="60f8" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">在转换中，我们将重命名列，我们必须映射旧列名和新列名，为此，我们将使用字典，因为它具有键-值对，我们将使用键作为旧列名，使用值作为新列名。下面我们已经声明了基于所有三个来源的所有新旧列。</p><pre class="kj kk kl km gt kx ky kz bn la lb bi"><span id="d9da" class="lc ld in ky b be le lf l lg lh"># constant.py<br/><br/>CITY_COL_DICT={<br/>     "ID": "city_id",<br/>     "Name": "city_name",<br/>     "CountryCode": "country_code",<br/>     "District": "city_district",<br/>     "Population": "city_population"<br/>}<br/>COUNTRY_COL_DICT={<br/>     "Code": "country_code",<br/>     "Name": "country_name",<br/>     "Continent": "continent",<br/>     "Region": "region",<br/>     "SurfaceArea": "surface_area",<br/>     "IndepYear": "independence_year",<br/>     "Population": "country_population",<br/>     "LifeExpectancy": "life_expectancy",<br/>     "GNP": "gross_national_product",<br/>     "GNPOld": "old_gross_national_product",<br/>     "LocalName": "local_name",<br/>     "GovernmentForm": "government_form",<br/>     "HeadOfState": "head_of_state",<br/>     "Capital": "capital",<br/>     "Code2": "country_code_2"<br/>}<br/>COUNTRY_LANGUAGE_COL_DICT={<br/>     "CountryCode": "country_code",<br/>     "Language": "language",<br/>     "IsOfficial": "is_official_language",<br/>     "Percentage": "language_percentage"<br/>}</span></pre><p id="cff0" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">正如我们所看到的,“country_code”在所有数据帧中都很常见，我们将使用该列进行连接操作，之后，我们将只选择必需的列，而不是所有列。所以我们也声明一下。</p><pre class="kj kk kl km gt kx ky kz bn la lb bi"><span id="07bf" class="lc ld in ky b be le lf l lg lh"># constant.py<br/><br/>JOIN_ON_COLUMNS=['country_code']<br/>JOIN_TYPE="left"<br/>SPEC_COLS=[<br/>     "country_code",<br/>     "country_name",<br/>     "region",<br/>     "surface_area",<br/>     "independence_year",<br/>     "country_population",<br/>     "life_expectancy",<br/>     "local_name",<br/>     "head_of_state",<br/>     "capital",<br/>     "country_code_2",<br/>     "city_id",<br/>     "city_name",<br/>     "city_district",<br/>     "city_population",<br/>     "language",<br/>     "is_official_language",<br/>     "language_percentage"<br/>]</span></pre><p id="a182" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">下面我们在 transform.py 中添加转换逻辑，如下图。</p><ol class=""><li id="f85f" class="li lj in jm b jn jo jr js jv lk jz ll kd lm kh ln lo lp lq bi translated">rename _ cols 重命名所有列。</li><li id="8a84" class="li lj in jm b jn lr jr ls jv lt jz lu kd lv kh ln lo lp lq bi translated">specific_cols —仅提取所需的列。</li><li id="29ac" class="li lj in jm b jn lr jr ls jv lt jz lu kd lv kh ln lo lp lq bi translated">join _ df 基于“国家代码”连接数据框架</li></ol><pre class="kj kk kl km gt kx ky kz bn la lb bi"><span id="17a4" class="lc ld in ky b be le lf l lg lh"># transform.py<br/><br/>from pyspark.sql import DataFrame<br/><br/><br/>def rename_cols(df: DataFrame, mapping_dict: dict) -&gt;DataFrame:<br/>    # Rename all the columns<br/>    '''<br/>    :param df: input dataframe<br/>    :param mapping_dict: dict of columns names<br/>    :return: ouput dataframe<br/>    '''<br/>    for key in mapping_dict.keys():<br/>        df=df.withColumnRenamed(key,mapping_dict.get(key))<br/>    return df<br/><br/><br/>def specific_cols(df: DataFrame, specific_cols: list):<br/>    # get specific cols df<br/>    '''<br/>    :param df: input dataframe <br/>    :param specific_cols: list of columns names<br/>    :return: ouput dataframe<br/>    '''<br/>    return df.select(specific_cols)<br/><br/><br/><br/>def join_df(left_df: DataFrame, right_df: DataFrame, ON_COLUMNS:list, JOIN_TYPE: str)-&gt;DataFrame:<br/>    # Join two dataframes<br/>    '''<br/>    :param left_df: input dataframe<br/>    :param right_df: input dataframe<br/>    :param ON_COLUMNS: list of columns to perform join<br/>    :param JOIN_TYPE: Join type<br/>    :return: ouput dataframe<br/>    '''<br/>    output_df=left_df.alias("left_df").join(right_df.alias\("right_df"), ON_COLUMNS, JOIN_TYPE)<br/>    return output_df</span></pre><p id="8b74" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">现在让我们创建 load.py 文件，并编写代码将清理后的数据存储到 MySQL 和文件系统中。</p><pre class="kj kk kl km gt kx ky kz bn la lb bi"><span id="6d1a" class="lc ld in ky b be le lf l lg lh"># load.py<br/><br/>from pyspark.sql import DataFrame<br/><br/><br/>def load(type: str, df: DataFrame, target: str):<br/>    # Load the data based on type<br/>    '''<br/>    :param type: Input Storage type (JDBC|CSV) Based on type data stored in MySQL or FileSystem<br/>    :param df: Input Dataframe<br/>    :param target: Input target <br/>             -For filesystem - Location where to store the data<br/>             -For MySQL - table name<br/>    '''<br/><br/>    # Write data on mysql database with table name<br/>    if type=="JDBC":<br/>       df.write.format("JDBC").mode("overwrite")\<br/>.options(url='jdbc:mysql://localhost/world',dbtable=target,driver='com.mysql.cj.jdbc.Driver',user='root',password='root').save()<br/>       print(f"Data succesfully loaded to MySQL Database !!")<br/>    <br/>    if type=="CSV":<br/>    # Write data on filesystem<br/>       df.write.format("CSV").mode("overwrite").options(header=True).save(target)<br/>       print(f"Data succesfully loaded to filesystem !!")</span></pre><p id="19bd" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">为了在 ETL 步骤中执行代码，我们将使用<strong class="jm io"> <em class="kt"> job.py </em> </strong>文件执行所有函数。我们将把所有的模块(extract.py，transform.py，load.py)和常量变量(constant.py)导入到 job.py 中</p><pre class="kj kk kl km gt kx ky kz bn la lb bi"><span id="d12a" class="lc ld in ky b be le lf l lg lh"># job.py<br/><br/># Imported required libraries and modules<br/>from pyspark.sql import SparkSession<br/>from metadata.constant import CITY_COL_DICT, COUNTRY_LANGUAGE_COL_DICT, COUNTRY_COL_DICT, \<br/>JOIN_TYPE,JOIN_ON_COLUMNS, SPEC_COLS, spark_inst<br/>from extract import extract<br/>from transform import rename_cols, join_df, specific_cols<br/>from load import load<br/><br/># Initiating and Calling SparkSession<br/>SPARK=spark_inst()<br/><br/>#### Extract ####<br/><br/># Extracting CITY and COUNTRY data from MYSQL<br/>city_df = extract(SPARK,"JDBC","city")<br/>country_df = extract(SPARK,"JDBC","country")<br/><br/># Extracting COUNTRYLANGUAGE data from FileSystem<br/>country_language_df = extract(SPARK,"CSV","filesystem/countrylanguage.csv")<br/><br/>#### Transformation ####<br/><br/># 1. Rename Columns<br/>city_df = rename_cols(city_df, CITY_COL_DICT)<br/>country_df = rename_cols(country_df, COUNTRY_COL_DICT)<br/>country_language_df = rename_cols(country_language_df, COUNTRY_LANGUAGE_COL_DICT)<br/><br/># 2. Join DF with common column "country_code"<br/>country_city_df=join_df(country_df, city_df, JOIN_ON_COLUMNS, JOIN_TYPE)<br/>country_city_language_df= join_df(country_city_df, country_language_df, JOIN_ON_COLUMNS, JOIN_TYPE)<br/><br/># 3. Get specific cols<br/>country_city_language_df = specific_cols(country_city_language_df, SPEC_COLS)<br/><br/><br/>#### Load Data ####<br/><br/># MySQL<br/>load("JDBC",country_city_language_df, "CountryCityLanguage")<br/><br/># FileSystem<br/>load("CSV",country_city_language_df, "output/countrycitylanguage.csv")</span></pre><p id="8202" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">现在，您的数据将作为 CSV 文件存储在 MySQL 数据库和文件系统中。</p><p id="da62" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">代码库:-<a class="ae lw" href="https://github.com/rohitrsp898/Basic_ETL_PySpark" rel="noopener ugc nofollow" target="_blank">https://github.com/rohitrsp898/Basic_ETL_PySpark</a></p><p id="d7ba" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">您还可以使用 Pandas 来了解基本的 ETL。</p><p id="e112" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">编码快乐！！</p></div></div>    
</body>
</html>