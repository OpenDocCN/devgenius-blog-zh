<html>
<head>
<title>Simplified Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">简化机器学习</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/simplified-machine-learning-66c7a47cde18?source=collection_archive---------29-----------------------#2020-07-13">https://blog.devgenius.io/simplified-machine-learning-66c7a47cde18?source=collection_archive---------29-----------------------#2020-07-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="530b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第四部分:使用主动回忆学习集成方法</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/1a80c53a3b439b9f2e680d4f51ff23ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_LGVNDG_IYJdkx4i"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk translated">肖恩·林在<a class="ae lb" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><h2 id="8100" class="lj lk iq bd ll lm ln dn lo lp lq dp lr jy ls lt lu kc lv lw lx kg ly lz ma mb bi translated">这次，让我们使用主动回忆技术来更有效地学习新概念</h2><div class="mc md gp gr me mf"><a href="https://getatomi.com/staffroom/what-is-active-recall-and-how-effective-is-it/" rel="noopener  ugc nofollow" target="_blank"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd ir gy z fp mk fr fs ml fu fw ip bi translated">什么是主动召回，效果如何？</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">好吧，如果你相信宣传，主动回忆比阅读好，比强调好，听着就喜欢…</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">getatomi.com</p></div></div><div class="mo l"><div class="mp l mq mr ms mo mt kv mf"/></div></div></a></div><p id="fc7c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过问自己以下问题，它鼓励我们积极地从大脑中检索信息。这种互动技术有助于我们将短期记忆中的知识转化为长期记忆。让我们开始吧…</p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><h1 id="d889" class="mu lk iq bd ll mv mw mx lo my mz na lr nb nc nd lu ne nf ng lx nh ni nj ma nk bi translated">从根本上说，什么是合奏？</h1><blockquote class="nl nm nn"><p id="9746" class="jn jo no jp b jq jr js jt ju jv jw jx np jz ka kb nq kd ke kf nr kh ki kj kk ij bi translated">分类器的集合是<strong class="jp ir">一组分类器(因此分类假设)</strong>，它们各自的决策以某种方式组合起来，以对新的例子进行分类。你可能认识的一些常见的合奏例子有<em class="iq">随机森林、AdaBoost、Bagging </em>等。我们将在下一节详细讨论这些技术。</p></blockquote><h2 id="0113" class="lj lk iq bd ll lm ln dn lo lp lq dp lr jy ls lt lu kc lv lw lx kg ly lz ma mb bi translated">什么因素确保集成优于单个分类器？</h2><ol class=""><li id="1016" class="ns nt iq jp b jq nu ju nv jy nw kc nx kg ny kk nz oa ob oc bi translated">分类器集合中的各个分类器必须<strong class="jp ir">彼此不一致</strong>，因此，它们的误差可以相互抵消</li><li id="23a6" class="ns nt iq jp b jq od ju oe jy of kc og kg oh kk nz oa ob oc bi translated">每个分类器以小于随机的速率产生<strong class="jp ir">不相关的误差</strong>(例如，对于二进制分类为 0.5)</li></ol><h2 id="1cbb" class="lj lk iq bd ll lm ln dn lo lp lq dp lr jy ls lt lu kc lv lw lx kg ly lz ma mb bi translated">是什么让合奏发挥作用？</h2><ol class=""><li id="1250" class="ns nt iq jp b jq nu ju nv jy nw kc nx kg ny kk nz oa ob oc bi translated"><strong class="jp ir">统计维度:</strong>与假设空间的大小相比，训练数据可能不够充分，集成方法对来自每个分类器的结果进行平均，并降低选择错误分类器的风险</li><li id="e2a3" class="ns nt iq jp b jq od ju oe jy of kc og kg oh kk nz oa ob oc bi translated"><strong class="jp ir">计算维度:</strong>在假设空间中从许多不同的起点运行搜索可以减少计算成本并避免陷入局部最优</li><li id="d723" class="ns nt iq jp b jq od ju oe jy of kc og kg oh kk nz oa ob oc bi translated"><strong class="jp ir">代表性维度:</strong>假设空间可能不包含真实的目标函数，分类器的加权组合可能能够到达假设空间之外</li></ol><h1 id="23cf" class="mu lk iq bd ll mv oi mx lo my oj na lr nb ok nd lu ne ol ng lx nh om nj ma nk bi translated">如何创建合奏？</h1><p id="6661" class="pw-post-body-paragraph jn jo iq jp b jq nu js jt ju nv jw jx jy on ka kb kc oo ke kf kg op ki kj kk ij bi translated">创建合奏的方法可以分为四种类型:</p><h2 id="3591" class="lj lk iq bd ll lm ln dn lo lp lq dp lr jy ls lt lu kc lv lw lx kg ly lz ma mb bi translated"><strong class="ak"> 1。通过子采样操作训练集:</strong></h2><ul class=""><li id="da1b" class="ns nt iq jp b jq nu ju nv jy nw kc nx kg ny kk oq oa ob oc bi translated">在<strong class="jp ir">不同的训练样本子集</strong>上运行几次学习算法，对于<strong class="jp ir">不稳定的</strong>学习算法<em class="no">(例如决策树、神经网络)</em>工作良好</li></ul><p id="71d3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="no">包括以下技术:</em></p><ul class=""><li id="4680" class="ns nt iq jp b jq jr ju jv jy or kc os kg ot kk oq oa ob oc bi translated"><strong class="jp ir">Bagging(bootstrap replicate)</strong>是这一类别下的典型集成方法，它使用原始训练集的 bootstrap replicate，类似于统计学中的 bootstrap 技术。</li></ul><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi ou"><img src="../Images/6385f99cc8043a71bb51aee0f07a83ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WqoNfHIcRwtFSaytx20E1Q.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk translated">Bagging 算法</figcaption></figure><ul class=""><li id="a8fd" class="ns nt iq jp b jq jr ju jv jy or kc os kg ot kk oq oa ob oc bi translated"><strong class="jp ir">交叉验证委员会</strong>将训练数据集分成 n 个子集，每次丢弃其中一个子集形成训练时间，然后这样迭代构建 10 个训练数据集。程序类似于<em class="no"> k 倍交叉验证</em></li><li id="4e20" class="ns nt iq jp b jq od ju oe jy of kc og kg oh kk oq oa ob oc bi translated"><strong class="jp ir"> Bagging vs 交叉验证委员会</strong> : bagging 分类器看起来更不一样，因为训练数据的差异更大，因此分类器彼此不一致</li><li id="ed9f" class="ns nt iq jp b jq od ju oe jy of kc og kg oh kk oq oa ob oc bi translated"><strong class="jp ir">增强</strong>调整训练实例的概率分布。与 Bagging 和 Cross Validated 委员会相比，它更容易过度适应噪声。增压是基于以下原则创建的:</li></ul><ol class=""><li id="d37b" class="ns nt iq jp b jq jr ju jv jy or kc os kg ot kk nz oa ob oc bi translated">希望生成的下一个假设在错误分类的例子上做得更好</li><li id="5d6e" class="ns nt iq jp b jq od ju oe jy of kc og kg oh kk nz oa ob oc bi translated">增加训练示例中<strong class="jp ir">不正确</strong>案例(而不是不正确假设)的权重，减少<strong class="jp ir">正确</strong>案例的权重</li><li id="d5cd" class="ns nt iq jp b jq od ju oe jy of kc og kg oh kk nz oa ob oc bi translated">通过每个假设的权重生成最终的集合假设(具有更多正确案例的 H 将具有更多权重)</li></ol><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="ov ow l"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk translated">AdaBoost 插图</figcaption></figure><h2 id="cf00" class="lj lk iq bd ll lm ln dn lo lp lq dp lr jy ls lt lu kc lv lw lx kg ly lz ma mb bi translated"><strong class="ak"> 2。操作输入功能</strong></h2><ul class=""><li id="e27f" class="ns nt iq jp b jq nu ju nv jy nw kc nx kg ny kk oq oa ob oc bi translated">操纵学习算法可用的输入特征集，当输入特征<strong class="jp ir">高度冗余</strong>时工作良好</li></ul><p id="5b6f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="no">包括以下技术:</em></p><ul class=""><li id="2e02" class="ns nt iq jp b jq jr ju jv jy or kc os kg ot kk oq oa ob oc bi translated"><strong class="jp ir">特征选择</strong>:重采样选择多少特征来训练数据</li></ul><h2 id="2e9b" class="lj lk iq bd ll lm ln dn lo lp lq dp lr jy ls lt lu kc lv lw lx kg ly lz ma mb bi translated"><strong class="ak"> 3。操纵输出目标</strong></h2><ul class=""><li id="677a" class="ns nt iq jp b jq nu ju nv jy nw kc nx kg ny kk oq oa ob oc bi translated"><strong class="jp ir">纠错输出编码(ECOC) </strong>:将 K 个输出标签划分成两个子集(A &amp; B)并在 A 和 B 上学习一个分类器，然后迭代过程 L 次，每次<strong class="jp ir">正确分类的类别得到一个投票</strong>，最后投票最多的类别就是集成的预测</li></ul><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi ox"><img src="../Images/bae4b80693cb37898d6251ae49c9f55d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s0k_V1ppTKQ9JbFYLggjLw.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk translated">ECOC 插图</figcaption></figure><h2 id="b587" class="lj lk iq bd ll lm ln dn lo lp lq dp lr jy ls lt lu kc lv lw lx kg ly lz ma mb bi translated">4.将随机性注入学习算法</h2><ul class=""><li id="e772" class="ns nt iq jp b jq nu ju nv jy nw kc nx kg ny kk oq oa ob oc bi translated">算法用于学习训练集，但是具有随机设置的不同初始权重</li><li id="d6b1" class="ns nt iq jp b jq od ju oe jy of kc og kg oh kk oq oa ob oc bi translated"><em class="no">例如，人工神经网络中不同的初始权重</em></li><li id="c55b" class="ns nt iq jp b jq od ju oe jy of kc og kg oh kk oq oa ob oc bi translated"><em class="no">例如，在决策树中选择随机分割标准</em></li></ul><h1 id="8c98" class="mu lk iq bd ll mv oi mx lo my oj na lr nb ok nd lu ne ol ng lx nh om nj ma nk bi translated">什么是随机森林？</h1><blockquote class="nl nm nn"><p id="630d" class="jn jo no jp b jq jr js jt ju jv jw jx np jz ka kb nq kd ke kf nr kh ki kj kk ij bi translated"><em class="iq">随机森林是由许多决策树组成的分类算法。它使用</em> <strong class="jp ir"> <em class="iq">装袋和特征随机性</em> </strong> <em class="iq">来创建一个</em> <strong class="jp ir"> <em class="iq">不相关的树的森林</em> </strong> <em class="iq">，其由委员会的预测比任何单个树的预测更准确。</em></p></blockquote><blockquote class="oy"><p id="7120" class="oz pa iq bd pb pc pd pe pf pg ph kk dk translated">随机森林=装袋+特征选择？</p></blockquote><h2 id="c4eb" class="lj lk iq bd ll lm pi dn lo lp pj dp lr jy pk lt lu kc pl lw lx kg pm lz ma mb bi translated">随机森林与装袋和特征选择有什么关系？</h2><ol class=""><li id="1b01" class="ns nt iq jp b jq nu ju nv jy nw kc nx kg ny kk nz oa ob oc bi translated"><strong class="jp ir">装袋:</strong>从训练数据集中随机抽取森林，并创建子集训练样本，那些未被选择的数据称为袋外(OOB)样本，用于评估随机森林的性能</li><li id="1365" class="ns nt iq jp b jq od ju oe jy of kc og kg oh kk nz oa ob oc bi translated"><strong class="jp ir">特征选择:</strong>通过选择特征子集中的重要特征来生长树，子集选择的随机性增加了模型的多样性</li><li id="77ae" class="ns nt iq jp b jq od ju oe jy of kc og kg oh kk nz oa ob oc bi translated">这两种方法用于构建森林中的每一棵树，然后通过迭代该树构建过程来形成随机森林</li></ol><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="ov ow l"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk translated">随机森林插图</figcaption></figure><p id="7d2d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="no">随机森林的重要性质:</em></p><ol class=""><li id="6134" class="ns nt iq jp b jq jr ju jv jy or kc os kg ot kk nz oa ob oc bi translated"><strong class="jp ir"> OOB(出袋)错误:</strong></li></ol><ul class=""><li id="f92c" class="ns nt iq jp b jq jr ju jv jy or kc os kg ot kk oq oa ob oc bi translated">可用于为每个节点找到随机选择的<strong class="jp ir">个属性</strong>的<strong class="jp ir">最佳范围</strong></li><li id="392d" class="ns nt iq jp b jq od ju oe jy of kc og kg oh kk oq oa ob oc bi translated">评估基于袋外样本的<strong class="jp ir">错误预测实例数</strong></li><li id="4989" class="ns nt iq jp b jq od ju oe jy of kc og kg oh kk oq oa ob oc bi translated"><strong class="jp ir"> OOB 误差估计:</strong>多数票不等于真实类的次数比例，所有情况下的平均值</li></ul><div class="mc md gp gr me mf"><a href="https://towardsdatascience.com/what-is-out-of-bag-oob-score-in-random-forest-a7fa23d710" rel="noopener follow" target="_blank"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd ir gy z fp mk fr fs ml fu fw ip bi translated">《随机森林》中的“出袋”(OOB)分数是多少？</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">这个博客试图解释 oob_score 的内部功能，当它在…</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="pn l mq mr ms mo mt kv mf"/></div></div></a></div><p id="4eb0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 2。可变重要性:</strong></p><ul class=""><li id="679c" class="ns nt iq jp b jq jr ju jv jy or kc os kg ot kk oq oa ob oc bi translated">可变重要性对于特征选择过程是至关重要的</li><li id="4710" class="ns nt iq jp b jq od ju oe jy of kc og kg oh kk oq oa ob oc bi translated">首先，取所有属性并<strong class="jp ir">选择其中一个</strong>并随机化属性值，学习一个集成</li><li id="2bb4" class="ns nt iq jp b jq od ju oe jy of kc og kg oh kk oq oa ob oc bi translated">如果具有随机属性的学习集成与没有随机分配的属性值的一样好，则该属性不重要，因此<strong class="jp ir">删除该属性</strong>并运行迭代过程</li><li id="3b6f" class="ns nt iq jp b jq od ju oe jy of kc og kg oh kk oq oa ob oc bi translated">如果两个属性高度相关，那么去掉其中一个可能不会有什么区别；如果把他们中的两个带出去，可能会产生巨大的变化</li></ul></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><h1 id="58be" class="mu lk iq bd ll mv mw mx lo my mz na lr nb nc nd lu ne nf ng lx nh ni nj ma nk bi translated">其他有用的资源</h1><p id="aa4e" class="pw-post-body-paragraph jn jo iq jp b jq nu js jt ju nv jw jx jy on ka kb kc oo ke kf kg op ki kj kk ij bi translated">关于集合方法的更多信息…</p><div class="mc md gp gr me mf"><a href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm" rel="noopener  ugc nofollow" target="_blank"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd ir gy z fp mk fr fs ml fu fw ip bi translated">随机森林.分类描述</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">本节给出了随机森林的简要概述以及关于该方法特性的一些评论。我们假设…</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">www.stat.berkeley.edu</p></div></div><div class="mo l"><div class="po l mq mr ms mo mt kv mf"/></div></div></a></div><div class="mc md gp gr me mf"><a href="https://towardsdatascience.com/understanding-random-forest-58381e0602d2" rel="noopener follow" target="_blank"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd ir gy z fp mk fr fs ml fu fw ip bi translated">了解随机森林</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">该算法如何工作以及为什么如此有效</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="pp l mq mr ms mo mt kv mf"/></div></div></a></div><div class="mc md gp gr me mf"><a href="https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205" rel="noopener follow" target="_blank"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd ir gy z fp mk fr fs ml fu fw ip bi translated">整体方法:装袋、助推和堆叠</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">理解集成学习的关键概念。</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="pq l mq mr ms mo mt kv mf"/></div></div></a></div><p id="3e12" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">更多机器学习主题…</p><div class="mc md gp gr me mf"><a href="https://medium.com/ai-in-plain-english/simplified-machine-learning-f5ca4e177bac" rel="noopener follow" target="_blank"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd ir gy z fp mk fr fs ml fu fw ip bi translated">简化机器学习</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">第 3 部分:决策树</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">medium.com</p></div></div><div class="mo l"><div class="pr l mq mr ms mo mt kv mf"/></div></div></a></div><div class="mc md gp gr me mf"><a href="https://medium.com/ai-in-plain-english/simplified-machine-learning-d66b3f5fa901" rel="noopener follow" target="_blank"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd ir gy z fp mk fr fs ml fu fw ip bi translated">简化机器学习</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">第二部分:概念学习</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">medium.com</p></div></div><div class="mo l"><div class="ps l mq mr ms mo mt kv mf"/></div></div></a></div><div class="mc md gp gr me mf"><a href="https://medium.com/ai-in-plain-english/simplified-machine-learning-concepts-ep1-dd794ee7dd0c" rel="noopener follow" target="_blank"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd ir gy z fp mk fr fs ml fu fw ip bi translated">简化机器学习</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">第 1 部分:机器学习简介</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">medium.com</p></div></div><div class="mo l"><div class="pt l mq mr ms mo mt kv mf"/></div></div></a></div></div></div>    
</body>
</html>