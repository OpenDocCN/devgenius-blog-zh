<html>
<head>
<title>Approaching Cluster Analysis In R</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">R 中聚类分析的探讨</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/cluster-analysis-in-r-b04b628b4bca?source=collection_archive---------5-----------------------#2022-01-06">https://blog.devgenius.io/cluster-analysis-in-r-b04b628b4bca?source=collection_archive---------5-----------------------#2022-01-06</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><div class=""><h2 id="4049" class="pw-subtitle-paragraph jk im in bd b jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb dk translated">如何处理 R 中的无监督学习问题？</h2></div><p id="786a" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">在这篇博客中，我将解释我们如何在  <em class="ky"> </em> <strong class="ke io"> <em class="ky"> R 编程语言中处理</em> <strong class="ke io"> <em class="ky">聚类分析问题。</em> </strong> <em class="ky">我在这里的方法是彻底调查我们可以遵循的过程和方法，以得出我们的结论。</em></strong></p><blockquote class="kz la lb"><p id="c72a" class="kc kd ky ke b kf kg jo kh ki kj jr kk lc km kn ko ld kq kr ks le ku kv kw kx ig bi translated">读完这篇文章后，你将能够回答以下问题:- </p><p id="eb59" class="kc kd ky ke b kf kg jo kh ki kj jr kk lc km kn ko ld kq kr ks le ku kv kw kx ig bi translated"><strong class="ke io">问:聚类问题的数据预处理？</strong></p><p id="4983" class="kc kd ky ke b kf kg jo kh ki kj jr kk lc km kn ko ld kq kr ks le ku kv kw kx ig bi translated"><strong class="ke io">问:聚类问题应遵循的单变量 EDA 技术？</strong></p><p id="dcf4" class="kc kd ky ke b kf kg jo kh ki kj jr kk lc km kn ko ld kq kr ks le ku kv kw kx ig bi translated"><strong class="ke io">问:双变量 EDA 技术可用于聚类问题吗？</strong></p><p id="815b" class="kc kd ky ke b kf kg jo kh ki kj jr kk lc km kn ko ld kq kr ks le ku kv kw kx ig bi translated"><strong class="ke io">问:多变量 EDA 技术可用于聚类问题吗？</strong></p><p id="8ba9" class="kc kd ky ke b kf kg jo kh ki kj jr kk lc km kn ko ld kq kr ks le ku kv kw kx ig bi translated"><strong class="ke io">问:如何决定最佳聚类数？</strong></p><p id="65b1" class="kc kd ky ke b kf kg jo kh ki kj jr kk lc km kn ko ld kq kr ks le ku kv kw kx ig bi translated"><strong class="ke io">问:如何做 k-means 聚类？</strong></p><p id="667a" class="kc kd ky ke b kf kg jo kh ki kj jr kk lc km kn ko ld kq kr ks le ku kv kw kx ig bi translated"><strong class="ke io">问:如何进行 k-medoids 聚类？</strong></p><p id="28b9" class="kc kd ky ke b kf kg jo kh ki kj jr kk lc km kn ko ld kq kr ks le ku kv kw kx ig bi translated"><strong class="ke io">问:如何做高斯混合模型聚类？</strong></p><p id="c5de" class="kc kd ky ke b kf kg jo kh ki kj jr kk lc km kn ko ld kq kr ks le ku kv kw kx ig bi translated"><strong class="ke io">问:如何做层次聚类？</strong></p><p id="d548" class="kc kd ky ke b kf kg jo kh ki kj jr kk lc km kn ko ld kq kr ks le ku kv kw kx ig bi translated"><strong class="ke io">问:如何可视化获得的集群？</strong></p><p id="1537" class="kc kd ky ke b kf kg jo kh ki kj jr kk lc km kn ko ld kq kr ks le ku kv kw kx ig bi translated"><strong class="ke io">问:如何可视化和解释聚类数据集？</strong></p><p id="6d7f" class="kc kd ky ke b kf kg jo kh ki kj jr kk lc km kn ko ld kq kr ks le ku kv kw kx ig bi translated"><strong class="ke io">问:如何比较车型？</strong></p><p id="76f5" class="kc kd ky ke b kf kg jo kh ki kj jr kk lc km kn ko ld kq kr ks le ku kv kw kx ig bi translated"><strong class="ke io">问:如何进行特征约简？</strong></p></blockquote><p id="7a39" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">那么，让我们开始吧</p><blockquote class="kz la lb"><p id="d2de" class="kc kd ky ke b kf kg jo kh ki kj jr kk lc km kn ko ld kq kr ks le ku kv kw kx ig bi translated">这是数据集的原始来源:- <a class="ae lf" href="https://archive-beta.ics.uci.edu/ml/datasets/wine" rel="noopener ugc nofollow" target="_blank">葡萄酒数据集。</a></p><p id="c3ba" class="kc kd ky ke b kf kg jo kh ki kj jr kk lc km kn ko ld kq kr ks le ku kv kw kx ig bi translated">我所使用的数据集不包含“类”列，它包含数据集中的功能名称。你可以在这里看一下数据集:- <a class="ae lf" href="https://raw.githubusercontent.com/kundan1rao/ml-wine-clustering/master/Wine%200.csv%20-%20Wine%200.csv.csv" rel="noopener ugc nofollow" target="_blank">葡萄酒数据集</a></p></blockquote><p id="dab4" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><em class="ky">我们会尝试聚类，找出</em> <a class="ae lf" href="https://raw.githubusercontent.com/kundan1rao/ml-wine-clustering/master/Wine%200.csv%20-%20Wine%200.csv.csv" rel="noopener ugc nofollow" target="_blank"> <em class="ky">这个葡萄酒数据集</em> </a> <em class="ky">中有多少种葡萄酒类型。如果你观察，你会发现有 178 种葡萄酒，每种都有 13 种特征。利用这些特征，我们需要对葡萄酒进行聚类。因此，我们可以将我们的目标定义为:- </em></p><p id="3bc3" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="ke io">目标:- </strong>找出上述葡萄酒数据集中存在的葡萄酒类型(聚类)。最后，我们可以通过哪些最基本的特征轻松识别特定葡萄酒的类别。</p><p id="423c" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="ke io">我在分析过程中遵循的主要步骤包括:- </strong></p><ol class=""><li id="3357" class="lg lh in ke b kf kg ki kj kl li kp lj kt lk kx ll lm ln lo bi translated"><strong class="ke io">数据探索和清理</strong></li><li id="a2fa" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx ll lm ln lo bi translated"><strong class="ke io">单变量 EDA </strong></li><li id="610b" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx ll lm ln lo bi translated"><strong class="ke io">多元 EDA </strong></li><li id="4768" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx ll lm ln lo bi translated"><strong class="ke io">主成分分析和特征探索</strong></li><li id="6917" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx ll lm ln lo bi translated"><strong class="ke io"> K 均值聚类</strong></li><li id="837c" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx ll lm ln lo bi translated"><strong class="ke io"> K-medoids 聚类</strong></li><li id="a744" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx ll lm ln lo bi translated"><strong class="ke io">高斯混合模型聚类</strong></li><li id="7541" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx ll lm ln lo bi translated"><strong class="ke io">层次聚类</strong></li><li id="93b4" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx ll lm ln lo bi translated"><strong class="ke io">比较型号</strong></li><li id="a655" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx ll lm ln lo bi translated"><strong class="ke io">特征约简和模型选择</strong></li><li id="dedf" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx ll lm ln lo bi translated"><strong class="ke io">结论</strong></li></ol><p id="d514" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">开始了，</p><p id="636c" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">如果你想一次完成整个项目，可以看看我的 Google colab 笔记本。为了更好、更深入地理解聚类算法，我强烈建议您阅读项目末尾提到的参考资料。</p><div class="lu lv gp gr lw lx"><a href="https://colab.research.google.com/drive/1egzFpDFoqXKzYbKyMtc_x-RJoaDcY1nZ#scrollTo=ZohJvCId9ZFu" rel="noopener  ugc nofollow" target="_blank"><div class="ly ab fo"><div class="lz ab ma cl cj mb"><h2 class="bd io gy z fp mc fr fs md fu fw im bi translated">谷歌联合实验室</h2><div class="me l"><h3 class="bd b gy z fp mc fr fs md fu fw dk translated">R 中的聚类分析</h3></div><div class="mf l"><p class="bd b dl z fp mc fr fs md fu fw dk translated">colab.research.google.com</p></div></div><div class="mg l"><div class="mh l mi mj mk mg ml mm lx"/></div></div></a></div><h1 id="0bca" class="mn mo in bd mp mq mr ms mt mu mv mw mx jt my ju mz jw na jx nb jz nc ka nd ne bi translated"><strong class="ak"> 1。数据探索和清理:- </strong></h1><p id="c6a0" class="pw-post-body-paragraph kc kd in ke b kf nf jo kh ki ng jr kk kl nh kn ko kp ni kr ks kt nj kv kw kx ig bi translated">我们可以通过以下代码加载数据并检查数据类型:-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="1257" class="nt mo in np b gy nu nv l nw nx"># importing the wine dataset</span><span id="b72a" class="nt mo in np b gy ny nv l nw nx"># library for read.csv() function<br/>library(readr)</span><span id="7848" class="nt mo in np b gy ny nv l nw nx">data&lt;-read.csv("https://raw.githubusercontent.com/kundan1rao/ml-wine-clustering/master/Wine%200.csv%20-%20Wine%200.csv.csv")</span><span id="ac66" class="nt mo in np b gy ny nv l nw nx"># Checking structure of the dataset and data types of variables<br/>str(data)</span></pre><p id="bf7c" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">数据集的结构如下所示</p><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi nz"><img src="../Images/70525563d758ede427b30a01a7199e3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XuQ1scIyjW4J1mOk60_iKA.png"/></div></div></figure><p id="5136" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">我们观察到</p><p id="5eb2" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><em class="ky"> 1。每个变量都是数字(双精度)数据类型</em></p><p id="09ff" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><em class="ky"> 2。13 个变量有 178 个观测值</em></p><p id="e3d5" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><em class="ky"> 3。“数据”在“data.frame”类中</em></p><blockquote class="kz la lb"><p id="0865" class="kc kd ky ke b kf kg jo kh ki kj jr kk lc km kn ko ld kq kr ks le ku kv kw kx ig bi translated">-上面的代码在本地可以很好地工作，并且我已经在我的 GitHub 帐户上托管了数据集。</p><p id="d539" class="kc kd ky ke b kf kg jo kh ki kj jr kk lc km kn ko ld kq kr ks le ku kv kw kx ig bi translated">-安装软件包，并在需要时加载它们。</p></blockquote><p id="226f" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">我们可以通过下面的命令来查看变量的名称、顶部的几行以及底部的几行</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="9c60" class="nt mo in np b gy nu nv l nw nx"># names<br/>names(data)</span><span id="0d65" class="nt mo in np b gy ny nv l nw nx"># head<br/>head(data)</span><span id="780a" class="nt mo in np b gy ny nv l nw nx"># tail<br/>tail(data)</span></pre><p id="30de" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">数据集的头部看起来像:-</p><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi og"><img src="../Images/fe80769d1531cd9a8d844defdfa546e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZBRXPjw1fiOwCsXkSzOUEg.png"/></div></div></figure><p id="2fc2" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">现在，我们可以通过下面的代码检查数据集中是否有 NA、NaN 或 infinity 值</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="18c4" class="nt mo in np b gy nu nv l nw nx"># Checking for null and other types of missing data<br/>library(dplyr) # library for data manipulation</span><span id="081f" class="nt mo in np b gy ny nv l nw nx"># total NAs<br/>f &lt;- function(x){sum(is.na(x))}<br/>sapply(data,f)%&gt;%as.data.frame()<br/></span><span id="afd9" class="nt mo in np b gy ny nv l nw nx"># Checking for total infinite values<br/>f &lt;- function(x){sum(is.infinite(x))}<br/>sapply(data,f) %&gt;% as.data.frame()<br/></span><span id="acad" class="nt mo in np b gy ny nv l nw nx"># total NaN values<br/>f &lt;- function(x){sum(is.nan(x))}<br/>sapply(data,f)%&gt;%as.data.frame()</span></pre><p id="ac45" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">我们可以观察到:-</p><ul class=""><li id="9e66" class="lg lh in ke b kf kg ki kj kl li kp lj kt lk kx oh lm ln lo bi translated">整个数据集中的任何变量既没有无穷大、NA、Null 值，也没有 NaN 值。</li></ul><h1 id="f049" class="mn mo in bd mp mq mr ms mt mu mv mw mx jt my ju mz jw na jx nb jz nc ka nd ne bi translated"><strong class="ak"> 2。单变量 EDA:- </strong></h1><p id="c94d" class="pw-post-body-paragraph kc kd in ke b kf nf jo kh ki ng jr kk kl nh kn ko kp ni kr ks kt nj kv kw kx ig bi translated">我们从总结开始，然后检查每个特征的均值和方差。稍后，我们绘制特征的箱线图，并查看这些特征的比例是否一致</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="6231" class="nt mo in np b gy nu nv l nw nx"># summary of the data<br/>summary(data)</span><span id="279f" class="nt mo in np b gy ny nv l nw nx"># mean of the features<br/>apply(data,2,mean) %&gt;% as.data.frame()</span><span id="4c7f" class="nt mo in np b gy ny nv l nw nx"># variance of the features<br/>apply(data,2,var)%&gt;%as.data.frame()</span><span id="4976" class="nt mo in np b gy ny nv l nw nx"># boxplot of features<br/>boxplot(data, xlab="features",ylab="values", main ="Boxplot of features", col = "green",border = "blue",cex.axis=.5)</span></pre><p id="c700" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">箱线图看起来如下</p><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/77cb54964a5691fff795aa1dc65d5d8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WyIE1c6W-vrmxaWcOUTS4w.png"/></div></div></figure><p id="51fe" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">我们观察到特征没有很好地缩放以应用聚类算法。因此，我们缩放这些特征，使得每个特征的平均值变为 0，标准偏差变为 1。</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="63fd" class="nt mo in np b gy nu nv l nw nx"># scaling the data set<br/>data.scaled &lt;- scale(data)</span><span id="12f4" class="nt mo in np b gy ny nv l nw nx"># summary of scaled data<br/>summary(data.scaled)</span><span id="f24a" class="nt mo in np b gy ny nv l nw nx"># mean of the scaled data set<br/>apply(data.scaled,2,mean)%&gt;%as.data.frame()</span><span id="6fcf" class="nt mo in np b gy ny nv l nw nx"># variance of the scaled dataset<br/>apply(data.scaled,2,var)%&gt;%as.data.frame()</span><span id="9dee" class="nt mo in np b gy ny nv l nw nx"># boxplot of scaled features<br/>boxplot(data.scaled, xlab="features",ylab="values", main ="Boxplot of features", col = "green",border = "blue",cex.axis=.5)</span></pre><p id="9f9d" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">缩放特征的箱线图看起来如下:-</p><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/c70cedb7b6e61312d6d7b385663d5ff6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V7zaLEuSM6Q01-yNOlC9Fw.png"/></div></div></figure><p id="ab47" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">观察:-</p><ul class=""><li id="7b3f" class="lg lh in ke b kf kg ki kj kl li kp lj kt lk kx oh lm ln lo bi translated">现在，这些特性已经很好地进行了缩放，可以随时进行比较和使用</li></ul><p id="825a" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">现在，我们将做单变量直方图和密度图，以查看各个特征中可能的聚类数</p><p id="8db8" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">r 代码制作图:-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="bb79" class="nt mo in np b gy nu nv l nw nx"># function to make the plots</span><span id="b611" class="nt mo in np b gy ny nv l nw nx"># required library<br/>library(stringr)<br/>library(ggplot2)</span><span id="aa24" class="nt mo in np b gy ny nv l nw nx"># function to create histogram and density plot<br/>histf&lt;-function(z){<br/>feature=str_replace_all(deparse(substitute(z)),"[data$]","")<br/>ggplot(data) +<br/>aes(x = z) +<br/>geom_histogram(aes(y=..density..), position="identity", alpha=0.5,bins = 14L, fill = "#497AD2", colour = "blue") +<br/>geom_density(alpha=0.2, fill = "#4411D2", colour = "#4411D2")+<br/>labs(x =  paste("Feature: ",feature),y = "No Of Obeservation",<br/>title = paste("Histogrem Plot Of ",feature),<br/>subtitle = paste("Distribution Of Feature ",feature),<br/>caption = "wine dataset") +<br/>theme_grey()<br/>}</span><span id="4955" class="nt mo in np b gy ny nv l nw nx"># calling function for different features<br/>histf(data$Alcohol)<br/>histf(data$Malic.acid)<br/>histf(data$Ash)<br/>histf(data$Alcalinity.of.ash)<br/>histf(data$Magnesium)<br/>histf(data$Total.phenols)<br/>histf(data$Flavanoids)<br/>histf(data$Nonflavanoid.phenols)<br/>histf(data$Proanthocyanins)<br/>histf(data$Color.intensity)<br/>histf(data$Hue)<br/>histf(data$OD280.OD315.of.diluted.wines)<br/>histf(data$Proline)</span></pre><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/c45d899c14796f68fc868be3e585b8ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aLcMZLp4RsgAwnII3zNtbQ.png"/></div></div><figcaption class="oj ok gj gh gi ol om bd b be z dk translated">酒精的直方图和密度图</figcaption></figure><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/4b6898f4b8d8a31a8e06cb23f9dfd7ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ms6djUchFLfJ1Utn07uRvg.png"/></div></div><figcaption class="oj ok gj gh gi ol om bd b be z dk translated">苹果酸的直方图和密度图</figcaption></figure><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/2e43cd960d18264f576aa7d2fbd91251.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6vqCnj_EZLv5Z6frE_KUJw.png"/></div></div><figcaption class="oj ok gj gh gi ol om bd b be z dk translated">灰分的直方图和密度图</figcaption></figure><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/ca82fa2177200782213355254df0e491.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yCheeikftlAEgZy5h4TUHQ.png"/></div></div><figcaption class="oj ok gj gh gi ol om bd b be z dk translated">灰分碱度的直方图和密度图</figcaption></figure><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/704a44ff07ee0bfa1df00f4b88498904.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nCaZWv6H9eGm5MyxaC8teg.png"/></div></div><figcaption class="oj ok gj gh gi ol om bd b be z dk translated">镁的直方图和密度图</figcaption></figure><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/a272bc40067e6d5bd50765d09f867e93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9u0VUtHXLkX7XqgLdE87IA.png"/></div></div><figcaption class="oj ok gj gh gi ol om bd b be z dk translated">总酚的直方图和密度图</figcaption></figure><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/7d3a0ce9d68c983d5f1d51f5f69908b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wwJ-hoFVlOb1jVckdq9oIQ.png"/></div></div><figcaption class="oj ok gj gh gi ol om bd b be z dk translated">黄酮类化合物的直方图和密度图</figcaption></figure><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/95d646c8b6b533c3923af11a7ce18b9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xP_FsmnDEkJNTLpmnbEjRw.png"/></div></div><figcaption class="oj ok gj gh gi ol om bd b be z dk translated">非类黄酮酚的直方图和密度图</figcaption></figure><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/131ee31f3f249502ba24649cc71e2402.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hFbm0lUiQuh9NUIfgcQFrw.png"/></div></div><figcaption class="oj ok gj gh gi ol om bd b be z dk translated">原花青素的直方图和密度图</figcaption></figure><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/2a5af011de87b3ce4e495244077580bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bsfjTyl1wziIflDVZIx8fg.png"/></div></div><figcaption class="oj ok gj gh gi ol om bd b be z dk translated">颜色强度的直方图和密度图</figcaption></figure><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/b51dff09bdb1c0599b7518b23b482117.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8AX6AWghanE4TegM69Y0RA.png"/></div></div><figcaption class="oj ok gj gh gi ol om bd b be z dk translated">色调的直方图和密度图</figcaption></figure><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/17e0294dd93b3c5b75e05fb7909e4bbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bpqqegZRzB6IWdapcO-zVQ.png"/></div></div><figcaption class="oj ok gj gh gi ol om bd b be z dk translated">稀释葡萄酒的 OD280 OD315 直方图和密度图</figcaption></figure><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/1cbf69e3af80ebed3cd36d340c4a854a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wZuUKk2uRA5NqCJgiDDI0A.png"/></div></div><figcaption class="oj ok gj gh gi ol om bd b be z dk translated">脯氨酸的直方图和密度图</figcaption></figure><p id="d95e" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">观察:-</p><blockquote class="kz la lb"><p id="a3d8" class="kc kd ky ke b kf kg jo kh ki kj jr kk lc km kn ko ld kq kr ks le ku kv kw kx ig bi translated">直方图或密度图的峰值意味着存在可能的聚类。</p></blockquote><ul class=""><li id="6886" class="lg lh in ke b kf kg ki kj kl li kp lj kt lk kx oh lm ln lo bi translated"><strong class="ke io">酒精</strong>似乎聚集成了<strong class="ke io"> 2 簇</strong>。</li><li id="3705" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx oh lm ln lo bi translated"><strong class="ke io">苹果酸</strong>似乎聚集成<strong class="ke io"> 3 个不同的簇</strong>。</li><li id="d072" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx oh lm ln lo bi translated"><strong class="ke io">灰</strong>好像有<strong class="ke io">多于 3 簇</strong>。</li><li id="a60c" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx oh lm ln lo bi translated"><strong class="ke io">灰分的碱度</strong>似乎呈正态分布。因此，我们可以说可能有三个聚类，两个相似类型具有低和高含量，一个聚类具有平均含量。</li><li id="fa02" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx oh lm ln lo bi translated"><strong class="ke io">镁</strong>似乎聚集在<strong class="ke io"> 3 个簇</strong>中，其中两个有些相似，具有中等镁含量，另一个具有高镁含量。</li><li id="df8f" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx oh lm ln lo bi translated"><strong class="ke io">总酚</strong>似乎聚集在<strong class="ke io"> 2 簇</strong>中。</li><li id="86a3" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx oh lm ln lo bi translated"><strong class="ke io">黄酮类化合物</strong>似乎聚集在<strong class="ke io"> 3 簇</strong>中。</li><li id="e790" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx oh lm ln lo bi translated"><strong class="ke io">非类黄酮酚类</strong>似乎聚集在<strong class="ke io"> 3 簇</strong>中。</li><li id="3951" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx oh lm ln lo bi translated"><strong class="ke io">原花青素</strong>似乎聚集在<strong class="ke io"> 3 簇</strong>中。</li><li id="1750" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx oh lm ln lo bi translated"><strong class="ke io">颜色强度</strong>似乎聚集在<strong class="ke io"> 3 簇</strong>中。</li><li id="95b7" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx oh lm ln lo bi translated"><strong class="ke io">色相</strong>似乎聚集在<strong class="ke io"> 2 簇</strong>中。</li><li id="511d" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx oh lm ln lo bi translated"><strong class="ke io">稀释酒</strong>的 OD280 OD315 似乎聚集在<strong class="ke io"> 2 簇</strong>中。</li><li id="285d" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx oh lm ln lo bi translated"><strong class="ke io">脯氨酸</strong>似乎聚集在<strong class="ke io"> 3 簇</strong>中。</li><li id="78e8" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx oh lm ln lo bi translated">这个葡萄酒数据集中似乎有<strong class="ke io"> 2 或 3 个聚类</strong>。现在我们将探讨多变量分析。</li></ul><h1 id="1a77" class="mn mo in bd mp mq mr ms mt mu mv mw mx jt my ju mz jw na jx nb jz nc ka nd ne bi translated"><strong class="ak"> 3。多元 EDA:- </strong></h1><p id="272f" class="pw-post-body-paragraph kc kd in ke b kf nf jo kh ki ng jr kk kl nh kn ko kp ni kr ks kt nj kv kw kx ig bi translated">我们将以相关性开始多元 EDA</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="32d2" class="nt mo in np b gy nu nv l nw nx">suppressPackageStartupMessages(<br/>library(dplyr)  # library for data manipulation<br/>)<br/>library(knitr)   # for using kable() function to render table</span><span id="8884" class="nt mo in np b gy ny nv l nw nx"># correlation among the features<br/>cor(data.scaled) %&gt;% kable()</span></pre><p id="c56c" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">输出表如下所示</p><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi on"><img src="../Images/0804c59696337e18ca7cb0b0391c5bdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-VH2bVn4hcjs77hJ4xGc1w.png"/></div></div></figure><p id="91cc" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">让我们看看所有特征的散点图:-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="1375" class="nt mo in np b gy nu nv l nw nx">#install.packages("GGally")</span><span id="798d" class="nt mo in np b gy ny nv l nw nx">library(GGally)<br/>ggpairs(data)</span></pre><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/e501c3d1fb00662ea38d888b88e092d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dHYbU01bejvsu75dW802Dg.png"/></div></div><figcaption class="oj ok gj gh gi ol om bd b be z dk translated">特征散点图</figcaption></figure><p id="402f" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">现在让我们看看相关图:-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="cf14" class="nt mo in np b gy nu nv l nw nx"># correlation plot<br/>options(repr.plot.width=12, repr.plot.height=9)<br/>ggcorr(data)</span></pre><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/3c8b6213e673b558d78a1c16b31701dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nbNkfT6iKtFfV7SahnY6YQ.png"/></div></div><figcaption class="oj ok gj gh gi ol om bd b be z dk translated">特征的相关图</figcaption></figure><p id="df40" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">从上表和曲线图中，我们可以观察到:-</p><ul class=""><li id="504a" class="lg lh in ke b kf kg ki kj kl li kp lj kt lk kx oh lm ln lo bi translated">酒精和脯氨酸似乎正相关</li><li id="c782" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx oh lm ln lo bi translated">酒精和颜色强度似乎是正相关的</li><li id="d499" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx oh lm ln lo bi translated">苹果酸和色调似乎是负相关的</li><li id="4a80" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx oh lm ln lo bi translated">还有许多相关的特性，这些特性提供了这些特性如何相互关联的总体思路</li></ul><p id="69b6" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">现在，让我们看看热图和相异矩阵图，以了解聚类趋势:-</p><p id="c2a1" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">热图:-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="958e" class="nt mo in np b gy nu nv l nw nx"># heatmap<br/><br/>#install.packages("gplots")<br/>library("gplots")<br/>heatmap.2(as.matrix(data.scaled),scale = "none")</span></pre><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/07e556b146eb872af5017faa52c055d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AuQCuGqc1zppk6rW_u8KNA.png"/></div></div><figcaption class="oj ok gj gh gi ol om bd b be z dk translated">数据集的热图</figcaption></figure><p id="bb53" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">相异矩阵图:-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="6073" class="nt mo in np b gy nu nv l nw nx"># Disimilarity matrix</span><span id="fd0d" class="nt mo in np b gy ny nv l nw nx">dist.eucl &lt;-dist(data.scaled, method = "euclidean")</span><span id="3020" class="nt mo in np b gy ny nv l nw nx"># Visualising disimiarity matrix<br/>#install.packages("factoextra")<br/>library(factoextra)   # useful library for visualising cluster analysis</span><span id="0225" class="nt mo in np b gy ny nv l nw nx">fviz_dist(dist.eucl)+labs(title="Wine data")</span></pre><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/320ba46381b5fdefe923c2d4e2d16d52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rUZLjBSk1crJWxCF5-gSLQ.png"/></div></div><figcaption class="oj ok gj gh gi ol om bd b be z dk translated">数据集的相异矩阵图</figcaption></figure><p id="553f" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">观察:-</p><ul class=""><li id="dd4e" class="lg lh in ke b kf kg ki kj kl li kp lj kt lk kx oh lm ln lo bi translated">上面的两个图，热图和相异矩阵图给出了我们的数据集中如何以及有多少聚类的总体概念。</li><li id="0b0e" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx oh lm ln lo bi translated">现在下结论还为时过早，但我们的数据集中似乎有三个潜在的聚类(可以通过查看热图中的聚类模式和相异矩阵图中的矩形框数量得出结论)。</li></ul><h1 id="bc93" class="mn mo in bd mp mq mr ms mt mu mv mw mx jt my ju mz jw na jx nb jz nc ka nd ne bi translated"><strong class="ak"> 4。主成分分析和特征探索</strong></h1><p id="9ccb" class="pw-post-body-paragraph kc kd in ke b kf nf jo kh ki ng jr kk kl nh kn ko kp ni kr ks kt nj kv kw kx ig bi translated">现在，我们将进行主成分分析，以找出能够解释数据集最大可变性的最少数量的特征，并通过绘制主成分来查看聚类趋势。</p><p id="ab50" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">寻找主成分:-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="cc9d" class="nt mo in np b gy nu nv l nw nx">#principle component</span><span id="047a" class="nt mo in np b gy ny nv l nw nx">pca.out&lt;-prcomp(data.scaled)<br/>summary(pca.out)</span></pre><p id="0517" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">这给出了输出:-</p><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oo"><img src="../Images/fee5c65be3c7e1110ba1a03d6c5209eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d2q53CAThcWhZhJy5nYK8Q.png"/></div></div><figcaption class="oj ok gj gh gi ol om bd b be z dk translated">主成分总结</figcaption></figure><p id="cc2e" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">图(由每个主成分解释的方差比例):-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="087a" class="nt mo in np b gy nu nv l nw nx">#PVE proportion of variance explained by each principle components</span><span id="9306" class="nt mo in np b gy ny nv l nw nx">pr.var &lt;- pca.out$sdev^2<br/>pve &lt;- pr.var/sum(pr.var)</span><span id="418a" class="nt mo in np b gy ny nv l nw nx"># plot of PVE explained by each principle component<br/>plot(pve, xlab="Principle Component", ylab = "Proportion of variance explained", ylim=c(0,1), type="b")</span></pre><p id="7b4c" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">获得的图是:-</p><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/e044c7722efb5abd4e59af16e451d22b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zdqprFxEvQiJf8zfJIsszQ.png"/></div></div></figure><p id="f78e" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">图(解释方差的累积比例):-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="3148" class="nt mo in np b gy nu nv l nw nx"># Cumulative proportion of variance explained</span><span id="f103" class="nt mo in np b gy ny nv l nw nx">plot(cumsum(pve), xlab="principle component",ylab="Cumulative proportion of variance explained", ylim=c(0,1), type="b")</span></pre><p id="4376" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">获得的输出图为:-</p><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/18fa7ceee706b64b7b154b6b8ade491c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6NGi5eYAIAYdHcVem5B2DA.png"/></div></div></figure><p id="3735" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">观察:- <em class="ky">我们可以观察由每个主成分解释的变异性的累积比例。PC1 到 PC9 解释了数据可变性的 0.9424 比例。</em></p><p id="e80e" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">现在，我们将通过可视化主成分来看到聚类趋势。</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="f0dc" class="nt mo in np b gy nu nv l nw nx"># biplot</span><span id="03f6" class="nt mo in np b gy ny nv l nw nx">fviz_pca_biplot(pca.out)</span></pre><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/b6b5b011b3dcbd93a883a8b323e0a6e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-TCfgTQrUY2YfE0P8kYCiA.png"/></div></div><figcaption class="oj ok gj gh gi ol om bd b be z dk translated">双波特</figcaption></figure><p id="40f4" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">观察:-</p><ol class=""><li id="b499" class="lg lh in ke b kf kg ki kj kl li kp lj kt lk kx ll lm ln lo bi translated">酒精、颜色强度、脯氨酸、镁、灰分似乎影响簇的形成。</li><li id="691a" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx ll lm ln lo bi translated">稀释葡萄酒的色调、总酚、黄酮类化合物、0D280/0D315 似乎影响另一个集群的形成。</li><li id="3818" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx ll lm ln lo bi translated">苹果酸、非类黄酮酚、灰分的碱性似乎影响又一个簇的形成。</li></ol><h1 id="02fd" class="mn mo in bd mp mq mr ms mt mu mv mw mx jt my ju mz jw na jx nb jz nc ka nd ne bi translated"><strong class="ak"> 5。k 均值聚类</strong></h1><p id="6770" class="pw-post-body-paragraph kc kd in ke b kf nf jo kh ki ng jr kk kl nh kn ko kp ni kr ks kt nj kv kw kx ig bi translated">现在我们将对葡萄酒数据集进行 kmeans 聚类。这些步骤包括</p><p id="08bb" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">1.寻找 kmeans 的最佳聚类数</p><p id="da5b" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">2.接下来，将进行 kmeans 聚类</p><p id="12d1" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">3.接下来，将显示集群</p><p id="6099" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">4.接下来，将进行总结</p><p id="6a18" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">最佳聚类数(肘形法):-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="bfd5" class="nt mo in np b gy nu nv l nw nx"># k-means clustering<br/># deciding optimul number of cluster<br/># Elbow method</span><span id="bb41" class="nt mo in np b gy ny nv l nw nx">fviz_nbclust(data.scaled, kmeans, method = "wss")+<br/>geom_vline(xintercept = 3, linetype = 2)+labs(subtitle = "Elbow method")</span></pre><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/b1306f45de0f6bedbe9c26061dcb330a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EZ2rB-BVd0x75j58YgnjaA.png"/></div></div></figure><p id="e161" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">观察:-我们可以在 k = 3 处看到肘部。</p><p id="25a1" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">最佳聚类数(剪影法):-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="3aa5" class="nt mo in np b gy nu nv l nw nx"># Silhouette method</span><span id="31d3" class="nt mo in np b gy ny nv l nw nx">fviz_nbclust(data.scaled, kmeans, method = "silhouette")+labs(subtitle = "Silhouette method")</span></pre><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/685807770de5af1c53f34d9140012d79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vEE6wNZoIPAR0wMWwr9muQ.png"/></div></div></figure><p id="5bd4" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">最佳聚类数(差距统计):-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="0bf4" class="nt mo in np b gy nu nv l nw nx"># Gap statistic<br/># nboot = 50 to keep the function speedy.<br/># recommended value: nboot= 500 for your analysis.<br/># Use verbose = FALSE to hide computing progression.</span><span id="e1ca" class="nt mo in np b gy ny nv l nw nx">set.seed(123)<br/>fviz_nbclust(data.scaled, kmeans, nstart = 25, method = "gap_stat", nboot = 50)+labs(subtitle = "Gap statistic method")</span></pre><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/72abf1eb8db9b3b639bf83c8cbcfee40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kRONBDriPSI1HDOQxpWKNw.png"/></div></div></figure><p id="001f" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">最佳聚类数(nb cluster()方法，多数法则):-</p><blockquote class="kz la lb"><p id="3ac2" class="kc kd ky ke b kf kg jo kh ki kj jr kk lc km kn ko ld kq kr ks le ku kv kw kx ig bi translated">nb cluster()使用两个统计信息来决定最佳的集群数量</p><p id="133b" class="kc kd ky ke b kf kg jo kh ki kj jr kk lc km kn ko ld kq kr ks le ku kv kw kx ig bi translated"><strong class="ke io">休伯特指数:- </strong>这是一种确定聚类数的图形方法。在休伯特指数图中，我们寻找对应于测量值显著增加的显著拐点，即休伯特指数二阶差图中的显著峰值。</p><p id="c4bd" class="kc kd ky ke b kf kg jo kh ki kj jr kk lc km kn ko ld kq kr ks le ku kv kw kx ig bi translated"><strong class="ke io">D 指数:</strong> -这是一种确定聚类数的图形方法。在 D 指数图中，我们寻找对应于测量值显著增加的显著拐点(din index 二阶差图中的显著峰值)。</p><p id="aac1" class="kc kd ky ke b kf kg jo kh ki kj jr kk lc km kn ko ld kq kr ks le ku kv kw kx ig bi translated">要了解更多详细信息，您可以参考他们的官方软件包文档。</p></blockquote><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="f7b1" class="nt mo in np b gy nu nv l nw nx">#install.packages("NbClust")</span><span id="a496" class="nt mo in np b gy ny nv l nw nx">library(NbClust)<br/>nb &lt;- NbClust(data.scaled, distance = "euclidean", min.nc = 2,<br/>max.nc = 10, method = "kmeans")</span><span id="c449" class="nt mo in np b gy ny nv l nw nx">suppressWarnings(<br/># visualising the result<br/>fviz_nbclust(nb)<br/>)</span></pre><p id="495c" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">我们得到的输出如下:-</p><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi op"><img src="../Images/d8be93afdf7a83c5d6244a0f3e3dac17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L-PX3wM01PK48XToCchssQ.png"/></div></div></figure><p id="f386" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><em class="ky">观察:-我们看到了确定 k 均值聚类的最佳聚类数的不同方法，最大值算法给出了 k=3 作为 k 均值聚类的最佳聚类数。</em></p><p id="d267" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">现在让我们做 k 均值聚类:-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="45e9" class="nt mo in np b gy nu nv l nw nx"># Compute k-means with k = 3</span><span id="6733" class="nt mo in np b gy ny nv l nw nx">set.seed(123)<br/>km.res &lt;- kmeans(data.scaled, 3, nstart = 25)</span></pre><p id="c3a1" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">可视化 k-means:-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="49a6" class="nt mo in np b gy nu nv l nw nx"># visualising k-means result</span><span id="6daf" class="nt mo in np b gy ny nv l nw nx">fviz_cluster(km.res, data = data.scaled,<br/>palette = c( "#00AFBB", "#E7B800", "#FC4E07"),<br/>ellipse.type = "euclid", # Concentration ellipse<br/>star.plot = TRUE, # Add segments from centroids to items<br/>repel = TRUE, # Avoid label overplotting (slow)<br/>ggtheme = theme_minimal())</span></pre><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/7defa4742873afae5f3c33d23003643c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Efq87qdn2NflOZ2rniULAQ.png"/></div></div><figcaption class="oj ok gj gh gi ol om bd b be z dk translated">kmeans 集群可视化</figcaption></figure><p id="0a1f" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">分类汇总(平均)表:-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="b5e5" class="nt mo in np b gy nu nv l nw nx"># making cluster as factor<br/>km.res$cluster &lt;- as.factor(km.res$cluster)</span><span id="67ea" class="nt mo in np b gy ny nv l nw nx"># assining cluster to the original wine data set<br/>data.clust.kmeans &lt;- cbind(data, cluster = km.res$cluster)</span><span id="b00c" class="nt mo in np b gy ny nv l nw nx"># aggregating the feature by cluster<br/>aggar.kmeans &lt;- aggregate(data.clust.kmeans[,1:13], by=list(data.clust.kmeans$cluster), mean) %&gt;% as.data.frame()</span><span id="4950" class="nt mo in np b gy ny nv l nw nx">aggar.kmeans%&gt;%kable()</span></pre><p id="2c49" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">该表看起来如下</p><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oq"><img src="../Images/bd1ab3593d8aa9e316449bd7932a8cb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SLLT4KCKOWDcKWu58WADMA.png"/></div></div></figure><p id="b9a4" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">可视化聚类数据:-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="5dbb" class="nt mo in np b gy nu nv l nw nx">suppressWarnings( ggpairs(data.clust.kmeans,aes(color=cluster, alpha=0.5),lower = list(combo = wrap("facethist", binwidth = 0.1))))</span></pre><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi or"><img src="../Images/7a55ab34b43ccbc6aa9e43ed83446662.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jjn4kNOkB_j1q3TFXGb2Rg.png"/></div></div></figure><h1 id="ad78" class="mn mo in bd mp mq mr ms mt mu mv mw mx jt my ju mz jw na jx nb jz nc ka nd ne bi translated"><strong class="ak"> 6。K Medoids 聚类</strong></h1><p id="5937" class="pw-post-body-paragraph kc kd in ke b kf nf jo kh ki ng jr kk kl nh kn ko kp ni kr ks kt nj kv kw kx ig bi translated">现在，我将对葡萄酒数据集进行 k-medoids 聚类。这些步骤包括</p><ol class=""><li id="06bc" class="lg lh in ke b kf kg ki kj kl li kp lj kt lk kx ll lm ln lo bi translated">寻找 k-medoids 的最佳簇数</li><li id="a2af" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx ll lm ln lo bi translated">接下来，将进行 k-medoids 聚类</li><li id="b74a" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx ll lm ln lo bi translated">接下来，将显示集群</li><li id="a4ca" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx ll lm ln lo bi translated">接下来，将进行总结</li></ol><blockquote class="kz la lb"><p id="7a52" class="kc kd ky ke b kf kg jo kh ki kj jr kk lc km kn ko ld kq kr ks le ku kv kw kx ig bi translated">注意:- k-medoids 算法也称为 PAM(medoids 周围的分区)。</p></blockquote><p id="c9d9" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">pam 的最佳聚类数(肘形法):-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="3c85" class="nt mo in np b gy nu nv l nw nx"># Elbow method</span><span id="429c" class="nt mo in np b gy ny nv l nw nx">fviz_nbclust(data.scaled, pam, method = "wss") +<br/>geom_vline(xintercept = 3, linetype = 2)+<br/>labs(subtitle = "Elbow method")</span></pre><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/f559cee568c04725e9ba7513071886ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TEMf-__T4dvArNsK6JC0-w.png"/></div></div></figure><p id="55eb" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">pam 中的最佳聚类数(剪影法):-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="a129" class="nt mo in np b gy nu nv l nw nx"># Silhouette method</span><span id="7270" class="nt mo in np b gy ny nv l nw nx">fviz_nbclust(data.scaled, pam, method = "silhouette")+<br/>labs(subtitle = "Silhouette method")</span></pre><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/209f1beb8476cab565266729445282c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EVQQooy1_bN2n5RC6Sn8wg.png"/></div></div></figure><p id="8073" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">pam 中的最佳聚类数(间隙统计):-</p><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/82bcee947e9fc7bbdb79ad10181e73ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-kVxEFGGI0SK77Zt-7qVbg.png"/></div></div></figure><p id="ee7a" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><em class="ky">观察:-我们看到上面的算法给出了 k=3 作为数据集中 k-medoids 聚类的最佳聚类数。</em></p><p id="b2dd" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">现在我们将进行 k-medoids 聚类:-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="d407" class="nt mo in np b gy nu nv l nw nx"># pam with k=3<br/>pam.res &lt;- pam(data.scaled, 3)</span></pre><p id="2e70" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">可视化 k-medoids 集群:-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="b256" class="nt mo in np b gy nu nv l nw nx"># visualising the clusters</span><span id="3deb" class="nt mo in np b gy ny nv l nw nx">fviz_cluster(pam.res,<br/>palette = c("#00AFBB", "#FC4E07","#00AB33"), # color palette<br/>ellipse.type = "t", # Concentration ellipse<br/>repel = TRUE, # Avoid label overplotting (slow)<br/>ggtheme = theme_classic()<br/>)</span></pre><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/e47021fcfdf2c75fec91a860291a2c1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pXFTBZZpZGeml-lsq0162g.png"/></div></div><figcaption class="oj ok gj gh gi ol om bd b be z dk translated">kmedoids 集群可视化</figcaption></figure><p id="fc11" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">分类汇总(平均)表:-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="09f7" class="nt mo in np b gy nu nv l nw nx"># making cluster as factor<br/>pam.res$cluster &lt;- as.factor(pam.res$cluster)</span><span id="3e32" class="nt mo in np b gy ny nv l nw nx"># assining cluster to the original wine data set<br/>data.clust.pam &lt;- cbind(data, cluster = pam.res$cluster)</span><span id="e5e3" class="nt mo in np b gy ny nv l nw nx"># aggregating the clustered data by cluster<br/>aggregate(data.clust.pam[,1:13], by=list(cluster=km.res$cluster), mean)%&gt;%kable()</span></pre><p id="00de" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">输出如下所示</p><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi os"><img src="../Images/e127ba3ae6626394e6aaadbf3ecb6b26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CHCbsHdHoU7e5u9pzTXTnw.png"/></div></div></figure><p id="7af3" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">可视化聚类数据:-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="7177" class="nt mo in np b gy nu nv l nw nx">suppressMessages( ggpairs(data.clust.pam,aes(color=cluster, alpha=0.5),lower = list(combo = wrap("facethist", binwidth = 0.1))))</span></pre><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi or"><img src="../Images/4d1cda969f16ab508cc5a9e39f13c80d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nv-6vxcZNZoaBl35jnP8Tg.png"/></div></div><figcaption class="oj ok gj gh gi ol om bd b be z dk translated">聚类数据集可视化(kmedoids)</figcaption></figure><h1 id="e63b" class="mn mo in bd mp mq mr ms mt mu mv mw mx jt my ju mz jw na jx nb jz nc ka nd ne bi translated"><strong class="ak"> 7。高斯混合模型聚类</strong></h1><p id="8943" class="pw-post-body-paragraph kc kd in ke b kf nf jo kh ki ng jr kk kl nh kn ko kp ni kr ks kt nj kv kw kx ig bi translated">现在，我将对葡萄酒数据集进行高斯混合模型聚类。这些步骤包括</p><ol class=""><li id="10cb" class="lg lh in ke b kf kg ki kj kl li kp lj kt lk kx ll lm ln lo bi translated">我将使用 Mclust 包中的 mclust()函数。它会自动找到一个最佳 k(聚类数),并将该聚类分配给观测值。</li><li id="5f34" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx ll lm ln lo bi translated">接下来，将显示集群</li><li id="64a5" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx ll lm ln lo bi translated">接下来，将进行总结</li></ol><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="4404" class="nt mo in np b gy nu nv l nw nx"># model based clustering<br/>#install.packages("mclust")</span><span id="0c64" class="nt mo in np b gy ny nv l nw nx">library(mclust)<br/>mc&lt;-Mclust(data.scaled)</span><span id="61c5" class="nt mo in np b gy ny nv l nw nx">mc$G</span></pre><p id="492b" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><em class="ky">我们再次看到 k=3 的最佳集群数。</em></p><p id="e9c4" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">可视化集群:-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="fdeb" class="nt mo in np b gy nu nv l nw nx"># Classification: plot showing the clustering</span><span id="86a0" class="nt mo in np b gy ny nv l nw nx">fviz_mclust(mc, "classification", geom = "point",pointsize = 1.5, palette = "jco")</span></pre><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/2d29ff91aa30881031630a362e70ff1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G620OTjgoKKrusdW3anYQw.png"/></div></div><figcaption class="oj ok gj gh gi ol om bd b be z dk translated">集群可视化(GMM)</figcaption></figure><p id="2498" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">分类汇总(平均)表:-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="6306" class="nt mo in np b gy nu nv l nw nx"># making cluster as factor<br/>mc$classification &lt;- as.factor(mc$classification)</span><span id="8e39" class="nt mo in np b gy ny nv l nw nx"># assining cluster to the original wine data set<br/>data.clust.gmm &lt;- cbind(data, cluster = mc$classification)</span><span id="b910" class="nt mo in np b gy ny nv l nw nx"># aggregating the clustered data by cluster<br/>aggregate(data.clust.gmm[,1:13], by=list(data.clust.gmm$cluster), mean)%&gt;%kable()</span></pre><p id="ed87" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">输出表如下所示</p><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi ot"><img src="../Images/4bb6539267d09ce7ec320d988efb0fd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EMZbOq7WhdSFvlXP-B2UWg.png"/></div></div></figure><p id="a509" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">可视化聚类数据:-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="7a92" class="nt mo in np b gy nu nv l nw nx">suppressMessages( ggpairs(data.clust.gmm,aes(color=cluster, alpha=0.5),lower = list(combo = wrap("facethist", binwidth = 0.1))))</span></pre><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi or"><img src="../Images/81a8a9586929fd51dfe5c9f923a1d6b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4O8bD0vKmaPd60yvXYnO-w.png"/></div></div><figcaption class="oj ok gj gh gi ol om bd b be z dk translated">聚类数据集可视化(GMM)</figcaption></figure><h1 id="3e45" class="mn mo in bd mp mq mr ms mt mu mv mw mx jt my ju mz jw na jx nb jz nc ka nd ne bi translated"><strong class="ak"> 8。层次聚类</strong></h1><p id="7fb7" class="pw-post-body-paragraph kc kd in ke b kf nf jo kh ki ng jr kk kl nh kn ko kp ni kr ks kt nj kv kw kx ig bi translated">为了简单起见，我们将使用以下内容。</p><p id="95d9" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="ke io">距离矩阵的方法:- </strong></p><p id="f23e" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">欧几里得的</p><p id="3292" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="ke io">聚类算法中的联动方法:- </strong></p><p id="1a4b" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">1.完成</p><p id="46db" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">2.沃德。D2</p><p id="eac2" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">3.平均的</p><p id="18c9" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">连锁方法影响最佳聚类数，但上述方法更一致且使用更广泛。因此，从现在开始，我将坚持这样做。</p><ul class=""><li id="fa97" class="lg lh in ke b kf kg ki kj kl li kp lj kt lk kx oh lm ln lo bi translated">首先，我将找出上述链接方法的最佳聚类数，</li><li id="7a48" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx oh lm ln lo bi translated">然后将进行聚类</li><li id="5c72" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx oh lm ln lo bi translated">然后将聚类可视化(树状图)</li><li id="ebb5" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx oh lm ln lo bi translated">然后会将该群集分配给原始数据集</li><li id="97ad" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx oh lm ln lo bi translated">然后，我将展示汇总表格</li></ul><p id="8b5e" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">用肘法进行层次聚类的最佳聚类数:-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="961d" class="nt mo in np b gy nu nv l nw nx"># Elbow method</span><span id="a6e8" class="nt mo in np b gy ny nv l nw nx">fviz_nbclust(data.scaled, hcut, method = "wss") +<br/>geom_vline(xintercept = 3, linetype = 2)+<br/>labs(subtitle = "Elbow method")</span></pre><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/c4e13b24108d1f80092fcad17125fa20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4G2-EB7ab5gtk_pT38zaXA.png"/></div></div></figure><p id="9620" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">剪影法聚类的最佳聚类数:-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="a5d1" class="nt mo in np b gy nu nv l nw nx"># Silhouette method</span><span id="5d95" class="nt mo in np b gy ny nv l nw nx">fviz_nbclust(data.scaled, hcut, method = "silhouette")+<br/>labs(subtitle = "Silhouette method")</span></pre><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/209f1beb8476cab565266729445282c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EVQQooy1_bN2n5RC6Sn8wg.png"/></div></div></figure><p id="965c" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">间隙统计法系统聚类的最佳聚类数:-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="bf65" class="nt mo in np b gy nu nv l nw nx"># Gap statistic<br/># nboot = 50 to keep the function speedy.<br/># recommended value: nboot= 500 for your analysis.<br/># Use verbose = FALSE to hide computing progression.</span><span id="0837" class="nt mo in np b gy ny nv l nw nx">set.seed(123)</span><span id="2f78" class="nt mo in np b gy ny nv l nw nx">fviz_nbclust(data.scaled, hcut, nstart = 25, method = "gap_stat",<br/>nboot = 50)+<br/>labs(subtitle = "Gap statistic method")</span></pre><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/25f462183e6bc2cf97d219789394b6cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h0qCd5WaXXDnn1k5ir6_EA.png"/></div></div></figure><p id="255d" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><em class="ky">观察:-我们看到，在分级聚类情况下，总体最佳聚类似乎也是 3。</em></p><p id="09ce" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">现在，我将通过<strong class="ke io"> NbClust </strong>包，从特定链接的角度更深入地探讨最佳集群。</p><p id="8610" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">多数法则下“完全连锁”系统聚类的最佳聚类数</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="4b19" class="nt mo in np b gy nu nv l nw nx">#optimum k<br/># complete linkage</span><span id="a01d" class="nt mo in np b gy ny nv l nw nx">nb.complete &lt;- NbClust(data.scaled, distance = "euclidean", min.nc = 2,max.nc = 10, method = "complete")<br/># visualising the result<br/>suppressWarnings( fviz_nbclust(nb.complete))</span></pre><p id="06d7" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">我们得到的输出如下:-</p><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oo"><img src="../Images/b4eb8287db3673043fffd8a42aa8740a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*40Fogrf5x2PGJ5kuOsHbpA.png"/></div></div></figure><p id="9cfd" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><em class="ky">观察:-这里最佳聚类数也是 k=3。</em></p><p id="8e8d" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">多数法则下“沃德连锁”系统聚类的最佳聚类数</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="7575" class="nt mo in np b gy nu nv l nw nx"># ward linkage</span><span id="8b68" class="nt mo in np b gy ny nv l nw nx">nb.ward2 &lt;- NbClust(data.scaled, distance = "euclidean", min.nc = 2,max.nc = 10, method = "ward.D")<br/>options(repr.plot.width=12, repr.plot.height=9)</span><span id="c305" class="nt mo in np b gy ny nv l nw nx"># visualising the result<br/>suppressWarnings( fviz_nbclust(nb.ward2))</span></pre><p id="306b" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">我们得到以下输出:-</p><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi ou"><img src="../Images/8d748549d1b6e0c8333c5d671732d616.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4cj2GBaH59f_gMnzxIo_mA.png"/></div></div></figure><p id="6fc1" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">观察:-同样，最佳聚类数为 k=3。</p><p id="3f90" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">多数法则下“平均连锁”系统聚类的最佳聚类数</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="ccdb" class="nt mo in np b gy nu nv l nw nx"># average linkage<br/>nb.avg &lt;- NbClust(data.scaled, distance = "euclidean", min.nc = 2,max.nc = 10, method = "average")</span><span id="753e" class="nt mo in np b gy ny nv l nw nx"># visualising the result<br/>suppressWarnings(fviz_nbclust(nb.avg))</span></pre><p id="32c2" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">我们得到的输出如下:-</p><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi ov"><img src="../Images/a67d6866587c59781abf25afd994cd91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JaN5KCnXPEGqJSRUlY0Dhw.png"/></div></div></figure><p id="1a06" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><em class="ky">观察:-此处最佳聚类数为 k=2，尽管 k=8 也是成为最佳数的良好候选。我将通过可视化树状图来探索它。</em></p><p id="5fab" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">现在，我将根据获得的最佳 k 值进行聚类，并相应地可视化和切割树状图。</p><p id="a241" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">进行层次聚类:-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="55af" class="nt mo in np b gy nu nv l nw nx"># hierarchical clustering</span><span id="a6f7" class="nt mo in np b gy ny nv l nw nx"># distance matrix<br/>dist.mat &lt;- dist(data.scaled, method = "euclidean")</span><span id="c207" class="nt mo in np b gy ny nv l nw nx"># doing the hierarchical clustering for diffrent linkage<br/>hc.comp&lt;-hclust(dist.mat,method = "complete")<br/>hc.ward&lt;-hclust(dist.mat,method = "ward.D2")<br/>hc.avg&lt;-hclust(dist.mat,method = "average")</span></pre><p id="fa6c" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">绘制树状图和聚类图</p><p id="8a4b" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">完全联动(k=3):-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="5cf8" class="nt mo in np b gy nu nv l nw nx"># Complete linkage (k=3)</span><span id="10fc" class="nt mo in np b gy ny nv l nw nx">suppressWarnings(<br/># Cut in 3 groups and color by groups<br/>fviz_dend(hc.comp, k = 3, # Cut in three groups<br/>cex = 0.5, # label size<br/>k_colors = c("#2E00DF", "#55AFBB", "#E7B800"),<br/>color_labels_by_k = TRUE, # color labels by groups<br/>rect = TRUE, # Add rectangle around groups<br/>rect_border = c("#2E66DF", "#00AFBB", "#E7B800"),<br/>rect_fill=F,main = "Dendrogram - Complete (k=3)",<br/>xlab = "Objects", ylab = "Distance", sub = "",<br/>ggtheme = theme_minimal()<br/>)<br/>)</span></pre><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/aa06321b4a5b55c221cbee8fdbc6c1ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hi5wdRxNvch1hqfJn04Uyg.png"/></div></div></figure><p id="f39e" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">沃德。D2 连接(k=3):-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="664a" class="nt mo in np b gy nu nv l nw nx"># Ward.D2 linkage (k=3)</span><span id="16fd" class="nt mo in np b gy ny nv l nw nx">suppressWarnings(<br/># Cut in 3 groups and color by groups</span><span id="263f" class="nt mo in np b gy ny nv l nw nx">fviz_dend(hc.ward, k = 3, # Cut in three groups<br/>cex = 0.5, # label size<br/>k_colors = c("#2E66DF", "#00AFBB", "#E7B800"),<br/>color_labels_by_k = TRUE, # color labels by groups<br/>rect = TRUE, # Add rectangle around groups<br/>rect_border = c("#2E66DF", "#00AFBB", "#E7B800"),rect_fill = F,<br/>main = "Dendrogram - ward.D2",xlab = "Objects", ylab = "Distance", sub = "",ggtheme = theme_minimal()<br/>)<br/>)</span></pre><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/3f9167964b431ef4be6e4ed920219a3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OA63HYNGRGBORsTyIXHUmQ.png"/></div></div></figure><p id="e2ad" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">平均链接(k=2):-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="91e3" class="nt mo in np b gy nu nv l nw nx"># Average linkage (k=2)</span><span id="cf9a" class="nt mo in np b gy ny nv l nw nx">suppressWarnings(<br/># Cut in 2 groups and color by groups</span><span id="2fc5" class="nt mo in np b gy ny nv l nw nx">fviz_dend(hc.avg, k = 2, # Cut in two groups<br/>cex = 0.5, # label size<br/>k_colors = c("#2E9F00", "#00AFBB"),<br/>color_labels_by_k = TRUE, # color labels by groups<br/>rect = TRUE, # Add rectangle around groups<br/>rect_border = c("#2E66DF", "#00AFBB"),rect_fill = F,main = "Dendrogram - Average(k=2)",<br/>xlab = "Objects", ylab = "Distance", sub = "",<br/>ggtheme = theme_minimal()<br/>)<br/>)</span></pre><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/17f55c63bc5bb4520a0af183671e719c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YPQkeICC3yn_rd3sauoBRg.png"/></div></div></figure><p id="5803" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">平均链接(k=8):-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="3846" class="nt mo in np b gy nu nv l nw nx"># Average linkage (k=8)</span><span id="b8d9" class="nt mo in np b gy ny nv l nw nx">suppressWarnings(<br/># Cut in eight groups and color by groups</span><span id="6525" class="nt mo in np b gy ny nv l nw nx">fviz_dend(hc.avg, k = 8, # Cut in eight groups<br/>cex = 0.5, # label size<br/>color_labels_by_k = TRUE, # color labels by groups<br/>rect = TRUE, # Add rectangle around groups<br/>rect_fill =F,main = "Dendrogram - Average(k=8)",xlab = "Objects", ylab = "Distance", sub = "",ggtheme = theme_minimal())<br/>)</span></pre><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/802573a04ac6e1afe6baa3d1b789cf15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rIRpnHhKaRknMLC0EMKSgA.png"/></div></div></figure><p id="c0c1" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">观察:-</p><ul class=""><li id="9d6b" class="lg lh in ke b kf kg ki kj kl li kp lj kt lk kx oh lm ln lo bi translated">从树状图中我们可以看出，k=3 的最优值更有意义，似乎是最佳的聚类数。</li><li id="4f92" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx oh lm ln lo bi translated">虽然我得到了 k=2 和 k=8 作为平均连锁最佳聚类数，但从图中可以清楚地看出，k=3 的聚类对该连锁似乎也更准确。</li><li id="769c" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx oh lm ln lo bi translated">因此，我将选择 k=3 的聚类作为(一般)层次聚类的最佳聚类，并做进一步的分析。</li></ul><p id="1b21" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">分类汇总(平均)表:-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="3185" class="nt mo in np b gy nu nv l nw nx"># cutting the dendrogram (complete linkage) at k=3<br/>grp.comp &lt;- cutree(hc.comp,3)</span><span id="9dad" class="nt mo in np b gy ny nv l nw nx"># making cluster as factor<br/>grp.comp &lt;- as.factor(grp.comp)</span><span id="3d31" class="nt mo in np b gy ny nv l nw nx"># assining cluster to the original wine data set<br/>data.clust.hier &lt;- cbind(data, cluster = grp.comp)</span><span id="7fcd" class="nt mo in np b gy ny nv l nw nx"># aggregating the clustered data by cluster<br/>aggregate(data.clust.hier[,1:13], by=list(data.clust.hier$cluster), mean)%&gt;%kable()</span></pre><p id="2e33" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">输出表如下所示</p><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi ow"><img src="../Images/9268ffee8e8a90e8f901c411a09b4e53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q_cVqHvULeTTzf4vFkr-hQ.png"/></div></div></figure><p id="3376" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">可视化聚类数据集:-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="2d0e" class="nt mo in np b gy nu nv l nw nx">suppressMessages( ggpairs(data.clust.hier,aes(color=cluster, alpha=0.5),lower = list(combo = wrap("facethist", binwidth = 0.1))))</span></pre><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi or"><img src="../Images/4400e380711ff7e83464018562c9ede3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W9S24hU7l-aaCloxKUZftw.png"/></div></div></figure><h1 id="4ea3" class="mn mo in bd mp mq mr ms mt mu mv mw mx jt my ju mz jw na jx nb jz nc ka nd ne bi translated"><strong class="ak"> 9。比较型号</strong></h1><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi ox"><img src="../Images/b9cd5696894972a980fd1de4b2e18a18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-mtZ6I2IDSl6bkAEIgvRaQ.png"/></div></div></figure><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/187f27bc05aef76e5312e81e4422fa59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*Q6fkSUu07Y5ynzVEzSvnng.png"/></div></figure><p id="b240" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="ke io"> <em class="ky">观察:- </em> </strong> <em class="ky"> <br/>在上面所有用不同方法得到的聚类列联表中我们可以看到最大数量的聚类出现的频率要么位于对角线上，要么位于不同的行和列上，这意味着几乎所有模型中的聚类分配都是好的。</em></p><h1 id="4052" class="mn mo in bd mp mq mr ms mt mu mv mw mx jt my ju mz jw na jx nb jz nc ka nd ne bi translated"><strong class="ak"> 10。特征简化和模型选择</strong></h1><p id="ac3e" class="pw-post-body-paragraph kc kd in ke b kf nf jo kh ki ng jr kk kl nh kn ko kp ni kr ks kt nj kv kw kx ig bi translated">现在，我将检查并选择满足“clValid”软件包提供的大多数参数的型号。有些是内部措施，有些是稳定措施。</p><p id="65a6" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">内部措施:-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="2ff4" class="nt mo in np b gy nu nv l nw nx">#install.packages("clValid")</span><span id="1c30" class="nt mo in np b gy ny nv l nw nx"># library for model selection<br/>suppressPackageStartupMessages( library(clValid))</span><span id="b94c" class="nt mo in np b gy ny nv l nw nx"># Computing clValid<br/>clmethods &lt;- c("hierarchical","kmeans","pam","model")  # different models</span><span id="272d" class="nt mo in np b gy ny nv l nw nx">suppressWarnings( intern &lt;- clValid(data.scaled, nClust = 2:8,clMethods = clmethods, validation = "internal",method="complete"))</span><span id="e6b9" class="nt mo in np b gy ny nv l nw nx"># Summary<br/>suppressWarnings( summary(intern))</span></pre><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oz"><img src="../Images/534d52769df6b9ff36b8858508903d3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jWOquJcRz4_jXCRKDuCWEA.png"/></div></div></figure><p id="d3f1" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><em class="ky">观察:-<br/>k = 3 的 k-means 在这里似乎很好用。</em></p><p id="292a" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">稳定性措施:-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="00d7" class="nt mo in np b gy nu nv l nw nx"># Stability measures</span><span id="306d" class="nt mo in np b gy ny nv l nw nx">clmethods &lt;- c("hierarchical","kmeans","pam","model")  # different models</span><span id="a973" class="nt mo in np b gy ny nv l nw nx">suppressWarnings( stab &lt;- clValid(data.scaled, nClust = 2:8, clMethods = clmethods,validation = "stability",method="complete"))</span><span id="4b3a" class="nt mo in np b gy ny nv l nw nx"># Display only optimal Scores<br/>optimalScores(stab)%&gt;%kable()</span></pre><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/50178b72e70bfa6cb95bd09f3e1d9a4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*S-487iizUli3MiNWMe3NaA.png"/></div></figure><p id="d202" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">同样，k=3 的 k-means 在这里似乎也很有效。</p><p id="8a3f" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="ke io">减少特征以提高模型的可解释性:- </strong></p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="1a85" class="nt mo in np b gy nu nv l nw nx">#devtools::install_github("o1iv3r/FeatureImpCluster")<br/>#install.packages("flexclust")<br/>library(FeatureImpCluster)<br/>library(flexclust)</span><span id="9e89" class="nt mo in np b gy ny nv l nw nx">set.seed(10)<br/>res &lt;- kcca(data.scaled,k=3)<br/>FeatureImp_res &lt;- FeatureImpCluster(res,as.data.table(data.scaled))<br/>plot(FeatureImp_res)</span></pre><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/a8b4cc93f421b602327eeb1bdaff8969.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HmGsU2bxdVcIIK58cnlBGQ.png"/></div></div></figure><p id="0b2e" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="ke io"> <em class="ky">观察:- </em> </strong> <em class="ky"> <br/>我们可以观察到误分类率最高的是“酒精”，其次是“脯氨酸”和“颜色强度”。因此，我们最终可以选择这些特征作为聚类的主要特征。现在，让我们基于这三个特性进行 kmeans 聚类，并将其与 kmeans 模型的所有特性进行比较，看看它在聚类方面是否工作良好。</em></p><p id="7d18" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="ke io">基于简化特征的 k 均值聚类:- </strong></p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="8484" class="nt mo in np b gy nu nv l nw nx"># making new data framed of reduced features<br/>data.scaled &lt;- as.data.frame(data.scaled)</span><span id="2c90" class="nt mo in np b gy ny nv l nw nx"># data (with reduced features) containing unscaled values of feature<br/>data.reduced &lt;- data[c("Alcohol","Proline","Color.intensity")]</span><span id="c001" class="nt mo in np b gy ny nv l nw nx"># data (with reduced feature) containing scaled fratures<br/>data.scaled.reduced &lt;- data.scaled[c("Alcohol","Proline","Color.intensity")]</span><span id="94f6" class="nt mo in np b gy ny nv l nw nx"># Compute k-means for reduced features with k = 3<br/>set.seed(123)<br/>km.res.reduced &lt;- kmeans(data.scaled.reduced, 3, nstart = 25)</span></pre><p id="cac8" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">可视化简化特征的集群:-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="1c88" class="nt mo in np b gy nu nv l nw nx"># visualising k-means result</span><span id="5a7a" class="nt mo in np b gy ny nv l nw nx">suppressWarnings(</span><span id="25a5" class="nt mo in np b gy ny nv l nw nx">fviz_cluster(km.res.reduced, data = data.scaled.reduced,<br/>palette = c( "#00AFBB", "#E7B800", "#FC4E07"),<br/>ellipse.type = "euclid", # Concentration ellipse<br/>star.plot = TRUE, # Add segments from centroids to items<br/>repel = TRUE, # Avoid label overplotting (slow)<br/>ggtheme = theme_minimal())<br/>)</span></pre><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/c0e2fed2e919963195a1644287e814a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l98AjE8Ylddg-NUbS13eHA.png"/></div></div><figcaption class="oj ok gj gh gi ol om bd b be z dk translated">具有简化特征的聚类(kmeans)可视化</figcaption></figure><p id="909a" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="ke io"> <em class="ky">观察:- </em> </strong> <em class="ky"> <br/>看起来类似于用所有特征进行聚类得到的聚类。这是一个很好的观点，表明我们减少的功能是好的。让我们更深入地探索它。</em></p><p id="7b8d" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">分类汇总(平均)表:-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="9c91" class="nt mo in np b gy nu nv l nw nx"># making cluster as factor<br/>km.res.reduced$cluster &lt;- as.factor(km.res.reduced$cluster)</span><span id="6da3" class="nt mo in np b gy ny nv l nw nx"># assining cluster to the original wine data set<br/>data.clust.reduced.kmeans &lt;- cbind(data.reduced, cluster = km.res.reduced$cluster)</span><span id="d65a" class="nt mo in np b gy ny nv l nw nx"># Aggregating the clustered data (reduced feature) by cluster<br/>aggar.reduced.kmeans &lt;- aggregate(data.clust.reduced.kmeans[,1:3], by=list(data.clust.reduced.kmeans$cluster), mean) %&gt;% as.data.frame()</span><span id="9ca6" class="nt mo in np b gy ny nv l nw nx">aggar.reduced.kmeans %&gt;% kable()</span></pre><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/eb11ef22726e45958436481265000194.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*LOov3LTjl8OImhETTl7gxQ.png"/></div></figure><p id="a48c" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">可视化聚类数据(具有简化的特征):-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="435f" class="nt mo in np b gy nu nv l nw nx">suppressMessages( ggpairs(data.clust.reduced.kmeans,aes(color=cluster, alpha=0.5),lower = list(combo = wrap("facethist", binwidth = 0.1))))</span></pre><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi or"><img src="../Images/ed31b33c6da8c5cb4bbb5a25ae070cf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HkCgZiD5daLulOp6EfJY2g.png"/></div></div></figure><p id="1438" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="ke io">观察:- </strong> <br/> <strong class="ke io">聚类 1:- </strong>酒精度高，颜色强度最高，脯氨酸含量低。<br/> <strong class="ke io">聚类 2:- </strong>酒精最高，脯氨酸最高，颜色强度中等。<br/> <strong class="ke io">集群 3:- </strong>一切最低。</p><p id="053d" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">识别和分类葡萄酒的表格形式(从现在开始可以通过视觉和味觉):-</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="0165" class="nt mo in np b gy nu nv l nw nx">Cluster &lt;- c("Cluster 1","Cluster 2","Cluster 3")<br/>Alcohol &lt;- c("High","Highest","Lowest")<br/>Proline &lt;- c("Low","Highest","Lowest")<br/>Colour.intensity &lt;- c("Highest","Medium","Lowest")<br/>df&lt;-data.frame(Cluster,Alcohol,Proline,Colour.intensity)<br/>df %&gt;% kable()</span></pre><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/3e1809ae92b49b8ee7fc48bc7dfeda28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*kaNtd20aoIgSXosnMPapBg.png"/></div></figure><p id="652d" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">将简化特征的 k 均值与所有特征的 k 均值进行比较:-</p><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi pd"><img src="../Images/73059f0d8f228b104624e005fb2b44ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X856NcZKZeAkylLx09se8Q.png"/></div></div></figure><p id="3799" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">列联表给出了良好的结果，因为对角线条目意味着两个结果中匹配的聚类远远大于不匹配的聚类。</p><blockquote class="pe"><p id="309b" class="pf pg in bd ph pi pj pk pl pm pn kx dk translated"><strong class="ak">最终模型选择:- </strong> <br/> <strong class="ak"> <em class="po">因此，我们最终为这个葡萄酒数据集选择了 kmeans 模型，用“酒精”、“脯氨酸”和“颜色强度”作为我们的简化特征。</em>T24】</strong></p></blockquote><h1 id="e471" class="mn mo in bd mp mq mr ms mt mu mv mw mx jt pp ju mz jw pq jx nb jz pr ka nd ne bi translated">11。结论</h1><p id="d370" class="pw-post-body-paragraph kc kd in ke b kf nf jo kh ki ng jr kk kl nh kn ko kp ni kr ks kt nj kv kw kx ig bi translated">我严格探索了用于葡萄酒数据集聚类的不同聚类算法(kmeans、kmedoids、hierarchical、gaussian mixture model)。<br/>从一开始，在进行多变量分析时，数据集中似乎有三个聚类，最后我们通过深入分析确认了这一点。<br/>最后我选择了 k-means 算法作为这个数据集的最佳聚类算法，该数据集具有简化的特征，即“酒精”、“脯氨酸”和“颜色强度”。</p><p id="a755" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">因此，在给定的数据集中，我们可以通过下表识别出三个聚类:-</p><figure class="nk nl nm nn gt oa gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/160e1f6d0318d3822b29b6076766ac62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*LKE0iCrat81f_7fQj6ZWlw.png"/></div></figure><h1 id="0202" class="mn mo in bd mp mq mr ms mt mu mv mw mx jt my ju mz jw na jx nb jz nc ka nd ne bi translated"><strong class="ak">参考文献:- </strong></h1><ol class=""><li id="436d" class="lg lh in ke b kf nf ki ng kl pt kp pu kt pv kx ll lm ln lo bi translated">统计学习导论(<a class="ae lf" href="https://www.statlearning.com/" rel="noopener ugc nofollow" target="_blank">https://www.statlearning.com/</a>)</li><li id="7fa2" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx ll lm ln lo bi translated">R 中聚类分析实用指南(<a class="ae lf" href="http://www.sthda.com/english/articles/25-clusteranalysis-in-r-practical-guide/" rel="noopener ugc nofollow" target="_blank">http://www . sth da . com/English/articles/25-Cluster Analysis-in-R-practical-Guide/</a>)</li><li id="c911" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx ll lm ln lo bi translated">cran.r-project 网站上的聚类分析包(<a class="ae lf" href="https://cran.r-project.org/view=Cluster" rel="noopener ugc nofollow" target="_blank">https://cran.r-project.org/view=Cluster</a>)</li><li id="ee79" class="lg lh in ke b kf lp ki lq kl lr kp ls kt lt kx ll lm ln lo bi translated">葡萄酒。(1991).UCI 机器学习知识库。(<a class="ae lf" href="https://archive-beta.ics.uci.edu/ml/datasets/wine" rel="noopener ugc nofollow" target="_blank">https://archive-beta.ics.uci.edu/ml/datasets/wine</a>)</li></ol><h1 id="d8b8" class="mn mo in bd mp mq mr ms mt mu mv mw mx jt my ju mz jw na jx nb jz nc ka nd ne bi translated"><strong class="ak">确认</strong></h1><p id="d2d9" class="pw-post-body-paragraph kc kd in ke b kf nf jo kh ki ng jr kk kl nh kn ko kp ni kr ks kt nj kv kw kx ig bi translated">我要感谢<a class="ae lf" href="https://datascience.cheenta.com/" rel="noopener ugc nofollow" target="_blank">奇恩塔统计与数据科学学院</a>和<a class="pw px ep" href="https://medium.com/u/a4bc34279e8?source=post_page-----b04b628b4bca--------------------------------" rel="noopener" target="_blank">斯里吉特慕克吉</a>的指导和宝贵建议。</p><p id="3507" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">如果你喜欢看这样的东西，你可以关注我的<a class="pw px ep" href="https://medium.com/u/cb15bfdd2b54?source=post_page-----b04b628b4bca--------------------------------" rel="noopener" target="_blank">昆丹·k·饶</a>(中)、<a class="ae lf" href="https://www.linkedin.com/in/kundan1rao/" rel="noopener ugc nofollow" target="_blank">昆丹·k·饶</a> (Linkedin)。</p><p id="b1d6" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">谢谢你的来访。继续学习！</p><p id="6c53" class="pw-post-body-paragraph kc kd in ke b kf kg jo kh ki kj jr kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">谢谢你。</p></div></div>    
</body>
</html>