<html>
<head>
<title>Introduction to Machine Learning: Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习导论:线性回归</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/introduction-to-machine-learning-linear-regression-fefab2aab1e7?source=collection_archive---------10-----------------------#2022-09-04">https://blog.devgenius.io/introduction-to-machine-learning-linear-regression-fefab2aab1e7?source=collection_archive---------10-----------------------#2022-09-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="3b20" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">机器学习</h2><div class=""/><div class=""><h2 id="7a39" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">如何用 SKLearn 做线性回归</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/90148ecd317c408974be9f669cb42b8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0qh_u0HfeEwbh4zt"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">机器学习简介:线性回归</figcaption></figure><p id="0a85" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">线性回归是一种在给定数据集的情况下创建线性方程的技术。当我们期望有一个线性相关性时，我们使用这个，也许像一个公寓的平方英尺与租金价格的比较。</p><p id="749d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">首先，我将通过 sklearn 向您展示一个线性回归如何工作的示例，然后我们将构建一个对. csv 文件运行线性回归的项目。对于这个项目，我们的示例数据将是公寓的平方英尺与租金价格的比较。</p><p id="9f31" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在我们完成简单的线性回归之后，我们将讨论一个有多个独立变量的线性回归的例子，通常也称为多元线性回归。</p><p id="a6c5" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在我们开始之前，如果您没有安装 numpy、pandas、sklearn 和 matplotlib 库，我们需要安装它们，我使用 pip，但是如果您使用 Anaconda Python 安装，您也可以使用 conda。</p><pre class="kp kq kr ks gt ma mb mc md aw me bi"><span id="44a7" class="mf mg iq mb b gy mh mi l mj mk">pip install numpy pandas sklearn matplotlib</span></pre><p id="a186" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们将从导入我们需要的库开始。</p><pre class="kp kq kr ks gt ma mb mc md aw me bi"><span id="defa" class="mf mg iq mb b gy mh mi l mj mk">libraries for linear regression with sklearn</span><span id="144d" class="mf mg iq mb b gy ml mi l mj mk">import numpy as np<br/>import math<br/>from sklearn.linear_model import LinearRegression<br/>import matplotlib.pyplot as plt<br/>from mpl_toolkits import mplot3d<br/>import random</span></pre><h1 id="829d" class="mm mg iq bd mn mo mp mq mr ms mt mu mv kf mw kg mx ki my kj mz kl na km nb nc bi translated">随机数据集的线性回归</h1><p id="f613" class="pw-post-body-paragraph le lf iq lg b lh nd ka lj lk ne kd lm ln nf lp lq lr ng lt lu lv nh lx ly lz ij bi translated">接下来，我们将从 x 值中随机生成一组 y 值。我们将使函数<code class="fe ni nj nk mb b">y = 2x</code>的方差为<code class="fe ni nj nk mb b">+/- 0.1</code></p><pre class="kp kq kr ks gt ma mb mc md aw me bi"><span id="36ae" class="mf mg iq mb b gy mh mi l mj mk">randomize a dataset for linear regression</span><span id="26c5" class="mf mg iq mb b gy ml mi l mj mk">arr = [1, 3, 5, 7, 9]<br/>def _func(x):<br/>    return 2 * x + random.uniform(-.1, .1)<br/>x_arr = np.array(arr).reshape(-1, 1)<br/>y_1 = [_func(a) for a in arr]<br/>y_arr = np.array(y_1)</span></pre><p id="8453" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">请注意，我将一组 x 值转换成了一个垂直数组，这对于调用 sklearn 的线性回归函数<br/>非常重要。现在让我们来看看我们的 x 和 y 值</p><pre class="kp kq kr ks gt ma mb mc md aw me bi"><span id="3ff9" class="mf mg iq mb b gy mh mi l mj mk">print(x_arr)<br/>print(y_arr)</span><span id="7a25" class="mf mg iq mb b gy ml mi l mj mk"># expected output<br/>[[1]<br/> [3]<br/> [5]<br/> [7]<br/> [9]]<br/>[ 2.07448972  5.9836436  10.02023471 13.99454233 17.97974717]</span></pre><p id="df13" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们将调用<code class="fe ni nj nk mb b">LinearRegression()</code>将我们的点拟合到一个模型中，然后我们将根据我们的原始点绘制生成的线</p><pre class="kp kq kr ks gt ma mb mc md aw me bi"><span id="3d3e" class="mf mg iq mb b gy mh mi l mj mk">run linear regression from sklearn</span><span id="813a" class="mf mg iq mb b gy ml mi l mj mk">model = LinearRegression().fit(x_arr, y_arr)<br/>plt.scatter(x_arr, y_arr)<br/>plt.plot(x_arr, model.predict(x_arr))<br/>plt.show()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/61096d762e76b63c8b99666d7747dedf.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/0*4UUUHmRAX8eiVcSH"/></div></figure><p id="9050" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">看起来它真的很适合，让我们检查我们的模型的系数和截距，以确保。我们期望截距接近 0，系数 1 应该接近 2</p><pre class="kp kq kr ks gt ma mb mc md aw me bi"><span id="f489" class="mf mg iq mb b gy mh mi l mj mk">print(model.coef_)<br/>print(model.intercept_)</span><span id="bc9d" class="mf mg iq mb b gy ml mi l mj mk"># expected output<br/>[1.99107068]<br/>0.05517809795209949</span></pre><p id="6886" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">很好，我们已经验证了线性回归返回的值接近我们预期的值，现在让我们检查每个预测的平均偏差。为此，我们将采用均方误差(MSE ),将其除以条目数，然后求平方根。</p><pre class="kp kq kr ks gt ma mb mc md aw me bi"><span id="0729" class="mf mg iq mb b gy mh mi l mj mk">mse = sum((y_arr - model.predict(x_arr))**2)<br/>avg_err = math.sqrt(mse/5)<br/>avg_err</span><span id="3c77" class="mf mg iq mb b gy ml mi l mj mk"># expected output<br/>0.02417347631277344</span></pre><p id="30cb" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">因为我们稍后将再次检查 mse 和平均误差，所以我在这里将它们定义为函数。</p><pre class="kp kq kr ks gt ma mb mc md aw me bi"><span id="cf00" class="mf mg iq mb b gy mh mi l mj mk">def mse(i, j):<br/>    return sum((i - j)**2)<br/>def avg_err(sum_err, _len):<br/>    return math.sqrt(sum_err/_len)</span></pre><p id="24c0" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">可以看到我们的平均误差也在 0.1 以内。当我们创建我们的函数时，我们添加了+/- 0.1 的随机偏移，并且我们每个条目的平均预测误差小于 0.1，这验证了我们的线性回归模型给出了良好的预测。</p><h1 id="e743" class="mm mg iq bd mn mo mp mq mr ms mt mu mv kf mw kg mx ki my kj mz kl na km nb nc bi translated">使用 CSV 中的真实数据进行线性回归</h1><p id="06ee" class="pw-post-body-paragraph le lf iq lg b lh nd ka lj lk ne kd lm ln nf lp lq lr ng lt lu lv nh lx ly lz ij bi translated">现在我们已经做了一个小例子，让我们继续线性回归的一些更适用的用途。首先，我们将读入一个. csv 文件，并由此形成 x 和 y 数组，然后我们将构建并检查这个新模型。</p><pre class="kp kq kr ks gt ma mb mc md aw me bi"><span id="7975" class="mf mg iq mb b gy mh mi l mj mk">run sklearn linear regression on real csv data</span><span id="7048" class="mf mg iq mb b gy ml mi l mj mk">arrs = np.genfromtxt("lin_reg_data.csv", delimiter=",", skip_header=1)<br/>x = arrs[:, 0].reshape(-1, 1)<br/>y = arrs[:, 1]<br/>model = LinearRegression().fit(x, y)</span></pre><p id="429f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">让我们检查一下每个条目的平均误差(我们期望这个误差在 100 以下)，系数和截距，看看我们的线相对于常规点是什么样的。</p><pre class="kp kq kr ks gt ma mb mc md aw me bi"><span id="130d" class="mf mg iq mb b gy mh mi l mj mk"># average error<br/>_len = y.size<br/>_avg_err = avg_err(mse(y, model.predict(x)), _len)<br/>print("The average price error per apartment from this model is", _avg_err)</span><span id="d53d" class="mf mg iq mb b gy ml mi l mj mk"># coefficients and intercepts<br/>print("The linear scaling coefficient for square feet in this model is", model.coef_[0])<br/>print("The offsetting price/intercept of this model is", model.intercept_)</span><span id="0877" class="mf mg iq mb b gy ml mi l mj mk"># plot<br/>plt.scatter(x, y)<br/>plt.plot(x, model.predict(x), color="black")</span><span id="b2ec" class="mf mg iq mb b gy ml mi l mj mk"># expected output<br/>The average price error per apartment from this model is 62.58835009983644<br/>The linear scaling coefficient for square feet in this model is 3.6575960237228298<br/>The offsetting price/intercept of this model is 1.3200558525759334</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/ee0834485da70a682de3b9cc514f7d6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/0*YgQiyd18fNxuZhHN"/></div></figure><p id="3a03" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">酷！这个模型看起来很好，我们的平均价格误差在 100 以内。这个城市(西雅图)的房租很贵，每平方英尺 3.66 美元。我们看到截距是 1.32，这看起来也是正确的，因为我们在模型中做了完全合理的假设，即公寓价格只随平方英尺直接变化。<br/>现在，对于我们的最后一个漫游项目，我们将尝试根据硬木树的高度和半径，为硬木树的重量(以吨为单位)拟合一个线性模型。</p><pre class="kp kq kr ks gt ma mb mc md aw me bi"><span id="4263" class="mf mg iq mb b gy mh mi l mj mk">sklearn linear regression on 3d csv data</span><span id="f57b" class="mf mg iq mb b gy ml mi l mj mk">arrs = np.genfromtxt("trees.csv", delimiter=",", skip_header=1)<br/>x = arrs[:, 0:2].reshape(-1, 2)<br/>y = arrs[:, 2]<br/>model = LinearRegression().fit(x, y)</span></pre><p id="6c2d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">现在让我们绘制模型(在 3D 中！)并获得<code class="fe ni nj nk mb b">R^2</code>分数</p><pre class="kp kq kr ks gt ma mb mc md aw me bi"><span id="8e7a" class="mf mg iq mb b gy mh mi l mj mk">plot 3D linear regression output</span><span id="3ae4" class="mf mg iq mb b gy ml mi l mj mk">x1 = []<br/>x2 = []<br/>for entry in x:<br/>    x1.append(entry[0])<br/>    x2.append(entry[1])<br/>fig = plt.figure()<br/>ax = plt.axes(projection = '3d')<br/>ax.scatter(x1, x2, y, alpha=0.3)<br/>ax.plot(x1, x2, model.predict(x), color='black')</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/8c2ed39712f841581a1d23959fe60a53.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/0*gy2el0Kie54y9ESd"/></div></figure><p id="83c5" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">用蓝色绘制点，用黑色绘制线性回归的平面，我们可以直观地看到，我们的线性回归模型预测实际值相当好，现在是时候用验证这一点了。来自 sklearn 的线性回归的 score()函数。</p><pre class="kp kq kr ks gt ma mb mc md aw me bi"><span id="5e3c" class="mf mg iq mb b gy mh mi l mj mk">print("Our R squared value is", model.score(x, y))</span><span id="0a6c" class="mf mg iq mb b gy ml mi l mj mk"># expected output<br/>Our R squared value is 0.9824677872384431</span></pre><p id="e56e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">哇，这是一个非常好的<code class="fe ni nj nk mb b">R^2</code>值，<code class="fe ni nj nk mb b">R^2</code>值越接近 1，线性模型就越精确。我们也来看看系数和截距。</p><pre class="kp kq kr ks gt ma mb mc md aw me bi"><span id="0268" class="mf mg iq mb b gy mh mi l mj mk">print(model.coef_)<br/>print(model.intercept_)</span><span id="0007" class="mf mg iq mb b gy ml mi l mj mk"># expected output<br/>[0.12629947 0.05877378]<br/>-3.575930068798057</span></pre><h1 id="549a" class="mm mg iq bd mn mo mp mq mr ms mt mu mv kf mw kg mx ki my kj mz kl na km nb nc bi translated">结论</h1><p id="9e23" class="pw-post-body-paragraph le lf iq lg b lh nd ka lj lk ne kd lm ln nf lp lq lr ng lt lu lv nh lx ly lz ij bi translated">查看我们的模型的系数和截距告诉我们，尽管我们的飞机在这些值上有多好的拟合，这个模型并不真正有意义。我们预测的值是树的重量，以吨为单位，所以截距-3.76 没有意义，我们应该期望截距为 0。</p><p id="2603" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">此外，通过肉眼检查“trees.csv”数据，我们会看到随着高度和半径的增加，模型的重量看起来越来越大。检查我们的图，我们可以看到，这些点显然形成了某种曲线，这种线性拟合效果很好的原因是因为我们的数据集中的值。</p><p id="a248" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这就结束了我们的线性回归模块。在本模块中，我们先看一个小的一维线性回归示例，然后看一个从 csv 文件读取的更大的示例，最后看一个用测试数据集验证的多元线性回归示例。在下一个模块中，我们将讲述逻辑回归。</p><h2 id="d319" class="mf mg iq bd mn no np dn mr nq nr dp mv ln ns nt mx lr nu nv mz lv nw nx nb iw bi translated">更多由作者</h2><ul class=""><li id="6434" class="ny nz iq lg b lh nd lk ne ln oa lr ob lv oc lz od oe of og bi translated"><a class="ae oh" href="https://pythonalgos.com/keras-optimizers-in-tensorflow-and-common-errors/" rel="noopener ugc nofollow" target="_blank">tensor flow Keras 优化器简介</a></li><li id="8e86" class="ny nz iq lg b lh oi lk oj ln ok lr ol lv om lz od oe of og bi translated"><a class="ae oh" href="https://pythonalgos.com/python-firebase-authentication-integration-with-fastapi/" rel="noopener ugc nofollow" target="_blank"> Python Firebase + Pyrebase 用户授权</a></li><li id="afe0" class="ny nz iq lg b lh oi lk oj ln ok lr ol lv om lz od oe of og bi translated"><a class="ae oh" href="https://pythonalgos.com/the-best-way-to-do-named-entity-recognition-ner/" rel="noopener ugc nofollow" target="_blank">用 Python 进行命名实体识别(NER)</a></li></ul><p id="8a5c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">如果你喜欢这篇文章，请在 Twitter 上分享！为了无限制地访问媒体文章，今天就注册成为<a class="ae oh" href="https://www.medium.com/@ytang07/membership" rel="noopener">媒体会员</a>！别忘了关注我，<a class="ae oh" href="https://www.medium.com/@ytang07" rel="noopener">唐</a>，获取更多关于增长、技术等方面的文章！</p></div></div>    
</body>
</html>