<html>
<head>
<title>Getting started with PySpark DataFrame API</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark DataFrame API 入门</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/getting-started-with-pyspark-dataframe-api-5af17af6a2aa?source=collection_archive---------5-----------------------#2022-10-30">https://blog.devgenius.io/getting-started-with-pyspark-dataframe-api-5af17af6a2aa?source=collection_archive---------5-----------------------#2022-10-30</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><p id="1ab8" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io"> PySpark、Python、Jupyter 笔记本</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b6a4d6d4b74002188e6c0ace9e4c8b5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6faBP6uq9s72e1IsHN4xZg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">火花数据帧 API</figcaption></figure><p id="e0ce" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我们已经介绍过 PySpark 设置。现在是时候探索它的特性了，它将帮助我们进行数据工程！编写我们自己的程序来执行数据转换任务可能相当具有挑战性。但是，Spark 有两个特性可以让您利用现有的技能来执行与数据相关的任务。我们已经谈到了这两个特性:DataFrame API 和 Spark SQL。我们可以利用这两者之一来清理、转换和塑造我们的数据。我们将讨论各种数据框架 API 操作，它们将帮助我们进行数据转换。上一篇文章的链接是<a class="ae ky" rel="noopener ugc nofollow" target="_blank" href="/setup-pyspark-locally-build-your-first-etl-pipeline-with-pyspark-91c3060c6133">这里</a>。</p><p id="bd20" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">完整的代码可在<a class="ae ky" href="https://github.com/hnawaz007/pythondataanalysis/tree/main/PySpark" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上获得。视频教程可以在 YouTube 上找到。</p><p id="c87a" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">PySpark DataFrame API 使我们能够:</p><ul class=""><li id="afb4" class="kz la in jm b jn jo jr js jv lb jz lc kd ld kh le lf lg lh bi translated">从各种来源读取数据，如平面文件、拼花、数据库和 API。</li><li id="31bb" class="kz la in jm b jn li jr lj jv lk jz ll kd lm kh le lf lg lh bi translated">使用数据框架轻松执行数据转换</li><li id="941d" class="kz la in jm b jn li jr lj jv lk jz ll kd lm kh le lf lg lh bi translated">执行选择列、筛选、连接、聚合功能</li></ul><p id="86db" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">大数据背景下的火花</strong></p><p id="5d31" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">让我们解释一下为什么 PySpark 在当前的数据领域很重要。我们都知道并且喜欢 Pandas 在数据操作方面的灵活性和多功能性。然而，由于数据的庞大，我们会遇到内存问题。然而，这不是熊猫的问题，而是运行它的机器的限制。如果我们的数据量大于机器的 ram，我们可以最大化单台机器上的 ram。这将引发内存错误。为了解决这个问题，我们可以使用块大小来批量处理数据，或者在将数据加载到 Pandas 之前简单地过滤数据。这是我在熊猫系列节目中经常被问到的问题之一。现在我们在 Spark 上有了 Pandas API，这是一个为处理大量数据而设计的分布式引擎。它提供了超越单台机器的可扩展性。Spark 上的 pandas API 可以很好地扩展到大型节点集群。给你一些背景资料，Databricks 有一个案例研究。Spark 集群能够在几秒钟内处理和执行 15TB 拼花数据集上的各种数据相关任务。</p><p id="33e1" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">数据框架 API 操作</strong></p><p id="4787" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">让我们打开笔记本，从 PySpark DataFrame API 开始。像往常一样，我们将在顶部导入所需的库。我们将 Java_home 变量设置为 Java 安装目录。这是 spark 所需要的。接下来，我们定义 spark 配置细节并启动 Spark 会话。这在<a class="ae ky" rel="noopener ugc nofollow" target="_blank" href="/setup-pyspark-locally-build-your-first-etl-pipeline-with-pyspark-91c3060c6133">上一节</a>中有详细介绍。</p><p id="adc1" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">让我们从 SQL 数据库中读取数据。我们定义了数据库细节和凭证。使用这些信息，我们构建一个 jdbc url。</p><pre class="kj kk kl km gt ln lo lp lq aw lr bi"><span id="bc92" class="ls lt in lo b gy lu lv l lw lx">database <strong class="lo io">=</strong> "AdventureWorksDW2019"<br/>table <strong class="lo io">=</strong> "DimProduct"<br/>password <strong class="lo io">=</strong> os<strong class="lo io">.</strong>environ['PGPASS']<br/>user <strong class="lo io">=</strong> os<strong class="lo io">.</strong>environ['PGUID']<br/>schema  <strong class="lo io">=</strong> "dbo"<br/>jdbc_url <strong class="lo io">=</strong> f'jdbc:sqlserver://localhost:1433;database={database};encrypt=true;trustServerCertificate=true;'</span></pre><p id="41a4" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我们调用 spark read 函数来导入数据。我们指定数据源是 jdbc 以及额外的选项，如 url、表、凭证和驱动程序的名称。这会将数据加载到数据帧中。现在我们有了一个数据框架，我们将能够运行数据框架 API 操作，以及针对它的 SQL 查询。</p><pre class="kj kk kl km gt ln lo lp lq aw lr bi"><span id="3a84" class="ls lt in lo b gy lu lv l lw lx">df <strong class="lo io">=</strong> spark<strong class="lo io">.</strong>read \<br/>    <strong class="lo io">.</strong>format("jdbc") \<br/>    <strong class="lo io">.</strong>option("url", jdbc_url) \<br/>    <strong class="lo io">.</strong>option("dbtable", f"{schema}.{table}") \<br/>    <strong class="lo io">.</strong>option("user", user) \<br/>    <strong class="lo io">.</strong>option("password", password) \<br/>    <strong class="lo io">.</strong>option("driver", "com.microsoft.sqlserver.jdbc.SQLServerDriver") \<br/>    <strong class="lo io">.</strong>load()</span></pre><p id="a821" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我们可以通过调用 show()方法来查看数据帧。默认情况下显示前 20 行。为了更好的可读性，我们也可以预览为熊猫数据帧。</p><pre class="kj kk kl km gt ln lo lp lq aw lr bi"><span id="f57e" class="ls lt in lo b gy lu lv l lw lx">df<strong class="lo io">.</strong>show()<br/># Convert to Pandas for better readability<br/>df<strong class="lo io">.</strong>limit(10)<strong class="lo io">.</strong>toPandas()</span></pre><p id="aab2" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">Dataframe API 允许我们轻松地执行各种操作。让我们来看几个例子。我们可以用 ColumnRename 函数重命名列<em class="ly">。它接受旧的和新的列名。我们可以用 count 函数检查数据帧包含多少行。我们可以通过选择从数据框架中选择列的子集。</em></p><pre class="kj kk kl km gt ln lo lp lq aw lr bi"><span id="f814" class="ls lt in lo b gy lu lv l lw lx"><em class="ly"># Rename column</em><br/>df <strong class="lo io">=</strong> df<strong class="lo io">.</strong>withColumnRenamed("EnglishProductName","ProductName")<br/>df<strong class="lo io">.</strong>show()<br/><em class="ly"># Row count</em><br/>df<strong class="lo io">.</strong>count(<br/><em class="ly"># Select subset of columns</em><br/>df <strong class="lo io">=</strong> df<strong class="lo io">.</strong>select("ProductKey", "ProductName", "Color")<br/>df<strong class="lo io">.</strong>show())</span></pre><p id="a907" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我们可以使用排序函数对数据集进行排序，并提供我们想要排序的列。默认情况下，排序顺序是升序。为了颠倒排序顺序，我们需要将函数从 spark sql 作为 f 导入。然后我们可以调用排序，并向函数提供关键字 desc 和列名。这将我们的数据集按降序排序。</p><pre class="kj kk kl km gt ln lo lp lq aw lr bi"><span id="96e6" class="ls lt in lo b gy lu lv l lw lx"># Ascending Sort<br/>df<strong class="lo io">.</strong>sort("ProductName")<strong class="lo io">.</strong>show()<br/># Descending Sort<br/><strong class="lo io">from</strong> pyspark.sql <strong class="lo io">import</strong> functions <strong class="lo io">as</strong> F<br/>df<strong class="lo io">.</strong>sort(F<strong class="lo io">.</strong>desc("ProductName"))<strong class="lo io">.</strong>show()</span></pre><p id="e7f1" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">接下来我们来看看过滤器。我们导入的数据将包含可能与分析无关的附加信息。我们可以用过滤器过滤掉不必要的数据。我们调用 select 函数并向它提供我们需要的列。在这一点上，我们可以链接过滤器。在过滤函数中，我们提供列名和值。在这种情况下，我们希望查看 ProductKey 等于 22 的记录。让我们用 show 预览一下数据。如果我们的列不包含任何空格，我们也可以编写不带引号的列。过滤功能也支持通配符操作。我们可以选择名称中包含头盔的产品。这将返回所有带有头盔字的产品。</p><pre class="kj kk kl km gt ln lo lp lq aw lr bi"><span id="082d" class="ls lt in lo b gy lu lv l lw lx"># Filter records with filter<br/>df<strong class="lo io">.</strong>select("ProductKey", "ProductName")<strong class="lo io">.</strong>filter("ProductKey = 22")<strong class="lo io">.</strong>show()<br/># We can also select columns without Quotes<br/>df<strong class="lo io">.</strong>select(df<strong class="lo io">.</strong>ProductKey, df<strong class="lo io">.</strong>ProductName)<strong class="lo io">.</strong>filter("ProductKey = 22")<strong class="lo io">.</strong>show()<br/># Filter records with wildcard<br/>df<strong class="lo io">.</strong>select("ProductKey", "ProductName")<strong class="lo io">.</strong>filter("ProductName like '%helmet%'")<strong class="lo io">.</strong>show()</span></pre><p id="2dad" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我们可以提供多种过滤条件。每个条件必须放在括号中，我们可以用&amp; OR 子句分隔这些条件。这里我们指定我们想要包含单词头盔的产品的记录，并且我们只想要黑色的产品。</p><pre class="kj kk kl km gt ln lo lp lq aw lr bi"><span id="d1ff" class="ls lt in lo b gy lu lv l lw lx"># Filter with multiple conditions<br/>df<strong class="lo io">.</strong>filter((df<strong class="lo io">.</strong>ProductName<strong class="lo io">.</strong>like('%helmet%')) <strong class="lo io">&amp;</strong> (df<strong class="lo io">.</strong>Color<strong class="lo io">==</strong>'Black'))<strong class="lo io">.</strong>show()</span></pre><p id="58c0" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">如果您更喜欢 SQL 并希望在 SQL 中执行过滤操作，那么只需创建一个基于数据帧的视图。一旦我们有了一个视图，我们就可以使用 Spark SQL 编写 SQL。我们可以在 where 子句中提供我们的过滤条件，并过滤出相关的数据。</p><pre class="kj kk kl km gt ln lo lp lq aw lr bi"><span id="0b71" class="ls lt in lo b gy lu lv l lw lx"><em class="ly"># Create a view</em><br/>df<strong class="lo io">.</strong>createOrReplaceTempView("Product")<br/>spark<strong class="lo io">.</strong>sql("select count(1) from Product")<strong class="lo io">.</strong>show()<br/># Filter data using SQL<br/>spark<strong class="lo io">.</strong>sql("select ProductKey, ProductName from Product where ProductKey = 22")<strong class="lo io">.</strong>show()<br/># Filter data using wildcard<br/>spark<strong class="lo io">.</strong>sql("select ProductKey, ProductName from Product where ProductName like '%helmet%'")<strong class="lo io">.</strong>show()</span></pre><p id="1892" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">接下来，我们将导入一个新的数据集。这包含我们产品的交易。用数据库术语来说，这是一个事实表。这有更多的行，因为我们产品的所有销售交易都记录在这里。我们将从该数据帧创建一个视图，并将缓存该视图以获得更好的性能。</p><pre class="kj kk kl km gt ln lo lp lq aw lr bi"><span id="a161" class="ls lt in lo b gy lu lv l lw lx"><em class="ly">#Let's get the fact table with product sales transactions</em><br/>sales <strong class="lo io">=</strong> spark<strong class="lo io">.</strong>read \<br/>    <strong class="lo io">.</strong>format("jdbc") \<br/>    <strong class="lo io">.</strong>option("url", jdbc_url) \<br/>    <strong class="lo io">.</strong>option("dbtable", f"dbo.FactInternetSales") \<br/>    <strong class="lo io">.</strong>option("user", user) \<br/>    <strong class="lo io">.</strong>option("password", password) \<br/>    <strong class="lo io">.</strong>option("driver", "com.microsoft.sqlserver.jdbc.SQLServerDriver") \<br/>    <strong class="lo io">.</strong>load()<br/># Create a view based on DataFrame<br/>sales<strong class="lo io">.</strong>createOrReplaceTempView("sales")<br/>sales<strong class="lo io">.</strong>cache()</span></pre><p id="60ea" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">为了得到我们产品的销售数字，我们需要把这两个视图结合起来。同样，我们可以使用 dataframe API 或 SQL 来执行此操作。我们可以在 Spark SQL 中发出以下 SQL 查询。在这里，我们选择产品名称和销售额。我们在 ProductKey 列中加入了产品和销售视图。我们正在筛选包含关键字头盔的产品记录。因为我们对销售额使用了聚合函数 SUM，所以我们需要按产品名称对其进行分组。我们根据销售额对查询进行降序排序。这将显示产品及其各自的销售数字。</p><pre class="kj kk kl km gt ln lo lp lq aw lr bi"><span id="c219" class="ls lt in lo b gy lu lv l lw lx">spark<strong class="lo io">.</strong>sql("""<br/>SELECT <br/>    p.ProductName,<br/>    SUM(s.SalesAmount) AS SalesAmount<br/>FROM  Product p<br/>    Inner join sales s on p.ProductKey = s.ProductKey<br/>where ProductName like '%helmet%'<br/>Group by <br/>    p.ProductName<br/>order by <br/>    SUM(s.SalesAmount) desc"""<br/>)<strong class="lo io">.</strong>show()</span></pre><p id="bbf7" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我们可以用 dataframe 执行连接操作。我们在 ProductKey 列上连接 sales 和 df 数据帧，并且我们只希望这两个数据帧中的匹配行带有关键字 Inner。</p><pre class="kj kk kl km gt ln lo lp lq aw lr bi"><span id="dc08" class="ls lt in lo b gy lu lv l lw lx">salesjoined <strong class="lo io">=</strong> sales<strong class="lo io">.</strong>join(df, ['ProductKey'],how<strong class="lo io">=</strong>'inner')<br/>salesjoined<strong class="lo io">.</strong>limit(10)<strong class="lo io">.</strong>toPandas()</span></pre><p id="905d" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">类似地，我们可以使用 group by 函数对连接的数据帧执行聚合。我们按照产品和颜色对数据集进行分组。聚合函数是 sum，我们将它应用于销售额。假设我们想要查看每种产品和颜色的最大销售额。我们最终将得到两个名称为销售额列。我们可以使用 Alias 函数来重命名列。</p><pre class="kj kk kl km gt ln lo lp lq aw lr bi"><span id="9ef1" class="ls lt in lo b gy lu lv l lw lx">salesjoined<strong class="lo io">.</strong>groupBy(["ProductName","Color"])<strong class="lo io">.</strong>agg(<br/>    F<strong class="lo io">.</strong>sum("SalesAmount")<strong class="lo io">.</strong>alias("TotalSalesAmounted"),\<br/>    F<strong class="lo io">.</strong>max("SalesAmount")<strong class="lo io">.</strong>alias("MaxSalesAmount")\<br/>    )<strong class="lo io">.</strong>show()</span></pre><p id="7d77" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">大多数情况下，我们会根据数据集中现有的列来派生新的列进行分析。我们可以通过各种方式实现这一目标。首先，我们用 Column 函数计算净销售额列<em class="ly">。我们提供新的列名，然后将这些列提供给 F 点 col 函数。我们导入上面 F 的火花函数。我们从销售额中减去税额，得到净销售额列。</em></p><pre class="kj kk kl km gt ln lo lp lq aw lr bi"><span id="ecf6" class="ls lt in lo b gy lu lv l lw lx">saleswithNet <strong class="lo io">=</strong> salesjoined<strong class="lo io">.</strong>withColumn("NetSales", F<strong class="lo io">.</strong>col("SalesAmount") <strong class="lo io">-</strong> F<strong class="lo io">.</strong>col("TaxAmt"))<br/>saleswithNet<strong class="lo io">.</strong>limit(10)<strong class="lo io">.</strong>toPandas()</span></pre><p id="7add" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我们还基于条件创建了一个列。我们可以像 construct 一样使用 SQL Case 语句。我们用 Column 函数调用<em class="ly">,并提供新的列名。当列值等于值时，我们为它提供一个值。在本例中，我们正在创建区域列。当销售区域关键字为 7 时，区域应为欧洲。我们指定多个 when 条件。如果该值不满足指定的条件，则属于“否则”。我们为美国设定了另外一个标准。让我们预览一下数据帧，在最后我们看到了区域列。我们可以检查该列中的不同值来验证我们的逻辑。</em></p><pre class="kj kk kl km gt ln lo lp lq aw lr bi"><span id="a9ef" class="ls lt in lo b gy lu lv l lw lx"><em class="ly">#Create a new column based on a condition</em><br/>saleswithNet <strong class="lo io">=</strong> saleswithNet<strong class="lo io">.</strong>withColumn(<br/>    'Region',<br/>    F<strong class="lo io">.</strong>when((F<strong class="lo io">.</strong>col("SalesTerritoryKey") <strong class="lo io">==</strong> 7), "Europe")\<br/>    <strong class="lo io">.</strong>when((F<strong class="lo io">.</strong>col("SalesTerritoryKey") <strong class="lo io">==</strong> 8) , "Europe")\<br/>    <strong class="lo io">.</strong>when((F<strong class="lo io">.</strong>col("SalesTerritoryKey") <strong class="lo io">==</strong> 9) , "Pacific")\<br/>    <strong class="lo io">.</strong>when((F<strong class="lo io">.</strong>col("SalesTerritoryKey") <strong class="lo io">==</strong> 10) , "Europe")\<br/>    <strong class="lo io">.</strong>otherwise("Americas")<br/>)<br/>saleswithNet<strong class="lo io">.</strong>limit(10)<strong class="lo io">.</strong>toPandas()<br/><em class="ly">#Check distinct values</em><br/>saleswithNet<strong class="lo io">.</strong>select('Region')<strong class="lo io">.</strong>distinct()<strong class="lo io">.</strong>collect()</span></pre><p id="b9b1" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">最后，我们将从列中删除空值。CarrieTrackingNumber 列为空。我们使用带有列的<em class="ly">函数和类似的结构。我们检查该列是否指定了一个值，否则，将其设置为一个现有值。这将用我们提供的值替换空值或无值。</em></p><pre class="kj kk kl km gt ln lo lp lq aw lr bi"><span id="6186" class="ls lt in lo b gy lu lv l lw lx"><em class="ly">#Replace null with 0</em><br/><strong class="lo io">from</strong> pyspark.sql.functions <strong class="lo io">import</strong> when, lit<br/>saleswithNet <strong class="lo io">=</strong> saleswithNet. WithColumn('CarrierTrackingNumber', when(saleswithNet<strong class="lo io">.</strong>CarrierTrackingNumber<strong class="lo io">.</strong>isNull(), <br/>lit('0'))<strong class="lo io">.</strong>otherwise(saleswithNet<strong class="lo io">.</strong>CarrierTrackingNumber))<br/>saleswithNet<strong class="lo io">.</strong>limit(10)<strong class="lo io">.</strong>toPandas()</span></pre><p id="47db" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">在某些情况下，我们可能希望删除一列。我们可以使用 drop 函数。我们向它提供想要删除的列。这将从我们的数据框架中删除该列。</p><pre class="kj kk kl km gt ln lo lp lq aw lr bi"><span id="cd21" class="ls lt in lo b gy lu lv l lw lx"><em class="ly"># Drop a columns</em><br/>saleswithNet<strong class="lo io">=</strong>saleswithNet<strong class="lo io">.</strong>drop("CustomerPONumber")<br/>saleswithNet<strong class="lo io">.</strong>limit(10)<strong class="lo io">.</strong>toPandas()</span></pre><p id="b441" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">这些是可用于数据转换的一些常见的 Dataframe API 操作。如果你想进一步探索这些，那么查看一下<a class="ae ky" href="https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html" rel="noopener ugc nofollow" target="_blank"> PySpark 文档</a>。</p><p id="ca92" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">结论</strong></p><ul class=""><li id="31d5" class="kz la in jm b jn jo jr js jv lb jz lc kd ld kh le lf lg lh bi translated">我们描述 PySpark DataFrame 是什么，以及如何使用这个 API 的各种特性。</li><li id="8f0b" class="kz la in jm b jn li jr lj jv lk jz ll kd lm kh le lf lg lh bi translated">我们展示了使用 PySpark DataFrame API 导入和操作数据是多么容易。</li><li id="2e19" class="kz la in jm b jn li jr lj jv lk jz ll kd lm kh le lf lg lh bi translated">我们从 SQL 数据库中读取数据，并使用 PySpark 对其进行操作。</li><li id="6d24" class="kz la in jm b jn li jr lj jv lk jz ll kd lm kh le lf lg lh bi translated">完整的代码可以在<a class="ae ky" href="https://github.com/hnawaz007/pythondataanalysis/blob/main/PySpark/PySpark%20Session%202.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到</li></ul></div></div>    
</body>
</html>