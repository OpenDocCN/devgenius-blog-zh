# ä½¿ç”¨ Spark è¿›è¡Œæ•°æ®å¤„ç†:é…¸æ€§åˆè§„æ€§

> åŸæ–‡ï¼š<https://blog.devgenius.io/data-processing-with-spark-acid-compliance-be89464073d9?source=collection_archive---------5----------------------->

åœ¨ä¹‹å‰çš„ Spark æ–‡ç« ä¸­ï¼Œæˆ‘è®²è¿°äº†å¦‚ä½•[è®¾ç½® Spark](https://ownyourdata.ai/wp/data-processing-with-spark-intro/) ï¼Œå¦‚ä½•è®¾ç½® [Hive æ•°æ®ç›®å½•](https://ownyourdata.ai/wp/data-processing-with-spark-data-catalog/)ä»¥åŠå¦‚ä½•å¤„ç†[æ¨¡å¼æ¼”åŒ–](https://ownyourdata.ai/wp/data-processing-with-spark-schema-evolution/)ã€‚ç°åœ¨æ˜¯æ—¶å€™ç»å† ACID å’Œæˆªè‡³ 2022 å¹´ 9 æœˆ Spark æœ‰å“ªäº›é€‰æ‹©:é˜¿å¸•å¥‡èƒ¡è¿ªï¼Œä¸‰è§’æ´²æ¹–å’Œé˜¿å¸•å¥‡å†°å±±ã€‚

# é…¸

åœ¨*ç®¡ç†çš„*ç¯å¢ƒä¸­æ•°æ®å¤„ç†çš„ 4 ä¸ªç‰¹å¾:

1.  åŸå­æ•°
2.  ä¸€è‡´æ€§
3.  éš”ç¦»
4.  æŒä¹…æ€§

å³ä½¿ä» Spark çš„æ–‡çŒ®ä¸­ï¼Œä¹ŸæŒ‡å‡ºä¸å°Šé‡åŸå­æ•°ï¼Œè¿™å½±å“äº†é…¸çš„å…¶ä»–ç‰¹æ€§ã€‚é˜…è¯»æ–‡æ¡£åŒºä¸­é“¾æ¥çš„ç¬¬äºŒç¯‡æ–‡ç« ï¼Œå®ƒå¾ˆå¥½åœ°è§£é‡Šäº†è¿™ç§æƒ…å†µã€‚

ä»€ä¹ˆæ—¶å€™åº”è¯¥å…³å¿ƒé…¸ï¼Ÿåœ¨æˆ‘çœ‹æ¥ï¼Œæ•°æ®å¤„ç†çš„æ‰€æœ‰æ­¥éª¤éƒ½ä¸éœ€è¦ ACIDã€‚åœ¨ç”¨æˆ·å±‚ï¼Œéœ€è¦ç¡®ä¿æ•°æ®ç®¡ç†ã€åˆè§„æ€§ä»¥åŠå¹¶å‘å’Œä¸€è‡´çš„è®¿é—®ï¼ŒACID æ˜¯åœ£æ¯ã€‚ä½†æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä¸´æ—¶/æš‚å­˜åŒºï¼ŒACID å°±ä¸é‚£ä¹ˆé‡è¦äº†ï¼Œå› ä¸ºå®ƒä¸ä¼šå½±å“æµç¨‹/ç”¨æˆ·ã€‚

å½“æˆ‘ä»¬è°ˆåˆ°è¦†ç›–æ¨¡å¼æ—¶ï¼ŒSpark ä¸­çš„ ACID è¢«ç ´åäº†ï¼Œå› ä¸º Spark å°†é¦–å…ˆåˆ é™¤æ•°æ®ï¼Œç„¶åå†å†™å…¥æ•°æ®ã€‚å› æ­¤ï¼Œæœ¬æ–‡å°†åªå…³æ³¨è¿™ç§ä¿å­˜æ¨¡å¼ã€‚

# æ¦‚å¿µéªŒè¯è®¾ç½®

é¦–å…ˆï¼Œæˆ‘éœ€è¦ä¸€äº›è™šæ‹Ÿæ•°æ®ï¼Œè¿™äº›æ•°æ®æ˜¯æˆ‘éšæœºç”Ÿæˆçš„ï¼Œå¹¶ç”¨ pandas ä¿å­˜åˆ°ç£ç›˜ä¸Š:

```
def get_some_data():     start_date = date(2021, 1, 1) end_date = date(2022, 6, 30)
    record_list = [] while start_date <= end_date:
        start_datetime = datetime(
            start_date.year, start_date.month, start_date.day, 
            random.randrange(6, 8), random.randrange(0, 59), random.randrange(0, 59)
        )
        start_kwh = 0.0
        record_list.append([start_datetime, start_kwh])
        for i in range(1, random.randrange(2, 15)):
            record_list.append(
                [
                    start_datetime+timedelta(minutes=i*45), 
                    round(i*random.uniform(1.10, 1.15), 2)
                ]
            )
        start_date = start_date + timedelta(1)

    df = pd.DataFrame(record_list)
    df.columns = ['time', 'produced_kwh']
    df.to_csv('/app/input_data/acid_example_data.txt', sep='\t', index=False)
```

ä¸ºäº†æ­£ç¡®å±•ç¤º Spark ä¸Šçš„å†™æ“ä½œï¼Œæˆ‘å°†æŒ‰æŠ¥å‘Šæœˆä»½å¯¹æ•°æ®è¿›è¡Œåˆ†åŒºï¼Œå¹¶å¼€å‘äº†ä¸¤ç§æ–¹æ³•:1 ç”¨äºå¿«ä¹æµï¼Œ1 ç”¨äºä¸­æ–­ Spark:

```
def get_input_df(spark_session):
    return spark_session.read.option(
        'delimiter', '\t').option(
        'header', 'true').csv(
        'input_data/acid_example_data.txt'
    ) def get_happy_flow(df_input):
    return df_input.withColumn(
        "time", to_timestamp("time")).withColumn(
        "produced_kwh", col("produced_kwh").cast("decimal(18,2)")).withColumn(
        "reporting_date", to_date(col("time"),"yyyy-MM-dd")).withColumn(
        "reporting_month", date_format(col("reporting_date"),"yyyyMM")) def get_error_flow(df_input):
    return df_input.withColumn(
        "produced_kwh", col("produced_kwh").cast("decimal(18,2)")).withColumn(
        "reporting_date", to_date(col("time"),"yyyy-MM-dd")).withColumn(
        "reporting_month", date_format(col("reporting_date"),"yyyyMM"))def get_data(spark_session, flow_type):
    df_input = get_input_df(spark_session)
    if flow_type == 'error':
        df_input = get_error_flow(df_input)
    else:
        df_input = get_happy_flow(df_input)

    return df_input
```

è®°ä½:Spark æ˜¯æƒ°æ€§çš„ï¼Œæ‰€ä»¥ get_error_flow åœ¨åˆ é™¤åä¼šå¤±è´¥ï¼Œå› ä¸ºæ—¶é—´æˆ³ä¸åŒ¹é…ã€‚

# é»˜è®¤ç«èŠ±è®¾ç½®

å¦‚ä½•è·å¾—é»˜è®¤çš„ spark ä¼šè¯:

```
def get_default_spark_session():
    spark_session = SparkSession.builder.appName('default').config(
        'spark.ui.enabled', 'false').getOrCreate()
    spark_session.sparkContext.setLogLevel('ERROR')
    return spark_session
```

åŠ è½½æ•°æ®:

```
def load_default_spark(flow_type='happy'):
    spark_session = get_default_spark_session()
    df_input = get_data(spark_session, flow_type) df_input.write.\
        partitionBy("reporting_month").\
        mode('overwrite').\
        format('parquet').\
        save(path)
```

è¯»å–æ•°æ®:

```
def read_data():
    spark_session = get_default_spark_session()

    number_of_records = spark_session.read.\
        format('parquet').\
        load(path).\
        count() print(f"Number of records: {number_of_records}") spark_session.read.\
        format('parquet').\
        load(path).\
        filter("reporting_date = '2022-03-01'").\
        show()
```

é‚£ä¹ˆå¿«ä¹æµä¸Šå‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ

```
In [1]: from acid.data_processing_acid_default_spark import load_default_spark, read_dataIn [2]: load_default_spark('happy')

In [3]: read_data()
Number of records: 4481
+-------------------+------------+--------------+---------------+
|               time|produced_kwh|reporting_date|reporting_month|
+-------------------+------------+--------------+---------------+
|2022-03-01 07:25:20|        0.00|    2022-03-01|         202203|
|2022-03-01 08:10:20|        1.11|    2022-03-01|         202203|
+-------------------+------------+--------------+---------------+
```

å…³äºé”™è¯¯çš„é‚£ä¸€ä¸ªå‘¢ï¼Ÿ

```
In [4]: load_default_spark('error')
22/09/22 09:20:18 ERROR Executor: Exception in task 0.0 in stage 10.0 (TID 14)
org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading to Spark >= 3.0: Fail to parse '2021-01-01 07:54:21' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.In [5]: read_data()
AnalysisException: Unable to infer schema for Parquet. It must be specified manually.
```

ä¸Šè¿°é”™è¯¯æ˜¯ç”±äºè·¯å¾„ä¸­ä¸å†æœ‰æ•°æ®è¿™ä¸€äº‹å®é€ æˆçš„ï¼Œå› ä¸º Spark é¦–å…ˆåˆ é™¤äº†æ•°æ®ï¼Œç„¶åè¯•å›¾å†™å…¥æ–°æ•°æ®ã€‚å› æ­¤ï¼ŒåŸå­æ€§ç‰¹å¾æ²¡æœ‰å¾—åˆ°å°Šé‡ï¼Œæˆ‘ä»¬ä¸¢å¤±äº†æ•°æ®å’Œä¸€è‡´æ€§ã€‚

åœ¨æˆ‘å†™è¿™ç¯‡æ–‡ç« çš„æ—¶å€™ï¼Œå›´ç»•ç€ data lakehouse å’Œ Spark æ˜¯å¦æ¯”å…¶ä»–æ•°æ®åº“æŠ€æœ¯æ›´å¥½è¿˜æœ‰äº‰è®®ã€‚æˆ‘æ‰¿è®¤æˆ‘æ›´å–œæ¬¢æ•°æ®åº“ï¼Œå› ä¸ºåœ¨ Spark ç”¨ null æ›¿æ¢æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå®ƒä»¬ä¼šåœ¨ write æ—¶å¤±è´¥ã€‚ä½†æˆ‘ç¡®å®è®¤ä¸ºå®ƒä»¬å¯ä»¥ç»“åˆèµ·æ¥åˆ›å»ºä¸€ä¸ªå¼ºå¤§çš„æ•°æ®å¹³å°ï¼Œä»¥å¤‡ä¸æ—¶ä¹‹éœ€ï¼

![](img/17ee57c769048b208e41234c9f9f34ae.png)

å¯ç”¨ ACID for Spark çš„å®ç”¨ç¨‹åºè¿˜é™„å¸¦äº†å…¶ä»–åŠŸèƒ½:æ’å…¥ã€åˆ é™¤å’Œæ—¶é—´æ—…è¡Œã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å°†ä»‹ç»æ¯ä¸ªå®ç”¨ç¨‹åºæ‰€éœ€çš„è®¾ç½®ï¼Œä½†é‡ç‚¹æ˜¯æµ‹è¯•åŸå­æ€§ã€‚

# é˜¿å¸•å¥‡èƒ¡è¿ª

å¦‚ä½•ä¸èƒ¡è¿ªäº§ç”Ÿç«èŠ±ï¼Ÿæˆ‘ä»¬éœ€è¦ç¡®ä¿åœ¨æˆ‘ä»¬çš„ä¼šè¯ä¸­æ‹¥æœ‰ jar ä¾èµ–æ€§ä»¥åŠèƒ¡è¿ªç‰¹å®šçš„é…ç½®:

```
def get_hudi_spark_session():
    spark_session = SparkSession.builder.appName('hudi').config(
        'spark.ui.enabled', 'false').config(
        'spark.jars.packages', 'org.apache.hudi:hudi-spark3.3-bundle_2.12:0.12.0').config(
        'spark.serializer', 'org.apache.spark.serializer.KryoSerializer').config(
        'spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.hudi.catalog.HoodieCatalog').config(
        'spark.sql.extensions', 'org.apache.spark.sql.hudi.HoodieSparkSessionExtension').getOrCreate()
    spark_session.sparkContext.setLogLevel('ERROR')
    return spark_session
```

åŠ è½½æ•°æ®:

```
def load_hudi_spark(flow_type='happy'):
    spark_session = get_hudi_spark_session()
    df_input = get_data(spark_session, flow_type)

    hudi_options = {
        'hoodie.table.name': 'acid_hudi_spark',
        'hoodie.datasource.write.recordkey.field': 'time',
        'hoodie.datasource.write.partitionpath.field': 'reporting_month',
        'hoodie.datasource.write.table.name': 'acid_hudi_spark',
        'hoodie.datasource.write.operation': 'insert_overwrite',
        'hoodie.datasource.write.precombine.field': 'time',
        'hoodie.upsert.shuffle.parallelism': 2,
        'hoodie.insert.shuffle.parallelism': 2,
        'hoodie.datasource.write.hive_style_partitioning': 'true'
    } df_input.write. \
        options(**hudi_options). \
        mode("append"). \
        format("hudi"). \
        save(path)
```

è¯·æ³¨æ„ï¼Œèƒ¡è¿ªéœ€è¦å¤„ç†å¤§é‡çš„é…ç½®:

1.  è®°å½•é”®ï¼Œè¿™æ˜¯ä¸»é”®
2.  åˆ†åŒºè·¯å¾„ï¼Œè¦åˆ†åŒºçš„å­—æ®µ
3.  å†™æ“ä½œï¼Œå¯ä»¥æ˜¯ upsertã€bulk_insert ç­‰
4.  é¢„ç»„åˆå­—æ®µï¼Œå¯¹äºæ—¶é—´æ—…è¡Œå’Œé‡å¤æ•°æ®åˆ é™¤éå¸¸é‡è¦
5.  hive_style_partitioningï¼Œå·²å¯ç”¨ï¼Œä»¥ä¾¿ä¸ºåˆ†åŒºè‡ªåŠ¨å‘ç°åˆ›å»ºæ–‡ä»¶å¤¹ç»“æ„

æ³¨æ„èƒ¡è¿ªæœ‰å®ƒè‡ªå·±çš„å†™æ“ä½œæ¨¡å¼ã€‚é€šè¿‡å°† Spark æ¨¡å¼è®¾ç½®ä¸º appendï¼Œå°†èƒ¡è¿ªæ¨¡å¼è®¾ç½®ä¸º insert_overwriteï¼Œæˆ‘ä»¬ç¡®ä¿äº†åŸå­æ€§ã€‚

è¯»å–æ•°æ®:

```
def read_data():
    spark_session = get_hudi_spark_session() number_of_records = spark_session.read.\
        format('hudi').\
        load(path).\
        count() print(f"Number of records: {number_of_records}") spark_session.read.\
        format('hudi').\
        load(path).\
        filter("reporting_date = '2022-03-01'").\
        show()
```

å¿«ä¹ä¹‹æµ:

```
In [1]: from acid.data_processing_acid_hudi import load_hudi_spark, read_dataIn [2]: load_hudi_spark('happy')

In [3]: read_data()
Number of records: 4481
+-------------------+--------------------+------------------+----------------------+--------------------+-------------------+------------+--------------+---------------+
|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|               time|produced_kwh|reporting_date|reporting_month|
+-------------------+--------------------+------------------+----------------------+--------------------+-------------------+------------+--------------+---------------+
|  20220922115641825|20220922115641825...|  1646119520000000|  reporting_month=2...|bb571e5b-6d19-473...|2022-03-01 07:25:20|        0.00|    2022-03-01|         202203|
|  20220922115641825|20220922115641825...|  1646122220000000|  reporting_month=2...|bb571e5b-6d19-473...|2022-03-01 08:10:20|        1.11|    2022-03-01|         202203|
+-------------------+--------------------+------------------+----------------------+--------------------+-------------------+------------+--------------+---------------+
```

æˆ‘ä»¬çœ‹åˆ°èƒ¡è¿ªå‘æ•°æ®ä¸­æ·»åŠ äº†ä¸€äº›å®¡è®¡åˆ—ï¼Œè¿™äº›åˆ—åœ¨å†…éƒ¨ç”¨äºæ•°æ®æ“ä½œã€‚

é”™è¯¯æµä¸Šå‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ

```
In [4]: load_hudi_spark('error')
22/09/22 12:01:57 ERROR Executor: Exception in task 0.0 in stage 50.0 (TID 212)
org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading to Spark >= 3.0: Fail to parse '2021-01-01 07:54:21' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.In [5]: read_data()
Number of records: 4481
+-------------------+--------------------+------------------+----------------------+--------------------+-------------------+------------+--------------+---------------+
|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|               time|produced_kwh|reporting_date|reporting_month|
+-------------------+--------------------+------------------+----------------------+--------------------+-------------------+------------+--------------+---------------+
|  20220922115641825|20220922115641825...|  1646119520000000|  reporting_month=2...|bb571e5b-6d19-473...|2022-03-01 07:25:20|        0.00|    2022-03-01|         202203|
|  20220922115641825|20220922115641825...|  1646122220000000|  reporting_month=2...|bb571e5b-6d19-473...|2022-03-01 08:10:20|        1.11|    2022-03-01|         202203|
+-------------------+--------------------+------------------+----------------------+--------------------+-------------------+------------+--------------+---------------+
```

ğŸ‰æˆåŠŸï¼æˆ‘ä»¬æœ‰åŸå­æ€§ï¼Œæˆ‘ä»¬çš„æ•°æ®ä¸ä¼šå› ä¸ºå¤„ç†ç®¡é“ä¸­çš„ä¸€äº›é”™è¯¯è€Œæ¶ˆå¤±ï¼

# ä¸‰è§’æ´²æ¹–

ä¸èƒ¡è¿ªä¸€æ ·ï¼ŒDelta Lake åœ¨ spark ä¼šè¯ä¸­éœ€è¦ä¸€ä¸ªé¢å¤–çš„ä¾èµ–é¡¹:

```
def get_delta_spark_session():
    spark_session = SparkSession.builder.appName('delta').config(
        'spark.ui.enabled', 'false').config(
        'spark.jars.packages', 'io.delta:delta-core_2.12:2.1.0').config(
        'spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension').config(
        'spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog').getOrCreate()
    spark_session.sparkContext.setLogLevel('ERROR')
    return spark_session
```

åŠ è½½æ•°æ®:

```
def load_delta_spark(flow_type='happy'):
    spark_session = get_delta_spark_session()
    df_input = get_data(spark_session, flow_type) df_input.write.\
        partitionBy("reporting_month").\
        mode("overwrite").\
        format("delta"). \
        save(path)
```

è¯»å–æ•°æ®:

```
def read_data():
    spark_session = get_delta_spark_session() number_of_records = spark_session.read.\
        format('delta').\
        load(path).\
        count() print(f"Number of records: {number_of_records}") spark_session.read.\
        format('delta').\
        load(path).\
        filter("reporting_date = '2022-03-01'").\
        show()
```

è¿™å¾ˆç®€å•ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬è¿è¡Œå¿«ä¹æµç¨‹:

```
In [1]: from acid.data_processing_acid_delta import load_delta_spark, read_dataIn [2]: load_delta_spark('happy')

In [3]: read_data()
Number of records: 4481
+-------------------+------------+--------------+---------------+
|               time|produced_kwh|reporting_date|reporting_month|
+-------------------+------------+--------------+---------------+
|2022-03-01 07:25:20|        0.00|    2022-03-01|         202203|
|2022-03-01 08:10:20|        1.11|    2022-03-01|         202203|
+-------------------+------------+--------------+---------------+
```

é”™è¯¯çš„é‚£ä¸ªå‘¢ï¼Ÿ

```
In [4]: load_delta_spark('error')
...
AnalysisException: Failed to merge fields 'time' and 'time'. Failed to merge incompatible data types TimestampType and StringTypeIn [5]: read_data()
Number of records: 4481
+-------------------+------------+--------------+---------------+
|               time|produced_kwh|reporting_date|reporting_month|
+-------------------+------------+--------------+---------------+
|2022-03-01 07:25:20|        0.00|    2022-03-01|         202203|
|2022-03-01 08:10:20|        1.11|    2022-03-01|         202203|
+-------------------+------------+--------------+---------------+
```

éå¸¸ç®€å•ï¼

# é˜¿å¸•å¥‡å†°å±±

å¦‚æœè¯´èƒ¡è¿ªå’Œå¾·å°”å¡”ä¸éœ€è¦æ•°æ®ç›®å½•(ä½†å®ƒä»¬å¯ä»¥é›†æˆåœ¨ä¸€èµ·)ï¼Œé‚£ä¹ˆå†°å±±å°±éœ€è¦ã€‚åŒæ ·å¯¹äº Icebergï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿å°†æ­£ç¡®çš„ä¾èµ–é¡¹æ·»åŠ åˆ°æˆ‘ä»¬çš„ä¼šè¯ä¸­:

```
def get_iceberg_spark_session():
    spark_session = SparkSession.builder.appName('iceberg').config(
        'spark.ui.enabled', 'false').config(
        'spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:0.14.1').config(
        'spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions').config(
        'spark.sql.catalog.spark_catalog', 'org.apache.iceberg.spark.SparkSessionCatalog').config(
        'spark.sql.catalog.spark_catalog.type', 'hive').config(
        'spark.sql.catalog.local', 'org.apache.iceberg.spark.SparkCatalog').getOrCreate()
    spark_session.sparkContext.setLogLevel('ERROR')
    return spark_session
```

æˆ‘ä»¬è¿˜éœ€è¦åˆ›å»ºè¡¨ï¼Œç„¶åæ‰èƒ½å‘å…¶ä¸­å†™å…¥æ•°æ®:

```
def create_table():
    spark_session = get_iceberg_spark_session() spark_session.sql(
        "CREATE or REPLACE TABLE iceberg_acid_example (  \
            time timestamp,  \
            produced_kwh decimal(18,2),  \
            reporting_date date, \
            reporting_month string ) \
        USING iceberg \
        PARTITIONED BY (reporting_month) \
        LOCATION '/app/output_data/acid_example/iceberg_spark'"
    )
```

åŠ è½½æ•°æ®:

```
def load_iceberg_spark(flow_type='happy'):
    spark_session = get_iceberg_spark_session()
    df_input = get_data(spark_session, flow_type) df_input.write.\
        mode("overwrite").\
        format("iceberg"). \
        save('iceberg_acid_example')
```

è¯»å–æ•°æ®:

```
def read_data():
    spark_session = get_iceberg_spark_session() number_of_records = spark_session.read.\
        format('iceberg').\
        table('iceberg_acid_example').\
        count() print(f"Number of records: {number_of_records}") spark_session.read.\
        format('iceberg').\
        table('iceberg_acid_example').\
        filter("reporting_date = '2022-03-01'").\
        show()
```

ç°åœ¨è®©æˆ‘ä»¬ğŸš€ï¼š

```
In [1]: from acid.data_processing_acid_iceberg import create_table, load_iceberg_spark, read_dataIn [2]: create_table()In [3]: load_iceberg_spark('happy')

In [4]: read_data()
Number of records: 4481
+-------------------+------------+--------------+---------------+
|               time|produced_kwh|reporting_date|reporting_month|
+-------------------+------------+--------------+---------------+
|2022-03-01 07:25:20|        0.00|    2022-03-01|         202203|
|2022-03-01 08:10:20|        1.11|    2022-03-01|         202203|
+-------------------+------------+--------------+---------------+
```

è€Œé”™è¯¯æµå‘¢ï¼Ÿ

```
In [5]: load_iceberg_spark('error')
...
AnalysisException: Cannot write incompatible data to table 'spark_catalog.default.iceberg_acid_example':
- Cannot safely cast 'time': string to timestampIn [6]: read_data()
Number of records: 4481
+-------------------+------------+--------------+---------------+
|               time|produced_kwh|reporting_date|reporting_month|
+-------------------+------------+--------------+---------------+
|2022-03-01 07:25:20|        0.00|    2022-03-01|         202203|
|2022-03-01 08:10:20|        1.11|    2022-03-01|         202203|
+-------------------+------------+--------------+---------------+
```

# ç»“è®º

è™½ç„¶ Spark ä¸èƒ½ç¡®ä¿ ACID åˆè§„æ€§ï¼Œä½†æˆ‘ä»¬å¯ä»¥åˆ©ç”¨é˜¿å¸•å¥‡èƒ¡è¿ªã€é˜¿å¸•å¥‡å†°å±±æˆ–å¾·å°”å¡”æ¹–æ¥å®ç°è¿™ä¸€ç‚¹ã€‚è¿™ 3 ä¸ªå·¥å…·å¸¦æ¥çš„ä¸ä»…ä»…æ˜¯ ACID:åŸºäºæ•°æ®æ¹–å­˜å‚¨è§£å†³æ–¹æ¡ˆçš„å®Œæ•´æ•°æ®ä»“åº“æŠ€æœ¯ã€‚

é‚£ä¹ˆï¼Œåœ¨è¿™ä¸ªæ¦‚å¿µéªŒè¯ä¸­ï¼Œå®ƒä»¬ä¹‹é—´çš„åŸºæœ¬åŒºåˆ«æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿ

![](img/3d6e55b84133bff2f6116f82f6c67df3.png)

æœ‰è¶£çš„æ˜¯ï¼Œæ ¹æ®æˆ‘ä½¿ç”¨çš„æ•°æ®ï¼Œèƒ¡è¿ªå°†åˆå§‹å­˜å‚¨å¢åŠ äº†å¤§çº¦ 8 å€ã€‚å­˜å‚¨è§„æ¨¡çš„å¢é•¿æ˜¯æœ‰æ„ä¹‰çš„ï¼Œå› ä¸ºèƒ¡è¿ªå°†å®¡è®¡ä¿¡æ¯é™„åŠ åˆ°æ¯ä¸ªè®°å½•ä¸­ã€‚

ä»ä»–ä»¬ä¸‰ä¸ªä¸­ï¼Œä¸‰è§’æ´²æ¹–æ˜¯æœ€å®¹æ˜“å»ºç«‹çš„ã€‚æœ€éš¾çš„æ˜¯ Apache Icebergï¼Œå› ä¸ºå®ƒä¾èµ–äº Hive(ä»¥åŠä¸æ”¯æŒ Java 11 çš„ Hadoop)ã€‚åŒæ ·ï¼Œæœ‰è¶£çš„æ˜¯è§‚å¯Ÿåˆ° Apache Iceberg é™„å¸¦äº†å¯¹ Hadoop çš„ä¾èµ–ï¼Œæ®ä¼ é—»ï¼Œè¿™ç§ä¾èµ–å³å°†ç»“æŸã€‚

Apache èƒ¡è¿ªå¸¦æ¥äº†å¦ä¸€ä¸ªæŒ‘æˆ˜:ä¸æ˜¯è®¾ç½®ï¼Œè€Œæ˜¯åœ¨ç¼–å†™ä»£ç æ—¶éœ€è¦è€ƒè™‘ç‰©ç†æ•°æ®æ¨¡å‹(ä¸»é”®ã€åˆ†åŒºç­‰)ã€‚æˆ‘è®¤ä¸ºè¿™æ˜¯ç§¯æçš„ï¼Œå› ä¸ºå®ƒä¿ƒä½¿æ‚¨åœ¨éƒ¨ç½²åˆ°äº§å“ä¹‹å‰è€ƒè™‘æ‚¨çš„æ•°æ®æ¨¡å¼ã€‚

å“ªä¸ªæœ€å¥½ï¼Ÿè¿™ä¹ˆå°çš„ POC éƒ½éš¾è¯´ï¼Œä½ è¯´å‘¢ï¼Ÿ

è¯·éšæ„æŸ¥çœ‹ Github repoï¼Œå¹¶è‡ªè¡Œè¯•ç”¨ï¼

# è¯æ˜æ–‡ä»¶

1.  [ACID](https://en.wikipedia.org/wiki/ACID)
2.  [ç«èŠ±:æ˜¯å¦ç¬¦åˆ ACID æ ‡å‡†](https://blog.knoldus.com/spark-acid-compliant-or-not/)
3.  [ç«èŠ±ä¿å­˜æ¨¡å¼](https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes)
4.  [å»ºè®¾æ•°æ®æ¹–åº“](https://www.databricks.com/p/ebook/building-the-data-lakehouse)
5.  [é˜¿å¸•å¥‡èƒ¡è¿ª](https://hudi.apache.org/)
6.  [ä¸‰è§’æ´²æ¹–](https://delta.io/learn/getting-started)
7.  [é˜¿å¸•å¥‡å†°å±±](https://iceberg.apache.org/docs/latest/getting-started/)
8.  [Data Lakehouse ç”µå­ä¹¦](https://www.databricks.com/p/ebook/building-the-data-lakehouse?utm_medium=paid+search&utm_source=google&utm_campaign=17159505560&utm_adgroup=137123059355&utm_content=ebook&utm_offer=building-the-data-lakehouse&utm_ad=596310483100&utm_term=data%20lakehouse%20databricks&gclid=Cj0KCQjwj7CZBhDHARIsAPPWv3d2fVr8yAoMM5gbCsd89dsgttYJwAKcYJf_BYT_SvUVSOZoQlRKORgaAozVEALw_wcB)
9.  [Github å›è´­](https://github.com/acirtep/ginlong-data-processing-spark)

æ–‡ç« æœ€åˆå‘è¡¨äº@ [ownyourdata.ai](https://ownyourdata.ai/wp/data-processing-with-spark-acid-compliance/) ã€‚