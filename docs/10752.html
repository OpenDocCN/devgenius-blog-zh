<html>
<head>
<title>Big Data Processing with HADOOP and SPARK in Python on COLAB</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在 COLAB 上用 Python 中的 HADOOP 和 SPARK 处理大数据</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/big-data-processing-with-hadoop-and-spark-in-python-on-colab-bff24d85782f?source=collection_archive---------1-----------------------#2022-11-27">https://blog.devgenius.io/big-data-processing-with-hadoop-and-spark-in-python-on-colab-bff24d85782f?source=collection_archive---------1-----------------------#2022-11-27</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><div class=""><h2 id="b187" class="pw-subtitle-paragraph jk im in bd b jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb dk translated">通过简单的步骤处理大数据</h2></div><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi kc"><img src="../Images/dafe8f9c394a327b6dc4add0777322a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AAHcvrW42etvbQNg"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">照片由<a class="ae ks" href="https://unsplash.com/es/@gerandeklerk?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Geran de Klerk </a>在<a class="ae ks" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="4e24" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated"><code class="fe lp lq lr ls b">HADOOP</code>或<strong class="kv io">H</strong>high<strong class="kv io">A</strong>可用性<strong class="kv io"> D </strong>分布式<strong class="kv io"> O </strong>对象<strong class="kv io"> O </strong>定向<strong class="kv io"> P </strong> latform，是一个基于 Java 的开源软件平台，负责管理大数据应用程序的数据处理和存储(参见<a class="ae ks" href="https://www.databricks.com/glossary/hadoop" rel="noopener ugc nofollow" target="_blank"> DataBricks 词汇表-HADOOP</a>)。Hadoop 不使用一台大型计算机来存储和处理数据，而是允许将多台计算机集群起来，以便更快地并行分析海量数据集(参见<a class="ae ks" href="https://aws.amazon.com/emr/details/hadoop/what-is-hadoop/" rel="noopener ugc nofollow" target="_blank">AWS-什么是 HADOOP </a>)。<code class="fe lp lq lr ls b">HADOOP</code>采用 MapReduce 编程范式；(1) Map 作业，它获取一组数据并将其转换为另一组数据，其中各个元素被分解为元组(键/值对)；(2)Reduce 作业，它将地图的输出作为输入，并将这些数据元组组合成更小的元组集(参见<a class="ae ks" href="https://www.ibm.com/my-en/topics/mapreduce" rel="noopener ugc nofollow" target="_blank">IBM-什么是 Map Reduce </a>)。尽管 HADOOP MapReduce 模型有许多优点，但它对于交互式查询和实时数据处理来说效率不高，因为它依赖于每个处理阶段之间的磁盘写入，因此<code class="fe lp lq lr ls b">HADOOP</code>的子项目<code class="fe lp lq lr ls b">SPARK</code>被提议通过使用内存中的数据存储来解决这个难题(参见<a class="ae ks" href="https://www.talend.com/resources/what-is-hadoop/" rel="noopener ugc nofollow" target="_blank">Talend-什么是 Hadoop </a>)。</p><p id="7acf" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">有一些关于 COLAB 笔记本代码的帖子，演示了 HADOOP(版本 3.3.0)和 SPARK(版本 3.3.0)在 GOOGLE COLAB 上的运行(参考<a class="ae ks" href="https://www.analyticsvidhya.com/blog/2021/05/integration-of-python-with-hadoop-and-spark/" rel="noopener ugc nofollow" target="_blank"> Neelu Tiwari 2021 </a>和<a class="ae ks" href="https://www.analyticsvidhya.com/blog/2020/11/a-must-read-guide-on-how-to-work-with-pyspark-on-google-colab-for-data-scientists/" rel="noopener ugc nofollow" target="_blank"> Aniruddha Bhandari 2020 </a>)。COLAB 代码的副本可以在这里找到，<a class="ae ks" href="https://colab.research.google.com/drive/14l6YBecumszYScDisUEIbHHWeprN7329?usp=sharing" rel="noopener ugc nofollow" target="_blank">https://COLAB . research . Google . com/drive/14l 6 ybecumszyscdisueibhhweprn 7329？usp =分享</a>。然而，随着时间的变化和新版本的引入，代码产生了如下 2 个错误。</p><pre class="kd ke kf kg gt lt ls lu bn lv lw bi"><span id="d06b" class="lx ly in ls b be lz ma l mb mc">--2022-11-27 01:59:09--  https://downloads.apache.org/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz<br/>Resolving downloads.apache.org (downloads.apache.org)... 88.99.95.219, 135.181.214.104, 2a01:4f8:10a:201a::2, ...<br/>Connecting to downloads.apache.org (downloads.apache.org)|88.99.95.219|:443... connected.<br/>HTTP request sent, awaiting response... 404 Not Found<br/>2022-11-27 01:59:10 ERROR 404: Not Found.</span></pre><p id="0e4a" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">第一个错误(上图)与“缺少 HADOOP 安装文件问题”有关，第二个错误(下图)与“安装位置缺少软件包问题”有关。</p><pre class="kd ke kf kg gt lt ls lu bn lv lw bi"><span id="0b5b" class="lx ly in ls b be lz ma l mb mc">---------------------------------------------------------------------------<br/>Py4JError                                 Traceback (most recent call last)<br/>&lt;ipython-input-12-c3c22d90951e&gt; in &lt;module&gt;<br/>      1 #creating a sparksession object and providing appName<br/>----&gt; 2 spark=SparkSession.builder.appName("local[*]").getOrCreate()<br/><br/>4 frames<br/>/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py in __getattr__(self, name)<br/>   1546         else:<br/>   1547             raise Py4JError(<br/>-&gt; 1548                 "{0}.{1} does not exist in the JVM".format(self._fqn, name))<br/>   1549 <br/>   1550     def _get_args(self, args):<br/><br/>Py4JError: org.apache.spark.api.python.PythonUtils.getPythonAuthSocketTimeout does not exist in the JVM</span></pre><p id="1226" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">所以本帖提出基于 HADOOP 3 . 3 . 0 版和 SPARK 3 . 3 . 1 版的一些代码更新。</p><h1 id="e42f" class="md ly in bd me mf mg mh mi mj mk ml mm jt mn ju mo jw mp jx mq jz mr ka ms mt bi translated">(0)先决条件</h1><pre class="kd ke kf kg gt lt ls lu bn lv lw bi"><span id="a69d" class="lx ly in ls b be lz ma l mb mc"># install java<br/>!apt-get install openjdk-8-jdk-headless -qq &gt; /dev/null</span></pre><pre class="mu lt ls lu bn lv lw bi"><span id="0647" class="lx ly in ls b be lz ma l mb mc">#create java home variable <br/>import os<br/>os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"</span></pre><h1 id="5f78" class="md ly in bd me mf mg mh mi mj mk ml mm jt mn ju mo jw mp jx mq jz mr ka ms mt bi translated">(1) HADOOP 安装</h1><pre class="kd ke kf kg gt lt ls lu bn lv lw bi"><span id="c160" class="lx ly in ls b be lz ma l mb mc">#download HADOOP (NEW DOWNLOAD LINK)<br/>!wget https://archive.apache.org/dist/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz</span></pre><pre class="mu lt ls lu bn lv lw bi"><span id="c2a2" class="lx ly in ls b be lz ma l mb mc">#extract the file<br/>!tar -xzvf hadoop-3.3.0.tar.gz</span></pre><pre class="mu lt ls lu bn lv lw bi"><span id="71d6" class="lx ly in ls b be lz ma l mb mc">#copy the hadoop file to user/local<br/>!cp -r hadoop-3.3.0/ /usr/local/</span></pre><pre class="mu lt ls lu bn lv lw bi"><span id="6726" class="lx ly in ls b be lz ma l mb mc">#find  the default Java path<br/>!readlink -f /usr/bin/java | sed "s:bin/java::"</span></pre><pre class="mu lt ls lu bn lv lw bi"><span id="8912" class="lx ly in ls b be lz ma l mb mc">#run Hadoop<br/>!/usr/local/hadoop-3.3.0/bin/hadoop</span></pre><pre class="mu lt ls lu bn lv lw bi"><span id="9776" class="lx ly in ls b be lz ma l mb mc">#create input folder for demonstration exercise<br/>!mkdir ~/testin</span></pre><pre class="mu lt ls lu bn lv lw bi"><span id="3e75" class="lx ly in ls b be lz ma l mb mc">#copy sample files to the input folder<br/>!cp /usr/local/hadoop-3.3.0/etc/hadoop/*.xml ~/testin</span></pre><pre class="mu lt ls lu bn lv lw bi"><span id="05f5" class="lx ly in ls b be lz ma l mb mc">#check that files have been successfully copied to the input folder<br/>!ls ~/testin</span></pre><pre class="mu lt ls lu bn lv lw bi"><span id="7ab9" class="lx ly in ls b be lz ma l mb mc">#run the mapreduce example program<br/>!/usr/local/hadoop-3.3.0/bin/hadoop jar /usr/local/hadoop-3.3.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar grep ~/testin ~/testout 'allowed[.]*'</span></pre><p id="7d5b" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">以下是<strong class="kv io"> Map-Reduce 框架报告部分</strong>的输出:</p><pre class="kd ke kf kg gt lt ls lu bn lv lw bi"><span id="3728" class="lx ly in ls b be lz ma l mv mc"> Map-Reduce Framework<br/>  Map input records=2<br/>  Map output records=2<br/>  Map output bytes=33<br/>  Map output materialized bytes=43<br/>  Input split bytes=111<br/>  Combine input records=0<br/>  Combine output records=0<br/>  Reduce input groups=2<br/>  Reduce shuffle bytes=43<br/>  Reduce input records=2<br/>  Reduce output records=2<br/>  Spilled Records=4<br/>  Shuffled Maps =1<br/>  Failed Shuffles=0<br/>  Merged Map outputs=1<br/>  GC time elapsed (ms)=0<br/>  Total committed heap usage (bytes)=2624585728<br/> Shuffle Errors<br/>  BAD_ID=0<br/>  CONNECTION=0<br/>  IO_ERROR=0<br/>  WRONG_LENGTH=0<br/>  WRONG_MAP=0<br/>  WRONG_REDUCE=0<br/> File Input Format Counters <br/>  Bytes Read=147<br/> File Output Format Counters <br/>  Bytes Written=34</span></pre><p id="ccc1" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">上述 MapReduce 任务基于 HADOOP 安装附带的示例文件。也可以用 Python 脚本编写 mapper 和 reducer。例如，<a class="ae ks" href="https://github.com/anjalysam/Hadoop" rel="noopener ugc nofollow" target="_blank"> Anjalysam (2020) </a>针对用 Python 脚本编写的 20 组新闻组数据集提供了一个 GitHub repo for MapReduce 任务。</p><p id="09be" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">以下是使用 Python 脚本运行 MapReduce 的步骤。</p><p id="19ed" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">(a)下载并提取目标数据集</p><pre class="kd ke kf kg gt lt ls lu bn lv lw bi"><span id="a385" class="lx ly in ls b be lz ma l mb mc">#download and extract 20 newsgroup dataset<br/>!wget http://qwone.com/~jason/20Newsgroups/20news-18828.tar.gz<br/>!tar -xzvf 20news-18828.tar.gz </span></pre><p id="3ee3" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">创建 mapper.py。</p><pre class="kd ke kf kg gt lt ls lu bn lv lw bi"><span id="623d" class="lx ly in ls b be lz ma l mb mc">import sys<br/>import io<br/>import re<br/>import nltk<br/>nltk.download('stopwords',quiet=True)<br/>from nltk.corpus import stopwords<br/>punctuations = '''!()-[]{};:'"\,&lt;&gt;./?@#$%^&amp;*_~'''<br/><br/>stop_words = set(stopwords.words('english'))<br/>input_stream = io.TextIOWrapper(sys.stdin.buffer, encoding='latin1')<br/>for line in input_stream:<br/>  line = line.strip()<br/>  line = re.sub(r'[^\w\s]', '',line)<br/>  line = line.lower()<br/>  for x in line:<br/>    if x in punctuations:<br/>      line=line.replace(x, " ") <br/><br/>  words=line.split()<br/>  for word in words: <br/>    if word not in stop_words:<br/>      print('%s\t%s' % (word, 1))</span></pre><p id="c9b8" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">创建 reducer.py。</p><pre class="kd ke kf kg gt lt ls lu bn lv lw bi"><span id="e5fc" class="lx ly in ls b be lz ma l mb mc">from operator import itemgetter<br/>import sys<br/><br/>current_word = None<br/>current_count = 0<br/>word = None<br/><br/># input comes from STDIN<br/>for line in sys.stdin:<br/>    # remove leading and trailing whitespace<br/>    line = line.strip()<br/>    line=line.lower()<br/><br/>    # parse the input we got from mapper.py<br/>    word, count = line.split('\t', 1)<br/>    try:<br/>      count = int(count)<br/>    except ValueError:<br/>      #count was not a number, so silently<br/>      #ignore/discard this line<br/>      continue<br/><br/>    # this IF-switch only works because Hadoop sorts map output<br/>    # by key (here: word) before it is passed to the reducer<br/>    if current_word == word:<br/>        current_count += count<br/>    else:<br/>        if current_word:<br/>            # write result to STDOUT<br/>            print ('%s\t%s' % (current_word, current_count))<br/>        current_count = count<br/>        current_word = word<br/><br/># do not forget to output the last word if needed!<br/>if current_word == word:<br/>    print( '%s\t%s' % (current_word, current_count))</span></pre><p id="2664" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">(d)运行以下脚本，为上述脚本文件设置权限。</p><pre class="kd ke kf kg gt lt ls lu bn lv lw bi"><span id="ada6" class="lx ly in ls b be lz ma l mb mc">!chmod u+rwx /content/mapper.py<br/>!chmod u+rwx /content/reducer.py</span></pre><p id="e262" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">(e)运行以下脚本来执行 MapReduce 任务。</p><pre class="kd ke kf kg gt lt ls lu bn lv lw bi"><span id="5890" class="lx ly in ls b be lz ma l mb mc">!/usr/local/hadoop-3.3.0/bin/hadoop jar /usr/local/hadoop-3.3.0/share/hadoop/tools/lib/hadoop-streaming-3.3.0.jar -input /content/20news-18828/alt.atheism/49960 -output ~/tryout -file /content/mapper.py  -file /content/reducer.py  -mapper 'python mapper.py'  -reducer 'python reducer.py'</span></pre><p id="8965" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">以下是<strong class="kv io"> Map-Reduce 框架报告部分</strong>的输出:</p><pre class="kd ke kf kg gt lt ls lu bn lv lw bi"><span id="2897" class="lx ly in ls b be lz ma l mv mc"> Map-Reduce Framework<br/>  Map input records=293<br/>  Map output records=1120<br/>  Map output bytes=10685<br/>  Map output materialized bytes=12931<br/>  Input split bytes=96<br/>  Combine input records=0<br/>  Combine output records=0<br/>  Reduce input groups=787<br/>  Reduce shuffle bytes=12931<br/>  Reduce input records=1120<br/>  Reduce output records=787<br/>  Spilled Records=2240<br/>  Shuffled Maps =1<br/>  Failed Shuffles=0<br/>  Merged Map outputs=1<br/>  GC time elapsed (ms)=79<br/>  Total committed heap usage (bytes)=605028352<br/> Shuffle Errors<br/>  BAD_ID=0<br/>  CONNECTION=0<br/>  IO_ERROR=0<br/>  WRONG_LENGTH=0<br/>  WRONG_MAP=0<br/>  WRONG_REDUCE=0<br/> File Input Format Counters <br/>  Bytes Read=11599<br/> File Output Format Counters <br/>  Bytes Written=7698<br/>2022-12-07 12:07:59,136 INFO streaming.StreamJob: Output directory: /root/tryout</span></pre><p id="4003" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">以下是包含字数统计 MapReduce 结果的输出文件的一部分:</p><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/10730657f0e7d28c4dbfba5843c91351.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*ppGtVxbSufdygDf530yLcQ.png"/></div></figure><p id="b926" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">在上面的例子中，HADOOP 使用了一个流实用程序<code class="fe lp lq lr ls b">hadoop-streaming-3.3.0.jar</code>，它使用任何可执行文件或脚本作为映射器和/或缩减器来简化映射/缩减作业。</p><p id="e22f" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">以上<strong class="kv io"> HADOOP 安装</strong>的 COLAB 笔记本代码可以在这里找到，<br/><a class="ae ks" href="https://colab.research.google.com/drive/1Fleb-LBbcLjPV8lxsmrr9foOhBFlmvNc?usp=sharing" rel="noopener ugc nofollow" target="_blank">https://COLAB . research . Google . com/drive/1 fleb-lbbcljv 8 LX smrr 9 foohbflmvnc？usp =共享</a></p><p id="420e" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">进一步阅读请访问:<a class="ae ks" href="https://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/" rel="noopener ugc nofollow" target="_blank">https://www . Michael-noll . com/tutorials/writing-an-Hadoop-MapReduce-program-in-python/</a></p><h1 id="934e" class="md ly in bd me mf mg mh mi mj mk ml mm jt mn ju mo jw mp jx mq jz mr ka ms mt bi translated">(2)火花安装</h1><pre class="kd ke kf kg gt lt ls lu bn lv lw bi"><span id="78ea" class="lx ly in ls b be lz ma l mb mc">#download SPARK (NEW DOWNLOAD LINK)<br/>!wget -q http://apache.osuosl.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz</span></pre><pre class="mu lt ls lu bn lv lw bi"><span id="2c6b" class="lx ly in ls b be lz ma l mb mc">#extract the spark file to the current folder<br/>!tar xf spark-3.3.1-bin-hadoop3.tgz</span></pre><pre class="mu lt ls lu bn lv lw bi"><span id="d38d" class="lx ly in ls b be lz ma l mb mc">#create spark home variable <br/>import os<br/>os.environ["SPARK_HOME"] = "/content/spark-3.3.1-bin-hadoop3"</span></pre><pre class="mu lt ls lu bn lv lw bi"><span id="3290" class="lx ly in ls b be lz ma l mb mc">#install findspark<br/>#findspark searches pyspark installation on the server <br/>#and adds pyspark installation path to sys.path at runtime <br/>#so that pyspark modules can be imported<br/><br/>!pip install -q findspark</span></pre><pre class="mu lt ls lu bn lv lw bi"><span id="61e3" class="lx ly in ls b be lz ma l mb mc">#import findspark<br/>import findspark<br/>findspark.init()</span></pre><pre class="mu lt ls lu bn lv lw bi"><span id="36c2" class="lx ly in ls b be lz ma l mb mc">#import pyspark (added by findspark during runtime)<br/>import pyspark<br/><br/>#import sparksession<br/>from pyspark.sql import SparkSession</span></pre><pre class="mu lt ls lu bn lv lw bi"><span id="1e85" class="lx ly in ls b be lz ma l mb mc">#create sparksession object and provide appName <br/>spark=SparkSession.builder.appName("local[*]").getOrCreate()</span></pre><pre class="mu lt ls lu bn lv lw bi"><span id="cd57" class="lx ly in ls b be lz ma l mb mc">#print spark version<br/>print("Apache Spark version: ", spark.version)</span></pre><p id="4ced" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">以上<strong class="kv io"> SPARK 装置</strong>的 COLAB 笔记本代码可以在这里找到，<a class="ae ks" href="https://colab.research.google.com/drive/1OS_TRKElENuS1mqYGjG_UeR7kyE1OKUl?usp=sharing" rel="noopener ugc nofollow" target="_blank">https://COLAB . research . Google . com/drive/1e 0 rxk 8 vjdnlxcxkwjejgyg 0 tvaaocsdo？usp =分享。</a></p><h1 id="2787" class="md ly in bd me mf mg mh mi mj mk ml mm jt mn ju mo jw mp jx mq jz mr ka ms mt bi translated">(3)使用 PIP 的火花安装</h1><p id="469a" class="pw-post-body-paragraph kt ku in kv b kw mx jo ky kz my jr lb lc mz le lf lg na li lj lk nb lm ln lo ig bi translated">SPARK 也可以使用 PIP 安装；比上述方法更简单的方法。</p><pre class="kd ke kf kg gt lt ls lu bn lv lw bi"><span id="e579" class="lx ly in ls b be lz ma l mb mc">#install pyspark<br/>!pip install pyspark</span></pre><pre class="mu lt ls lu bn lv lw bi"><span id="cd19" class="lx ly in ls b be lz ma l mb mc">#import sparksession<br/>from pyspark.sql import SparkSession</span></pre><pre class="mu lt ls lu bn lv lw bi"><span id="eeeb" class="lx ly in ls b be lz ma l mb mc">#create sparksession object and provide appName <br/>spark=SparkSession.builder.appName("local[*]").getOrCreate()</span></pre><pre class="mu lt ls lu bn lv lw bi"><span id="9875" class="lx ly in ls b be lz ma l mb mc">#print spark version<br/>print("Apache Spark version: ", spark.version)</span></pre><p id="8a1a" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">上面使用 Pip 安装<strong class="kv io"> SPARK 的 COLAB 笔记本代码可以在这里找到，<a class="ae ks" href="https://colab.research.google.com/drive/1yBGRPaC5A52ukus1u0-6AHHsWSir6hba?usp=sharing" rel="noopener ugc nofollow" target="_blank">https://COLAB . research . Google . com/drive/1 ybgrpac 5a 52 uk us 1 u 0-6 ahswsir 6 HBA？usp =分享</a></strong></p><h1 id="329b" class="md ly in bd me mf mg mh mi mj mk ml mm jt mn ju mo jw mp jx mq jz mr ka ms mt bi translated">(4)火花代码演示</h1><p id="7ed5" class="pw-post-body-paragraph kt ku in kv b kw mx jo ky kz my jr lb lc mz le lf lg na li lj lk nb lm ln lo ig bi translated"><a class="ae ks" href="https://sparkbyexamples.com/pyspark/pandas-vs-pyspark-dataframe-with-examples/" rel="noopener ugc nofollow" target="_blank"> SparkByExamples </a>提供了一些代码示例来演示 SPARK 数据帧的使用，如下所示。其工作原理类似于 Python 熊猫数据框。</p><pre class="kd ke kf kg gt lt ls lu bn lv lw bi"><span id="8e25" class="lx ly in ls b be lz ma l mb mc">#create sample data for spark<br/>data = [("James","","Smith",30,"M",60000),<br/>        ("Michael","Rose","",50,"M",70000),<br/>        ("Robert","","Williams",42,"",400000),<br/>        ("Maria","Anne","Jones",38,"F",500000),<br/>        ("Jen","Mary","Brown",45,"F",0)]<br/><br/>columns = ["first_name","middle_name","last_name","Age","gender","salary"]<br/>pysparkDF = spark.createDataFrame(data = data, schema = columns)<br/>pysparkDF.printSchema()<br/>pysparkDF.show(truncate=False)</span></pre><pre class="mu lt ls lu bn lv lw bi"><span id="6f4d" class="lx ly in ls b be lz ma l mv mc">root<br/> |-- first_name: string (nullable = true)<br/> |-- middle_name: string (nullable = true)<br/> |-- last_name: string (nullable = true)<br/> |-- Age: long (nullable = true)<br/> |-- gender: string (nullable = true)<br/> |-- salary: long (nullable = true)<br/><br/>+----------+-----------+---------+---+------+------+<br/>|first_name|middle_name|last_name|Age|gender|salary|<br/>+----------+-----------+---------+---+------+------+<br/>|James     |           |Smith    |30 |M     |60000 |<br/>|Michael   |Rose       |         |50 |M     |70000 |<br/>|Robert    |           |Williams |42 |      |400000|<br/>|Maria     |Anne       |Jones    |38 |F     |500000|<br/>|Jen       |Mary       |Brown    |45 |F     |0     |<br/>+----------+-----------+---------+---+------+------+</span></pre><p id="424f" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">使用该数据框架，可以对大数据进行一些处理，如均值和平均值，如下所示(参见<a class="ae ks" href="https://sparkbyexamples.com/pyspark/pandas-vs-pyspark-dataframe-with-examples/" rel="noopener ugc nofollow" target="_blank"> SparkByExamples </a>)。</p><pre class="kd ke kf kg gt lt ls lu bn lv lw bi"><span id="c0bf" class="lx ly in ls b be lz ma l mb mc">from pyspark.sql.functions import mean, col, max<br/>#Example 1<br/>df2=pysparkDF.select(mean("age"),mean("salary")).show()<br/>#Example 2<br/>pysparkDF.groupBy("gender") \<br/>         .agg(mean("age"),mean("salary"),max("salary")) \<br/>         .show()</span></pre><pre class="mu lt ls lu bn lv lw bi"><span id="f04c" class="lx ly in ls b be lz ma l mv mc">+--------+-----------+<br/>|avg(age)|avg(salary)|<br/>+--------+-----------+<br/>|    41.0|   206000.0|<br/>+--------+-----------+<br/><br/>+------+--------+-----------+-----------+<br/>|gender|avg(age)|avg(salary)|max(salary)|<br/>+------+--------+-----------+-----------+<br/>|     M|    40.0|    65000.0|      70000|<br/>|     F|    41.5|   250000.0|     500000|<br/>|      |    42.0|   400000.0|     400000|<br/>+------+--------+-----------+-----------+</span></pre><p id="70a9" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">进一步阅读:<a class="ae ks" href="https://sparkbyexamples.com/" rel="noopener ugc nofollow" target="_blank">https://sparkbyexamples.com/</a></p></div></div>    
</body>
</html>