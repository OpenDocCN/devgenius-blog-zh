<html>
<head>
<title>Data Flow from Excel to PostgreSQL DB with Airflow— Feat. Pyspark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从 Excel 到 PostgreSQL 数据库的数据流。Pyspark</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/dag-flow-from-excel-to-postgresql-db-with-pyspark-293eb203b2a0?source=collection_archive---------2-----------------------#2022-12-08">https://blog.devgenius.io/dag-flow-from-excel-to-postgresql-db-with-pyspark-293eb203b2a0?source=collection_archive---------2-----------------------#2022-12-08</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><h1 id="928b" class="jk jl in bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated"><strong class="ak">目标:</strong></h1><p id="ed2b" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">使用 Airflow scheduler 提供了三条到 PostgreSQL 数据库的路径。<br/>探索这些路线的选择和挑战。</p><p id="125f" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">1) PostgreSQL 运算符，在 Airflow 中内部建立连接。</p><p id="bf31" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">2)使用 Python 操作符和 psycopg2 库将数据写入数据库表。</p><p id="65bb" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">3)使用通过 helper 函数调用的 pyspark 会话，然后将数据写入数据库。将使用 JDBC 驱动程序。</p><p id="a5b6" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">三者中最简单的是使用 Pyspark 会话。(<a class="ae ll" href="https://medium.com/@kamaljp/pyspark-pipeline-multi-table-excel-worksheet-to-ml-pipeline-and-prediction-storage-to-database-75b79b8468fe" rel="noopener">更简单的选择是不使用气流。在这里了解一下</a>上游任务可以将 excel 文件转换成 csv 格式并写入目标文件夹。相同的代码是<a class="ae ll" href="https://github.com/Kamalabot/dagflowPostgres" rel="noopener ugc nofollow" target="_blank">在回购</a>中共享。在你开始问之前，管道有多快？这是运行时间数据。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi lm"><img src="../Images/110e1cc21aee82f87dc0f6a463b6341d.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*9-rrlJmDvFWh3botSF4m9A.png"/></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk translated">显示两个任务的 DAG 流程</figcaption></figure><p id="26bc" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">上面是两个任务的时间线。t1 将 excel 文件转换为 csv 文件，并将其写入目标。t2 获取该 csv 并将其写入 PostgreSQL 服务器。该图直接取自气流。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/b99ffb0605b7f0f79fbc21fb4b83bd52.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*6flY9JQUW_Y7gsFX_BdrFg.png"/></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk translated">将 excel 文件转换为 csv</figcaption></figure><p id="b29d" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">第二张图是将 excel 文件转换为 csv 文件的时间表。几乎不需要 12 秒。Pyspark 用了更多的时间。这是因为，它正在创建 spark 会话，使用 JDBC 驱动程序读取 CSV 文件并将其写入 PG 数据库。您可以看到气流调度程序记录了所有这些内容。</p><p id="4920" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">数据工程很复杂，就像你的生活伴侣或爱人。你需要爱它，优雅地处理它，然后带着它走向下一个复杂的问题。(数据工程师在一些公司被称为数据战士是有原因的)</p><p id="fa8d" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">但是以 pyspark 为特色的内部气流，看起来还是很复杂…告诉我们为什么要用它？</p><h1 id="69fc" class="jk jl in bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">我为什么使用 Pyspark:</h1><p id="7910" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">因为，PostgreSQL 操作符由于某种原因没有安装在我的本地系统中。以下是我采取的故障诊断步骤。</p><p id="e9b6" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">0.重新安装 PostgreSQL 提供程序，甚至尝试安装 Amazon PostgreSQL 提供程序。不起作用。</p><ol class=""><li id="ede9" class="lz ma in kk b kl lg kp lh kt mb kx mc lb md lf me mf mg mh bi translated">创建了新的<strong class="kk io">虚拟环境并安装了 Airflow </strong>及其提供商。很顺利。</li></ol><pre class="ln lo lp lq gt mi mj mk bn ml mm bi"><span id="d48e" class="mn jl in mj b be mo mp l mq mr">&gt; python3 -m venv airflow-env<br/>&gt; source airflow-env/bin/activate<br/>&gt; pip install --upgrade pip<br/>&gt; pip install apache-airflow apache-airflow-providers-postgres</span></pre><p id="7f8f" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">2)启动气流网络服务器。没问题，一切从零开始</p><p id="c903" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">3)尝试为虚拟环境内的气流设置默认数据库。<em class="ms">它与父文件系统中已有的气流冲突。</em></p><p id="f162" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">4)想到用 Dockers 做气流 Web 服务器+调度器。我的 2gb 的机器卡住了，然后重新启动。</p><p id="f616" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">5)已检查重启是否正常工作。没有。</p><h1 id="a0f2" class="jk jl in bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated"><strong class="ak">目标和失败的重要性:</strong></h1><p id="c3dd" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">当我决定使用气流将 XL 文件中的数据移动到 PostgreSQL 数据库时，<strong class="kk io">成功条件</strong>是 XL 数据应该在 PostgreSQL 数据库中，并且应该通过气流完成。</p><p id="87a2" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">无论我使用 Bash 操作符、Ssh 操作符还是其他操作符，都不是一个条件。此外，我是在 PostgresOperator 失败后才知道这些运算符的。所以失败很重要。<br/>另一个目标是我必须学习。如果 PostgreSQL 操作符起作用，我可能仍然会尝试使用 Pyspark。但不会研究如何使用其他运算符。让我们继续前进。</p><h1 id="cfee" class="jk jl in bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">将 PG 连接到气流的命令行:</h1><pre class="ln lo lp lq gt mi mj mk bn ml mm bi"><span id="d56a" class="mn jl in mj b be mo mp l mq mr">airflow connections add 'my_new_db' --conn-uri 'postgresql://postgres:password@127.0.0.1:5432/database</span></pre><p id="c8c1" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">上述连接字符串在 Airflow 服务器和 PG 数据库以及 PostgreSQL 操作符之间创建连接。我没有使用这条路线，但是在这里分享它，因为通过 Airflow GUI 创建连接可能会令人困惑。</p><p id="d26a" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">气流 UI，可能有点混乱。原来模式意味着表名。大多不言自明。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/f6e3e18c31b0e8ac814ef328b69ea17f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*6eaC6VlIqYAhUzFi0KsO_A.png"/></div></figure><p id="7800" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">像 PostgreSQL / SQL 这样的连接类型会在 pip 安装完库并重启机器后出现。不仅仅是气流服务器。经过两个小时的修改，我发现了一个错误，并找到了一条替代路线</p><h1 id="1b41" class="jk jl in bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">在进入代码之前:</h1><p id="abfd" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">在气流的情况下，还有一些额外的摔跤要做。dag 脚本必须位于 airflow 安装的 DAG 文件夹中，通常是/home/user _ { your NAME }/airflow/DAGs。然后是<br/>气流实例。</p><pre class="ln lo lp lq gt mi mj mk bn ml mm bi"><span id="df3f" class="mn jl in mj b be mo mp l mq mr">&gt; airflow db init ==&gt; Will start the native database server and <br/>                      allow airflow to use it. <br/>&gt; airflow webserver -p 8081 ==&gt; Will start the Airflow Webserver where <br/>                          the Dags are displayed. <br/>&gt; airflow scheduler ==&gt; Will start the scheduler, which finds the airflow <br/>                        webserver and attaches itself (or the opposite way, <br/>                          not sure)</span></pre><p id="7d63" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">调度程序和 web 服务器必须相互通信。在我的系统中，调度程序一直处于离线状态。我不得不手动终止调度程序并重启它。这些日志可用于排除此类故障。</p><p id="76e3" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated"><strong class="kk io">DAG 脚本可能会有导入错误。(和 Postgres 操作符错误一样，我之前说过)</strong>支持模块文件必须在 Dag 脚本所在的同一个文件夹中。那是在/home/user _ {你的名字}/airflow/dags/里面。</p><p id="7fca" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">可以编写另一个 DAG，以便根据触发器仅将文件从您正在编码的文件夹复制到 airflow dags 文件夹。(一个练习可能是！！)</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/11248eb619553464590de38d244f1e22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*VYwxvl7GsF-DEEadhYufJw.png"/></div></figure><p id="c387" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">新的 Dag 需要几分钟才能在 Airflow 服务器上显示出来。我想数据库读取新的 dag 会有一个时间延迟，所以一直刷新页面，直到 Dag 显示出来，或者在顶部弹出一个错误。</p><h1 id="f0f6" class="jk jl in bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">气流需要功能:</h1><p id="f7d0" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">我花了一些时间才意识到用于创建 dag 的 dag_script.py 是 python 脚本。变量可以像我们在 python 脚本中一样声明。请小心，因为 DAG 代码可以通过 docker 或 github repo 转移到完全不同的环境中。如果你见过 Kaggle 笔记本，那些由 dockers<a class="ae ll" href="https://github.com/Kaggle/docker-python" rel="noopener ugc nofollow" target="_blank">https://github.com/Kaggle/docker-python</a>创造的紧凑环境。</p><p id="9049" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">最后，一些代码…符合 dag _ scriptthat，它创建 dag(有向无环图)来移动数据。</p><pre class="ln lo lp lq gt mi mj mk bn ml mm bi"><span id="a467" class="mn jl in mj b be mo mp l mq mr">import os<br/>import warnings<br/>warnings.filterwarnings('ignore')<br/>from dagHelper import *<br/>import configparser<br/>from airflow import DAG<br/>from airflow.utils.dates import days_ago<br/>from airflow.operators.python import PythonOperator<br/><br/>#The import line creating the error is below<br/>#from airflow.providers.postgres.operators.postgres import PostgresOperator<br/><br/>#The import functions and variables declations are not shared for brevity<br/>with DAG(<br/>        dag_id='Excel_CSV_Spark_writer',<br/>        default_args=args,<br/>        schedule_interval='0 0 * * *',<br/>        start_date=days_ago(2)<br/>) as dag:<br/>    t1 = PythonOperator(<br/>            task_id='t1',<br/>            python_callable=transformXL,<br/>            op_kwargs={<br/>                'fileLocation':sourceName,<br/>                'fileDestination':newDest,<br/>                'worksheet':'StoreData'<br/>                }<br/>            )<br/><br/>    t2 = PythonOperator(<br/>            task_id='t2',<br/>            python_callable=transformDB,<br/>            op_kwargs={<br/>                'fileLocation':newDest,<br/>                'tableName':tableName,<br/>                'config':config}<br/>        )<br/>    t1 &gt;&gt; t2<br/>if __name__ == "__main__":<br/>    dag.cli()</span></pre><p id="0aef" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">这看起来很简单。Python-callable 是隐藏在 dagHelper.py 文件/模块中的函数。这就是两个主要的支持功能。光彩照人。</p><pre class="ln lo lp lq gt mi mj mk bn ml mm bi"><span id="36f6" class="mn jl in mj b be mo mp l mq mr"><br/>import pyspark<br/>from pyspark.sql import SparkSession<br/>from pyspark.sql.functions import *<br/>import pandas as pd<br/>#transformation function<br/>def transformXL(fileLocation,fileDestination,worksheet = 'DataSource'):<br/>    <br/>    sourceDF = pd.read_excel(fileLocation,worksheet,header=0)<br/>    <br/>    frameCols = sourceDF.columns<br/>    dropColumns = []<br/>    <br/>    for x in frameCols:<br/>        if x.split(':')[0] == 'Unnamed':<br/>            dropColumns.append(x)<br/>    sourceDF.drop(dropColumns,inplace=True,axis=1)<br/>    <br/>    #Formatting the columns headers to <br/>    frameCols = sourceDF.columns<br/>    cols = []<br/>    for x in frameCols:<br/>        temp = x.replace(" ",'_')<br/>        temp = temp.replace("-","_")<br/>        temp = temp.replace(":","_")<br/>        cols.append(temp)<br/>        <br/>    sourceDF.columns = cols<br/>    <br/>    sourceDF.to_csv(fileDestination,index=False)<br/><br/>def transformDB(fileLocation,tableName,config):<br/>    #Starting spark with database connectivity<br/>    #database connection data<br/>    db ='dabases' <br/>    user ='postgres' <br/>    passwd = password <br/>    port = 5432<br/>    host ='localhost' <br/><br/>    spark = SparkSession.builder.appName("KPI"). \<br/>            config('spark.jars','/usr/share/java/postgresql-42.2.26.jar'). \<br/>            getOrCreate()<br/>    sparkread = spark.read<br/>    sparkcon = spark.sparkContext<br/>    <br/>    sourceDF = sparkread.csv(fileLocation,inferSchema=True,<br/>                            header=True)<br/>    print(f"jdbc:postgresql://{host}:{port}/{db}")<br/>    sourceDF.write.format("jdbc") \<br/>        .option("url",f"jdbc:postgresql://{host}:{port}/{db}") \<br/>        .option("dbtable",f"{tableName}") \<br/>        .option("user",f"{user}") \<br/>        .option("password",f"{passwd}") \<br/>        .option("driver","org.postgresql.Driver") \<br/>        .save(mode='overwrite')</span></pre><h1 id="c635" class="jk jl in bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">嘿，它们只是 python 函数:</h1><p id="28bd" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">这就是我写了自己的气流 DAGs 后的感受。在气流的情况下，DAG 在服务器中运行，它与本地<br/>文件系统进行文件通信，并从模块中读取函数。单任务 dag 很容易，但是 Pyspark 环境下的 DAG 写入数据库要花很多时间。因为我没有给出正确的数据库表名。</p><p id="61a3" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">有些错误的发生是因为我们不知道或不记得一个概念。当多个系统交互时，理解与服务器、文件系统、spark 上下文和数据库服务器相关的概念是很重要的。否则，理解错误将成为一个挑战。更糟的情况会阻止我们前进。这就是所有的人…</p></div></div>    
</body>
</html>