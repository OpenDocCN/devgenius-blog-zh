<html>
<head>
<title>Analyzing Reddit data using Scala, Spark and Spark-SQL</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Scala、Spark 和 Spark-SQL 分析 Reddit 数据</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/analyzing-reddit-data-using-scala-spark-and-spark-sql-6246c75463c6?source=collection_archive---------11-----------------------#2022-09-10">https://blog.devgenius.io/analyzing-reddit-data-using-scala-spark-and-spark-sql-6246c75463c6?source=collection_archive---------11-----------------------#2022-09-10</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><figure class="gl gn jl jm jn jo gh gi paragraph-image"><div class="gh gi jk"><img src="../Images/fa43551071a5c5e78acb6769b3cd1add.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*-6M5z0XU9eaA2sfW5HEAMA.png"/></div><figcaption class="jr js gj gh gi jt ju bd b be z dk translated">多亏了 Spark-SQL，我们可以对 Reddit 数据运行 SQL 查询</figcaption></figure><p id="e006" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">不久前，我开始熟悉 Scala 和 Spark。真正强大而有趣的技术，我对自己说。因此，我自然决定用一个真实的用例来测试它。</p><p id="56f2" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">一个有趣的数据是 Reddit。鉴于这是一个巨大的社交网络，这些数据为许多有趣的项目提供了空间。我还发现一个名为<a class="ae kt" href="https://files.pushshift.io/reddit/" rel="noopener ugc nofollow" target="_blank"> Pushshift </a>的网站正在存档 Reddit 数据。所以我试了一下。</p><p id="ccdc" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">我感兴趣的是摘录一些帖子和贴在上面的评论。该资源先按文件夹，然后按每月或每天的快照对它们进行组织。在这篇文章中，我们将看看如何读取和分析这些数据，以及如何使用 Scala、Spark 和 Spark-SQL 来完成。</p><p id="117e" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">像往常一样，如果您想继续学习，请参考 Github 库。</p><h1 id="a255" class="ku kv in bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">入门指南</h1><p id="e48f" class="pw-post-body-paragraph jv jw in jx b jy ls ka kb kc lt ke kf kg lu ki kj kk lv km kn ko lw kq kr ks ig bi translated">首先，我们从<a class="ae kt" href="https://files.pushshift.io/reddit/" rel="noopener ugc nofollow" target="_blank"> Pushshift </a>网站下载了两份每日数据摘录——一份是提交的内容，一份是与之相关的评论。在那里发现的另一个有用的东西是样本数据，对于<a class="ae kt" href="https://files.pushshift.io/reddit/submissions/sample.json" rel="noopener ugc nofollow" target="_blank">提交</a>和<a class="ae kt" href="https://files.pushshift.io/reddit/comments/sample_data.json" rel="noopener ugc nofollow" target="_blank">评论</a>，它可以用来在处理数据之前理解数据的结构。从文件中可以看出，这些是 JSONL(换行符分隔的 JSON 文件)。</p><p id="69ef" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">另一件要注意的事情是，所提供的文件是压缩的(GZ 或 ZSTD)，所以我们需要提供适当的选项来允许动态解压缩或预先提取档案。在我的例子中，我们的文件是 gz 加密的，我必须将<strong class="jx io"> hadoop-xz </strong>库添加到我的<strong class="jx io"> build.sbt </strong>文件中。</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="9d2e" class="mg kv in mc b gy mh mi l mj mk"><em class="ml">libraryDependencies </em>++= Seq(<br/>  "org.apache.spark" %% "spark-core" % sparkVersion,<br/>  "org.apache.spark" %% "spark-sql" % sparkVersion,<br/>  "org.scalactic" %% "scalactic" % "3.2.9",<br/>  "org.scalatest" %% "scalatest" % "3.2.9" % "test",<br/>  "io.sensesecure" % "hadoop-xz" % "1.4"<br/>)</span></pre><h1 id="f941" class="ku kv in bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">数据形状</h1><p id="0623" class="pw-post-body-paragraph jv jw in jx b jy ls ka kb kc lt ke kf kg lu ki kj kk lv km kn ko lw kq kr ks ig bi translated">接下来，查看提供的样本文件，我们需要选择与我们想要分析的内容相关的列。基于此，我为提交和评论创建了一个 Case 类。</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="6c37" class="mg kv in mc b gy mh mi l mj mk">case class Submission(<br/>                       selftext: String,<br/>                       title: String,<br/>                       permalink: String,<br/>                       id: String,<br/>                       created_utc: BigInt,<br/>                       author: String,<br/>                       retrieved_on: BigInt,<br/>                       score: BigInt,<br/>                       subreddit_id: String,<br/>                       subreddit: String,<br/>                     )</span><span id="75b0" class="mg kv in mc b gy mm mi l mj mk">case class Comment(<br/>                    author: String,<br/>                    body: String,<br/>                    score: BigInt,<br/>                    subreddit_id: String,<br/>                    subreddit: String,<br/>                    id: String,<br/>                    parent_id: String,<br/>                    link_id: String,<br/>                    retrieved_on: BigInt,<br/>                    created_utc: BigInt,<br/>                    permalink: String<br/>                  )</span></pre><p id="40c9" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">然后，我们可以使用 case 类来构建 Spark 数据帧模式。</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="7b81" class="mg kv in mc b gy mh mi l mj mk">val <em class="ml">commentSchema </em>= ScalaReflection.<em class="ml">schemaFor</em>[Comment].dataType.asInstanceOf[StructType]<br/><br/>val <em class="ml">submissionSchema </em>= ScalaReflection.<em class="ml">schemaFor</em>[Submission].dataType.asInstanceOf[StructType]</span></pre><p id="90a3" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">然后，可以创建数据帧</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="5962" class="mg kv in mc b gy mh mi l mj mk">val <em class="ml">submissions </em>= <em class="ml">ss</em>.read.schema(<em class="ml">submissionSchema</em>)<br/>  .option("io.compression.codecs","io.sensesecure.hadoop.xz.XZCodec")<br/>  .json(s"<strong class="mc io">$</strong><em class="ml">assetsPath</em>/RS_2018-02-01.xz").as[Submission]<br/><br/>val <em class="ml">comments </em>= <em class="ml">ss</em>.read.schema(<em class="ml">commentSchema</em>)<br/>  .option("io.compression.codecs","io.sensesecure.hadoop.xz.XZCodec")<br/>  .json(s"<strong class="mc io">$</strong><em class="ml">assetsPath</em>/RC_2018-02-01.xz").as[Comment]</span></pre><p id="2f2f" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">我们现在可以快速计算一下，看看每个数据帧有多少个条目，代表一天的 Reddit 数据。</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="abed" class="mg kv in mc b gy mh mi l mj mk">//3194211 comments<br/>println(comments.count())<br/><br/>//387140 submissions<br/>println(submissions.count())</span></pre><p id="047e" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">现在，让我们使用 Spark SQL 来查询这些数据。我们首先将数据帧别名化为视图，使它们在 Spark SQL 上下文中可用。</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="eebf" class="mg kv in mc b gy mh mi l mj mk"><em class="ml">submissions</em>.createOrReplaceTempView("submissions")<br/><br/><em class="ml">comments</em>.createOrReplaceTempView("comments")</span></pre><p id="081c" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">我们现在可以对这两个逻辑视图运行 SQL 查询。在下面的查询中，我们加入了提交的文章和它们各自的评论，只保留子编辑“世界新闻”(类似于子论坛)上的文章，只过滤一个文章 id。我们将 id/link_id 列连接起来，其中“t3_”部分是文章类型，在这里详细描述<a class="ae kt" href="https://www.reddit.com/dev/api/" rel="noopener ugc nofollow" target="_blank"/>。</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="5216" class="mg kv in mc b gy mh mi l mj mk"><em class="ml">ss</em>.sql(<br/>  """<br/>    |SELECT * FROM submissions s<br/>    | join comments c on replace(c.link_id,"t3_","") = s.id<br/>    | where s.subreddit='worldnews' and s.id = '7uktsn'<br/>    |<br/>    |<br/>    |""".stripMargin).show()</span></pre><figure class="lx ly lz ma gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mn"><img src="../Images/1284657ffb5d070466843e32e0a5b3cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BgpekHjwX2nvvnPmfoBNgw.png"/></div></div></figure><p id="e614" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">我们可以通过访问以下网址在网站上看到这篇文章。</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="fe40" class="mg kv in mc b gy mh mi l mj mk"><a class="ae kt" href="https://www.reddit.com/r/worldnews/comments/7uktsn" rel="noopener ugc nofollow" target="_blank">https://www.reddit.com/r/worldnews/comments/7uktsn</a></span></pre><p id="c8fb" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">我们还可以运行一个查询来找出在特定的一天中发表最多帖子的作者的平均分数:</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="5d8b" class="mg kv in mc b gy mh mi l mj mk"><em class="ml">ss</em>.sql(<br/>  """<br/>    |SELECT author, COUNT(score), AVG(score)<br/>    |FROM submissions s<br/>    |WHERE subreddit='worldnews'<br/>    |GROUP BY author<br/>    |ORDER BY 2 DESC<br/>    |LIMIT 10<br/>    |<br/>    |""".stripMargin).show()</span></pre><h1 id="0f3b" class="ku kv in bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">其他考虑</h1><ul class=""><li id="f60f" class="ms mt in jx b jy ls kc lt kg mu kk mv ko mw ks mx my mz na bi translated">如果您想查询 ZSTD 压缩的数据，您可能想看看下面的<a class="ae kt" href="https://gist.github.com/cnstlungu/3e284fc674f1550be150d9d970ff5a09" rel="noopener ugc nofollow" target="_blank">示例</a>。</li><li id="a6af" class="ms mt in jx b jy nb kc nc kg nd kk ne ko nf ks mx my mz na bi translated">请注意规模和性能，归档时相当于一个月的十几 GB 数据，解压缩时相当于 180 多 GB 的数据。小规模测试，仅提取您需要的内容，并了解如何配置和调优 Spark 作业(也适用于我)。</li><li id="6a4a" class="ms mt in jx b jy nb kc nc kg nd kk ne ko nf ks mx my mz na bi translated">这样的分析放在 Jupyter 笔记本里会很整洁。我们可以为 Jupyter 设置一个 Scala 内核。我的工作流程是这里的<a class="ae kt" href="https://gist.github.com/cnstlungu/50a3a7d51a262677224e51e49803771e" rel="noopener ugc nofollow" target="_blank">这里的</a>，一个例子是这里的<a class="ae kt" href="https://gist.github.com/cnstlungu/b58ccd3eaa07c368ff548295d014a2cf" rel="noopener ugc nofollow" target="_blank"/>。</li></ul><h1 id="6a31" class="ku kv in bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">结论</h1><p id="de51" class="pw-post-body-paragraph jv jw in jx b jy ls ka kb kc lt ke kf kg lu ki kj kk lv km kn ko lw kq kr ks ig bi translated">在这篇短文中，我们已经了解了如何利用 Scala 和 Spark 来读取和处理 Reddit 数据，使用优秀的 SQL，为一系列有趣的应用铺平了道路。感谢阅读！</p><p id="8ce1" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated"><em class="ml">善意提醒一下，这篇文章的配套源代码可以在</em><a class="ae kt" href="https://github.com/cnstlungu/scala-spark-reddit-example" rel="noopener ugc nofollow" target="_blank"><em class="ml">Github</em></a><em class="ml">上找到。</em></p></div></div>    
</body>
</html>