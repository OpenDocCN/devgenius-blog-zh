<html>
<head>
<title>Become 10x more productive in Data Science.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">将数据科学的工作效率提高10倍。</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/become-10x-more-productive-in-data-science-c80242e9d0ae?source=collection_archive---------20-----------------------#2020-06-10">https://blog.devgenius.io/become-10x-more-productive-in-data-science-c80242e9d0ae?source=collection_archive---------20-----------------------#2020-06-10</a></blockquote><div><div class="fc if ig ih ii ij"/><div class="ik il im in io"><figure class="iq ir gq gs is it gi gj paragraph-image"><div role="button" tabindex="0" class="iu iv di iw bf ix"><div class="gi gj ip"><img src="../Images/b6fa825345790ba94df5068de98f047a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qJ5at238aI6hQ2rTtWb74g.jpeg"/></div></div><figcaption class="ja jb gk gi gj jc jd bd b be z dk translated">来自<a class="ae je" href="https://www.pexels.com/photo/white-clock-reading-at-2-12-1537268/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">佩克斯</a>的<a class="ae je" href="https://www.pexels.com/@stasknop?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">斯塔斯·克诺普</a>的照片</figcaption></figure><div class=""/><div class=""><h2 id="3d42" class="pw-subtitle-paragraph ke jg jh bd b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv dk translated">这太简单了。</h2></div><p id="2645" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ik bi translated">以下可能对一些人来说是显而易见的，但话说回来，这些年来我一直在处理各种各样的DS/ML问题，我刚刚明白了这一点，所以也许我是一个迟钝的学习者，问题如下:</p><p id="0d48" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ik bi translated">你得到了一个全新的数据集，并被赋予了分析、预测或应用一些ML技术的任务，然后你开始尝试一些模型和技术，到目前为止还不错，但我们越来越多地得到了由数千甚至数百万行组成的庞大数据集，而你管道中的每一步都需要一些时间。另一方面，你可能正在学习一个新的工具、框架或DS，这也需要大量的试错。这些问题加在一起会占据你一天的大部分时间，而时间就是这个故事中的坏人。</p><p id="ca6a" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ik bi translated">我们被拷问，并被灌输一种感觉，即我们得到的数据是需要清理和尊重的东西，所以也许这就是为什么我从来没有想过为了更快地进行迭代，需要降低数据的质量而不是改进它。</p><p id="ba78" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ik bi translated">这项技术的警告是，你应该只使用它来探索解决方案、工具(<em class="ls">即你的管道</em>)和框架，你仍然必须在完全清理和恢复的数据集上运行你的模型，用图形术语来说这就是我们将要做的:</p><figure class="lu lv lw lx gu it gi gj paragraph-image"><div role="button" tabindex="0" class="iu iv di iw bf ix"><div class="gi gj lt"><img src="../Images/8d58fb69b0da21e84e0b7eeb26e122ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jwwEgTOD27zBbYJ7w4FFRQ.png"/></div></div></figure><pre class="lu lv lw lx gu ly lz ma mb aw mc bi"><span id="16fd" class="md me jh lz b gz mf mg l mh mi"><strong class="lz ji">1.</strong> Degrade your training Dataset and setup your environment, iterate on models and evaluate metrics, fine tune your pipeline etc, etc.</span><span id="35ea" class="md me jh lz b gz mj mg l mh mi"><strong class="lz ji">2. </strong>Run the resulting process on your Original Dataset.</span><span id="d0a6" class="md me jh lz b gz mj mg l mh mi"><strong class="lz ji">3. </strong>Continue with the rest of your pipeline with your Original Dataset.</span></pre></div><div class="ab cl mk ml hv mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="ik il im in io"><h2 id="2ebc" class="md me jh bd mr ms mt dn mu mv mw dp mx lf my mz na lj nb nc nd ln ne nf ng nh bi translated">一个实际的例子:</h2><p id="4390" class="pw-post-body-paragraph kw kx jh ky b kz ni ki lb lc nj kl le lf nk lh li lj nl ll lm ln nm lp lq lr ik bi translated">我当前的数据集具有以下形状和读取时间:</p><pre class="lu lv lw lx gu ly lz ma mb aw mc bi"><span id="f9f3" class="md me jh lz b gz mf mg l mh mi">Training dataset:</span><span id="3cb8" class="md me jh lz b gz mj mg l mh mi">df.shape<br/>&gt;&gt;&gt;(<strong class="lz ji">501808, 313</strong>) so 313 Columns and half a Million Rows.</span><span id="003e" class="md me jh lz b gz mj mg l mh mi">Reading time: <strong class="lz ji">~15s</strong>  (Specs: Windows, i5 8thGen, 32GB RAM)</span><span id="65e5" class="md me jh lz b gz mj mg l mh mi">This might not seem like much but once you start running your pipeline time will add considerably, gradient boosting (xgboost) for instance takes about 5-10 minutes and my machine fans really start to take off.</span></pre><p id="a640" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ik bi translated">为了以某种方式优雅地降低数据集，我最近使用的方法是删除每隔一行和/或一列:</p><pre class="lu lv lw lx gu ly lz ma mb aw mc bi"><span id="82ed" class="md me jh lz b gz mf mg l mh mi"><strong class="lz ji">FOR ROWS :</strong></span><span id="bddb" class="md me jh lz b gz mj mg l mh mi">f = "training_data.csv"<br/>num_lines = sum(1 for l in open(f))-1<br/><br/># Take every N-th (in this case 2nd) row<br/>n = 2</span><span id="96a8" class="md me jh lz b gz mj mg l mh mi"># The row indices to skip - make sure 0 is not included to keep the header.</span><span id="4a4d" class="md me jh lz b gz mj mg l mh mi"><strong class="lz ji">skip_idx</strong> = [x for x in range(1, num_lines) if x % n != 0]</span><span id="ff0e" class="md me jh lz b gz mj mg l mh mi">training_data = pd.read_csv(f, header=0, skiprows=<strong class="lz ji">skip_idx</strong>)</span><span id="7add" class="md me jh lz b gz mj mg l mh mi">training_data.to_csv("training_decimated_1.csv", index=False)</span><span id="c923" class="md me jh lz b gz mj mg l mh mi"><br/><strong class="lz ji">FOR COLUMNS/FEATURES :</strong></span><span id="2345" class="md me jh lz b gz mj mg l mh mi">f = "training_data.csv"<br/>training_data = pd.read_csv(f, header=0)</span><span id="7f25" class="md me jh lz b gz mj mg l mh mi">features = [c for c in training_data if c.startswith("feature")]<br/>oddFeatures = features[0::2]</span><span id="24ec" class="md me jh lz b gz mj mg l mh mi">training_data.drop(oddFeatures, axis = 1, inplace = True)</span><span id="e31f" class="md me jh lz b gz mj mg l mh mi">training_data.to_csv("training_decimated_2.csv", index=False)</span></pre><p id="bb23" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ik bi translated">每个数据集都是不同的，这里我假设有要素列和数据行，但是在进行剪切之前，您需要了解自己的数据结构，例如，该数据集有预测阶段所需的附加列，这些列被单独留下。</p><pre class="lu lv lw lx gu ly lz ma mb aw mc bi"><span id="5269" class="md me jh lz b gz mf mg l mh mi">After running the above a few times ( 3x rows, 1x columns) my resulting dataset has this shape and reading times.</span><span id="f5b2" class="md me jh lz b gz mj mg l mh mi"><strong class="lz ji">df.shape</strong><br/>&gt;&gt;&gt; [62726 rows x 158 columns]<br/><strong class="lz ji">Reading time:</strong> ~1s <br/><strong class="lz ji">xgboost:</strong> ~ 1 minute.<br/></span></pre><p id="4cdc" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ik bi translated">这意味着您在数据集上花费的时间减少了1/10，理论上还可以更少，但是…</p></div><div class="ab cl mk ml hv mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="ik il im in io"><h2 id="5bf9" class="md me jh bd mr ms mt dn mu mv mw dp mx lf my mz na lj nb nc nd ln ne nf ng nh bi translated">子采样？</h2><p id="9b99" class="pw-post-body-paragraph kw kx jh ky b kz ni ki lb lc nj kl le lf nk lh li lj nl ll lm ln nm lp lq lr ik bi translated">如果花费50%-80%的时间来设置您的数据科学项目和选择正确的模型，您就节省了大量时间，但实际上使用降级的数据集运行整个管道又如何呢？</p><p id="32e0" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ik bi translated">好吧，你会进行二次采样，不幸的是会删除信息，所以你的准确度很可能会下降。您仍然可以使用降级的数据集来试验诸如超参数调整和模型变量之类的东西，并且您有时(<em class="ls">但不总是</em>)会得到影响您的特定问题的指示，每个案例都是不同的，因此由您来确定这是否是一个好的时间投资，以及是否有任何关系。</p></div><div class="ab cl mk ml hv mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="ik il im in io"><h2 id="381c" class="md me jh bd mr ms mt dn mu mv mw dp mx lf my mz na lj nb nc nd ln ne nf ng nh bi translated">可供选择的事物</h2><p id="bc22" class="pw-post-body-paragraph kw kx jh ky b kz ni ki lb lc nj kl le lf nk lh li lj nl ll lm ln nm lp lq lr ik bi translated">有更复杂的替代方法来优化读取和扩展您的管道以处理更大的数据集，但它们确实需要一个学习过程，可能并不适用于所有情况，以下两种方法可能是一个很好的起点:</p><p id="6ac3" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ik bi translated"><a class="ae je" href="https://github.com/wesm/feather" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ji">羽化</strong> </a> <strong class="ky ji"> : </strong>将你的数据帧转换成这种格式以便更快的读写。</p><p id="4281" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ik bi translated"><a class="ae je" href="https://dask.org" rel="noopener ugc nofollow" target="_blank"><strong class="ky ji">Dask</strong></a><strong class="ky ji">:</strong>主要旨在扩大<strong class="ky ji"> </strong>您的项目，但经过高度优化，可以减少您的读/写次数，只需对现有代码进行最小的更改。</p></div><div class="ab cl mk ml hv mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="ik il im in io"><p id="9f1d" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ik bi translated">我希望这有助于您提高工作效率，减少处理大型数据集或新项目时的挫折感。</p><p id="7b9a" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ik bi translated">感谢阅读！</p></div></div>    
</body>
</html>