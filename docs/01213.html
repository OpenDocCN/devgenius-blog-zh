<html>
<head>
<title>Foreground-Background Separation using Core-ML</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于Core-ML的前景背景分离</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/foreground-background-separation-using-core-ml-82efbe7e7fc8?source=collection_archive---------1-----------------------#2020-06-27">https://blog.devgenius.io/foreground-background-separation-using-core-ml-82efbe7e7fc8?source=collection_archive---------1-----------------------#2020-06-27</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><figure class="gl gn jl jm jn jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi jk"><img src="../Images/7af269f513ceeca24387cb97cd877fee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-IKKrMUiVgkkluI5"/></div></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">阿里·沙阿·拉哈尼在<a class="ae jz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="40a6" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">最近，我对使用深度学习的图像分割非常感兴趣，每当我学习到与机器和深度学习相关的东西时，它们都只是停留在Jupiter笔记本上，我没有对这些模型做任何事情，也没有任何部署它们的经验。因此，在查看了ML-Kit、Flask和IOS等许多框架之后，我决定创建一个简单的应用程序。</p><p id="29eb" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">最后，我决定使用IOS，原因有两个</p><ol class=""><li id="7e10" class="ky kz in kc b kd ke kh ki kl la kp lb kt lc kx ld le lf lg bi translated">我从未在IOS上工作过，所以通过做一个IOS项目，我会学到一些新的东西，在阅读了关于Core-ML的内容后，我很兴奋，并渴望用它做点什么。</li><li id="0c60" class="ky kz in kc b kd lh kh li kl lj kp lk kt ll kx ld le lf lg bi translated">我只是苹果迷。</li></ol><p id="a0cb" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">对于这篇文章，我不打算详细介绍图像分割，我会留到其他时间，我相信你们都知道什么是图像分割，这就是为什么你登陆这个页面。</p><p id="a10b" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">但是，仅供参考:<strong class="kc io">图像分割</strong>是将图像分割成区域，以便提取图像中的多个不同对象。下面是一个简单的例子。图像分割用于医疗成像、安全、自动驾驶汽车等各种任务，最近我们大多数人都在视频通话应用中使用这种技术，如hangout、zoom或teams，因为它们提供了改变背景的功能。</p><figure class="ln lo lp lq gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi lm"><img src="../Images/0785a9ad6f5d655907e25d06cc361784.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NrSQdqlIIgMTn44MAXaBjg.jpeg"/></div></div></figure><p id="144f" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">我保持这个项目有点简单，而不是在视频上工作，我将在静态图像上做图像分割，主要任务是前景提取。作为参考，这是我们希望使用该应用程序实现的。</p><figure class="ln lo lp lq gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi lr"><img src="../Images/93245bf85b4339d29e15ecce970929fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KAaAycVVJNkhYWSd6SAoeQ.png"/></div></div></figure><h1 id="637b" class="ls lt in bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">DeepLab (V3):</h1><p id="370f" class="pw-post-body-paragraph ka kb in kc b kd mq kf kg kh mr kj kk kl ms kn ko kp mt kr ks kt mu kv kw kx ig bi translated">对于图像分割，我将使用谷歌DeepLab(V3)模型，该模型已被用于创建Pixel 2和Pixel 2 XL智能手机的“肖像”模式。这是一个用于语义图像分割的先进的深度学习模型，其目标是为输入图像中的每个像素分配语义标签(例如，人、狗、猫等)。</p><p id="93ec" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">使用该模型的主要原因是我们不能将Pytorch和Tensorlflow模型直接用于Core-ML，而是苹果提供了一个已经转换的版本<a class="ae jz" href="https://developer.apple.com/machine-learning/models/" rel="noopener ugc nofollow" target="_blank"> DeepLabV3 </a>，这使得工作更加容易，但是您可以训练自己的模型，并使用<a class="ae jz" href="https://coremltools.readme.io/docs" rel="noopener ugc nofollow" target="_blank"> CoreML-Tools </a>转换模型。但对于这一点，我会坚持使用苹果提供的模型。</p><h1 id="07e0" class="ls lt in bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">入门指南</h1><h2 id="b8cc" class="mv lt in bd lu mw mx dn ly my mz dp mc kl na nb mg kp nc nd mk kt ne nf mo ng bi translated">创建新项目:</h2><p id="04af" class="pw-post-body-paragraph ka kb in kc b kd mq kf kg kh mr kj kk kl ms kn ko kp mt kr ks kt mu kv kw kx ig bi translated">前往Xcode并创建一个新项目，选择一个单一视图应用程序作为模板并填写所有信息，并确保选择“故事板”作为用户界面。</p><div class="ln lo lp lq gt ab cb"><figure class="nh jo ni nj nk nl nm paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><img src="../Images/d78c6592568234886c744efc5f6e70a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*Ke9MESwinCqsoc14xjmzhA.png"/></div></figure><figure class="nh jo nn nj nk nl nm paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><img src="../Images/682faff284c1113c7d239793d328dceb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*1P7qMFxhHsNmXEbVBuCk_w.png"/></div></figure></div><h1 id="c4a5" class="ls lt in bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">设置视图</h1><p id="53e5" class="pw-post-body-paragraph ka kb in kc b kd mq kf kg kh mr kj kk kl ms kn ko kp mt kr ks kt mu kv kw kx ig bi translated">现在让我们用按钮和徽标来设置ViewController。我们将有一个图像视图，和一个UI按钮来启动遮罩。</p><figure class="ln lo lp lq gt jo"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="f488" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">现在，我们需要添加功能来选择图像。为此，我决定在导航栏上创建两个按钮。一个将从照片库中上传图像，另一个用于从相机中拍摄照片。让我们将它们添加到<code class="fe nq nr ns nt b"><strong class="kc io">viewdidload</strong></code> <strong class="kc io"> </strong>中，然后我们将设置我们的视图和布局。</p><figure class="ln lo lp lq gt jo"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="39a1" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">在上面的代码中，我添加了一个带有两个UI按钮的导航栏，这将在导航栏的右边给你两个按钮。</p><p id="0e55" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">更改Info.plist文件并添加一个属性很重要，这样就可以向用户解释为什么我们需要访问相机和库。在“<em class="nu">隐私—图片库使用说明</em>”中添加一些文字。</p><p id="0711" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">现在我们需要为所有三个按钮创建一个“选择器动作”方法，记住我们为照片库、照相机和分段按钮各有一个按钮，每次按下相应的按钮时都会调用这些方法。</p><figure class="ln lo lp lq gt jo"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="d6b8" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">在按下相机和照片库按钮时，我们希望<code class="fe nq nr ns nt b">Image Picker Controller</code>变为活动状态，它是一个控制器，用于管理拍照、录制电影和从用户的媒体库中选择项目的系统界面。让我们创建一个方法<code class="fe nq nr ns nt b">showImagePickerController</code>，为我们激活控制器。</p><figure class="ln lo lp lq gt jo"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="403f" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">在这种情况下，我们将传递图像控制器的源，它可以是<code class="fe nq nr ns nt b">.photogallery</code>或<code class="fe nq nr ns nt b">.camera.</code>，现在我们需要创建一个回调实例函数，通知代理用户选择了一个静态图像或电影，这样我们就可以在我们的UI视图上显示选择的图像。</p><figure class="ln lo lp lq gt jo"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="cdc5" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">这个函数获取图像并将其分配给imageView，然后通过调用图像上的<code class="fe nq nr ns nt b">image.fixOrientation()</code>方法做一些有趣的事情，我将在后面解释。这里的<code class="fe nq nr ns nt b">self.originalImage</code>只是一个简单的<code class="fe nq nr ns nt b">UIImage</code>类型的变量。</p><h1 id="8404" class="ls lt in bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">核心-ML</h1><p id="72e8" class="pw-post-body-paragraph ka kb in kc b kd mq kf kg kh mr kj kk kl ms kn ko kp mt kr ks kt mu kv kw kx ig bi translated">现在有趣的部分开始了。创建一个Core-ML组，并添加您下载的所有Deep-Lab模型文件。</p><figure class="ln lo lp lq gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi nv"><img src="../Images/8050d9011f6538089c36611b842c9072.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vloxumfmdw5cWOLKEAAi3w.png"/></div></div></figure><p id="1daa" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">现在我们需要建立一个视觉，这是苹果的框架，用我们的Core-ML模型处理计算机视觉相关的工作。</p><p id="1cc2" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">首先，创建一个Core-ML模型的实例和类型为<code class="fe nq nr ns nt b"><a class="ae jz" href="https://developer.apple.com/documentation/vision/vncoremlrequest" rel="noopener ugc nofollow" target="_blank">VNCoreMLRequest</a></code>的模型属性，然后我们将设置一个模型，如果你仔细观察的话，我们在<code class="fe nq nr ns nt b">viewDidLoad()</code>中调用了<code class="fe nq nr ns nt b">setUpModel()</code>，现在我们将添加它要做的事情。</p><figure class="ln lo lp lq gt jo"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="f6f5" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">在上面的代码中，我们首先创建一个vision模型，然后将该模型传递给一个请求处理程序，该处理程序将创建一个vision请求，我们还将使用对象的完成处理程序来指定一个方法，以便在您运行请求后从该模型接收结果。</p><p id="e405" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">当我们的模型设置完成后，我们可以选择一幅图像，现在剩下的唯一事情就是开始分割，为此我们用要处理的图像创建<code class="fe nq nr ns nt b"><a class="ae jz" href="https://developer.apple.com/documentation/vision/vnimagerequesthandler" rel="noopener ugc nofollow" target="_blank">VNImageRequestHandler</a></code>对象。</p><figure class="ln lo lp lq gt jo"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="19b4" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">这里发生的事情是，我们使用<code class="fe nq nr ns nt b"><a class="ae jz" href="https://developer.apple.com/documentation/vision/vnimagerequesthandler" rel="noopener ugc nofollow" target="_blank">VNImageRequestHandler</a></code>创建获取图像的请求，按照Core-Ml模型的指定进行预处理，将图像发送到Core-ML模型，并获得输出。这里<code class="fe nq nr ns nt b">perform([])</code>方法传入一个视觉请求数组。</p><p id="99a2" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">现在唯一剩下的事情就是处理Core-ML模型的结果。但在此之前，我们应该注意到，我们的Core-ML模型的输出只是数组大小为<code class="fe nq nr ns nt b">512</code>乘以<code class="fe nq nr ns nt b">512</code>。数组内容只是一个整数值(0，7，15，9等)。每个数字代表一种颜色。如果我们的CoreML模型识别出图像中存在杂项/背景区域，那么构成背景/杂项区域的所有像素将被赋予一个值，比如说<code class="fe nq nr ns nt b">0</code>。这只是一个数组，不是图像，所以我们不能直接用它作为原始图像的遮罩。</p><p id="64b8" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">仅供参考的输出是<code class="fe nq nr ns nt b">MLMultiArray</code>的类型，它只是苹果版本的2D阵列。</p><h2 id="d3b4" class="mv lt in bd lu mw mx dn ly my mz dp mc kl na nb mg kp nc nd mk kt ne nf mo ng bi translated">MLMultiArray -&gt; UIImage</h2><p id="38e2" class="pw-post-body-paragraph ka kb in kc b kd mq kf kg kh mr kj kk kl ms kn ko kp mt kr ks kt mu kv kw kx ig bi translated">添加一个新文件<code class="fe nq nr ns nt b">MLMultiArrayToUIImage</code>并将这段代码添加到下面的代码中。大部分代码取自<a class="ae jz" href="https://github.com/hollance/CoreMLHelpers" rel="noopener ugc nofollow" target="_blank"> CoreMLHelpers </a>，但我做了一些调整以满足我的需求。</p><figure class="ln lo lp lq gt jo"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="0163" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">在这里，我们迭代MLMultiArray并将这些值转换为灰度像素，我对语义分割不感兴趣，语义分割意味着对图像中的不同类别进行分类，我只对从前景中去除背景感兴趣，因此所有属于类别<code class="fe nq nr ns nt b">0 </code>的背景都被分配了<code class="fe nq nr ns nt b">255(white)</code>像素，并且所有被识别的对象都被转换为黑色<code class="fe nq nr ns nt b">0</code>像素。</p><p id="e6f7" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">现在我们有了一个<code class="fe nq nr ns nt b">UInt8</code>像素的数组，我们将使用这些像素值将其转换为灰度图像，为此我们必须使用像素数据创建一个<code class="fe nq nr ns nt b">CGContext</code>，然后使用这些像素数据创建一个图像，这是通过上面代码中的<code class="fe nq nr ns nt b">fromByteArray</code>函数完成的。我已经为灰度做了这个，但是你也可以为<code class="fe nq nr ns nt b">RGB</code>调整这个。</p><h2 id="e1e6" class="mv lt in bd lu mw mx dn ly my mz dp mc kl na nb mg kp nc nd mk kt ne nf mo ng bi translated">调整遮罩图像的大小</h2><p id="4388" class="pw-post-body-paragraph ka kb in kc b kd mq kf kg kh mr kj kk kl ms kn ko kp mt kr ks kt mu kv kw kx ig bi translated">在创建一个处理程序来处理我们的Core-ML模型的输出之前，还有一件事要做。记住，Deep-Lab模型返回一个<code class="fe nq nr ns nt b">512 x 512</code>蒙版，但是我们的原始图像可能有不同的大小，所以我们不能直接将蒙版应用到图像上。所以我们有两个选择，要么缩小原始图像，要么放大蒙版图像，谢天谢地，有一个简单的方法可以做到这一点。继续创建一个文件<code class="fe nq nr ns nt b">UIImageExtension.swift</code>和下面的代码。</p><figure class="ln lo lp lq gt jo"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="4ce0" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">这里，我们再次创建了一个上下文，但这次没有任何像素数据，然后在<code class="fe nq nr ns nt b">CGcontex</code> t中绘制我们的<code class="fe nq nr ns nt b">maskImage</code>，并将图像调整到所需的形状。</p><h2 id="4691" class="mv lt in bd lu mw mx dn ly my mz dp mc kl na nb mg kp nc nd mk kt ne nf mo ng bi translated">处理分割结果</h2><p id="ac04" class="pw-post-body-paragraph ka kb in kc b kd mq kf kg kh mr kj kk kl ms kn ko kp mt kr ks kt mu kv kw kx ig bi translated">现在我们需要添加处理程序来处理分割结果。Vision请求的完成处理程序指示请求是成功了还是导致了错误。如果成功，它的<code class="fe nq nr ns nt b"><a class="ae jz" href="https://developer.apple.com/documentation/vision/vnrequest/2867238-results" rel="noopener ugc nofollow" target="_blank">results</a></code>属性包含<code class="fe nq nr ns nt b"><a class="ae jz" href="https://developer.apple.com/documentation/vision/vncoremlfeaturevalueobservation" rel="noopener ugc nofollow" target="_blank">VNCoreMLFeatureValueObservation</a></code>，这是由我们的Core-ML模型生成的输出字典，字典中的第一个对象是我们的结果。</p><figure class="ln lo lp lq gt jo"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="5d69" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">这个结果字典包含了<code class="fe nq nr ns nt b">MLfeaturevalue</code>一个将我们的结果和它的值包装在一起的对象，我们只需要从中取出<code class="fe nq nr ns nt b">multiArrayValue</code>。然后我们如上所述将这个数组转换成一个图像，然后根据原始图像的大小调整image的大小。调整图像大小后，我们需要对原始图像应用遮罩。内联<code class="fe nq nr ns nt b">9</code>我们正在执行<code class="fe nq nr ns nt b">maskOriginalImage()</code>函数，让我们来实现它吧。</p><h2 id="0764" class="mv lt in bd lu mw mx dn ly my mz dp mc kl na nb mg kp nc nd mk kt ne nf mo ng bi translated">掩蔽原始图像</h2><p id="e183" class="pw-post-body-paragraph ka kb in kc b kd mq kf kg kh mr kj kk kl ms kn ko kp mt kr ks kt mu kv kw kx ig bi translated">为了遮蔽图像，我们首先需要从我们的遮罩创建一个图像遮罩，然后将该图像遮罩应用于我们的原始图像。</p><figure class="ln lo lp lq gt jo"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="3457" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">最后，我们已经把所有的部分都准备好了，但是等一下，如果你记得当我们从<code class="fe nq nr ns nt b">UIImagePickerController.</code>中选择一个图像时，我给图像添加了<code class="fe nq nr ns nt b">fixOrientation()</code>，这背后的原因是UIImage在转换成<code class="fe nq nr ns nt b">CGImage</code>时失去了它的方向。它会影响自动设置为风景模式的肖像图像。我不知道为什么会发生这种情况，但是图像被旋转了<code class="fe nq nr ns nt b">90⁰</code>，你可以通过调用<code class="fe nq nr ns nt b">fixOrientation()</code>方法的注释行来检查这一点。</p><p id="5903" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">要解决这个问题，请在<code class="fe nq nr ns nt b">UIImageExtension.swift</code>中添加以下代码。您可以了解更多关于ios 上<a class="ae jz" href="https://medium.com/@eorvain_app/image-orientation-on-ios-abaf8321820b" rel="noopener">定向如何工作的信息。</a></p><figure class="ln lo lp lq gt jo"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="8e70" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">最后，我们添加了所有功能，现在我们可以构建代码并进行测试。这是示例输出。</p><div class="ln lo lp lq gt ab cb"><figure class="nh jo nw nj nk nl nm paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><img src="../Images/cc2edf9b819c5fa73e6cf1e2396c8984.png" data-original-src="https://miro.medium.com/v2/resize:fit:1470/format:webp/1*3QpnXFoPw_ViUQVn5I9A9w.jpeg"/></div></figure><figure class="nh jo nx nj nk nl nm paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><img src="../Images/13cf175c4f3cf159504509224f334105.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*Epw7OQ22yQtaC0Zivfo-Ng.png"/></div></figure></div><p id="258c" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">我们可以很容易地注意到，它工作得很好，但不是在边缘，原因是Deep-Lab是一个图像分割模型，不是为了显著性检测，对于这项任务，我们需要一个为显著对象检测设计的深度学习模型，我最近了解到的一个模型是<a class="ae jz" href="https://github.com/NathanUA/BASNet" rel="noopener ugc nofollow" target="_blank"> BasNet </a>，它工作得非常好，这里有一些样本图像遮罩。</p><div class="ln lo lp lq gt ab cb"><figure class="nh jo ny nj nk nl nm paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><img src="../Images/7d908f814d7845c439412e19e23eeb75.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*8Uq2uAhHz9XJ7vVUCaVkxg.jpeg"/></div></figure><figure class="nh jo nz nj nk nl nm paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><img src="../Images/341fe92477a9b0eb6b03846fde9c7275.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*3QpnXFoPw_ViUQVn5I9A9w.jpeg"/></div></figure><figure class="nh jo oa nj nk nl nm paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><img src="../Images/3a6ceaf8c6453125dd03738840edd015.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*T5Epne-ebUV2HEv8EJQIjg.jpeg"/></div></figure></div><div class="ab cb"><figure class="nh jo ob nj nk nl nm paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><img src="../Images/23ac5832242cf3dbdd8c5fae5e8226a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*RddkicZLiYBLqzLw2P82Ow.png"/></div></figure><figure class="nh jo oc nj nk nl nm paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><img src="../Images/ea351d625c894ed332ed104496aaf773.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*Y24M-d4DQaQFUivxZN7BXg.png"/></div></figure><figure class="nh jo od nj nk nl nm paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><img src="../Images/75a2b7b68763abe86b70872c9273a40f.png" data-original-src="https://miro.medium.com/v2/resize:fit:396/format:webp/1*Ysk9MJA1qbg6YAtCoEmGnQ.png"/></div></figure></div><p id="8d21" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">我们可以很容易地注意到掩模图像之间的差异，以及它在检测对象边界方面有多好。但不幸的是，没有Core-ML模型，我使用Python创建了这些图像。有一些转换工具可以将PyTorch模型转换成Core-ML，但是我不太喜欢这些工具，因为它们仍然有很多限制。我正致力于将<code class="fe nq nr ns nt b">BasNet </code>转换成Core-ML，一旦我成功了，我会更新我的博客并分享这个模型，但在此之前，这就是我们所拥有的。</p></div><div class="ab cl oe of hr og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="ig ih ii ij ik"><p id="74a8" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">我希望你喜欢并学到了新的东西，如果你喜欢，请留下一些掌声。👏👏👏</p><p id="4797" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">[参考文献]</p><ol class=""><li id="762d" class="ky kz in kc b kd ke kh ki kl la kp lb kt lc kx ld le lf lg bi translated">秦，x，张，z，黄，c，高，Dehghan，m .，&amp; Jagersand，M. (2019)。BASNet:边界感知显著目标检测。<em class="nu">IEEE计算机视觉和模式识别会议(CVPR) </em>。</li><li id="96f6" class="ky kz in kc b kd lh kh li kl lj kp lk kt ll kx ld le lf lg bi translated">陈良杰、朱玉坤、乔治·帕潘德里欧、弗洛里安·施罗夫和哈特维格·亚当(2018)。用于语义图像分割的阿特鲁可分离卷积编解码器。在<em class="nu"> ECCV </em>。</li></ol></div></div>    
</body>
</html>