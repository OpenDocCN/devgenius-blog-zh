<html>
<head>
<title>Pyspark DataFrame Joins, GroupBy, UDF, and Handling Missing Values</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Pyspark 数据帧连接、分组、UDF 和处理缺失值</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/pyspark-for-begineers-part-3-pyspark-dataframe-db02f0fcd275?source=collection_archive---------11-----------------------#2022-08-15">https://blog.devgenius.io/pyspark-for-begineers-part-3-pyspark-dataframe-db02f0fcd275?source=collection_archive---------11-----------------------#2022-08-15</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><blockquote class="jk jl jm"><p id="793f" class="jn jo jp jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ig bi translated">DataFrame 现在是一个行业流行语，人们倾向于在各种情况下使用它。在上一篇文章中，我们了解了 Pyspark 中的数据帧，它的特性、重要性、创建以及 Pyspark 数据帧的一些基本功能。在本文中，我们将了解更多关于 Groupby 和聚合函数、连接、填充缺失值以及 Pyspark DataFrame 中的其他概念。</p></blockquote><p id="1e20" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated">要了解我之前关于 Pyspark DataFrame 的文章，请点击此链接</p><div class="kp kq gp gr kr ks"><a href="https://muttinenisairohith.medium.com/pyspark-for-begineers-part-2-pyspark-dataframe-60008da53e30" rel="noopener follow" target="_blank"><div class="kt ab fo"><div class="ku ab kv cl cj kw"><h2 class="bd io gy z fp kx fr fs ky fu fw im bi translated">初学者用 Pyspark |第 2 部分:Pyspark 数据框架</h2><div class="kz l"><h3 class="bd b gy z fp kx fr fs ky fu fw dk translated">DataFrame 现在是一个行业流行语，人们倾向于在各种情况下使用它。在这篇文章中，我们将学习…</h3></div><div class="la l"><p class="bd b dl z fp kx fr fs ky fu fw dk translated">muttinenisairohith.medium.com</p></div></div><div class="lb l"><div class="lc l ld le lf lb lg lh ks"/></div></div></a></div><p id="2375" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated"><strong class="jq io">分组和聚合函数:</strong></p><p id="6aad" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated">与 SQL <code class="fe li lj lk ll b">GROUP BY</code>子句类似，PySpark <code class="fe li lj lk ll b">groupBy()</code>函数用于将相同的数据收集到 DataFrame 上的组中，并对分组的数据执行 count、sum、avg、min 和 max 函数。</p><p id="b6f5" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated">开始之前，让我们创建一个简单的数据框架。使用的 CSV 文件可以在 这里找到<a class="ae lm" href="https://github.com/muttinenisairohith/Encoding-Categorical-Data/blob/ffb93bdf21a91a03362d0a3ab5d56c94d24ed962/data/test2.csv" rel="noopener ugc nofollow" target="_blank"> <strong class="jq io">。</strong></a></p><pre class="ln lo lp lq gt lr ll ls lt aw lu bi"><span id="439e" class="lv lw in ll b gy lx ly l lz ma">from pyspark.sql import SparkSession</span><span id="a870" class="lv lw in ll b gy mb ly l lz ma">spark =SparkSession.builder.appName("Practice").getOrCreate()</span><span id="3453" class="lv lw in ll b gy mb ly l lz ma">df_pyspark= spark.read.csv("test2.csv",header=True,inferSchema=True)<br/>df_pyspark.show()</span></pre><figure class="ln lo lp lq gt md gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/1c4c8d20cf6e6e759912dddd6fddd8bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*ZRDd-NpiWhAKI8IyjiDDBA.png"/></div></figure><p id="7cec" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated"><strong class="jq io"> groupBy(): </strong></p><p id="67a5" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated">在数据框的<code class="fe li lj lk ll b">Departments</code>列上<code class="fe li lj lk ll b">groupBy()</code>，然后使用<code class="fe li lj lk ll b">sum()</code>函数找到各部门的<code class="fe li lj lk ll b">salary</code>之和。</p><pre class="ln lo lp lq gt lr ll ls lt aw lu bi"><span id="1d15" class="lv lw in ll b gy lx ly l lz ma">#group by Departments which gives summation of salaries</span><span id="a761" class="lv lw in ll b gy mb ly l lz ma">df_pyspark.groupBy("Departments").sum("salary").show()</span></pre><figure class="ln lo lp lq gt md gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/1a58a0fca4423e2cd30b3a6412edcccb.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*HXYuOlcLujLP7sImIP0ufg.png"/></div></figure><p id="aaff" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated">类似地，我们可以使用 groupBy 函数执行 min、max、mean、avg 和 count。</p><pre class="ln lo lp lq gt lr ll ls lt aw lu bi"><span id="7503" class="lv lw in ll b gy lx ly l lz ma">df_pyspark.groupBy("Departments").min("salary").show()<br/>df_pyspark.groupBy("Departments").max("salary").show()<br/>df_pyspark.groupBy("Departments").avg("salary").show()<br/>df_pyspark.groupBy("Departments").mean("salary").show()</span><span id="ef48" class="lv lw in ll b gy mb ly l lz ma">df_pyspark.groupBy("Departments").count().show()  #count of number of people in each Department</span></pre><p id="2117" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated"><strong class="jq io">使用多列的 groupBy()</strong></p><pre class="ln lo lp lq gt lr ll ls lt aw lu bi"><span id="8e0c" class="lv lw in ll b gy lx ly l lz ma">df_pyspark.groupBy("Name","Departments").sum("salary").show()</span></pre><p id="7caf" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated"><strong class="jq io"> groupBy()和 agg()函数</strong></p><pre class="ln lo lp lq gt lr ll ls lt aw lu bi"><span id="fd67" class="lv lw in ll b gy lx ly l lz ma">df_pyspark.groupBy("Departments").agg(({"salary":"sum"})).show()</span></pre><figure class="ln lo lp lq gt md gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/045ccbceb8ffd91331f0268bd915c0dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*xHjjAMJ858YWVDScrSw1mw.png"/></div></figure><p id="b6c8" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated">我们也可以在整个数据帧上执行 agg()函数，而不用 groupBy()</p><pre class="ln lo lp lq gt lr ll ls lt aw lu bi"><span id="3324" class="lv lw in ll b gy lx ly l lz ma">df_pyspark.agg(({"salary":"sum"})).show()</span><span id="6474" class="lv lw in ll b gy mb ly l lz ma">+-----------+ <br/>|sum(salary)| <br/>+-----------+ <br/>|      73000| <br/>+-----------+</span></pre><p id="9951" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated">使用<strong class="jq io">Pivot/UnPivot</strong>—Spark SQL 提供<code class="fe li lj lk ll b">pivot()</code>函数将数据从一列旋转到多列(转置行到列)。这是一种聚合，其中一个分组列值被转置到具有不同数据的各个列中。可以使用类似的 UnPivot。</p><pre class="ln lo lp lq gt lr ll ls lt aw lu bi"><span id="2e99" class="lv lw in ll b gy lx ly l lz ma">df_pyspark.groupBy("Departments").pivot("Name").sum("salary").show()</span></pre><figure class="ln lo lp lq gt md gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mh"><img src="../Images/bba380efd7293893d0fa33228128e2d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*Nb4-aBKABCl9v0ewWnLtMw.png"/></div></div></figure><p id="af36" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated"><strong class="jq io">处理缺失值 Pyspark </strong></p><p id="29b0" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated">对于这个任务，我们将使用这个<a class="ae lm" href="https://github.com/muttinenisairohith/Encoding-Categorical-Data/blob/b0bb96f293adbb803e24c26b7780e078372d3703/data/test3.csv" rel="noopener ugc nofollow" target="_blank"> <strong class="jq io"> CSV 文件</strong> </a></p><pre class="ln lo lp lq gt lr ll ls lt aw lu bi"><span id="7099" class="lv lw in ll b gy lx ly l lz ma">df_pyspark1=spark.read.csv("test3.csv",header=True,inferSchema=True)<br/>df_pyspark1.show()</span></pre><figure class="ln lo lp lq gt md gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mm"><img src="../Images/922f68fd9c3acd7babab10b307f936aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*odwYkGZN-erEb89ubYJ9IA.png"/></div></div></figure><p id="ce74" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated">删除基于空值的行</p><pre class="ln lo lp lq gt lr ll ls lt aw lu bi"><span id="396b" class="lv lw in ll b gy lx ly l lz ma">df_pyspark1.na.drop().show()</span></pre><figure class="ln lo lp lq gt md gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mn"><img src="../Images/1ca5cb68c80905f426aaf660030e4af8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_LzN3RuUW1oGHAX-c8_3tg.png"/></div></div></figure><p id="7b9b" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated">drop()有以下参数— how、thresh 和 subset</p><pre class="ln lo lp lq gt lr ll ls lt aw lu bi"><span id="5d0e" class="lv lw in ll b gy lx ly l lz ma">1. df_pyspark1.na.drop(how="all").show() # if all values in rows are null then drop # default any</span><span id="48a4" class="lv lw in ll b gy mb ly l lz ma">2. df_pyspark1.na.drop(how="any",thresh=2).show() #atleast 2 non null values should be present</span><span id="12d1" class="lv lw in ll b gy mb ly l lz ma">3. df_pyspark1.na.drop(how="any",subset=["salary"]).show() # only in that column rows get deleted</span></pre><p id="cd85" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated">填充缺失值-单值</p><pre class="ln lo lp lq gt lr ll ls lt aw lu bi"><span id="156b" class="lv lw in ll b gy lx ly l lz ma">df_pyspark1.na.fill('Missing Values').show() #string values will get replaced as string is given as input</span><span id="00e4" class="lv lw in ll b gy mb ly l lz ma">df_pyspark1.na.fill(0).show() #integer values will get replaced as integer is given as input</span></pre><p id="a9b4" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated">借助估算函数，使用平均值、中值或众数填充缺失值</p><pre class="ln lo lp lq gt lr ll ls lt aw lu bi"><span id="2626" class="lv lw in ll b gy lx ly l lz ma">#filling with mean</span><span id="aac2" class="lv lw in ll b gy mb ly l lz ma">from pyspark.ml.feature import Imputer</span><span id="c3c7" class="lv lw in ll b gy mb ly l lz ma">imputer = Imputer(inputCols=["age"],outputCols=["age_imputed"]).setStrategy("mean")</span></pre><p id="1388" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated">在 setStrategy 中，我们可以使用均值、中值或众数。</p><pre class="ln lo lp lq gt lr ll ls lt aw lu bi"><span id="1ed3" class="lv lw in ll b gy lx ly l lz ma">imputer.fit(df_pyspark1).transform(df_pyspark1).show()</span></pre><figure class="ln lo lp lq gt md gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mo"><img src="../Images/22c2a57f5f7a661f7d8198180706e645.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g8MHQ1uyPyx4CUEixsy_kw.png"/></div></div></figure><p id="013d" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated"><strong class="jq io">Pyspark 数据帧中的 orderBy()和 sort()</strong></p><p id="5a75" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated">我们将切换回以前使用的 CSV 文件</p><figure class="ln lo lp lq gt md gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/576569d2ba3f450ce8a3b213222abaa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*TH5yqf0cOO38FBOnmjiyoA.png"/></div></figure><p id="47e4" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated"><strong class="jq io"><em class="jp">sort()</em></strong>-使用一列或多列对数据帧进行排序，默认-升序</p><figure class="ln lo lp lq gt md gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/3b8abe625a061635b7f69065811250fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*pKpQk_Ra3JYSFBALAwYB2Q.png"/></div></figure><pre class="ln lo lp lq gt lr ll ls lt aw lu bi"><span id="540b" class="lv lw in ll b gy lx ly l lz ma">df_pyspark.sort("salary").show() # Sort based on single column</span><span id="39de" class="lv lw in ll b gy mb ly l lz ma">df_pyspark.sort(df_pyspark["salary"].desc()).show() # sort based on descending order</span><span id="7b7e" class="lv lw in ll b gy mb ly l lz ma">df_pyspark.sort("salary","Name").show() # Sort based on first column then second column</span></pre><p id="11e8" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated">或者，也可以使用 orderBy。</p><pre class="ln lo lp lq gt lr ll ls lt aw lu bi"><span id="c896" class="lv lw in ll b gy lx ly l lz ma">df_pyspark.orderBy("salary").show() # Sort based on single column</span></pre><p id="290b" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated"><strong class="jq io">使用 pyspark 连接()</strong></p><p id="40a5" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated">PySpark Join 用于组合两个数据帧，通过链接这些数据帧，您可以连接多个数据帧；它支持传统 SQL 中可用的所有基本连接类型操作，如<code class="fe li lj lk ll b">INNER</code>、<code class="fe li lj lk ll b">LEFT OUTER</code>、<code class="fe li lj lk ll b">RIGHT OUTER</code>、<code class="fe li lj lk ll b">LEFT ANTI</code>、<code class="fe li lj lk ll b">LEFT SEMI</code>、<code class="fe li lj lk ll b">CROSS</code>、<code class="fe li lj lk ll b">SELF</code>连接。</p><figure class="ln lo lp lq gt md gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mr"><img src="../Images/95ee0e5670061c3093819a7252b8b830.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i4c1GH-HF6pnkXPM_T2OCg.png"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk translated">连接</figcaption></figure><p id="2de4" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated">让我们从创建两个数据帧开始:</p><pre class="ln lo lp lq gt lr ll ls lt aw lu bi"><span id="cc87" class="lv lw in ll b gy lx ly l lz ma">emp = [(1,"Smith",-1,"2018","10","M",3000),(2, "Rose",1 , "2010", "20","M", 4000),(3,"Williams",1,"2010","10","M",1000),(4, "Jones",2 ,"2005","10","F",2000),(5,"Brown",2,"2010","40","",-1),(6, "Brown", 2, "2010","50","",-1)]</span><span id="89c0" class="lv lw in ll b gy mb ly l lz ma">empColumns = ["emp_id","name","superior_emp_id","year_joined", "emp_dept_id","gender","salary"]<br/><br/>empDF = spark.createDataFrame(data=emp, schema = empColumns)<br/>empDF.printSchema()<br/>empDF.show()<br/><br/>dept = [("Finance",10),("Marketing",20),("Sales",30),("IT",40)]<br/>deptColumns = ["dept_name","dept_id"]<br/>deptDF = spark.createDataFrame(data=dept, schema = deptColumns)<br/>deptDF.printSchema()<br/>deptDF.show()</span></pre><figure class="ln lo lp lq gt md gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mn"><img src="../Images/fb7cfc44178c1716379e1641607299de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OLjD4SUaBhCZQNV0H4_r9w.png"/></div></div></figure><pre class="ln lo lp lq gt lr ll ls lt aw lu bi"><span id="3f27" class="lv lw in ll b gy lx ly l lz ma">empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,"inner") .show()</span></pre><figure class="ln lo lp lq gt md gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mw"><img src="../Images/ad41ea8555648a6adb62973400c9fed1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nIvryEc5qQ1ThoYTm52LMA.png"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk translated">内部连接</figcaption></figure><pre class="ln lo lp lq gt lr ll ls lt aw lu bi"><span id="2645" class="lv lw in ll b gy lx ly l lz ma">empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,"outer").show()<br/>#Or<br/>empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,"full").show()<br/>#Or<br/>empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,"fullouter").show()</span></pre><figure class="ln lo lp lq gt md gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mm"><img src="../Images/ce3213f229cca38e5cede7a82593d473.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vECaQGoD1EL6AJYTyz_E5Q.png"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk translated">外部连接</figcaption></figure><pre class="ln lo lp lq gt lr ll ls lt aw lu bi"><span id="9f6f" class="lv lw in ll b gy lx ly l lz ma">empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,"left").show()<br/>#Or<br/>empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,"leftouter").show()</span></pre><figure class="ln lo lp lq gt md gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mm"><img src="../Images/a2ae27f8c8b55de88bba5e631d125b0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F2BmFvhI5aOukZiv7jzMiA.png"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk translated">左连接</figcaption></figure><pre class="ln lo lp lq gt lr ll ls lt aw lu bi"><span id="bda4" class="lv lw in ll b gy lx ly l lz ma">empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,"right").show()<br/>#Or<br/>empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,"rightouter").show()</span></pre><figure class="ln lo lp lq gt md gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mx"><img src="../Images/3e1e5a3efb51d7718342c5f34ddb7b6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rZ5oRTv-D_7C2X3c40w7OA.png"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk translated">右连接</figcaption></figure><pre class="ln lo lp lq gt lr ll ls lt aw lu bi"><span id="aab0" class="lv lw in ll b gy lx ly l lz ma">empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,"leftsemi").show()</span></pre><figure class="ln lo lp lq gt md gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi my"><img src="../Images/0ecaeaccc6f052ff4db334c01c0a1e80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R8Peyj-prRVlyMEBNX6-Xw.png"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk translated">左半连接</figcaption></figure><pre class="ln lo lp lq gt lr ll ls lt aw lu bi"><span id="a94d" class="lv lw in ll b gy lx ly l lz ma">empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,"leftanti").show()</span></pre><figure class="ln lo lp lq gt md gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/b6256bfe27dee6096e3f2dce2c2b3cc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*d8gh85PXYXR6czVh6phM-w.png"/></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk translated">左反连接</figcaption></figure><p id="367d" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated">Pyspark DataFrame 中没有可用的自连接，但是可以使用上面任何可用的方法来完成。</p><p id="2ddf" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated"><strong class="jq io"> Union()使用 pyspark </strong></p><p id="7c53" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated">要合并相同模式或结构的两个或多个数据帧，需要使用 union()。</p><pre class="ln lo lp lq gt lr ll ls lt aw lu bi"><span id="f129" class="lv lw in ll b gy lx ly l lz ma">unionDF = df.union(df2)</span></pre><p id="ede9" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated">要合并而不重复，distinct()与 union()一起使用</p><pre class="ln lo lp lq gt lr ll ls lt aw lu bi"><span id="3569" class="lv lw in ll b gy lx ly l lz ma">disDF = df.union(df2).distinct()</span></pre><p id="5c6f" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated"><strong class="jq io">用户定义的功能(UDF): </strong></p><p id="2015" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated">UDF 用于扩展框架的功能，并在多个数据框架上重用这些功能。例如，如果我们想把一个名字串中一个单词的每一个首字母都转换成小写。PySpark 内置功能没有此功能，因此我们可以创建一个 UDF，并根据需要在许多数据框上重复使用。UDF 一旦创建，就可以在几个数据帧和 SQL 表达式中重用。</p><pre class="ln lo lp lq gt lr ll ls lt aw lu bi"><span id="e060" class="lv lw in ll b gy lx ly l lz ma">def lowerCase(str):<br/>     return str.lower()</span></pre><p id="efac" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated">随着 lowerCase()函数的创建，让我们来看看 UDF 代码。</p><pre class="ln lo lp lq gt lr ll ls lt aw lu bi"><span id="c0bf" class="lv lw in ll b gy lx ly l lz ma">from pyspark.sql.functions import udf</span><span id="dca7" class="lv lw in ll b gy mb ly l lz ma">upperCaseUDF = udf(lambda z:upperCase(z))</span><span id="1135" class="lv lw in ll b gy mb ly l lz ma">deptDF.withColumn("dept_name", ulowerCaseUDF(deptDF["dept_name"])).show()</span></pre><figure class="ln lo lp lq gt md gh gi paragraph-image"><div class="gh gi na"><img src="../Images/a899c75e1d622585cadaa2a2f1beb1ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*zs-7ojIgeMp3Mlgb0g3gHQ.png"/></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk translated">南非民主统一战线(United Democratic Front)</figcaption></figure><p id="1d6f" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated">我想我有 Pyspark DataFrame 中的大部分主题。我写这篇文章是为了学习 Pyspark，并在某个地方写些东西，这样我可以在需要的时候引用它们。希望这能有所帮助。</p><p id="8ef0" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated">快乐编码和学习…</p><p id="0eec" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated"><strong class="jq io">参考文献:</strong></p><p id="d7e4" class="pw-post-body-paragraph jn jo in jq b jr js jt ju jv jw jx jy km ka kb kc kn ke kf kg ko ki kj kk kl ig bi translated">SparkbyExamples.com</p></div></div>    
</body>
</html>