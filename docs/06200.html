<html>
<head>
<title>How to train a Graph Convolutional Network on the Cora dataset with PyTorch Geometric</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用PyTorch Geometric在Cora数据集上训练图卷积网络</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/how-to-train-a-graph-convolutional-network-on-the-cora-dataset-with-pytorch-geometric-847ed5fab9cb?source=collection_archive---------1-----------------------#2021-12-21">https://blog.devgenius.io/how-to-train-a-graph-convolutional-network-on-the-cora-dataset-with-pytorch-geometric-847ed5fab9cb?source=collection_archive---------1-----------------------#2021-12-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/f191899b9d007f8016863ee827e08d08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Ehhlol-dG4DhHu5S"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">艾莉娜·格鲁布尼亚克在<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="f51a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">现实中充满了图表</strong>。道路？图表。社交网络？图表。分子？图表。你明白了:图是我们拥有的最重要的数据结构之一。</p><p id="e786" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">幸运的是，今天有很多很好的资源，可以让你了解将机器学习应用于这类数据所需的一切。这篇文章告诉你如何将斯坦福<strong class="kf ir">斯坦福</strong>的<strong class="kf ir">带图的机器学习</strong>课程提出的各种主题之一付诸实践。</p><p id="994a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇文章的主题只是我正在“转换成代码”的课程中的一课。如果你想看看其他的，<strong class="kf ir">所有当前的笔记本</strong>(在撰写本文时正在开发中)<strong class="kf ir">和斯坦福课程的有用链接，可以在这里</strong> 找到 <a class="ae kc" href="https://github.com/mnslarcher/cs224w-slides-to-code" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir">。</strong></a></p><p id="89d6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如前所述，已经有很多学习与<strong class="kf ir">图形ML </strong>相关的理论的材料，特别是<strong class="kf ir">图形神经网络</strong>，所以我在这里就不再解释了。如果你需要复习或者还没有看过理论，除了刚才提到的课程，另外两个资源可能对你有用:</p><ul class=""><li id="b37e" class="lb lc iq kf b kg kh kk kl ko ld ks le kw lf la lg lh li lj bi translated">原<strong class="kf ir">图卷积网络</strong> <strong class="kf ir">论文</strong> : <a class="ae kc" href="https://arxiv.org/abs/1609.02907" rel="noopener ugc nofollow" target="_blank">用图卷积网络进行半监督分类</a></li><li id="90d2" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated">论文作者的<strong class="kf ir">博文</strong>，<strong class="kf ir">托马斯·基普夫</strong> : <a class="ae kc" href="https://tkipf.github.io/graph-convolutional-networks/" rel="noopener ugc nofollow" target="_blank">图卷积网络</a></li></ul><p id="b54d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">好了，介绍够了，我们开始吧！</p><p id="48ae" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">在<strong class="kf ir"> Google Colab </strong>(或任何笔记本)上安装<strong class="kf ir"> PyTorch几何</strong>的</strong>:</p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lp"><img src="../Images/d44e4bd594f86e81397a189aed291d05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FQWQ53d2ekV4W9bQgO9b1Q.png"/></div></div></figure><p id="6c35" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一些进口:</p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lu"><img src="../Images/5323afafa3d04283c6df0c5b900694c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1DSxau765cNudoJDVrxQWw.png"/></div></div></figure><h1 id="5c7a" class="lv lw iq bd lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms bi translated">Cora数据集</h1><p id="e592" class="pw-post-body-paragraph kd ke iq kf b kg mt ki kj kk mu km kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">我们将在其上测试我们闪亮的新技术的数据集也是它的创造者使用的数据集之一，Cora数据集。来自Cora数据集带代码页的<a class="ae kc" href="https://paperswithcode.com/dataset/cora" rel="noopener ugc nofollow" target="_blank">文件:</a></p><blockquote class="my mz na"><p id="17ab" class="kd ke nb kf b kg kh ki kj kk kl km kn nc kp kq kr nd kt ku kv ne kx ky kz la ij bi translated">Cora数据集包括2708份科学出版物，分为七类。引文网络由5429个链接组成。数据集中的每个出版物由0/1值的词向量来描述，该词向量指示字典中相应词的存在与否。这部词典由1433个独特的单词组成。</p></blockquote><p id="8970" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们来探索这个数据集，以了解它是如何制作的，我提醒你，这里提供的所有代码都可以在笔记本“6。图神经网络1: GNN模型”在<a class="ae kc" href="https://github.com/mnslarcher/cs224w-slides-to-code" rel="noopener ugc nofollow" target="_blank">这</a>页。</p><p id="84d3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">来自<a class="ae kc" href="https://arxiv.org/abs/1609.02907" rel="noopener ugc nofollow" target="_blank"> Kipf &amp;威灵(ICLR 2017) </a>:</p><blockquote class="my mz na"><p id="1493" class="kd ke nb kf b kg kh ki kj kk kl km kn nc kp kq kr nd kt ku kv ne kx ky kz la ij bi translated">[…]在包含1，000个标记示例的测试集上评估预测准确性。[…]500个标记超参数优化示例的验证集(所有图层的辍学率、第一个GCN图层的L2正则化因子和隐藏单元的数量)。我们不使用验证集标签进行培训。</p></blockquote><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nf"><img src="../Images/885f99fed23287857d567a305e0b7119.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AcIWoy81wvf1fFJnSMPtpg.png"/></div></div></figure><p id="197b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">出局:</p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ng"><img src="../Images/230df290cad489fd7812fc9f74ae2a52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fGoAJN0bOJufamZPFqQr8Q.png"/></div></div></figure><p id="688a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从这里，我们已经可以观察到一些事情。</p><p id="6435" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，为了获得正确的边数，我们必须将数据属性“num_edges”除以2，这是因为<strong class="kf ir">py torch Geometric</strong>“<a class="ae kc" href="https://github.com/pyg-team/pytorch_geometric/issues/343#issuecomment-496501421" rel="noopener ugc nofollow" target="_blank">将每个链接保存为两个方向的无向边</a>”。</p><p id="1ff7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">即使这样做，数字加起来也不完美，显然是因为“<a class="ae kc" href="https://github.com/pyg-team/pytorch_geometric/issues/852#issuecomment-563200790" rel="noopener ugc nofollow" target="_blank">Cora数据集包含重复的边</a>”。</p><p id="f33d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另一个奇怪的事实是，除去标签用于训练、验证和测试的节点，还有其他节点没有被考虑用于这些目的。</p><p id="c576" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们可以看到Cora数据集实际上只包含一个图。</p><p id="1959" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">来自<a class="ae kc" href="https://arxiv.org/abs/1609.02907" rel="noopener ugc nofollow" target="_blank"> Kipf &amp;威灵(ICLR 2017) </a>:</p><blockquote class="my mz na"><p id="1d62" class="kd ke nb kf b kg kh ki kj kk kl km kn nc kp kq kr nd kt ku kv ne kx ky kz la ij bi translated">我们使用Glorot &amp; Bengio (2010)中描述的初始化来初始化权重，并相应地(行)归一化输入特征向量。</p></blockquote><p id="3419" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">默认情况下，<strong class="kf ir"> Glorot初始化</strong>由PyTorch Geometric完成，相反，必须明确添加行的<strong class="kf ir">规范化，</strong>以便每个节点的特征总和为1:</p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nh"><img src="../Images/08931635257d5c79ad761b88f13634e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a63U0Kvaz9sZKcVNhcJoMQ.png"/></div></div></figure><p id="5d35" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">出局:</p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ni"><img src="../Images/df51e4adb52bcca695ab8f680238758d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T7HKkZWRrenD8JK-BxWs0g.png"/></div></div></figure><h1 id="9a6e" class="lv lw iq bd lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms bi translated">图形卷积网络</h1><p id="a277" class="pw-post-body-paragraph kd ke iq kf b kg mt ki kj kk mu km kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">现在我们有了数据，是时候定义我们的<strong class="kf ir">图卷积网络</strong> ( <strong class="kf ir"> GCN </strong>)！</p><p id="366e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">来自<a class="ae kc" href="https://arxiv.org/abs/1609.02907" rel="noopener ugc nofollow" target="_blank"> Kipf &amp;威灵(ICLR 2017) </a>:</p><blockquote class="my mz na"><p id="794a" class="kd ke nb kf b kg kh ki kj kk kl km kn nc kp kq kr nd kt ku kv ne kx ky kz la ij bi translated">我们使用Adam (Kingma &amp; Ba，2015)对所有模型进行最多200个时期(训练迭代)的训练，学习率为0.01，窗口大小为10的早期停止，即如果验证损失连续10个时期没有减少，我们就停止训练。</p></blockquote><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nj"><img src="../Images/5c424281281cdce83e3cd45d358bd032.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UEqGfUTJoqn1XR-bn6ONDw.png"/></div></div></figure><p id="5ca9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">出局:</p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nk"><img src="../Images/51c2c152c6de80a1ecb04b37bcf0095a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*audWsps7IQB7AlPeFmPQaw.png"/></div></div></figure><p id="8aaa" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您已经查看了PyTorch几何文档中的实现，甚至是Thomas Kipf的框架中的实现，您可能会发现一些不一致之处(例如，两个漏失层而不是一个)。实际上这是因为<a class="ae kc" href="https://github.com/tkipf/pygcn/issues/20" rel="noopener ugc nofollow" target="_blank">都不忠实于TensorFlow </a>中的原始实现。</p><h1 id="3a5d" class="lv lw iq bd lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms bi translated">培训和评估</h1><p id="6868" class="pw-post-body-paragraph kd ke iq kf b kg mt ki kj kk mu km kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">在编写训练循环之前，我们应该准备一个训练和评估步骤:</p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nl"><img src="../Images/4dd1acc8212941a798b21507c53b63a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oO0uRii1KZCFNGQtgTGrnA.png"/></div></div></figure><p id="5803" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里我们可以看到，该模型将整个图形作为输入，而<strong class="kf ir">输出</strong>和<strong class="kf ir">目标</strong>被<strong class="kf ir">不同地屏蔽</strong>，这取决于它是处于训练、验证还是测试阶段。</p><p id="4bdf" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，一些代码被注释掉了，因此没有真正使用，这是因为它代表了一种尝试，即像在最初的实现中一样，仅将<strong class="kf ir"> L2正则化</strong>应用于第一层。不幸的是，通常情况下，使用PyTorch不可能100%地复制TensorFlow中所做的事情。在我们的例子中，使用我们将使用的优化器的“weight_decay”参数可以获得最佳结果:<strong class="kf ir"> Adam </strong>。</p><p id="823f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">来自<a class="ae kc" href="https://arxiv.org/abs/1609.02907" rel="noopener ugc nofollow" target="_blank"> Kipf &amp;威灵(ICLR 2017) </a>:</p><blockquote class="my mz na"><p id="54fa" class="kd ke nb kf b kg kh ki kj kk kl km kn nc kp kq kr nd kt ku kv ne kx ky kz la ij bi translated">我们使用Adam (Kingma &amp; Ba，2015)对所有模型进行最多200个时期(训练迭代)的训练，学习率为0.01，窗口大小为10的早期停止，即如果验证损失连续10个时期没有减少，我们就停止训练。</p></blockquote><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nm"><img src="../Images/b02038104277de11ead701a0feb82104.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6ORosH-03n15JBi2jfQJWA.png"/></div></div></figure><p id="8270" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">需要注意的是，与论文中描述的相比，<strong class="kf ir">提前停止</strong>逻辑的官方实现(此处复制)略有不同:在验证损失不减少的十次之后，训练不会停止，但当它大于其最后十次值的平均值时，训练会停止。</p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nn"><img src="../Images/82771fd49702b13486406a0f777dd096.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l1HoKrbtUUusR3WcFcvrTQ.png"/></div></div></figure><p id="616d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">出局:</p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi no"><img src="../Images/6e22d31401df5a01dc5b3751a00b10a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wZfKDqUgL0dozaIlJM0xzw.png"/></div></div></figure><p id="597e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">没错。<strong class="kf ir">我们获得了与原论文</strong>中报道一致的测试精度(论文中为81.5%)。请注意，由于这是一个小数据集，这些结果对选择的随机种子很敏感。缓解这个问题的一个可能的解决方案是像作者所做的那样，取100(或更多)次运行的平均值。</p><p id="acee" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们来看看<strong class="kf ir">损耗</strong>和<strong class="kf ir">精度</strong> <strong class="kf ir">曲线</strong>。</p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi np"><img src="../Images/50ac38f7f2ae48268b98156a6ef0d8b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ORAk-IE3f0fDoTkMNA4oNQ.png"/></div></div></figure><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nq"><img src="../Images/b3a986c1a20455b5c9bc38cd193c3dd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X4sNH0Ef1ntqmN9zgKN9pQ.png"/></div></div></figure><h1 id="9b9b" class="lv lw iq bd lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms bi translated">最后的想法</h1><p id="7ad3" class="pw-post-body-paragraph kd ke iq kf b kg mt ki kj kk mu km kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">如果你已经走了这么远，<strong class="kf ir">谢谢你</strong>！如果你喜欢这篇文章，我会很高兴如果你<strong class="kf ir">拍</strong>让我知道。如果我想在社交媒体上留下一个<strong class="kf ir">明星</strong>的<a class="ae kc" href="https://github.com/mnslarcher/cs224w-slides-to-code" rel="noopener ugc nofollow" target="_blank">回购</a>或<strong class="kf ir">分享</strong>它，你真的是一个朋友。</p><p id="883f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">啊，我差点忘了，让我们保持联系，你可以在这里找到我:</p><ul class=""><li id="9ee4" class="lb lc iq kf b kg kh kk kl ko ld ks le kw lf la lg lh li lj bi translated"><strong class="kf ir">领英</strong>:<a class="ae kc" href="https://www.linkedin.com/in/mnslarcher" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/mnslarcher</a></li><li id="51fd" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated"><strong class="kf ir">推特</strong>:<a class="ae kc" href="https://twitter.com/mnslarcher" rel="noopener ugc nofollow" target="_blank">https://twitter.com/mnslarcher</a></li></ul><div class="nr ns gp gr nt nu"><a href="https://mnslarcher.medium.com/membership" rel="noopener follow" target="_blank"><div class="nv ab fo"><div class="nw ab nx cl cj ny"><h2 class="bd ir gy z fp nz fr fs oa fu fw ip bi translated">通过我的推荐链接加入Medium-Mario Nam Tao shian ti Larcher</h2><div class="ob l"><h3 class="bd b gy z fp nz fr fs oa fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="oc l"><p class="bd b dl z fp nz fr fs oa fu fw dk translated">mnslarcher.medium.com</p></div></div><div class="od l"><div class="oe l of og oh od oi jw nu"/></div></div></a></div></div></div>    
</body>
</html>