<html>
<head>
<title>K8s Service— iptables and ipvs Implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">K8s 服务— iptables 和 ipvs 实施</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/k8s-service-iptables-and-ipvs-implementation-1d52260c25c3?source=collection_archive---------0-----------------------#2022-08-28">https://blog.devgenius.io/k8s-service-iptables-and-ipvs-implementation-1d52260c25c3?source=collection_archive---------0-----------------------#2022-08-28</a></blockquote><div><div class="fc if ig ih ii ij"/><div class="ik il im in io"><div class=""/><div class=""><h2 id="a4c1" class="pw-subtitle-paragraph jo iq ir bd b jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf dk translated">iptables 和 ipv 如何实现 K8s 服务</h2></div><figure class="kg kh ki kj gu kk gi gj paragraph-image"><div class="ab gv cl kl"><img src="../Images/cadd5ef092dc49ce2b2893e9b73b62a0.png" data-original-src="https://miro.medium.com/v2/format:webp/0*iumBQTxpiOFGl-xh.png"/></div></figure><p id="133b" class="pw-post-body-paragraph ko kp ir kq b kr ks js kt ku kv jv kw kx ky kz la lb lc ld le lf lg lh li lj ik bi translated">在我的“<a class="ae lk" rel="noopener ugc nofollow" target="_blank" href="/k8s-service-introduction-b1197f5d0ab4"> K8s —服务介绍</a>”一文中，我提到 K8s 服务使用<code class="fe ll lm ln lo b">iptables</code>和<code class="fe ll lm ln lo b">ipvs</code>来实现负载均衡。让我们来谈谈本文中的<code class="fe ll lm ln lo b">iptables</code>和<code class="fe ll lm ln lo b">ipvs</code>是如何实现的。</p><h1 id="e79f" class="lp lq ir bd lr ls lt lu lv lw lx ly lz jx ma jy mb ka mc kb md kd me ke mf mg bi translated">主机名应用演示摘要</h1><p id="60f0" class="pw-post-body-paragraph ko kp ir kq b kr mh js kt ku mi jv kw kx mj kz la lb mk ld le lf ml lh li lj ik bi translated">让我们回顾一下我们在上一篇文章中创建的名为“主机名”的服务。</p><pre class="kg kh ki kj gu mm lo mn mo aw mp bi"><span id="c4ff" class="mq lq ir lo b gz mr ms l mt mu">apiVersion: v1<br/>kind: Service<br/>metadata:<br/>  name: hostnames<br/>spec:<br/>  selector:<br/>    app: hostnames<br/>  ports:<br/>  - name: default<br/>    protocol: TCP<br/>    port: 80<br/>    targetPort: 9376</span></pre><p id="0ca5" class="pw-post-body-paragraph ko kp ir kq b kr ks js kt ku kv jv kw kx ky kz la lb lc ld le lf lg lh li lj ik bi translated">在此“主机名”服务背后，有三个单元，由以下部署控制:</p><pre class="kg kh ki kj gu mm lo mn mo aw mp bi"><span id="c007" class="mq lq ir lo b gz mr ms l mt mu">apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  name: hostnames<br/>spec:<br/>  selector:<br/>    matchLabels:<br/>      app: hostnames<br/>  replicas: 3<br/>  template:<br/>    metadata:<br/>      labels:<br/>        app: hostnames<br/>    spec:<br/>      containers:<br/>      - name: hostnames<br/>        image: k8s.gcr.io/serve_hostname<br/>        ports:<br/>        - containerPort: 9376<br/>          protocol: TCP</span></pre><p id="3ee1" class="pw-post-body-paragraph ko kp ir kq b kr ks js kt ku kv jv kw kx ky kz la lb lc ld le lf lg lh li lj ik bi translated">该应用程序每次访问端口 9376 时都会返回自己的主机名。服务 VIP 是 10.0.1.175:</p><pre class="kg kh ki kj gu mm lo mn mo aw mp bi"><span id="8eb6" class="mq lq ir lo b gz mr ms l mt mu">$ kubectl get svc hostnames<br/>NAME        TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE<br/>hostnames   ClusterIP   10.0.1.175   &lt;none&gt;        80/TCP    5s</span><span id="0c92" class="mq lq ir lo b gz mv ms l mt mu">$ curl 10.0.1.175:80<br/>hostnames-0uton</span><span id="947d" class="mq lq ir lo b gz mv ms l mt mu">$ curl 10.0.1.175:80<br/>hostnames-yp2kp</span><span id="9451" class="mq lq ir lo b gz mv ms l mt mu">$ curl 10.0.1.175:80<br/>hostnames-bvc05</span></pre><h1 id="9f5f" class="lp lq ir bd lr ls lt lu lv lw lx ly lz jx ma jy mb ka mc kb md kd me ke mf mg bi translated">iptables 规则</h1><h2 id="d667" class="mq lq ir bd lr mw mx dn lv my mz dp lz kx na nb mb lb nc nd md lf ne nf mf ng bi translated">服务规则</h2><p id="83f9" class="pw-post-body-paragraph ko kp ir kq b kr mh js kt ku mi jv kw kx mj kz la lb mk ld le lf ml lh li lj ik bi translated">引擎盖下，服务一提交给 K8s，<code class="fe ll lm ln lo b">kube-proxy</code>就能通过服务告密者感知到这样一个服务对象的加入。作为对此事件的响应，它在主机上创建了这样一个<code class="fe ll lm ln lo b">iptables</code>规则(您可以通过 iptables-save 看到它)，如下所示:</p><pre class="kg kh ki kj gu mm lo mn mo aw mp bi"><span id="cd39" class="mq lq ir lo b gz mr ms l mt mu">-A KUBE-SERVICES -d 10.0.1.175/32 -p tcp -m comment --comment "default/hostnames: cluster IP" -m tcp --dport 80 -j KUBE-SVC-NWV5X2332I4OT4T3</span></pre><p id="c2cc" class="pw-post-body-paragraph ko kp ir kq b kr ks js kt ku kv jv kw kx ky kz la lb lc ld le lf lg lh li lj ik bi translated">上面的<code class="fe ll lm ln lo b">iptables</code>规则告诉 K8s:任何目的地址为 10.0.1.175，目的端口为 80 的 IP 包都要跳转到另一个名为<strong class="kq is">KUBE-SVC-nwv5x 2332 i4 ot 4 T3</strong>的<code class="fe ll lm ln lo b">iptables</code>链进行处理。</p><p id="61a9" class="pw-post-body-paragraph ko kp ir kq b kr ks js kt ku kv jv kw kx ky kz la lb lc ld le lf lg lh li lj ik bi translated">而正如我们之前看到的，10.0.1.175 是这个服务的 VIP。所以这个规则为这个服务设置了一个固定的入口地址。而且，由于 10.0.1.175 只是一个<code class="fe ll lm ln lo b">iptables</code>规则上的配置，并没有真正的网络设备，所以如果 ping 这个地址，也不会有响应。</p><p id="c130" class="pw-post-body-paragraph ko kp ir kq b kr ks js kt ku kv jv kw kx ky kz la lb lc ld le lf lg lh li lj ik bi translated">那么，这个链条"<strong class="kq is">KUBE-SVC-nwv5x 2332 i4 ot 4t 3 "</strong>长什么样呢？事实上，它是如下规则的集合:</p><pre class="kg kh ki kj gu mm lo mn mo aw mp bi"><span id="1d0a" class="mq lq ir lo b gz mr ms l mt mu">-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment "default/hostnames:" -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-WNBA2IHDGP2BOBGZ</span><span id="eaec" class="mq lq ir lo b gz mv ms l mt mu">-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment "default/hostnames:" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-X3P2623AGDH6CDF3</span><span id="79a6" class="mq lq ir lo b gz mv ms l mt mu">-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment "default/hostnames:" -j KUBE-SEP-57KPRZ3JQVENLNBR</span></pre><p id="3c10" class="pw-post-body-paragraph ko kp ir kq b kr ks js kt ku kv jv kw kx ky kz la lb lc ld le lf lg lh li lj ik bi translated">如你所见，这套规则实际上是一套随机模式的<code class="fe ll lm ln lo b">iptables</code>链(<code class="fe ll lm ln lo b">–mode random</code>)。随机转发目的地是</p><ul class=""><li id="a671" class="nh ni ir kq b kr ks ku kv kx nj lb nk lf nl lj nm nn no np bi translated">KUBE-九月-WNBA2IHDGP2BOBGZ</li><li id="9d43" class="nh ni ir kq b kr nq ku nr kx ns lb nt lf nu lj nm nn no np bi translated">KUBE-SEP-X3P2623AGDH6CDF3</li><li id="b9bb" class="nh ni ir kq b kr nq ku nr kx ns lb nt lf nu lj nm nn no np bi translated">KUBE</li></ul><p id="ce03" class="pw-post-body-paragraph ko kp ir kq b kr ks js kt ku kv jv kw kx ky kz la lb lc ld le lf lg lh li lj ik bi translated">这三条链指向的最终目的地实际上是服务代理的<strong class="kq is">三个吊舱。因此，这组规则是服务实现负载平衡的地方。</strong></p><p id="b945" class="pw-post-body-paragraph ko kp ir kq b kr ks js kt ku kv jv kw kx ky kz la lb lc ld le lf lg lh li lj ik bi translated">需要注意的是，<code class="fe ll lm ln lo b">iptables</code>规则的匹配是自上而下一条一条进行的，所以为了保证上述三条规则中的每一条都有相同的被选中概率，我们要将它们的概率字段的值设置为<code class="fe ll lm ln lo b">1/3 (0.333 …)</code>、<code class="fe ll lm ln lo b">1/2</code>和<code class="fe ll lm ln lo b">1</code>。</p><p id="d5cc" class="pw-post-body-paragraph ko kp ir kq b kr ks js kt ku kv jv kw kx ky kz la lb lc ld le lf lg lh li lj ik bi translated">这个设置的原理很简单:第一个规则被选中的概率是 1/3；而如果第一条规则没有被选中，那么此时就只剩下两条规则了，所以第二条规则被选中的概率必须设为 1/2；同样，最后一个条目必须设置为 1。</p><h2 id="446a" class="mq lq ir bd lr mw mx dn lv my mz dp lz kx na nb mb lb nc nd md lf ne nf mf ng bi translated">Pod 规则</h2><p id="5905" class="pw-post-body-paragraph ko kp ir kq b kr mh js kt ku mi jv kw kx mj kz la lb mk ld le lf ml lh li lj ik bi translated">通过查看以上三个链条的细节，我们很容易理解业务转发的具体原理，如下图所示:</p><pre class="kg kh ki kj gu mm lo mn mo aw mp bi"><span id="635b" class="mq lq ir lo b gz mr ms l mt mu">-A KUBE-SEP-57KPRZ3JQVENLNBR -s 10.244.3.6/32 -m comment --comment "default/hostnames:" -j MARK --set-xmark 0x00004000/0x00004000<br/>-A KUBE-SEP-57KPRZ3JQVENLNBR -p tcp -m comment --comment "default/hostnames:" -m tcp -j DNAT --to-destination 10.244.3.6:9376</span><span id="19fa" class="mq lq ir lo b gz mv ms l mt mu">-A KUBE-SEP-WNBA2IHDGP2BOBGZ -s 10.244.1.7/32 -m comment --comment "default/hostnames:" -j MARK --set-xmark 0x00004000/0x00004000<br/>-A KUBE-SEP-WNBA2IHDGP2BOBGZ -p tcp -m comment --comment "default/hostnames:" -m tcp -j DNAT --to-destination 10.244.1.7:9376</span><span id="52ce" class="mq lq ir lo b gz mv ms l mt mu">-A KUBE-SEP-X3P2623AGDH6CDF3 -s 10.244.2.3/32 -m comment --comment "default/hostnames:" -j MARK --set-xmark 0x00004000/0x00004000<br/>-A KUBE-SEP-X3P2623AGDH6CDF3 -p tcp -m comment --comment "default/hostnames:" -m tcp -j DNAT --to-destination 10.244.2.3:9376</span></pre><p id="fefc" class="pw-post-body-paragraph ko kp ir kq b kr ks js kt ku kv jv kw kx ky kz la lb lc ld le lf lg lh li lj ik bi translated">可以看出，这三条链其实就是三条 DNAT 规则。但是在 DNAT 规则之前，<code class="fe ll lm ln lo b">iptables</code>还会在传入的 IP 数据包上设置一个“标记”(<code class="fe ll lm ln lo b">--set-xmark</code>)。</p><p id="83ad" class="pw-post-body-paragraph ko kp ir kq b kr ks js kt ku kv jv kw kx ky kz la lb lc ld le lf lg lh li lj ik bi translated">DNAT 规则的作用是在预路由检查点之前，即在路由之前，将传入 IP 数据包的目的地址和端口更改为由<code class="fe ll lm ln lo b">–-to-destination</code>指定的新目的地址和端口。如您所见，目的地址和端口正是代理 Pod 的 IP 地址和端口。</p><p id="ca95" class="pw-post-body-paragraph ko kp ir kq b kr ks js kt ku kv jv kw kx ky kz la lb lc ld le lf lg lh li lj ik bi translated">这样，访问服务 VIP 的 IP 包经过上述<code class="fe ll lm ln lo b">iptables</code>处理后，就变成了访问特定后端 Pod 的 IP 包。不难理解，这些端点对应的<code class="fe ll lm ln lo b">iptables</code>规则是由<code class="fe ll lm ln lo b">kube-proxy</code>通过监控 Pod 变化事件在主机上生成和维护的。</p><p id="0030" class="pw-post-body-paragraph ko kp ir kq b kr ks js kt ku kv jv kw kx ky kz la lb lc ld le lf lg lh li lj ik bi translated">流程图如下所示:</p><figure class="kg kh ki kj gu kk gi gj paragraph-image"><div role="button" tabindex="0" class="nw nx di ny bf nz"><div class="gi gj nv"><img src="../Images/55fc7eff77517926f16c800e9426957a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p0MfLPJOOHRQunw2OBn8-g.png"/></div></div></figure><h1 id="c323" class="lp lq ir bd lr ls lt lu lv lw lx ly lz jx ma jy mb ka mc kb md kd me ke mf mg bi translated">ipv</h1><p id="dfea" class="pw-post-body-paragraph ko kp ir kq b kr mh js kt ku mi jv kw kx mj kz la lb mk ld le lf ml lh li lj ik bi translated">通过上面的<code class="fe ll lm ln lo b">iptables</code>解释，可以看到<code class="fe ll lm ln lo b">kube-proxy</code>通过<code class="fe ll lm ln lo b">iptables</code>处理服务的过程实际上需要在主机上设置大量的<code class="fe ll lm ln lo b">iptables</code>规则。另外，<code class="fe ll lm ln lo b">kube-proxy</code>需要在控制循环中不断刷新这些规则，以确保它们总是正确的。</p><p id="0cd4" class="pw-post-body-paragraph ko kp ir kq b kr ks js kt ku kv jv kw kx ky kz la lb lc ld le lf lg lh li lj ik bi translated">不难想象，当你的主机上有大量的 pod 时，成百上千的<code class="fe ll lm ln lo b">iptables</code>规则不断刷新，会占用主机大量的 CPU 资源，甚至会让主机“卡”在进程中。</p><p id="a952" class="pw-post-body-paragraph ko kp ir kq b kr ks js kt ku kv jv kw kx ky kz la lb lc ld le lf lg lh li lj ik bi translated">所以<code class="fe ll lm ln lo b">iptables-based</code>服务实现一直是制约 K8s 项目承载更多 pod 的主要障碍。</p><p id="b542" class="pw-post-body-paragraph ko kp ir kq b kr ks js kt ku kv jv kw kx ky kz la lb lc ld le lf lg lh li lj ik bi translated"><code class="fe ll lm ln lo b">ipvs</code>模式的服务是解决这一问题的有效方法。<code class="fe ll lm ln lo b">ipvs</code>模式的工作原理与<code class="fe ll lm ln lo b">iptables</code>模式类似。在我们创建了前面的服务之后，<code class="fe ll lm ln lo b">kube-proxy</code>将首先在主机上创建一个虚拟网卡(名为:<strong class="kq is"> kube-ipvs0 </strong>)，并为其分配服务 VIP 作为 IP 地址，如下所示:</p><pre class="kg kh ki kj gu mm lo mn mo aw mp bi"><span id="4f3c" class="mq lq ir lo b gz mr ms l mt mu"># ip addr<br/>  ...<br/>  73：kube-ipvs0：&lt;BROADCAST,NOARP&gt;  mtu 1500 qdisc noop state DOWN qlen 1000<br/>  link/ether  1a:ce:f5:5f:c1:4d brd ff:ff:ff:ff:ff:ff<br/>  inet 10.0.1.175/32  scope global kube-ipvs0<br/>  valid_lft forever  preferred_lft forever</span></pre><p id="26b0" class="pw-post-body-paragraph ko kp ir kq b kr ks js kt ku kv jv kw kx ky kz la lb lc ld le lf lg lh li lj ik bi translated">接下来，<code class="fe ll lm ln lo b">kube-proxy</code>将通过 Linux IPVS 模块为该 IP 地址设置三个<code class="fe ll lm ln lo b">ipvs</code>虚拟主机，并将这三个虚拟主机之间的轮询模式(rr)设置为负载均衡策略。我们可以通过<code class="fe ll lm ln lo b">ipvsadm</code>查看该设置，如下所示:</p><pre class="kg kh ki kj gu mm lo mn mo aw mp bi"><span id="33df" class="mq lq ir lo b gz mr ms l mt mu"># ipvsadm -ln<br/> IP Virtual Server version 1.2.1 (size=4096)<br/>  Prot LocalAddress:Port Scheduler Flags<br/>    -&gt;  RemoteAddress:Port           Forward  Weight ActiveConn InActConn     <br/>  TCP  10.102.128.4:80 rr<br/>    -&gt;  10.244.3.6:9376    Masq    1       0          0         <br/>    -&gt;  10.244.1.7:9376    Masq    1       0          0<br/>    -&gt;  10.244.2.3:9376    Masq    1       0          0</span></pre><p id="2c6e" class="pw-post-body-paragraph ko kp ir kq b kr ks js kt ku kv jv kw kx ky kz la lb lc ld le lf lg lh li lj ik bi translated">可以看到，三个 IPVS 虚拟主机的 IP 地址和端口对应于三个代理的 pod。</p><p id="90e1" class="pw-post-body-paragraph ko kp ir kq b kr ks js kt ku kv jv kw kx ky kz la lb lc ld le lf lg lh li lj ik bi translated">此时，发送到 10.102.128.4:80 的任何请求都将被 IPVS 模块转发到后端 Pod。</p><p id="2ce9" class="pw-post-body-paragraph ko kp ir kq b kr ks js kt ku kv jv kw kx ky kz la lb lc ld le lf lg lh li lj ik bi translated">流程图如下所示:</p><figure class="kg kh ki kj gu kk gi gj paragraph-image"><div role="button" tabindex="0" class="nw nx di ny bf nz"><div class="gi gj oa"><img src="../Images/48c9e7a1e4e00e8992ef6aa99dee5782.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WdrIgZaRlRKyS1eewQGkrg.png"/></div></div></figure><h1 id="8292" class="lp lq ir bd lr ls lt lu lv lw lx ly lz jx ma jy mb ka mc kb md kd me ke mf mg bi translated">iptables 与 ipv</h1><p id="3fd7" class="pw-post-body-paragraph ko kp ir kq b kr mh js kt ku mi jv kw kx mj kz la lb mk ld le lf ml lh li lj ik bi translated">与<code class="fe ll lm ln lo b">iptables</code>相比，<code class="fe ll lm ln lo b">ipvs</code>在内核中的实现实际上是基于 Netfilter 的 NAT 模式，所以在转发层，理论上<code class="fe ll lm ln lo b">ipvs</code>并没有显著的性能提升。</p><p id="bd0b" class="pw-post-body-paragraph ko kp ir kq b kr ks js kt ku kv jv kw kx ky kz la lb lc ld le lf lg lh li lj ik bi translated">而 IPVS 不需要在主机上为每个 Pod 设置<code class="fe ll lm ln lo b">iptables</code>规则，而是把这些“规则”的处理放到内核状态，大大降低了维护这些规则的成本。</p><p id="cd6d" class="pw-post-body-paragraph ko kp ir kq b kr ks js kt ku kv jv kw kx ky kz la lb lc ld le lf lg lh li lj ik bi translated">不过需要注意的是，<code class="fe ll lm ln lo b">ipvs</code>模块只负责上述负载均衡和代理功能。然而，一个完整的服务流程正常工作所需的操作，如包过滤和 SNAT，仍然依赖于<code class="fe ll lm ln lo b">iptables</code>。</p><p id="fef6" class="pw-post-body-paragraph ko kp ir kq b kr ks js kt ku kv jv kw kx ky kz la lb lc ld le lf lg lh li lj ik bi translated">但是这些辅助<code class="fe ll lm ln lo b">iptables</code>规则在数量上是有限的，并不随着吊舱数量的增加而增加。因此，在大规模集群中，我强烈建议您将<code class="fe ll lm ln lo b">--proxy-mode=ipvs</code>设置为<code class="fe ll lm ln lo b">kube-proxy</code>来启用这个特性。它给 K8s 集群规模带来的提升还是巨大的。</p></div></div>    
</body>
</html>