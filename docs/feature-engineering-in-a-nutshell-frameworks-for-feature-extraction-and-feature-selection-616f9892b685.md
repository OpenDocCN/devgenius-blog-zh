# 简而言之的特征工程:特征提取和特征选择的框架

> 原文：<https://blog.devgenius.io/feature-engineering-in-a-nutshell-frameworks-for-feature-extraction-and-feature-selection-616f9892b685?source=collection_archive---------2----------------------->

# 0.摘要

在这篇文章中，我将尝试使用一种很少使用但很实用的框架，对一些最重要的特征提取降维技术进行分类。我将尝试展示核心思想和技术背后的数学，尽可能多地保留必要的数学来更好地理解概念，尽可能少地保留可读性。我将关注这些技术的作用，以及在什么情况下使用它们更有意义。

其次，我将使用不同的框架对特征选择技术进行分类，并且解释一些最常用的算法。但首先要做一个简短的介绍，以提高对这个问题重要性的认识。

# 1.信息爆炸与维数灾难

当一个人刚开始学习数据科学时，他往往会觉得越多越好，包括数据和维度。这可能是真的，如果你从特征中获得新奇的信息，而事实并非总是如此。还有“没有免费的午餐定理”,意思是总有代价要付出，但你可以选择代价有多高，以及何时付出，可能在部署时，也可能更早……

即使这个话题并不新鲜，但由于过去几十年的信息爆炸，它变得越来越不重要，这意味着数据生产的增长速度远远超过了我们分析数据的能力。这导致了一个分析缺口，为了弥补这个缺口，我们一直致力于减少功能数量，以提高速度和性能(如果可能的话)。

![](img/699ee929d5f00d5998dd88935a197bee.png)

信息爆炸[2]

但是，很多在低维空间不出现的问题，在高维空间开始出现。这就是我所说的维度诅咒。您可以在 ML 中看到一个这样的例子，随机采样的高维向量通常非常稀疏，这增加了过度拟合的风险，使得在没有大量训练数据的情况下很难识别数据中的模式。

# 2.特征工程:为什么？我们有什么选择？

如前所述，您希望在训练数据中包含相关特征，而不是太多不相关的特征。想出一套好的相关特性是任何 ML 项目的关键部分，通常项目中投入的大部分时间都用于特性工程。这也是关于维数减少，因为我们想简化数据而不丢失太多的信息。我们可以获得的一些优势是:

*   如果我们将数据提供给另一个 ML 算法，它会运行得更快
*   数据将占用更少的内存空间
*   如果我们消除冗余或无用的信息，它甚至可能表现得更好
*   更好地可视化数据，深入了解最重要的功能

一如既往，有一些缺点值得考虑:

*   信息丢失，性能可能下降
*   可能计算量很大
*   ML 管道更加复杂
*   难以解释变形的特征

现在，你有什么选择。请参见下图，了解用于特征提取和特征选择的最重要可用选项的分类。

![](img/319aa8be5c8c2092ad16a2513cad9254.png)

FE 和 FS 方法的分类(图片由作者提供)

# 2.1.特征抽出

特征提取的一个简单例子是当我们将几个相关的特征合并成一个时。如果汽车的里程与车龄非常相关，我们可以实现一个降维算法，将它们合并成一个表示汽车磨损的特征。然而，当我们有成百上千个特征时，手动地做这件事是不可行的。这里的主要降维技术涉及矩阵分解或邻居图[3]。

# 2.1.1.矩阵分解

这里的目标是将一个矩阵表示为两个较小矩阵的乘积。在下图中，蓝色矩阵是您的数据，其中每行是一个样本，每列是一个要素。原型是您将用来重构数据的最简单的形式。

![](img/71c0bfd03e41624558a6f93dd00ffaa3.png)

矩阵分解[3]

你的一行数据将被表达为原型的线性组合。线性组合的系数(图中红色部分)是低维表示。这基本上就是矩阵分解。

![](img/0d65f39c581cc1ff54a1f51da3d32cc1.png)

行作为原型的线性组合[3]

您可能已经看到了降维技术中的分类，这取决于数据转换，是线性的还是非线性的。正如你现在看到的，我们讨论的是线性的。

用数学符号表示，可以写成:

![](img/393a44ac20d28102c038e1c87b34ffeb.png)

你可以看到它不是相等的，而是近似的。这是因为我们在一些约束条件下，尽量减少一些错误或损失。不同的矩阵分解技术基本上取决于这些约束。

## 2.1.1.1.PCA:主成分分析

如果你以前在降维方面做过一些工作，你肯定尝试过 PCA，因为它是一种标准。基本上，我们在这里做的是选择损失作为平方误差。如果你在统计学 101 的回归课上有闪回，那很好，因为本质上就是这样。

![](img/c71063cb75191b3d579d724a5b868f35.png)

PCA 中最小化的函数[3]

在文献中，您可能已经看到 PCA 和类似的方法被归在“投影技术”下，这也是真实的，因为主成分只是形成超平面的轴，然后我们将数据投影到该超平面上。这些轴试图保持尽可能多的差异，或者换句话说，它试图最小化原始数据集与其投影之间的均方距离。所以同一个想法有不同的名字…最小化平方误差。

还记得我们的矩阵乘法与原始数据不完全相等吗？这里值得一提的是近似误差，也称为“重建误差”，定义为原始数据和重建数据(压缩然后解压缩)之间的均方距离。有了 *sklearn* ，我们只用几行代码就可以评估这一点:

```
pca = PCA(n_components = 154) 
X_reduced = pca.fit_transform(X_train) 
X_recovered = pca.inverse_transform(X_reduced)
```

将其应用于包含手写数字的 MNIST 数据集，我们可以看到如果我们希望保留 95%的方差，情况会是怎样:

![](img/20c3dd928c9d79be15364566d32699ba.png)

保留 95%方差的 MNIST 压缩[1]

这 95%被称为“解释方差比”，它基本上是数据集方差在每个主成分中所占的比例。

如果你想知道什么时候使用 PCA，如果你不确定还有什么更好的方法，我建议你把它作为默认的降维技术。关于何时使用替代品，请查看以下章节。

## 2.1.1.2.其他人

除了经典的 PCA 之外，一些矩阵分解备选方案也值得一提:

*   随机 PCA:
    -什么？我们不使用奇异值分解来寻找主成分，而是使用随机算法
    -什么时候？如果你想快点。这很容易比传统的 PCA 快 100 倍
*   增量 PCA (IPCA):
    -什么？小批量处理数据集
    -何时？如果您有大量的训练集和/或在线应用 PCA。

如果你对其他的感兴趣，你可以检查非负矩阵分解或概率 PCA。

# 2.1.2.邻居图

邻居图背后的核心思想是:构建图的方法和嵌入图的方法。在本节中，我将使用 S 数据集，它在解释流形学习时经常使用。基本思想可以在下一张图中看到，其中线性变换没有用，所以我们尝试构建一个连接这些点和它们的 k-最近邻的图。

![](img/a3bd4c42da4d71ec6cc006e1bffdb9e7.png)

来自 S 数据集的点

![](img/7c6313adcb0b80bfe8c480e232dac1e0.png)

相同的图像，其中点与图形相链接[3]

值得一提的是，这些图片可能是欺骗性的，因为它们适用于 3D，但它们有助于获得对正在发生的事情的直觉。

在数学上更深入一点，这里是如何将流形的机制应用到一组有限的点上[5]:我们选择一个参数 *k* 和*k*——一个点的最近邻居形成围绕 *x* 的开放邻域。这些邻域将是坐标面片，并且与欧几里得空间中的开集微分同胚。有时选择邻域半径来计算开放邻域。

![](img/e8658616e292e461c8b3789579f3b2df.png)

从欧几里得空间提取的样本数据集[5]

我们假设数据点实际上位于一个 D 维子流形上，其中 *d < D* 。那么任务就是‘学习’流形。离散集合上的坐标面片通过计算每个点 *x* 的 *k-* 最近邻来构建。

![](img/657f905b4f2c2d31ea76290e7c327b2a.png)

给定这些坐标片，流形学习的目标是找到微分同胚

![](img/733a9cfa6e4cfbf78af4e5ab8ec54a3b.png)

微分同胚[5]

这里的核心思想仍然是，我们想要构建一个图，根据我们如何构建它，以及我们如何将它嵌入到低维中，我们会得到不同的算法，正如我们接下来将看到的。

## 2.1.2.1.Isomap

为了在这里构建图形，我们试图保留实例之间的测地线距离(在*测地线空间*中的距离)。*测地线*是直线概念到弯曲流形的推广，它定义了流形上两点之间沿其弯曲几何形状的最短路径。在下面的图片中，我们可以将它表示为连接红点的红线。

![](img/c37f5d4a3eb4cbf37d907d78ba472e98.png)

流形上两点之间的最短路径[3]

如何把这个嵌入到一个更低维的空间？假设我们在欧几里得空间中，我们只是将权重放在形成所谓的*邻接*矩阵的点上。不需要深究数学，因为我们现在有一个矩阵，当我们有矩阵时，如何到达一个低维空间？如果你读到这里，你会知道我们又在讨论矩阵分解了😊

## 2.1.2.2.t-SNE

这个算法中的数学可能很复杂，但你可以理解为取 *k* 最近邻，我们将改变内核的大小以匹配 *k* 最近邻。为了将它嵌入到一个较低的维度中，该算法所做的是试图保持相似的实例靠近，而不相似的实例分开。

如果您想知道何时需要使用它，它主要用于可视化，特别是可视化高维空间中的实例集群。

## 2.1.2.3.LLE:局部线性嵌入

简而言之，该算法测量每个训练实例与其最近邻居之间的距离，并寻找训练集的低维表示，其中局部关系得到最佳保留。与前面类似，我们通过形成编码这些线性关系的权重矩阵来实现这一点。

为了将其映射到低维空间，我们希望保持权重固定，并在低维空间中找到实例图像的最佳位置。

如果你想展开扭曲的流形，这种算法是好的，但是对于大数据集来说是不好的，因为这种算法伸缩性很差。

## 2.1 代码比较

当可视化这些算法的结果时，我们得到以下结果

![](img/fd13317c3344213862d1d41c24bf9919.png)

比较 S-dataset 中的不同算法[4]

我们可以看到 SNE 霸王龙是最慢的，这种情况经常发生。我在这里没有谈到多维尺度(MDS)，但我们可以看到，它显示了良好的结果，虽然它需要很长的计算时间。还值得一提的是，对于这个特定的数据集，随机投影的效果出奇地好，并且比其他算法快得多。

![](img/1adcf4829cc9dd2221a390a18c4f01ee.png)

S 数据集中的随机投影[4]

# 2.2.特征选择

顾名思义，我们在这里选择最有用的功能进行训练。为了保持本文结构的一致性，我将首先对主要的特征选择技术进行分类，然后给出一些最重要算法的例子。如果您对特征选择技术的更深入的概述感兴趣，我鼓励您阅读[6]。

特征选择技术最初可以分为无监督的(如果这些方法忽略目标变量，而是使用例如相关性来去除冗余变量)和有监督的(使用目标变量来去除冗余特征)，但是在这两种情况下都使用 3 种主要算法:过滤方法、包装方法和嵌入方法。

# 2.2.1.过滤方法

这些算法应用统计测量来给特征分配分数。这个分数用于对特性进行排序，根据这个分数，我们将选择保留哪些特性，放弃哪些特性。

这些方法可以分为单变量(独立考虑特征)或多变量(考虑因变量)

## 2.2.1.1.检验

它是在每个特征和特征之间计算的。选择具有最佳卡方得分的特征。为此，必须满足三个条件:

*   该特征必须是分类的
*   它是独立采样的
*   该特征的预期频率高于 5

## 2.2.1.2.信息增益

在这里，我们正在计算转换的熵的减少。虽然熵的概念起源于热力学，作为一种分子无序的度量，它扩展到其他领域，如*信息论*，在那里它测量一条消息的平均信息内容。在 ML 中，当一个集合只包含一个类的实例时，它的熵为零。

## 2.2.1.3.相关系数得分

寻找与其他要素不相关的要素。

# 2.2.2.包装方法

这里，一组特征的选择被认为是一个搜索问题，其中不同的组合被准备、评估和相互比较。为了评估特征的组合并基于模型准确度分配分数，使用了预测模型。根据搜索过程，可以对算法进行细分:

*   有条不紊的搜索，如最佳优先搜索
*   随机搜索，例如随机爬山算法
*   使用类似向前和向后传递的试探法来添加/移除特征

值得一提的是，由于重复的学习步骤和交叉验证，包装器方法在计算上比过滤器方法更昂贵。从好的方面来看，这些方法比过滤方法更准确。

## 2.2.2.1.正向特征选择

它迭代地选择最佳性能特征。然后，它会选择与第一个特性结合使用时性能最佳的第二个特性。它不断迭代，直到达到期望的性能。

## 2.2.2.2.向后特征选择

这个想法基本上与前向特征选择相反。它开始考虑所有特征，并在每次迭代中删除最不重要的特征，从而提高模型的性能。

## 2.2.2.3.递归特征选择

这里的目标是通过递归地考虑较小的特征集来选择特征。给出了特征的权重估计。训练估计器，获得每个特征的重要性，然后从集合中删除最不重要的特征。该过程在删减集上递归重复，直到达到期望的特征数量。

# 2.2.3.嵌入式方法

在创建模型时，嵌入式方法可以了解哪些特性对模型的性能贡献最大。最常见的类型是正则化(或惩罚)方法，这种方法在预测算法的优化中引入了额外的约束，从而以更高的复杂性惩罚模型。

与提到的替代方法相比，嵌入方法在计算上不如包装方法密集，但是它们具有学习模型特有的缺点。

## 2.2.3.1.拉索正规化 L1

LASSO(最小绝对收缩和选择操作符)倾向于将权重向下推至零。这导致了稀疏模型，其中除了最重要的权重之外，所有的权重都为零。

## 2.2.3.2.弹性网

LASSO 在某些情况下可能表现不稳定，例如，当几个要素高度相关或要素多于行时。在这种情况下，弹性网是首选，虽然它增加了一个额外的超参数。

# 3.结论

当谈到特征提取中的维度技术时，它有助于使用矩阵分解+邻居图的框架来尝试对任何遇到的技术进行分类。这可以帮助你理解几乎所有降维算法背后的核心思想。

我们也看到了一些维度技术，它们如何比较以及何时使用。最后，我们还看到了特征选择技术的分类以及一些最常见的算法。

这篇文章的续篇可以是关于通过收集新数据来创建新特性的文章，或者是关于自动编码器的文章。

参考资料:

[1]使用 Scikit-Learn、Keras 和 TensorFlow 进行机器实践学习，第二版，作者 Aurelien Geron

[2]由 Carlos Huertas (NxGTR)提出的维数约简— Kaggle Days SF

[3]利兰·麦金尼斯的《虚张声势者降维指南》

[4]Stefan küHN 的《用于数据可视化和特征工程的流形学习和降维》

[5]贾努·维尔马著《流形学习》

[6]功能选择—详尽概述