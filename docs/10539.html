<html>
<head>
<title>Data extraction with Python and BeautifulSoup</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Python 和 BeautifulSoup 进行数据提取</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/data-extraction-with-python-and-beautifulsoup-7fb4d79d1cca?source=collection_archive---------5-----------------------#2022-11-09">https://blog.devgenius.io/data-extraction-with-python-and-beautifulsoup-7fb4d79d1cca?source=collection_archive---------5-----------------------#2022-11-09</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><figure class="gl gn jl jm jn jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi jk"><img src="../Images/5c5e56b1ffbd714f61ffe861347d08e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gsRB0j6CLlxmaHVc"/></div></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">罗曼·辛克维奇·🇺🇦在<a class="ae jz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="fff5" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">数据驱动决策的一个必要步骤是获取数据。</p><p id="e2e3" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">数据提取或数据收集是获取有助于数据分析的相关数据的过程。并非所有数据都相关，因此需要注意收集对数据分析最有用的相关数据。</p><p id="7a89" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">这篇文章的目的是提供一个简单的指南，说明如何从提供 HTML 元素的网站中提取数据。并不是每个网站都有可以用来获取数据的公共 API。因此，在没有 API 的情况下，从网站上搜集数据可能会成为一种趋势。</p><h1 id="6414" class="ky kz in bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">安装所需的库</h1><p id="05dc" class="pw-post-body-paragraph ka kb in kc b kd lw kf kg kh lx kj kk kl ly kn ko kp lz kr ks kt ma kv kw kx ig bi translated">开始之前，请确保您的计算机上安装了 python。python==3.7 或更高版本会更好。</p><p id="0e15" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">创建一个文件夹，并启动指向创建的文件夹的命令提示符。</p><p id="1562" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">一旦命令提示符指向文件夹位置，请键入以下内容</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="70a8" class="mk kz in mg b gy ml mm l mn mo">pip install beautifulsoup4</span><span id="4d69" class="mk kz in mg b gy mp mm l mn mo">pip install requests</span><span id="fe82" class="mk kz in mg b gy mp mm l mn mo">pip install jupyter</span></pre><p id="b673" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">对于本教程，我将提取美国所有州的列表及其缩写州名的网站内容。</p><p id="5856" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">网站可以在这里找到<a class="ae jz" href="https://abbreviations.yourdictionary.com/articles/state-abbrev.html" rel="noopener ugc nofollow" target="_blank">美国和 ABBR </a></p><p id="c7e1" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">安装成功后，在命令提示符下键入</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="d689" class="mk kz in mg b gy ml mm l mn mo">jupyter notebook</span></pre><p id="d592" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">这将启动一个 jupyter 编码环境的实例。</p><figure class="mb mc md me gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi mq"><img src="../Images/bbad63286c7b1f329d7181734ad612f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*EKzq6X21W-Sr6Smu.png"/></div></div></figure><p id="27ed" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">要获得您的浏览器用户代理，请使用您的浏览器搜索引擎键入不带引号的“我的用户代理”。</p><p id="6563" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">在第 1 行，导入所需的库。在第 2 行和第 3 行，设置标题和要抓取的网站的 URL，并使用请求库向网站发出请求。来自网站的响应被保存到名为 response 的变量中。</p><figure class="mb mc md me gt jo gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/9bfd992661306afcaf3673521e4df003.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JG2vj_GxPJbuEcMH.png"/></div></figure><p id="b74c" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">在第 4 行，使用 python 和 open()命令，来自网站的响应被保存到 usa_states.html。这是为了避免对网站进行重复的 HTTP 调用。</p><p id="4797" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">在第 5 行，读取保存的文件，并使用 BeautifulSoup 库，解析转换成 python 对象的网站内容。</p><p id="648c" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">在第 6 行，使用 soup.find()方法搜索<table>元素。找到后，搜索作为 table 元素的子元素的所有<tr>标签。</tr></table></p><figure class="mb mc md me gt jo gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/185eaeaa6366afd63376ffcf8293a2d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/0*b9Efscx7GTzYVxvY.png"/></div></figure><p id="9d80" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">结果是一个保存在 states_table 变量中的列表。</p><figure class="mb mc md me gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi mt"><img src="../Images/3605c5f84a5dfb7c60781210eb83aa3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*cVOQNnMB5VoOgxW3.png"/></div></div></figure><p id="490d" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">因为结果是一个列表，所以我们遍历 states_table 列表来查找作为<tr>元素的子元素的<td>元素。找到后，将结果追加到声明的 states_names 变量中。states_names.clear()方法用于删除任何以前保存的列表内容，以避免重复。</td></tr></p><p id="5e82" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">在第 10 行，使用 CSV 内置模块，将结果写入名为 usa_states.csv 的 CSV 文件。</p><figure class="mb mc md me gt jo gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/2db8ce04ded2d3455c108f703e5ae40f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*ubj_1-keZPIrCgfJDg_3CQ.png"/></div></figure><p id="a641" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">最后，我们删除 usa_states.html 文件，该文件用于存储 html 网站的内容。</p><p id="ce32" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">在这里，这是一个简单的抓取网站数据的方法。虽然网站抓取有时很难处理，但我希望这篇文章能给你一个如何抓取网站的基本知识。</p></div></div>    
</body>
</html>