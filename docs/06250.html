<html>
<head>
<title>Ridge, Lasso &amp; Elastic Net Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">脊、套索和弹性网回归</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/ridge-lasso-elastic-net-regression-2ea752186e51?source=collection_archive---------3-----------------------#2021-12-25">https://blog.devgenius.io/ridge-lasso-elastic-net-regression-2ea752186e51?source=collection_archive---------3-----------------------#2021-12-25</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><figure class="gl gn jl jm jn jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi jk"><img src="../Images/4c7fd1eaf9264e2df09dd0a7ec729006.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-aJ9MXozOKv6joiX.png"/></div></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">正规化技术(<a class="ae jz" href="https://miro.medium.com/max/788/1*q53XbAJdKv_l3sw-sUlCVA.png" rel="noopener">来源</a>)</figcaption></figure><p id="4af0" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">在本文中，我们将抛弃常用的线性回归算法，尝试探索一些其他的回归替代方法，即<em class="ky">套索、脊和弹性网</em>。到本文结束时，您将对上述三种算法的区别和工作原理、它们在机器学习问题中的应用以及何时使用它们有了很好的了解。</p></div><div class="ab cl kz la hr lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="ig ih ii ij ik"><h1 id="9041" class="lg lh in bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">偏差-方差权衡:</h1><p id="2e7f" class="pw-post-body-paragraph ka kb in kc b kd me kf kg kh mf kj kk kl mg kn ko kp mh kr ks kt mi kv kw kx ig bi translated">在训练和测试我们的机器学习模型时，我们经常会遇到这样的情况，训练集和测试集之一或两者产生的错误比预期的要高。这可能由于几个原因而发生，其中之一是<em class="ky">偏差-方差权衡</em>。</p><p id="4bff" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><em class="ky">偏差</em>是模型做出与实际值不同的预测的趋势，而<em class="ky">方差</em>是不同数据样本的预测偏差。具有高偏差的模型试图过度简化模型，而具有高方差的模型无法对看不见的数据进行概括。在减少偏差时，模型变得易受高方差的影响，反之亦然。因此，这两种度量之间的权衡或平衡是定义好的预测模型的要素。</p><figure class="mk ml mm mn gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi mj"><img src="../Images/79d58a2407929b4e963bcde29562a422.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bPbddWfUW2tqD_su.png"/></div></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">偏差-方差权衡(<a class="ae jz" href="https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Bias_and_variance_contributing_to_total_error.svg/1920px-Bias_and_variance_contributing_to_total_error.svg.png" rel="noopener ugc nofollow" target="_blank">来源</a>)</figcaption></figure><p id="3da0" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">具有低偏差和高方差的模型被称为o <em class="ky">过拟合</em>，这是模型在训练集上表现非常好，但在看不见的实例集上表现不佳，导致高误差值的情况。解决过度拟合的一种方法是<em class="ky">正则化</em>，这正是下面讨论的回归技术所使用的。</p><h1 id="6e85" class="lg lh in bd li lj mo ll lm ln mp lp lq lr mq lt lu lv mr lx ly lz ms mb mc md bi translated">正规化:</h1><p id="c7d2" class="pw-post-body-paragraph ka kb in kc b kd me kf kg kh mf kj kk kl mg kn ko kp mh kr ks kt mi kv kw kx ig bi translated">正则化是一种用于避免过度拟合的技术，其中系数(如果需要的话)被限制或缩减为零。减少不太重要的特征的影响会直接影响预测的质量，因为它<em class="ky">会减少自由度</em>，这反过来会使模型更难变得复杂或过度拟合数据。</p><figure class="mk ml mm mn gt jo gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/0ca5888565d1f05a1731a0f15d37517f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/0*SE8FAMFxuH1g3yAE.png"/></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">线性回归成本函数</figcaption></figure><p id="ff3d" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">之前讨论的所有内容都是通过修改上面显示的传统线性回归成本函数来完成的。<em class="ky">一个惩罚项</em>被添加到成本函数中，它让我们控制将要在手边的模型上执行的正则化的类型和数量。如果你不熟悉上面显示的等式，你应该看看这个视频。</p><h1 id="9223" class="lg lh in bd li lj mo ll lm ln mp lp lq lr mq lt lu lv mr lx ly lz ms mb mc md bi translated">岭回归:</h1><p id="c03d" class="pw-post-body-paragraph ka kb in kc b kd me kf kg kh mf kj kk kl mg kn ko kp mh kr ks kt mi kv kw kx ig bi translated">这种类型的正则化回归将代表L2范数的一半平方的惩罚项添加到成本函数中。这迫使学习算法不仅要适应数据，还要保持模型权重尽可能小。岭回归方程如下所示。</p><blockquote class="mu mv mw"><p id="57ce" class="ka kb ky kc b kd ke kf kg kh ki kj kk mx km kn ko my kq kr ks mz ku kv kw kx ig bi translated">L2范数是特征向量上预测值和目标值之差的平方和。它也被称为欧几里得距离和均方根误差(RMSE)。</p></blockquote><figure class="mk ml mm mn gt jo gh gi paragraph-image"><div class="gh gi na"><img src="../Images/1aa58ac4e730f9ceaed5fe042ab67fed.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/0*YRLATDD3UCUaP9cf.png"/></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">岭回归成本函数</figcaption></figure><p id="7fd8" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">收缩超参数λ (lambda)控制正则化的量，需要正确选择，因为如果λ = 0，则岭回归与线性回归相同，另一方面，如果λ非常大，则所有权重最终都非常接近于0，导致模型欠拟合。选择正确λ的一个好方法是进行交叉验证。</p><h1 id="df9f" class="lg lh in bd li lj mo ll lm ln mp lp lq lr mq lt lu lv mr lx ly lz ms mb mc md bi translated">套索回归:</h1><p id="636c" class="pw-post-body-paragraph ka kb in kc b kd me kf kg kh mf kj kk kl mg kn ko kp mh kr ks kt mi kv kw kx ig bi translated">简称为<em class="ky">最小绝对收缩和选择算子回归</em>，这种类型的正则化回归使用L1范数而不是L2范数的一半平方作为代价函数中的惩罚项。Lasso回归的一个重要特征是，它倾向于完全消除最不重要的要素的权重，从而自动执行要素选择。</p><blockquote class="mu mv mw"><p id="2f32" class="ka kb ky kc b kd ke kf kg kh ki kj kk mx km kn ko my kq kr ks mz ku kv kw kx ig bi translated">L1范数是特征向量上预测值和目标值之间的差值的大小的总和，或者可以理解为绝对差值的总和。它也被称为曼哈顿距离、出租车标准和平均绝对误差(MAE)。</p></blockquote><figure class="mk ml mm mn gt jo gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/fbed0615a3db0307a0b76cbe07ac2f74.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/0*IaufTtPdCTV_i_Sn.png"/></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">Lasso回归成本函数</figcaption></figure><p id="a250" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">收缩超参数λ的工作方式类似于岭回归，太小会导致无正则化，而太大则会导致模型欠拟合。</p><p id="5e4f" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">岭回归和套索回归之间的主要区别在于，即使两种回归技术都将系数缩小到接近于零，但如果收缩参数足够大，只有套索回归实际上会将系数设置为零。因此，产生了具有选定特征集的模型(稀疏模型),使其更易于解释和使用。</p><h1 id="e46a" class="lg lh in bd li lj mo ll lm ln mp lp lq lr mq lt lu lv mr lx ly lz ms mb mc md bi translated">弹性净回归；</h1><p id="b291" class="pw-post-body-paragraph ka kb in kc b kd me kf kg kh mf kj kk kl mg kn ko kp mh kr ks kt mi kv kw kx ig bi translated">这种回归是岭回归和套索回归的简单混合。弹性网中的惩罚项是绝对值和平方值惩罚的组合。</p><blockquote class="mu mv mw"><p id="784a" class="ka kb ky kc b kd ke kf kg kh ki kj kk mx km kn ko my kq kr ks mz ku kv kw kx ig bi translated">弹性网最早出现是由于对Lasso的批评，其变量选择可能过于依赖数据，因此不稳定。解决方案是结合岭回归和套索的惩罚，以获得两全其美。(<a class="ae jz" href="https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net" rel="noopener ugc nofollow" target="_blank">来源</a></p></blockquote><figure class="mk ml mm mn gt jo gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/e835a11b3a1cc7e7938889305f8e331a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/0*W3CKcnEIlhqXJRVO.png"/></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">弹性网络成本函数</figcaption></figure><p id="51ca" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">脊和套索正则化之间的混合可以通过比率超参数(r)来控制。当r = 0时，弹性网等价于岭回归，当r = 1时，等价于套索回归。</p><figure class="mk ml mm mn gt jo gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/7e5053aeec050f962d1564b20c0cf823.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*qo2E47gWBGUVBHAN.png"/></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">山脊vs套索vs弹性网(<a class="ae jz" href="https://www.oreilly.com/library/view/machine-learning-with/9781787121515/assets/03902148-aac8-4968-a384-3ac2c2180e21.png" rel="noopener ugc nofollow" target="_blank">来源</a>)</figcaption></figure><h1 id="a2a1" class="lg lh in bd li lj mo ll lm ln mp lp lq lr mq lt lu lv mr lx ly lz ms mb mc md bi translated">需要记住的几点:</h1><ul class=""><li id="1acb" class="ne nf in kc b kd me kh mf kl ng kp nh kt ni kx nj nk nl nm bi translated">正如我们在介绍中看到的，正则化可以极大地帮助避免过度拟合，因此一点正则化几乎总是优于一般的线性回归。</li><li id="faaa" class="ne nf in kc b kd nn kh no kl np kp nq kt nr kx nj nk nl nm bi translated">在实施上述三种技术中的任何一种之前，缩放数据非常重要，因为它们都对输入要素的比例非常敏感。这可以使用<em class="ky"> sklearn的标准定标器</em>来完成。</li><li id="ff64" class="ne nf in kc b kd nn kh no kl np kp nq kt nr kx nj nk nl nm bi translated"><em class="ky">岭回归</em>是一个很好的起点，但是如果有可能只有少数特征实际上是有用的，<em class="ky">套索和弹性网</em>可以派上用场，因为它们往往会将无用特征的权重降低到零。</li><li id="03e5" class="ne nf in kc b kd nn kh no kl np kp nq kt nr kx nj nk nl nm bi translated">另一方面，如果特征较少，并且所有特征似乎对目标都很重要，那么<em class="ky">岭回归</em>应该是首选，因为它倾向于给出较小但分布良好的权重。</li><li id="5278" class="ne nf in kc b kd nn kh no kl np kp nq kt nr kx nj nk nl nm bi translated"><em class="ky">当特征的数量大于训练实例的数量或者当几个特征高度相关时，弹性网</em>优于<em class="ky">套索回归</em>，因为在这种情况下，套索会增加一点偏差。</li></ul></div><div class="ab cl kz la hr lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="ig ih ii ij ik"><h1 id="6a30" class="lg lh in bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">结论:</h1><p id="cc00" class="pw-post-body-paragraph ka kb in kc b kd me kf kg kh mf kj kk kl mg kn ko kp mh kr ks kt mi kv kw kx ig bi translated">在这篇文章中，我们了解了偏差-方差的权衡，为什么它是必要的，以及如何通过正则化来达到一个良好的平衡。然后，我们看了三种类型的回归技术:<em class="ky">脊、套索和弹性网</em>，它们的数学表达式，最后，我们回顾了选择一种技术时需要记住的区别和要点。谢谢大家！</p></div></div>    
</body>
</html>