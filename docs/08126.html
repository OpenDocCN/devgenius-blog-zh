<html>
<head>
<title>Clustering Data into Groups, Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">将数据分组，第 2 部分</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/clustering-data-into-groups-part-2-3f7e1d25e67e?source=collection_archive---------8-----------------------#2022-05-18">https://blog.devgenius.io/clustering-data-into-groups-part-2-3f7e1d25e67e?source=collection_archive---------8-----------------------#2022-05-18</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><h2 id="3cda" class="il im in bd b dl io ip iq ir is it dk iu translated" aria-label="kicker paragraph">文章</h2><div class=""/><div class=""><h2 id="9f1e" class="pw-subtitle-paragraph jt iw in bd b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk dk translated"><em class="kl">来自</em> <a class="ae km" href="https://www.manning.com/books/data-science-bookcamp?utm_source=medium&amp;utm_medium=organic&amp;utm_campaign=book_apeltsin_data_9_6_19" rel="noopener ugc nofollow" target="_blank"> <em class="kl">数据科学图书营</em> </a> <em class="kl">作者 Leonard Apeltsin </em></h2></div><p id="3c2e" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated"><em class="lj">这个由 3 部分组成的文章系列包括:</em></p><ul class=""><li id="6cc1" class="lk ll in kp b kq kr kt ku kw lm la ln le lo li lp lq lr ls bi translated"><em class="lj">通过中心性对数据进行聚类</em></li><li id="3e55" class="lk ll in kp b kq lt kt lu kw lv la lw le lx li lp lq lr ls bi translated"><em class="lj">按密度聚类数据</em></li><li id="95a7" class="lk ll in kp b kq lt kt lu kw lv la lw le lx li lp lq lr ls bi translated"><em class="lj">聚类算法之间的权衡</em></li><li id="20f4" class="lk ll in kp b kq lt kt lu kw lv la lw le lx li lp lq lr ls bi translated"><em class="lj">使用 scikit-learn 库执行聚类</em></li><li id="8085" class="lk ll in kp b kq lt kt lu kw lv la lw le lx li lp lq lr ls bi translated"><em class="lj">使用熊猫遍历集群</em></li></ul></div><div class="ab cl ly lz hr ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="ig ih ii ij ik"><p id="1ed7" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated">在<a class="ae km" href="https://www.manning.com/?utm_source=medium&amp;utm_medium=organic&amp;utm_campaign=book_apeltsin_data_9_6_19" rel="noopener ugc nofollow" target="_blank">manning.com</a>的结账处，将<strong class="kp ix"> fccapeltsin </strong>输入折扣代码框，即可享受<a class="ae km" href="https://www.manning.com/books/data-science-bookcamp?utm_source=medium&amp;utm_medium=organic&amp;utm_campaign=book_apeltsin_data_9_6_19" rel="noopener ugc nofollow" target="_blank"> <em class="lj">数据科学图书营</em></a>35%的折扣。</p></div><div class="ab cl ly lz hr ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="ig ih ii ij ik"><p id="0eb8" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated">如果你错过了，你可以在这里查看<a class="ae km" href="https://manningbooks.medium.com/clustering-data-into-groups-part-1-2a438c46de95" rel="noopener">第一部分</a>。</p><h2 id="2920" class="mf mg in bd mh mi mj dn mk ml mm dp mn kw mo mp mq la mr ms mt le mu mv mw it bi translated"><strong class="ak"> K-means:用于将数据分组为 K 个中心组的聚类算法</strong></h2><p id="8fd7" class="pw-post-body-paragraph kn ko in kp b kq mx jx ks kt my ka kv kw mz ky kz la na lc ld le nb lg lh li ig bi translated">K-means 算法假设输入的数据点围绕<em class="lj"> K </em>不同的中心旋转。每个中心坐标就像一个隐藏的靶心，被分散的数据点所包围。该算法的目的是揭示这些隐藏的中心坐标。</p><p id="9488" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated">我们通过首先选择<em class="lj"> K </em>来初始化 K-means，这是我们将搜索的中心坐标的数量。在我们的镖靶分析中，<em class="lj"> K </em>被设置为 2，尽管通常<em class="lj"> K </em>可以等于任何整数。该算法随机选择<em class="lj"> K </em>个数据点。这些数据点被视为真正的中心。然后，算法通过更新选择的中心位置进行迭代，数据科学家称之为<em class="lj">质心</em>。在单次迭代中，每个数据点被分配到其最近的中心，导致形成<em class="lj"> K </em>组。接下来，更新每个组的中心。新的中心等于组坐标的平均值。如果我们重复这个过程足够长的时间，组均值将收敛到<em class="lj"> K </em>个代表中心(图 6)。收敛性在数学上是有保证的。然而，我们无法预先知道收敛所需的迭代次数。一个常见的技巧是，当新计算的中心都没有明显偏离其前身时，停止迭代。</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nc"><img src="../Images/d5ce96a945d8ccc0e1d9b1ee9590fb5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*go7KdLCBzUG4s7wn.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk translated">图 6。K-means 算法从两个随机选择的质心迭代收敛到实际的靶心质心</figcaption></figure><p id="aa1b" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated">K-means 也不是没有局限性。该算法基于我们对 K 的了解:要寻找的聚类数。通常，这种知识是不存在的。此外，虽然 K-means 通常可以找到合理的中心，但从数学上来说，它不能保证找到数据中可能的最佳中心。有时，K-means 会由于算法初始化阶段随机质心选择不当而返回非直觉或次优组。最后，K-means 假设数据中的聚类实际上围绕<em class="lj"> K </em>中心位置旋转。但是正如我们在本节后面所了解到的，这个假设并不总是成立的。</p><h2 id="cc44" class="mf mg in bd mh mi mj dn mk ml mm dp mn kw mo mp mq la mr ms mt le mu mv mw it bi translated"><strong class="ak">使用 scikit-learn 的 K 均值聚类</strong></h2><p id="b971" class="pw-post-body-paragraph kn ko in kp b kq mx jx ks kt my ka kv kw mz ky kz la na lc ld le nb lg lh li ig bi translated">如果 K-means 算法被有效地实现，它可以在合理的时间内运行。通过外部 scikit-learn 库可以快速实现该算法。Scikit-learn 是一个非常流行的机器学习工具包，构建在 NumPy 和 SciPy 之上。它具有各种核心分类、回归和聚类算法，当然包括 K-means。让我们安装库。然后我们导入 scikit-learn 的<code class="fe ns nt nu nv b">KMeans</code>集群类。</p><p id="65a0" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated">从命令行终端调用<code class="fe ns nt nu nv b">pip install scikit-learn</code>来安装 scikit-learn 库。</p><p id="b8bb" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated"><strong class="kp ix">清单 8。从 scikit 导入</strong><code class="fe ns nt nu nv b"><strong class="kp ix">KMeans</strong></code><strong class="kp ix">-学习</strong></p><pre class="nd ne nf ng gt nw nv nx ny aw nz bi"><span id="0985" class="mf mg in nv b gy oa ob l oc od">from sklearn.cluster import Kmeans</span></pre><p id="88fb" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated">将<code class="fe ns nt nu nv b">KMeans</code>应用到我们的<code class="fe ns nt nu nv b">darts</code>数据很容易。首先我们需要运行<code class="fe ns nt nu nv b">KMeans(n_clusters=2)</code>，这将创建一个能够找到两个靶心的<code class="fe ns nt nu nv b">cluster_model</code>对象。然后我们可以通过运行<code class="fe ns nt nu nv b">cluster_model.fit_predict(darts)</code>来执行 K-means。该方法调用将返回一个存储每个飞镖的靶心索引的<code class="fe ns nt nu nv b">assigned_bulls_eyes</code>数组。</p><p id="2ba5" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated"><strong class="kp ix">清单 9。使用 scikit-learn 的 K-means 聚类</strong></p><pre class="nd ne nf ng gt nw nv nx ny aw nz bi"><span id="3dda" class="mf mg in nv b gy oa ob l oc od">cluster_model = KMeans(n_clusters=2) ❶<br/> assigned_bulls_eyes = cluster_model.fit_predict(darts) ❷<br/>  <br/> print("Bull's-eye assignments:")<br/> print(assigned_bulls_eyes)<br/> Bull's-eye assignments:<br/> [0 0 0 ... 1 1 1]</span></pre><p id="7b55" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated">❶ <strong class="kp ix">创建一个聚类模型对象，其中中心的数量被设置为 2 </strong></p><p id="bfb3" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated">❷ <strong class="kp ix">使用 K-means 算法优化两个中心，并返回为每个镖分配的聚类</strong></p><p id="1643" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated">让我们根据它们的聚类分配来给我们的飞镖着色，以验证结果(图 7)。</p><p id="31b1" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated"><strong class="kp ix">清单 10。绘制 K-均值聚类分配图</strong></p><pre class="nd ne nf ng gt nw nv nx ny aw nz bi"><span id="58e2" class="mf mg in nv b gy oa ob l oc od">for bs_index in range(len(bulls_eyes)):<br/>     selected_darts = [darts[i] for i in range(len(darts))<br/>                       if bs_index == assigned_bulls_eyes[i]]<br/>     x_coordinates, y_coordinates = np.array(selected_darts).T<br/>     plt.scatter(x_coordinates, y_coordinates,<br/>                 color=['g', 'k'][bs_index])<br/> plt.show()</span></pre><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi oe"><img src="../Images/89ab3f6b42f97c7e827d272b93e393db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KzIDyisJ2cdmQC0m.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk translated">图 7。scikit-learn 返回的 K-means 聚类结果与我们的预期一致。</figcaption></figure><p id="13d0" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated">我们的聚类模型已经定位了数据中的质心。现在我们可以重用这些质心来分析模型以前没有见过的新数据点。执行<code class="fe ns nt nu nv b">cluster_model.predict([x, y])</code>将质心分配给由<code class="fe ns nt nu nv b">x</code>和<code class="fe ns nt nu nv b">y</code>定义的数据点。我们使用<code class="fe ns nt nu nv b">predict</code>方法对两个新数据点进行聚类。</p><p id="c07c" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated"><strong class="kp ix">清单 11。使用</strong> <code class="fe ns nt nu nv b"><strong class="kp ix">cluster_model</strong></code> <strong class="kp ix">对新数据进行聚类</strong></p><pre class="nd ne nf ng gt nw nv nx ny aw nz bi"><span id="9709" class="mf mg in nv b gy oa ob l oc od">new_darts = [[500, 500], [-500, -500]]<br/> new_bulls_eye_assignments = cluster_model.predict(new_darts)<br/> for i, dart in enumerate(new_darts):<br/>     bulls_eye_index = new_bulls_eye_assignments[i]<br/>     print(f"Dart at {dart} is closest to bull's-eye {bulls_eye_index}")<br/> Dart at [500, 500] is closest to bull's-eye 0<br/> Dart at [-500, -500] is closest to bull's-eye 1</span></pre><h2 id="31ff" class="mf mg in bd mh mi mj dn mk ml mm dp mn kw mo mp mq la mr ms mt le mu mv mw it bi translated"><strong class="ak">使用肘法选择最佳 K</strong></h2><p id="c3fe" class="pw-post-body-paragraph kn ko in kp b kq mx jx ks kt my ka kv kw mz ky kz la na lc ld le nb lg lh li ig bi translated">K-means 依赖于输入的<em class="lj"> K </em>。当事先不知道数据中真实聚类的数量时，这可能是一个严重的障碍。然而，我们可以使用称为<em class="lj">弯头法</em>的技术来估计<em class="lj"> K </em>的适当值。</p><p id="51ae" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated">肘方法依赖于一个叫做<em class="lj">惯性</em>的计算值，它是每个点和它最近的 K 均值中心之间距离的平方和。如果<em class="lj"> K </em>为 1，那么惯性等于到数据集均值的所有平方距离的总和。如第 5 节所述，该值与方差成正比。方差反过来是离差的度量。因此，如果<em class="lj"> K </em>为 1，则惯性是对离差的估计。即使<em class="lj"> K </em>大于 1，该属性也成立。基本上，惯性估计了我们计算的平均值周围的总离差。</p><p id="4d29" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated">通过估计离差，我们可以确定我们的<em class="lj"> K </em>值是太高还是太低。例如，假设我们将<em class="lj"> K </em>设为 1。潜在地，我们的许多数据点将位于离一个中心太远的地方。我们的离差会很大，惯性也会很大。当我们将 K 向一个更合理的数值增加时，额外的中心将导致惯性减小。最终，如果我们走极端，将<em class="lj"> K </em>设置为等于点的总数，每个数据点都将落入自己的私有簇中。色散将被消除，惯性将下降到零(图 8)。</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi of"><img src="../Images/c1e3f8cb0d5b684450d097a5e84bb263.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qLY609wmPXflVqr4.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk translated">图 8。编号为 1 到 6 的六个点被标绘在 2D 空间中。由星星标记的中心是通过不同的<em class="kl"> K </em>值计算出来的。从每个点到最近的中心画一条线。惯性是通过将六条线的平方长度相加计算出来的。<em class="kl">(A)K</em>= 1。所有六条线都从一个中心延伸出来。惯性挺大的。(B) <em class="kl"> K </em> = 2。点 5 和 6 非常靠近第二个中心。惯性减小。<em class="kl"> K </em> = 3。点 1 和 3 基本上更靠近新形成的中心。点 2 和 4 也基本上更靠近新形成的中心。惯性已经急剧下降。(D) <em class="kl"> K </em> = 4。点 1 和 3 现在与其中心重叠。它们对惯性的贡献已经从非常低的值转移到零。其余四个点及其相关中心之间的距离保持不变。因此，将<em class="kl"> K </em>从 3 增加到 4 会导致惯性非常小的降低。</figcaption></figure><p id="4231" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated">有些惯性值太大。其他的太低了。介于两者之间的某个值可能正好合适。我们如何找到它？</p><p id="5236" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated">让我们想出一个解决办法。我们首先在大范围的<em class="lj"> K </em>值上绘制我们的镖靶数据集的惯性(图 9)。为每个 scikit-learn <code class="fe ns nt nu nv b">KMeans</code>对象自动计算惯性。我们可以通过模型的<code class="fe ns nt nu nv b">_inertia</code>属性来访问这个存储值。</p><p id="3f80" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated"><strong class="kp ix">清单 12。绘制 K-均值惯性</strong></p><pre class="nd ne nf ng gt nw nv nx ny aw nz bi"><span id="4032" class="mf mg in nv b gy oa ob l oc od">k_values = range(1, 10)<br/> inertia_values = [KMeans(k).fit(darts).inertia_<br/>                   for k in k_values]<br/>  <br/> plt.plot(k_values, inertia_values)<br/> plt.xlabel('K')<br/> plt.ylabel('Inertia')<br/> plt.show()</span></pre><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi og"><img src="../Images/06a512634aa9b00a01f63ceaf15d114f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*L4DElIC6-xs6cDkJ.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk translated">图 9。包含两个靶心目标的飞镖靶模拟的惯性图。这个情节就像一只胳膊在肘部弯曲。肘部直接指向 2 的一个<em class="kl"> K </em>。</figcaption></figure><p id="d3c0" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated">生成的图类似于肘部弯曲的手臂，肘部指向的<em class="lj"> K </em>值为 2。正如我们已经知道的，这个<em class="lj"> K </em>精确地捕捉了我们已经预编程到数据集中的两个中心。</p><p id="6b12" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated">如果现有中心的数量增加，这种方法还适用吗？我们可以通过在投掷飞镖模拟中添加一个额外的靶心来找出答案。在我们将集群数量增加到三个之后，我们重新生成我们的惯性图(图 10)。</p><p id="1909" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated">清单 13。绘制 3 镖靶模拟的惯性图</p><pre class="nd ne nf ng gt nw nv nx ny aw nz bi"><span id="3d5e" class="mf mg in nv b gy oa ob l oc od">new_bulls_eye = [12, 0]<br/> for _ in range(5000):<br/>     x = np.random.normal(new_bulls_eye[0], variance ** 0.5)<br/>     y = np.random.normal(new_bulls_eye[1], variance ** 0.5)<br/>     darts.append([x, y])<br/>  <br/> inertia_values = [KMeans(k).fit(darts).inertia_<br/>                   for k in k_values]<br/>  <br/> plt.plot(k_values, inertia_values)<br/> plt.xlabel('K')<br/> plt.ylabel('Inertia')<br/> plt.show()</span></pre><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi og"><img src="../Images/68c692d2c5b637c2130f6f0451fe46aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YVxLwh0eThT2cmFv.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk translated">图 10。包含三个靶心目标的飞镖靶模拟的惯性图。这个情节就像一只胳膊在肘部弯曲。弯头的最低部分指向 3 的 K。</figcaption></figure><p id="8db7" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated">添加第三个中心导致新弯头的最低倾斜度指向 3 的<em class="lj"> K </em>值。本质上，我们的肘形图描绘了每个增量<em class="lj"> K </em>捕获的离差。连续的<em class="lj"> K </em>值之间惯性的快速下降意味着分散的数据点已经被分配到更紧密的聚类中。随着惯性曲线变平，惯性的减小逐渐失去其影响。这种从垂直下降到较平缓角度的过渡导致我们的图中出现一个肘形。我们可以使用肘部的位置在 K-means 算法中选择一个合适的<em class="lj"> K </em>。</p><p id="32e6" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated">肘方法选择标准是一个有用的启发，但并不保证在所有情况下都有效。在某些情况下，肘部在多个<em class="lj"> K </em>值上缓慢变平，这使得很难选择单个有效的聚类计数。</p><p id="67bb" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated">还有更强大的<em class="lj"> K </em>选择方法，比如<em class="lj">剪影得分</em>，它捕捉每个点到相邻聚类的距离。对剪影配乐的彻底讨论超出了本书的范围。然而，我们鼓励你使用<code class="fe ns nt nu nv b">sklearn.metrics.silhouette_score</code>方法自己探索乐谱。</p><h2 id="2eaa" class="mf mg in bd mh mi mj dn mk ml mm dp mn kw mo mp mq la mr ms mt le mu mv mw it bi translated">k-均值聚类方法</h2><ul class=""><li id="f37c" class="lk ll in kp b kq mx kt my kw oh la oi le oj li lp lq lr ls bi translated"><code class="fe ns nt nu nv b">k_means_model = KMeans(n_clusters=K)</code> —创建一个 K 均值模型来搜索<em class="lj"> K 个</em>不同的质心。我们需要将这些质心与输入的数据相匹配。</li><li id="c38f" class="lk ll in kp b kq lt kt lu kw lv la lw le lx li lp lq lr ls bi translated"><code class="fe ns nt nu nv b">clusters = k_means_model.fit_predict(data)</code> —使用初始化的<code class="fe ns nt nu nv b">KMeans</code>对象对输入的数据执行 K-means。返回的<code class="fe ns nt nu nv b">clusters</code>数组包含范围从 0 到<em class="lj"> K </em>的簇 id。<code class="fe ns nt nu nv b">data[i]</code>的集群 ID 等于<code class="fe ns nt nu nv b">clusters[i]</code>。</li><li id="015d" class="lk ll in kp b kq lt kt lu kw lv la lw le lx li lp lq lr ls bi translated"><code class="fe ns nt nu nv b">clusters = KMeans(n_clusters=K).fit_predict(data)</code> —在单行代码中执行 K-means，并返回结果聚类。</li><li id="385f" class="lk ll in kp b kq lt kt lu kw lv la lw le lx li lp lq lr ls bi translated"><code class="fe ns nt nu nv b">new_clusters = k_means_model.predict(new_data)</code>-使用数据优化<code class="fe ns nt nu nv b">KMeans</code>对象中的现有质心，找到与以前未见过的数据最近的质心。</li><li id="2edf" class="lk ll in kp b kq lt kt lu kw lv la lw le lx li lp lq lr ls bi translated"><code class="fe ns nt nu nv b">inertia = k_means_model.inertia_</code> —返回与数据优化的<code class="fe ns nt nu nv b">KMeans</code>对象相关联的惯性。</li><li id="fb05" class="lk ll in kp b kq lt kt lu kw lv la lw le lx li lp lq lr ls bi translated"><code class="fe ns nt nu nv b">inertia = KMeans(n_clusters=K).fit(data).inertia_</code> —在单行代码中执行 K-means，并返回结果惯量。</li></ul><p id="38cf" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated">肘方法并不完美，但是如果数据集中在<em class="lj"> K </em> distinct means 上，它表现得相当好。当然，这假设我们的数据集群由于中心性而不同。然而，在许多情况下，由于空间中数据点的密度不同，数据聚类也不同。让我们探讨一下密度驱动的集群的概念，它不依赖于中心性。</p><h2 id="00a2" class="mf mg in bd mh mi mj dn mk ml mm dp mn kw mo mp mq la mr ms mt le mu mv mw it bi translated"><strong class="ak">使用密度发现集群</strong></h2><p id="36d8" class="pw-post-body-paragraph kn ko in kp b kq mx jx ks kt my ka kv kw mz ky kz la na lc ld le nb lg lh li ig bi translated">假设一位天文学家在太阳系遥远的边缘发现了一颗新行星。这颗行星很像土星，有多个环围绕其中心在恒定的轨道上旋转。每个环都是由成千上万的岩石形成的。我们将这些岩石建模为由 x 和 y 坐标定义的独立点。让我们使用 scikit-learn 的<code class="fe ns nt nu nv b">makes_circles</code>函数生成三个由许多岩石组成的岩石环(图 11)。</p><p id="b1eb" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated"><strong class="kp ix">清单 14。模拟行星周围的光环</strong></p><pre class="nd ne nf ng gt nw nv nx ny aw nz bi"><span id="7517" class="mf mg in nv b gy oa ob l oc od">from sklearn.datasets import make_circles<br/>  <br/> x_coordinates = []<br/> y_coordinates = []<br/> for factor in [.3, .6, 0.99]:<br/>     rock_ring, _ = make_circles(n_samples=800, factor=factor, ❶<br/>                                 noise=.03, random_state=1)<br/>     for rock in rock_ring:<br/>         x_coordinates.append(rock[0])<br/>         y_coordinates.append(rock[1])<br/>  <br/> plt.scatter(x_coordinates, y_coordinates)<br/> plt.show()</span></pre><p id="8171" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated">❶<strong class="kp ix">make _ circles 函数在 2D 中创建两个同心圆。较小圆的半径相对于较大圆的比例由“因子”参数决定。</strong></p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi ok"><img src="../Images/ddc7e16c96638e712da6ab6b8cd41b3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2KfmwNMrsWI70wqQ.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk translated">图 11。围绕一个中心点的三个岩石环的模拟</figcaption></figure><p id="87a1" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated">三个环组清楚地出现在图中。让我们通过将<em class="lj"> K </em>设置为 3 来使用 K-means 搜索这三个集群(图 12)。</p><p id="c0b1" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated"><strong class="kp ix">清单 15。使用 K-均值聚类环</strong></p><pre class="nd ne nf ng gt nw nv nx ny aw nz bi"><span id="6187" class="mf mg in nv b gy oa ob l oc od">rocks = [[x_coordinates[i], y_coordinates[i]]<br/>           for i in range(len(x_coordinates))]<br/> rock_clusters = KMeans(3).fit_predict(rocks)<br/>  <br/> colors = [['g', 'y', 'k'][cluster] for cluster in  rock_clusters]<br/> plt.scatter(x_coordinates, y_coordinates, color=colors)<br/> plt.show()</span></pre><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi ol"><img src="../Images/4e3dbcdafa2aaeef5f7c105724f63e25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*u0g88AsuGB7Scbdx.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk translated">图 12。k-均值聚类未能正确识别三个不同的岩石环。</figcaption></figure><p id="d1ae" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated">输出是一个彻底的失败！K-means 将数据分解成三个对称的段，每个段跨越多个环。这个解决方案不符合我们的直觉预期，即每个环都应该属于自己独特的组。哪里出了问题？K-means 假设三个星团由三个独特的中心定义，但实际的环围绕一个中心点旋转。集群之间的差异不是由中心性驱动的，而是由密度驱动的。每一个环都是由密集的点构成的，人口稀少的空白区域作为环之间的边界。</p><p id="8ed7" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated">我们需要设计一种算法，在密集的空间区域聚集数据。这样做要求我们定义一个给定的区域是密集的还是稀疏的。<em class="lj">密度</em>的一个简单定义如下:只有当一个点位于<em class="lj"> Y </em>其他点的距离<em class="lj"> X </em>内时，该点才处于密集区域。我们将把<em class="lj"> X </em>和<em class="lj"> Y </em>分别称为<code class="fe ns nt nu nv b">epsilon</code>和<code class="fe ns nt nu nv b">min_points</code>。以下代码将<code class="fe ns nt nu nv b">epsilon</code>设置为 0.1，将<code class="fe ns nt nu nv b">min_points</code>设置为 10。因此，如果我们的岩石在至少 10 个其他岩石的 0.1 半径范围内，它们就存在于空间的密集区域。</p><p id="e656" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated"><strong class="kp ix">清单 16。指定密度参数</strong></p><pre class="nd ne nf ng gt nw nv nx ny aw nz bi"><span id="72f0" class="mf mg in nv b gy oa ob l oc od">epsilon = 0.1<br/> min_points = 10</span></pre><p id="10cc" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated">让我们分析一下<code class="fe ns nt nu nv b">rocks</code>列表中第一块岩石的密度。我们从搜索<code class="fe ns nt nu nv b">rocks[0]</code>的<code class="fe ns nt nu nv b">epsilon</code>个单位内的所有其他岩石开始。我们将这些相邻岩石的索引存储在一个<code class="fe ns nt nu nv b">neighbor_indices</code>列表中。</p><p id="7381" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated"><strong class="kp ix">清单 17。</strong>寻找邻居<code class="fe ns nt nu nv b"><strong class="kp ix">rocks[0]</strong></code></p><pre class="nd ne nf ng gt nw nv nx ny aw nz bi"><span id="ae02" class="mf mg in nv b gy oa ob l oc od">neighbor_indices = [i for i, rock in enumerate(rocks[1:])<br/>                     if euclidean(rocks[0], rock) &lt;= epsilon]</span></pre><p id="f7ab" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated">现在，我们将邻居的数量与<code class="fe ns nt nu nv b">min_points</code>进行比较，以确定<code class="fe ns nt nu nv b">rocks[0]</code>是否位于密集的空间区域。</p><p id="0fdf" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated"><strong class="kp ix">清单 18。检查</strong>的密度<code class="fe ns nt nu nv b"><strong class="kp ix">rocks[0]</strong></code></p><pre class="nd ne nf ng gt nw nv nx ny aw nz bi"><span id="4dee" class="mf mg in nv b gy oa ob l oc od">num_neighbors = len(neighbor_indices)<br/> print(f"The rock at index 0 has {num_neighbors} neighbors.")<br/>  <br/> if num_neighbors &gt;= min_points:<br/>     print("It lies in a dense region.")<br/> else:<br/>     print("It does not lie in a dense region.")<br/> The rock at index 0 has 40 neighbors.<br/> It lies in a dense region.</span></pre><p id="f66f" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated">指数为 0 的岩石位于稠密的空间区域。<code class="fe ns nt nu nv b">rocks[0]</code>的邻居也共享那个密集的空间区域吗？这是一个很难回答的问题。毕竟，有可能每个邻居的邻居都少于自己的邻居。根据我们严格的密度定义，我们不会认为这些邻居是密集点。然而，这将导致一种可笑的情况，其中密集区域仅由一个点组成:<code class="fe ns nt nu nv b">rocks[0]</code>。我们可以通过更新密度定义来避免这种荒谬的结果。让我们正式定义<em class="lj">密度</em>如下:</p><ul class=""><li id="93f6" class="lk ll in kp b kq kr kt ku kw lm la ln le lo li lp lq lr ls bi translated">如果一个点位于<code class="fe ns nt nu nv b">min_point</code>邻居的<code class="fe ns nt nu nv b">epsilon</code>距离内，则该点位于空间的密集区域。</li><li id="a38a" class="lk ll in kp b kq lt kt lu kw lv la lw le lx li lp lq lr ls bi translated">一个密集空间区域中的一个点的每个邻居也聚集在该空间中。</li></ul><p id="da32" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated">基于我们更新的定义，我们可以将<code class="fe ns nt nu nv b">rocks[0]</code>和它的邻居组合成一个单一的密集集群。</p><p id="bf47" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated"><strong class="kp ix">清单 19。创建密集集群</strong></p><pre class="nd ne nf ng gt nw nv nx ny aw nz bi"><span id="bfe3" class="mf mg in nv b gy oa ob l oc od">dense_region_indices = [0] + neighbor_indices<br/> dense_region_cluster = [rocks[i] for i in dense_region_indices]<br/> dense_cluster_size = len(dense_region_cluster)<br/> print(f"We found a dense cluster containing {dense_cluster_size} rocks")<br/> We found a dense cluster containing 41 rocks</span></pre><p id="9c33" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated">指数为 0 的岩石和它的邻居形成了一个 41 元素的致密星团。是否有邻居的邻居属于空间的密集区域？如果是这样，那么按照我们更新的定义，这些岩石也属于致密星团。因此，通过分析额外的邻近点，我们可以扩展<code class="fe ns nt nu nv b">dense_region_cluster</code>的大小。</p><p id="0660" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated"><strong class="kp ix">清单 20。扩展密集集群</strong></p><pre class="nd ne nf ng gt nw nv nx ny aw nz bi"><span id="b46e" class="mf mg in nv b gy oa ob l oc od">dense_region_indices = set(dense_region_indices) ❶<br/> for index in neighbor_indices:<br/>     point = rocks[index]<br/>     neighbors_of_neighbors = [i for i, rock in enumerate(rocks)<br/>                               if euclidean(point, rock) &lt;= epsilon]<br/>     if len(neighbors_of_neighbors) &gt;= min_points:<br/>         dense_region_indices.update(neighbors_of_neighbors)<br/>  <br/> dense_region_cluster = [rocks[i] for i in dense_region_indices]<br/> dense_cluster_size = len(dense_region_cluster)<br/> print(f"We expanded our cluster to include {dense_cluster_size} rocks")<br/> We expanded our cluster to include 781 rocks</span></pre><p id="5b40" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated">❶ <strong class="kp ix">将密集区域索引转换为集合。这允许我们用额外的索引更新集合，而不用担心重复。</strong></p><p id="1047" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated">我们遍历了邻居的邻居，将我们的密集集群扩展了近 20 倍。为什么停在那里？通过分析新遇到的邻居的密度，我们可以进一步扩展我们的集群。反复重复我们的分析将增加我们的聚类边界的宽度。最终，边界会扩展到完全包围我们的一个岩石环。然后，由于没有新的邻居要吸收，我们可以对到目前为止还没有分析过的<code class="fe ns nt nu nv b">rocks</code>元素重复迭代分析。这种重复将导致更多致密环的聚集。</p><p id="63a5" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated">刚刚描述的过程被称为 DBSCAN。DBSCAN 算法根据数据的空间分布来组织数据。</p><p id="ad3f" class="pw-post-body-paragraph kn ko in kp b kq kr jx ks kt ku ka kv kw kx ky kz la lb lc ld le lf lg lh li ig bi translated">点击查看<a class="ae km" href="https://manningbooks.medium.com/clustering-data-into-groups-part-3-7369148da274" rel="noopener">第三部分。感谢阅读。</a></p></div></div>    
</body>
</html>