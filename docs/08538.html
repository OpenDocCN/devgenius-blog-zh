<html>
<head>
<title>PCA: A data science technique to throw out the noisy stuff</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PCA:一种剔除噪声的数据科学技术</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/pca-a-data-science-technique-to-throw-out-the-noisy-stuff-ff8ab81ca65f?source=collection_archive---------17-----------------------#2022-06-22">https://blog.devgenius.io/pca-a-data-science-technique-to-throw-out-the-noisy-stuff-ff8ab81ca65f?source=collection_archive---------17-----------------------#2022-06-22</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><p id="be22" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">PCA 或主成分分析是一种降维方法。它主要查看数据的每个特征维度(特征向量)(标准化后)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/39c0f4a74b76fa76afa90d7017a1e4e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GjPSIYjBLggsHIFyIoWTbA.jpeg"/></div></div></figure><p id="6faf" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我喜欢把这个过程想象成在一个成功的万圣节后挑选吃什么糖果，当然我们喜欢吃所有的糖果(保持数据的所有维度)，但是如果我们这样做了，我们会呕吐——或者耗尽我们服务器上的空间。因此，我们决定按照美味度来组织糖果——维度(在我们的例子中，美味度等于维度中的方差，因为捕获的方差越多=数据越丰富)。所以我们挑了 M 块最美味的糖果，在爸爸妈妈把剩下的藏起来之前把它们藏起来。</p><p id="b3ca" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">例如，假设我们在 3D 空间中有一组点，但我们想将这些数据压缩到 2D 平面中，并尽可能少地丢失信息。我们将选择具有最高特征值的两个特征向量。</p><p id="aae2" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">特征值是该维度中方差的良好代表，PCA 的目标是找到我们的数据具有高方差的特征维度，并保持这些特征维度，因为我们具有的方差越大，通过删除该维度丢失的数据就越多。</p><p id="cf04" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">现在你已经理解了 PCA 背后的直觉，让我们直接进入代码吧！如果你想继续的话，下面是 colabs 笔记本的链接。</p><div class="ku kv gp gr kw kx"><a href="https://colab.research.google.com/drive/1tWPjWUAOPgNIc0OHceKLZ232Vvhb5AD4#scrollTo=dTnylQeR3BT1" rel="noopener  ugc nofollow" target="_blank"><div class="ky ab fo"><div class="kz ab la cl cj lb"><h2 class="bd io gy z fp lc fr fs ld fu fw im bi translated">谷歌联合实验室</h2><div class="le l"><h3 class="bd b gy z fp lc fr fs ld fu fw dk translated">编辑描述</h3></div><div class="lf l"><p class="bd b dl z fp lc fr fs ld fu fw dk translated">colab.research.google.com</p></div></div><div class="lg l"><div class="lh l li lj lk lg ll ks kx"/></div></div></a></div><h2 id="347a" class="lm ln in bd lo lp lq dn lr ls lt dp lu jv lv lw lx jz ly lz ma kd mb mc md me bi translated">生成数据</h2><p id="dc7e" class="pw-post-body-paragraph jk jl in jm b jn mf jp jq jr mg jt ju jv mh jx jy jz mi kb kc kd mj kf kg kh ig bi translated">我们的第一步是生成数据。我使用方差不同的正态分布生成了我们的 x、y 和 z 坐标，然后将它们绘制在一个漂亮的 3D 图形中。</p><h2 id="ac7d" class="lm ln in bd lo lp lq dn lr ls lt dp lu jv lv lw lx jz ly lz ma kd mb mc md me bi translated">常化</h2><p id="f5c6" class="pw-post-body-paragraph jk jl in jm b jn mf jp jq jr mg jt ju jv mh jx jy jz mi kb kc kd mj kf kg kh ig bi translated">现在为了比较苹果和苹果(或者糖块和糖块)。我们需要将数据标准化。同样，我们不能将一袋家庭大小的彩虹糖与一根有趣大小的巧克力棒相比较，我们需要用一些东西来衡量它们(在我们的糖果案例中，我们可能评估相同的卡路里数，在我们的案例中，我们使用标准的正常值)。</p><h2 id="d718" class="lm ln in bd lo lp lq dn lr ls lt dp lu jv lv lw lx jz ly lz ma kd mb mc md me bi translated">相关矩阵</h2><p id="7204" class="pw-post-body-paragraph jk jl in jm b jn mf jp jq jr mg jt ju jv mh jx jy jz mi kb kc kd mj kf kg kh ig bi translated">现在我们可以计算相关矩阵(标准正态分布的协方差矩阵)，它告诉我们不同的维度是如何相互关联的。我们可以看到对角线都是 1，这是有意义的，因为维度和它自身有 1 对 1 的关联。</p><h2 id="8d31" class="lm ln in bd lo lp lq dn lr ls lt dp lu jv lv lw lx jz ly lz ma kd mb mc md me bi translated"><strong class="ak">计算特征向量和值</strong></h2><p id="7d30" class="pw-post-body-paragraph jk jl in jm b jn mf jp jq jr mg jt ju jv mh jx jy jz mi kb kc kd mj kf kg kh ig bi translated">相关矩阵允许我们获得特征向量和值，然后根据值对它们进行排序，以获得按公国顺序排序的维度向量——首先是美味的糖果。</p><h2 id="647d" class="lm ln in bd lo lp lq dn lr ls lt dp lu jv lv lw lx jz ly lz ma kd mb mc md me bi translated">挑选 M 个最大特征值的向量</h2><p id="c86f" class="pw-post-body-paragraph jk jl in jm b jn mf jp jq jr mg jt ju jv mh jx jy jz mi kb kc kd mj kf kg kh ig bi translated">然后，我们挑选与最大的两个(M)特征向量相对应的两列，以获得一个 3x2 矩阵，然后将其乘以 2x3 转置，以获得一个 3x3 矩阵，我们可以用它来转换我们的原始 3 维数据。</p><p id="a773" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">最后，我们将显示我们压缩的二维数据和构成这些点的单位向量。我们还在新的空间中显示了这两个主向量对应的 3D 单位向量。</p><p id="d2d2" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">好了，我们已经使用 PCA 成功地压缩了数据，从 N =&gt; M 维开始，我们节省的空间是(N-M)/N %,或者在我们的例子中是(3–2)/3 = &gt; 33%！请注意，这是一种有损压缩，一旦我们将原始 3D 数据压缩到这个 2D 平面，我们就无法恢复原始 3D 数据。</p></div></div>    
</body>
</html>