<html>
<head>
<title>Creating a Randomly Sampled Working Data in Spark and Python from Original Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从原始数据集在 Spark 和 Python 中创建随机采样的工作数据</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/creating-a-randomly-sampled-working-data-in-spark-sql-and-python-from-original-dataset-7b8251027700?source=collection_archive---------5-----------------------#2022-01-03">https://blog.devgenius.io/creating-a-randomly-sampled-working-data-in-spark-sql-and-python-from-original-dataset-7b8251027700?source=collection_archive---------5-----------------------#2022-01-03</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><p id="0b32" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">了解如何从大型数据集创建具有统计显著性的样本，以加速迭代开发，消除 ML 训练中的偏差等。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1f72970550a995c1d955bc5ed742bf78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AoFD5pgFQwkzqUEyiBee_Q.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated"><a href="’&lt;a" class="ae ky" rel="noopener ugc nofollow" target="_blank">https://www.freepik.com/photos/food'</a>T13】freepik 创作的美食照片—<a class="ae ky" href="http://www.freepik.com" rel="noopener ugc nofollow" target="_blank">www.freepik.com</a>T14】/a&gt;</figcaption></figure><h1 id="d2bc" class="kz la in bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">为什么是 PySpark</h1><p id="c174" class="pw-post-body-paragraph jk jl in jm b jn lx jp jq jr ly jt ju jv lz jx jy jz ma kb kc kd mb kf kg kh ig bi mc translated">使用 python 停放已经成为数据工程系统的一种非常流行的方法。它结合了所有东西的精华:Spark 用于更快的内存处理(当然，假设你有内存)；以及有用的 python 库的普遍存在和精通该语言的开发人员的可用性。将它与 SQL 结合起来，SQL 是数据处理的事实语言，如分析、清理、争论等等，你就有了一个像样的数据工程系统。</p><h1 id="eb1b" class="kz la in bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">大数据集的挑战</h1><p id="3330" class="pw-post-body-paragraph jk jl in jm b jn lx jp jq jr ly jt ju jv lz jx jy jz ma kb kc kd mb kf kg kh ig bi translated">在开发应用程序时，您经常需要使用组织中的真实数据集；不是虚构的或者不相关的，你在学习时会用到的。但是有一个小问题:真实的数据集通常非常大，当你编写程序时，你想要一个非常小的子集来加快开发过程。也许您正在一个内存更少的更小的服务器、一个云容器，甚至是内存非常有限的笔记本电脑上开发它。一个完整的数据集可能会导致内存不足的错误，尤其是在执行像<em class="ml"> collect </em>()这样的操作时，或者至少会减慢您的迭代代码开发。</p><p id="e827" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">你能做什么？您可以使用简单的 Unix 工具(如<code class="fe mm mn mo mp b">head</code>)获取实际数据文件的一部分；但这将使你的数据可疑地失真。文件的前<em class="ml"> n </em>行可能属于一个客户；因此，当您对文件进行操作以执行识别分布模式或训练 ML 模型等任务时，它将导致糟糕的开发、无效的模型并掩盖任何潜在的错误。您需要该文件的一个较小的子集；但是必须从原始文件中随机选择，即必须是<em class="ml">统计上</em>显著的样本。在本文中，您将学习如何从大文件中创建数据样本，以表示实际数据的特定百分比；而是随机选择的。您还将学习如何根据实际分布来影响该百分比，以进一步消除偏差。</p><h1 id="3082" class="kz la in bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">建立</h1><p id="2e53" class="pw-post-body-paragraph jk jl in jm b jn lx jp jq jr ly jt ju jv lz jx jy jz ma kb kc kd mb kf kg kh ig bi mc translated">你将需要一个实际的数据文件。出于本文的目的，让我们获得一个公开可用的真实数据集。它是美国所有州所有县的 COVID 和数据。你可以从<a class="ae ky" href="https://www.openicpsr.org/openicpsr/project/128303/version/V1/view?path=/openicpsr/128303/fcr:versions/V1/Live-Data/us-counties.txt&amp;type=file" rel="noopener ugc nofollow" target="_blank">这里</a>下载文件。这是数据集的引用:</p><blockquote class="mq mr ms"><p id="2570" class="jk jl ml jm b jn jo jp jq jr js jt ju mt jw jx jy mu ka kb kc mv ke kf kg kh ig bi translated">纽约时报。冠状病毒(新冠肺炎)在美国的数据:现场数据:us counties.txt. Ann Arbor，MI:大学间政治和社会研究联合会[经销商]，2020–12–08。<a class="ae ky" href="https://doi.org/10.3886/E128303V1-105700" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.3886/E128303V1-105700</a></p></blockquote><p id="ba86" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">下载文件并将其保存在您要处理的文件夹中。在这一点上，我假设你有；</p><ul class=""><li id="be5a" class="mw mx in jm b jn jo jr js jv my jz mz kd na kh nb nc nd ne bi translated">正在运行的 Spark 集群(甚至是笔记本电脑上的本地 Spark 集群)。我计划稍后再写一篇关于如何安装和使用本地 Spark 安装的文章。</li><li id="378c" class="mw mx in jm b jn nf jr ng jv nh jz ni kd nj kh nb nc nd ne bi translated">您已经安装了 PySpark 包。</li></ul><p id="05ca" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">要确保安装良好，请在命令行中输入以下内容:</p><pre class="kj kk kl km gt nk mp nl nm aw nn bi"><span id="8b5c" class="no la in mp b gy np nq l nr ns"><strong class="mp io">pyspark</strong></span></pre><p id="fb31" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">您应该会看到类似这样的屏幕(当然，您的 Spark 和 Python 版本可能会有所不同)</p><pre class="kj kk kl km gt nk mp nl nm aw nn bi"><span id="f4d1" class="no la in mp b gy np nq l nr ns">Python 3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:04:45) [MSC v.1900 32 bit (Intel)] on win32</span><span id="03e9" class="no la in mp b gy nt nq l nr ns">Type “help”, “copyright”, “credits” or “license” for more information.</span><span id="c520" class="no la in mp b gy nt nq l nr ns">Using Spark’s default log4j profile: org/apache/spark/log4j-defaults.properties</span><span id="2315" class="no la in mp b gy nt nq l nr ns">Setting default log level to “WARN”.</span><span id="0999" class="no la in mp b gy nt nq l nr ns">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).<br/>Welcome to<br/>      ____              __<br/>     / __/__  ___ _____/ /__<br/>    _\ \/ _ \/ _ `/ __/  '_/<br/>   /__ / .__/\_,_/_/ /_/\_\   version 2.4.5<br/>      /_/</span><span id="7a24" class="no la in mp b gy nt nq l nr ns">Using Python version 3.6.4 (v3.6.4:d48eceb, Dec 19 2017 06:04:45)<br/>SparkSession available as ‘spark’.<br/>&gt;&gt;&gt;</span></pre><p id="67d9" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">屏幕将等待熟悉的 python 提示符(" &gt; &gt; &gt; ")。正如它在屏幕上显示的那样，已经有了我们需要的一组变量。</p><p id="36fd" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io"> sc </strong>:这是 Spark 上下文。我们不需要使用这个</p><p id="fbea" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io"> spark </strong>:这是 Spark SQL 会话。这将被大量使用。如果在上面的输出中没有看到这一点，可以通过执行以下命令在 PySpark 实例中创建它</p><pre class="kj kk kl km gt nk mp nl nm aw nn bi"><span id="5a75" class="no la in mp b gy np nq l nr ns"><strong class="mp io">from pyspark.sql import *<br/>spark = SparkSession.builder.appName(‘Arup’).getOrCreate()</strong></span></pre><p id="91ba" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">就是这样。让我们开始讨论今天目标的实质内容。</p><h1 id="7431" class="kz la in bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">读取数据</h1><p id="5a72" class="pw-post-body-paragraph jk jl in jm b jn lx jp jq jr ly jt ju jv lz jx jy jz ma kb kc kd mb kf kg kh ig bi mc translated">首先，我们需要从下载的 CSV 文件中创建一个数据帧。检查文件，我们看到它有一个头，这使我们很容易映射到数据帧的列。它负责列的名称；但是数据类型呢？我们可以提供给他们。但是我们也可以使用一个称为模式推理的巧妙技巧，Spark 将读取几行内容，并尝试从实际数据中猜测数据类型。下面是我们将文件读入名为<code class="fe mm mn mo mp b">df</code>的数据帧的代码。我们指定文件有一个标题(它应该提供列名),并且应该推断出模式。</p><pre class="kj kk kl km gt nk mp nl nm aw nn bi"><span id="7b6b" class="no la in mp b gy np nq l nr ns">&gt;&gt;&gt; <strong class="mp io">df = spark.read.options(header=True, inferSchema=True).csv(“us-counties.txt”)</strong></span></pre><p id="fbfb" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">让我们确认我们得到了所有的记录:</p><pre class="kj kk kl km gt nk mp nl nm aw nn bi"><span id="8930" class="no la in mp b gy np nq l nr ns">&gt;&gt;&gt; <strong class="mp io">df.count()</strong><br/>758243</span></pre><p id="14ba" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">它是否导入了所有列的名称并推断出了数据类型？我们可以检查数据帧的模式。</p><pre class="kj kk kl km gt nk mp nl nm aw nn bi"><span id="ec39" class="no la in mp b gy np nq l nr ns">&gt;&gt;&gt; <strong class="mp io">df.printSchema()</strong></span><span id="2cd6" class="no la in mp b gy nt nq l nr ns">root<br/>| — date: timestamp (nullable = true)<br/>| — county: string (nullable = true)<br/>| — state: string (nullable = true)<br/>| — fips: integer (nullable = true)<br/>| — cases: integer (nullable = true)<br/>| — deaths: integer (nullable = true)</span></pre><p id="519c" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">目前看来不错。让我们检查一个记录:</p><pre class="kj kk kl km gt nk mp nl nm aw nn bi"><span id="6e13" class="no la in mp b gy np nq l nr ns">&gt;&gt;&gt; <strong class="mp io">df.first()</strong></span><span id="b0b8" class="no la in mp b gy nt nq l nr ns">Row(date=datetime.datetime(2020, 1, 21, 0, 0), county=’Snohomish’, state=’Washington’, fips=53061, cases=1, deaths=0)</span></pre><p id="2d8e" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">在导入过程中，一切看起来都很好。</p><h1 id="0091" class="kz la in bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">固定抽样</h1><p id="836c" class="pw-post-body-paragraph jk jl in jm b jn lx jp jq jr ly jt ju jv lz jx jy jz ma kb kc kd mb kf kg kh ig bi mc translated"><span class="l md me mf bm mg mh mi mj mk di"> N </span>如何从这个数据帧创建一个样本。Spark 提供了一个名为<code class="fe mm mn mo mp b">sample()</code>的函数，它接受一个参数——要采样的总数据的百分比。让我们使用它。现在，让我们使用 0.001%，或 0.00001 作为采样比率。此外，由于我们只是在试验，我们不会保存到另一个数据帧；我们将检查结果。</p><pre class="kj kk kl km gt nk mp nl nm aw nn bi"><span id="8d0f" class="no la in mp b gy np nq l nr ns">&gt;&gt;&gt; <strong class="mp io">df.sample(.00001).show()</strong></span><span id="5372" class="no la in mp b gy nt nq l nr ns">+-------------------+----------+--------------+-----+-----+------+<br/>|               date|    county|         state| fips|cases|deaths|<br/>+-------------------+----------+--------------+-----+-----+------+<br/>|2020-04-13 00:00:00|       Bay|      Michigan|26017|   59|     2|<br/>|2020-04-27 00:00:00|   Lubbock|         Texas|48303|  504|    43|<br/>|2020-05-12 00:00:00|    Kimble|         Texas|48267|    1|     0|<br/>|2020-05-24 00:00:00|    Thomas|      Nebraska|31171|    1|     0|<br/>|2020-06-21 00:00:00|     Cedar|      Nebraska|31027|    9|     0|<br/>|2020-07-02 00:00:00|Los Alamos|    New Mexico|35028|    8|     0|<br/>|2020-07-13 00:00:00|  Montrose|      Colorado| 8085|  218|    12|<br/>|2020-09-13 00:00:00|   Mathews|      Virginia|51115|   23|     0|<br/>|2020-11-02 00:00:00|     Surry|North Carolina|37171| 2009|    33|<br/>+-------------------+----------+--------------+-----+-----+------+</span></pre><p id="b6a8" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我们得到了 9 条记录，大约是文件中 758243 条记录的 0.001%。让我们再运行一次这个命令:</p><pre class="kj kk kl km gt nk mp nl nm aw nn bi"><span id="a3b7" class="no la in mp b gy np nq l nr ns">&gt;&gt;&gt; <strong class="mp io">df.sample(.00001).show()</strong></span><span id="b68a" class="no la in mp b gy nt nq l nr ns">+-------------------+-------------------+-------------+-----+-----+------+<br/>|               date|             county|        state| fips|cases|deaths|<br/>+-------------------+-------------------+-------------+-----+-----+------+</span><span id="ca73" class="no la in mp b gy nt nq l nr ns">|2020-03-30 00:00:00|              Vilas|    Wisconsin|55125|    3|     0|<br/>|2020-04-20 00:00:00|             Coffee|      Alabama| 1031|   64|     0|<br/>|2020-05-16 00:00:00|               Clay|West Virginia|54015|    2|     0|<br/>|2020-06-12 00:00:00|          Christian|     Missouri|29043|   29|     0|<br/>|2020-07-22 00:00:00|               Wood|West Virginia|54107|  216|     3|<br/>|2020-07-28 00:00:00|            El Paso|        Texas|48141|13552|   239|<br/>|2020-08-16 00:00:00|             Zapata|        Texas|48505|  203|     3|<br/>|2020-09-11 00:00:00|            Chester| Pennsylvania|42029| 6187|   362|<br/>|2020-09-30 00:00:00|        Roger Mills|     Oklahoma|40129|   59|     1|<br/>|2020-10-16 00:00:00|           McLennan|        Texas|48309| 9290|   134|<br/>|2020-11-02 00:00:00|Virginia Beach city|     Virginia|51810| 8248|   107|<br/>+-------------------+-------------------+-------------+-----+-----+------+</span></pre><p id="8d51" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我们得到了 11 个记录，也有不同的记录，这是意料之中的。抽样是随机的；因此每次提取都会选择不同的记录。让我们再试一次:</p><pre class="kj kk kl km gt nk mp nl nm aw nn bi"><span id="93af" class="no la in mp b gy np nq l nr ns">&gt;&gt;&gt; <strong class="mp io">df.sample(.00001).show()</strong></span><span id="cb70" class="no la in mp b gy nt nq l nr ns">+ — — — — — — — — — -+ — — — — -+ — — — — — — + — — -+ — — -+ — — — +<br/>| date              | county  | state  | fips|cases|deaths|<br/>+ — — — — — — — — — -+ — — — — -+ — — — — — — + — — -+ — — -+ — |2020–07–02 00:00:00| Paulding| Georgia       |13223|  638|  16|<br/>|2020–07–17 00:00:00| Doña Ana| New Mexico    |35013| 1656|  12|<br/>|2020–08–22 00:00:00|Mendocino| California    | 6045|  624|  16|<br/>|2020–09–01 00:00:00| Brantley| Georgia       |13025|  310|   8|<br/>|2020–10–04 00:00:00|Lancaster| Pennsylvania  |42071| 8195| 459|<br/>|2020–11–21 00:00:00| Grant   | Kentucky      |21081|  540|   6|<br/>+ — — — — — — — — — -+ — — — — -+ — — — — — — + ----+ —---+ — — — +</span></pre><p id="da7f" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我们只拿到了 6 张唱片，而且与前两次不同。每次数字都不同的原因是因为采样百分比是近似值，而不是精确值。所以这些数字大约是原始记录数的 0.001%；但不完全一样。</p><h1 id="972e" class="kz la in bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">播种</h1><p id="79c9" class="pw-post-body-paragraph jk jl in jm b jn lx jp jq jr ly jt ju jv lz jx jy jz ma kb kc kd mb kf kg kh ig bi mc translated"><span class="l md me mf bm mg mh mi mj mk di"> T </span>他的每次提取不同记录和不同数量的特性(当然作为真正随机性的一个属性)对于许多开发案例和您可能需要的东西来说可能是很好的。然而，在其他情况下，你会想要一个样本；但是希望该样本对于每次调用都是可预测的，即该样本在第一次应该是随机的，但是不应该随着后续调用而改变。你如何实现这一点？</p><p id="2e34" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">为了再现相同的输出，幸运的是该函数带有一个可选参数 seed。这是你如何使用它。在这个例子中，我使用 9876543210 作为种子。</p><pre class="kj kk kl km gt nk mp nl nm aw nn bi"><span id="7018" class="no la in mp b gy np nq l nr ns">&gt;&gt;&gt; <strong class="mp io">df.sample(.00001,9876543210).show()</strong></span><span id="b617" class="no la in mp b gy nt nq l nr ns">+ — — — — — — — — — -+ — — — — — + — — — — — -+ — — -+ — — -+ — — — +<br/>| date| county| state| fips|cases|deaths|<br/>+ — — — — — — — — — -+ — — — — — + — — — — — -+ — — -+ — — -+ — — — +<br/>|2020–07–24 00:00:00| Delaware| Indiana|18035| 541| 53|<br/>|2020–08–04 00:00:00| Blaine| Idaho|16013| 571| 6|<br/>|2020–08–10 00:00:00| Auglaize| Ohio|39011| 256| 6|<br/>|2020–09–14 00:00:00|Deer Lodge| Montana|30023| 87| 0|<br/>|2020–09–28 00:00:00| Campbell| Virginia|51031| 453| 4|<br/>|2020–10–19 00:00:00| Clay|Mississippi|28025| 672| 21|<br/>+ — — — — — — — — — -+ — — — — — + — — — — — -+ — — -+ — — -+ — — — +</span></pre><p id="5d3f" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">下次运行它:</p><pre class="kj kk kl km gt nk mp nl nm aw nn bi"><span id="c15c" class="no la in mp b gy np nq l nr ns">&gt;&gt;&gt; <strong class="mp io">df.sample(.00001,9876543210).show()</strong></span><span id="d3b8" class="no la in mp b gy nt nq l nr ns">+ — — — — — — — — — -+ — — — — — + — — — — — -+ — — -+ — — -+ — — — +<br/>| date| county| state| fips|cases|deaths|<br/>+ — — — — — — — — — -+ — — — — — + — — — — — -+ — — -+ — — -+ — — — +<br/>|2020–07–24 00:00:00| Delaware| Indiana|18035| 541| 53|<br/>|2020–08–04 00:00:00| Blaine| Idaho|16013| 571| 6|<br/>|2020–08–10 00:00:00| Auglaize| Ohio|39011| 256| 6|<br/>|2020–09–14 00:00:00|Deer Lodge| Montana|30023| 87| 0|<br/>|2020–09–28 00:00:00| Campbell| Virginia|51031| 453| 4|<br/>|2020–10–19 00:00:00| Clay|Mississippi|28025| 672| 21|<br/>+ — — — — — — — — — -+ — — — — — + — — — — — -+ — — -+ — — -+ — — — +</span></pre><p id="4bfd" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">每次都是同样的样本。如果你想要不同的样本呢？简单地使用不同的种子，或者完全省略种子。</p><pre class="kj kk kl km gt nk mp nl nm aw nn bi"><span id="2d42" class="no la in mp b gy np nq l nr ns">&gt;&gt;&gt; <strong class="mp io">df.sample(.00001,8765432109).show()</strong></span><span id="331c" class="no la in mp b gy nt nq l nr ns">+ — — — — — — — — — -+ — — — — + — — — — — — + — — -+ — — -+ — — — +<br/>| date              | county | state      | fips|cases|deaths|<br/>+ — — — — — — — — — -+ — — — — + — — — — — — + — — -+ — — -+ — — — +<br/>|2020–03–27 00:00:00|Franklin| Washington |53021| 12| 0|<br/>|2020–05–08 00:00:00| Pasco  | Florida    |12101| 291| 9|<br/>|2020–05–10 00:00:00|Mariposa| California | 6043| 15| 0|<br/>|2020–06–28 00:00:00| Gregg  | Texas      |48183| 340| 14|<br/>|2020–09–02 00:00:00| Benson |North Dakota|38005| 239| 4|<br/>|2020–10–24 00:00:00| Steuben| New York   |36101| 951| 69|<br/>+ — — — — — — — — — -+ — — — — + — — — — — — + — — -+ — — -+ — — — +</span></pre><h1 id="8eb7" class="kz la in bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">允许重复吗？</h1><p id="de2a" class="pw-post-body-paragraph jk jl in jm b jn lx jp jq jr ly jt ju jv lz jx jy jz ma kb kc kd mb kf kg kh ig bi mc translated">采样时，当函数遇到之前已经选取的值时，它会抑制重复的值。你可能无法接受。如果数据分布是单个值，例如“纽约”碰巧出现了很多次，那么它很可能会出现很多次。但是，由于 sample()取消了它之前选取的值，因此在采样过程中会人为地取消选取该值。该函数有另一个参数来实现这一点。</p><p id="a146" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">为了允许可能的重复值，您可以将第一个参数<em class="ml">替换为默认为 False 的</em>。通过将设置为 true，可以获得可能的重复值。这里有一个例子。我用 1 作为种子，这样我每次都能得到相同的样本。</p><pre class="kj kk kl km gt nk mp nl nm aw nn bi"><span id="f3bf" class="no la in mp b gy np nq l nr ns">&gt;&gt;&gt; <strong class="mp io">df.select(“state”).sample(True, .00001, seed=1).show()</strong></span><span id="1d94" class="no la in mp b gy nt nq l nr ns">+ — — — — — +<br/>| state     |<br/>+ — — — — — +<br/>| Wisconsin |<br/>| Vermont   |<br/>| Texas     |<br/>|Washington |<br/>| Georgia   |<br/>|New Mexico |<br/>| Tennessee |<br/>| Louisiana |<br/>| Hawaii    |<br/>| Georgia   |<br/>| Wisconsin |<br/>| Wyoming   |<br/>+ — — — — — +</span></pre><p id="cf3c" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">您可以看到输出中有一些条目，例如“Wisconsin”重复出现。该函数仅仅表示它在采样过程中选取的内容；它没有拒绝之前摘的。现在让我们重新发出同样的命令，将参数设置为 False，或者完全省略参数，因为 False 是默认值。</p><pre class="kj kk kl km gt nk mp nl nm aw nn bi"><span id="0f2b" class="no la in mp b gy np nq l nr ns">&gt;&gt;&gt; <strong class="mp io">df.select(“state”).sample(False, .00001, seed=1).show()</strong></span><span id="f7db" class="no la in mp b gy nt nq l nr ns">+ — — — — — +<br/>| state     |<br/>+ — — — — — +<br/>| Illinois  |<br/>| Georgia   |<br/>|Washington |<br/>| Virginia  |<br/>| Minnesota |<br/>+ — — — — — +</span></pre><p id="0fbf" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">如你所见，这些条目都是独一无二的；不重复。</p><h1 id="ac14" class="kz la in bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">减少采样中的偏斜</h1><p id="91ce" class="pw-post-body-paragraph jk jl in jm b jn lx jp jq jr ly jt ju jv lz jx jy jz ma kb kc kd mb kf kg kh ig bi mc translated">现在你明白了抽样是如何工作的，你可能意识到我使用的抽样百分比(0.001%)将适用于所有情况。这意味着出现频率较高的值将被更频繁地选取。如果那是你想要的，你不需要再进一步；但是有时您可能希望减少经常重复的数据元素的偏斜效应。请允许我用一个例子来解释。</p><p id="1b86" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">首先，让我们获得每个州的记录数:</p><pre class="kj kk kl km gt nk mp nl nm aw nn bi"><span id="9f58" class="no la in mp b gy np nq l nr ns">&gt;&gt;&gt; <strong class="mp io">from pyspark.sql.functions import desc</strong></span><span id="c81e" class="no la in mp b gy nt nq l nr ns">&gt;&gt;&gt; <strong class="mp io">df.groupBy(“state”).count().alias(“cnt”).sort(desc(“cnt.count”)).show()</strong></span><span id="c6e4" class="no la in mp b gy nt nq l nr ns">+ — — — — — — — — — — + — — -+<br/>| state|count|<br/>+ — — — — — — — — — — + — — -+<br/>| Texas|57096|<br/>| Georgia|38969|<br/>| Virginia|31699|<br/>| Kentucky|28132|<br/>| Missouri|26770|<br/>| Illinois|24258|<br/>| North Carolina|24108|</span><span id="51a7" class="no la in mp b gy nt nq l nr ns"><em class="ml">… output truncated for brevity …</em></span><span id="1fd2" class="no la in mp b gy nt nq l nr ns">| Massachusetts| 3859|<br/>| Arizona| 3813|<br/>| Vermont| 3703|<br/>| Nevada| 3678|<br/>| New Hampshire| 2705|<br/>| Connecticut| 2258|<br/>| Rhode Island| 1482|<br/>| Hawaii| 1050|<br/>| Delaware| 983|<br/>| Virgin Islands| 824|<br/>|Northern Mariana …| 410|<br/>|District of Columbia| 261|<br/>| Guam| 253|<br/>+ — — — — — — — — — — + — — -+</span></pre><p id="81cf" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">如您所见，一些州有大量记录，例如德克萨斯州，而关岛有少量病例。如果你以固定的 0.001%的比率抽样，关岛出现的机会很小；事实上，它甚至可能不会出现。同样的，高发州像德州，佐治亚等。会出现更多。这可能会导致 ML 模型的测试结果或训练结果不正确。</p><p id="8277" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">为了解决这个问题，您可能希望创建一个有条纹的抽样率，而不是对所有记录应用一个单一的抽样百分比，您希望根据州对不同的记录应用不同的抽样百分比。为了简单起见，让我们这样计划:</p><ul class=""><li id="26dc" class="mw mx in jm b jn jo jr js jv my jz mz kd na kh nb nc nd ne bi translated">对于拥有 16，000 条或更多记录的州，税率为 0.001%</li><li id="59a9" class="mw mx in jm b jn nf jr ng jv nh jz ni kd nj kh nb nc nd ne bi translated">对于记录少于 16，000 条的州，税率为 0.01%。</li></ul><p id="09dc" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">幸运的是，Spark 提供了另一个函数 sampleBy()允许我们这样做。这个函数有两个参数:</p><ul class=""><li id="a26b" class="mw mx in jm b jn jo jr js jv my jz mz kd na kh nb nc nd ne bi translated"><em class="ml">列名</em>:将被检查得出抽样百分比的列。在这种情况下，它将是“状态”</li><li id="5ef7" class="mw mx in jm b jn nf jr ng jv nh jz ni kd nj kh nb nc nd ne bi translated"><em class="ml">分数</em>:dict 格式的那个列值将应用什么分数。例如，如果设置为:</li></ul><pre class="kj kk kl km gt nk mp nl nm aw nn bi"><span id="7c0d" class="no la in mp b gy np nq l nr ns">{<br/>  ‘Texas’ : 0.00001,<br/>  ‘Guam’ : 0.0001,<br/>   … and so on …<br/>}</span></pre><p id="9dd7" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">然后，将根据“状态”列中的值应用采样率。</p><p id="e31f" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">还有第三个参数——<em class="ml">seed</em>——就像前面的<em class="ml"> sample </em>()函数一样。它是可选的，就像那种情况一样。</p><p id="9e6f" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">但是，不可能为 fractions 参数手动构造 dict 对象。手动完成的工作量相当大。此外，分布可能会改变，我们应该自动化它以与实际的数据模式保持一致。我们是这样做的。首先，让我们将计数放入一个新的数据帧:</p><pre class="kj kk kl km gt nk mp nl nm aw nn bi"><span id="f02d" class="no la in mp b gy np nq l nr ns"><strong class="mp io">df1 = df.groupBy(“state”).count().alias(“cnt”).sort(desc(“cnt.count”))</strong></span></pre><p id="80aa" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我们将使用一个名为 lit()的函数向数据帧添加文字。同时，让我们也导入一些我们需要的其他函数。</p><pre class="kj kk kl km gt nk mp nl nm aw nn bi"><span id="76d5" class="no la in mp b gy np nq l nr ns"><strong class="mp io">from pyspark.sql.functions import lit, when, col</strong></span></pre><p id="3bd0" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">使用 lit()我们将把采样率加到相应的状态上。我们将把它存储在另一个数据帧中。采样比率存储在名为“stratum”的列中。</p><pre class="kj kk kl km gt nk mp nl nm aw nn bi"><span id="04c8" class="no la in mp b gy np nq l nr ns"><strong class="mp io">df2 = df1.withColumn("stratum",<br/>when (col(“count”) &gt; (16000), lit(0.00001))<br/>     .otherwise(lit(0.0001))<br/>)</strong></span></pre><p id="f4ec" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">现在，我们将创建一个名为 fractions 的 dict 对象来创建我们需要的状态到采样率的映射:</p><pre class="kj kk kl km gt nk mp nl nm aw nn bi"><span id="d311" class="no la in mp b gy np nq l nr ns">fractions = df2.select(“state”,”stratum”).rdd.collectAsMap()</span></pre><p id="e3ad" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">让我们来看看 dict 变量:</p><pre class="kj kk kl km gt nk mp nl nm aw nn bi"><span id="2033" class="no la in mp b gy np nq l nr ns">&gt;&gt;&gt; <strong class="mp io">print(fractions)</strong></span><span id="040f" class="no la in mp b gy nt nq l nr ns">{‘Texas’: 1e-05, ‘Georgia’: 1e-05, ‘Virginia’: 1e-05, ‘Kentucky’: 1e-05, ‘Missouri’: 1e-05, ‘Illinois’: 1e-05, ‘North Carolina’: 1e-05, ‘Iowa’: 1e-05, ‘Tennessee’: 1e-05, ‘Kansas’: 1e-05, ‘Indiana’: 1e-05, ‘Ohio’: 1e-05, ‘Minnesota’: 1e-05, ‘Michigan’: 1e-05, ‘Mississippi’: 1e-05, ‘Nebraska’: 1e-05, ‘Arkansas’: 1e-05, ‘Oklahoma’: 1e-05, ‘Wisconsin’: 1e-05, ‘Florida’: 1e-05, ‘Pennsylvania’: 1e-05, ‘Alabama’: 1e-05, ‘Louisiana’: 0.0001, ‘Puerto Rico’: 0.0001, ‘Colorado’: 0.0001, ‘New York’: 0.0001, ‘California’: 0.0001, ‘South Dakota’: 0.0001, ‘West Virginia’: 0.0001, ‘North Dakota’: 0.0001, ‘South Carolina’: 0.0001, ‘Montana’: 0.0001, ‘Washington’: 0.0001, ‘Idaho’: 0.0001, ‘Oregon’: 0.0001, ‘New Mexico’: 0.0001, ‘Utah’: 0.0001, ‘Maryland’: 0.0001, ‘New Jersey’: 0.0001, ‘Alaska’: 0.0001, ‘Wyoming’: 0.0001, ‘Maine’: 0.0001, ‘Massachusetts’: 0.0001, ‘Arizona’: 0.0001, ‘Vermont’: 0.0001, ‘Nevada’: 0.0001, ‘New Hampshire’: 0.0001, ‘Connecticut’: 0.0001, ‘Rhode Island’: 0.0001, ‘Hawaii’: 0.0001, ‘Delaware’: 0.0001, ‘Virgin Islands’: 0.0001, ‘Northern Mariana Islands’: 0.0001, ‘District of Columbia’: 0.0001, ‘Guam’: 0.0001}</span></pre><p id="3d0d" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">这正是我们需要传递给<em class="ml"> sampleBy </em>()函数的内容。以上可能不是非常可读。可以用下面的来轻松阅读。</p><pre class="kj kk kl km gt nk mp nl nm aw nn bi"><span id="0c26" class="no la in mp b gy np nq l nr ns">&gt;&gt;&gt; <strong class="mp io">for i in fractions:</strong><br/>    … <strong class="mp io">print(i, fractions[i])</strong><br/>    …</span><span id="2280" class="no la in mp b gy nt nq l nr ns">Washington 0.0001<br/>Illinois 1e-05<br/>California 1e-05<br/>Arizona 0.0001<br/>Massachusetts 0.0001<br/><em class="ml">… output truncated …</em></span></pre><p id="1441" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">现在，让我们对数据集进行采样:</p><pre class="kj kk kl km gt nk mp nl nm aw nn bi"><span id="723d" class="no la in mp b gy np nq l nr ns"><strong class="mp io">sampleDF = df.sampleBy("state",fractions,1).show(1000)<br/>sampleDF.show(1000)</strong></span><span id="b7fa" class="no la in mp b gy nt nq l nr ns">+-------------------+--------------------+-------------+-----+-----+------+<br/>|               date|              county|        state| fips|cases|deaths|<br/>+-------------------+--------------------+-------------+-----+-----+------+<br/>|2020-03-24 00:00:00|                Yuma|      Arizona| 4027|    2|     0|<br/>|2020-03-25 00:00:00|             Webster|    Louisiana|22119|    5|     1|<br/>|2020-04-11 00:00:00|             Orleans|    Louisiana|22071| 5535|   232|<br/>|2020-05-09 00:00:00|         Musselshell|      Montana|30065|    1|     0|<br/>|2020-05-10 00:00:00|              Vernon|    Louisiana|22115|   18|     2|<br/>|2020-05-20 00:00:00|               Glenn|   California| 6021|   12|     0|<br/>|2020-05-23 00:00:00|         Carson City|       Nevada|32510|   83|     4|<br/>|2020-06-01 00:00:00|Dillingham Census...|       Alaska| 2070|    1|     0|</span><span id="3e4e" class="no la in mp b gy nt nq l nr ns"><em class="ml">… output truncated …</em></span></pre><p id="1d24" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">仅此而已；您的<em class="ml">工作</em>数据集是<strong class="jm io"> sampleDF </strong>，它已经根据实际数据分布使用不同的采样率进行了仔细采样，这将减少偏斜效应。这只是一个简单的例子，说明如何应用两个比率。您可以修改创建 df2 数据帧的代码，根据需要添加任意多的比率。最终目标是创建字典对象，并将其传递给<em class="ml"> sampleBy </em>()函数。</p><h1 id="cb2a" class="kz la in bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">外卖食品</h1><p id="e286" class="pw-post-body-paragraph jk jl in jm b jn lx jp jq jr ly jt ju jv lz jx jy jz ma kb kc kd mb kf kg kh ig bi translated">下面是这篇文章的简要概述。</p><ol class=""><li id="f9af" class="mw mx in jm b jn jo jr js jv my jz mz kd na kh nu nc nd ne bi translated">Spark 提供了一个名为<em class="ml"> sample </em>()的函数，它从原始文件中抽取一个随机的数据样本。所有记录的采样率都是固定的。由于这是均匀随机的、频繁出现的值，因此会更频繁地出现在样本中，从而扭曲数据。</li><li id="59da" class="mw mx in jm b jn nf jr ng jv nh jz ni kd nj kh nu nc nd ne bi translated">Spark 提供了另一个名为<em class="ml"> sampleBy </em>()的函数，该函数也提取随机样本；但是采样率可以依赖于文件的内容。这使您可以减少结果样本中常用值的出现，从而减少偏差。</li></ol><h1 id="0a87" class="kz la in bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">进一步阅读</h1><p id="436b" class="pw-post-body-paragraph jk jl in jm b jn lx jp jq jr ly jt ju jv lz jx jy jz ma kb kc kd mb kf kg kh ig bi translated">你可以在 Apache Spark 官方文档页面<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameStatFunctions.sampleBy.html" rel="noopener ugc nofollow" target="_blank">https://Spark . Apache . org/docs/latest/API/python/reference/API/py Spark . SQL . data frame statfunctions . sample by . html</a>阅读更多关于<em class="ml"> sampleBy </em>()函数的信息。然而，我必须警告，文档非常稀少。但是父文档页面显示了本文中显示的许多命令的扩展语法，这可能会有所帮助。</p></div></div>    
</body>
</html>