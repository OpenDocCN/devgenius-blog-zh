<html>
<head>
<title>Extract and Load Football Statistics to Google Cloud Storage &amp; BigQuery with Airflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">提取足球统计数据并加载到 Google 云存储&amp; BigQuery with Airflow</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/extract-and-load-football-statistics-to-google-cloud-storage-bigquery-with-airflow-1a217227dbd1?source=collection_archive---------6-----------------------#2022-12-05">https://blog.devgenius.io/extract-and-load-football-statistics-to-google-cloud-storage-bigquery-with-airflow-1a217227dbd1?source=collection_archive---------6-----------------------#2022-12-05</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><div class=""><h2 id="b0ef" class="pw-subtitle-paragraph jk im in bd b jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb dk translated"><em class="kc">利用气流自动提取和装载管道的演示</em></h2></div><figure class="ke kf kg kh gt ki gh gi paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="gh gi kd"><img src="../Images/0c61bd9d120209526a9a6382c9cbe7ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IZMi5wQt01RY04IffpkxPg.jpeg"/></div></div><figcaption class="kp kq gj gh gi kr ks bd b be z dk translated">图片来自马修·施瓦特的 Unsplash</figcaption></figure><p id="5318" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">如果你碰巧看到这篇文章，那么我想你目前正在学习或从事一个涉及 Apache Airflow 的项目。<strong class="kv io"> Apache Airflow </strong>是最流行的开源工作流编排工具。它将您的代码转换成一个您可以计划、运行和观察的工作流。在数据领域，许多数据团队使用这个工具来<strong class="kv io">自动化他们的 ETL 管道</strong>(提取、传输和加载)，例如在数据库、数据湖和数据仓库之间复制或传输数据。</p><h2 id="9b15" class="lp lq in bd lr ls lt dn lu lv lw dp lx lc ly lz ma lg mb mc md lk me mf mg mh bi translated">目标</h2><p id="fe37" class="pw-post-body-paragraph kt ku in kv b kw mi jo ky kz mj jr lb lc mk le lf lg ml li lj lk mm lm ln lo ig bi translated">在这篇文章中，我将展示如何从外部数据源提取数据并加载到 Google 云存储(GCS)和 BigQuery (BQ)中。为此，我将使用 Astro CLI。Astro CLI 是一个开源的 CLI 工具，用于 Apache Airflow 的数据编排。它为构建、测试和部署气流 Dag 和任务提供了一个自包含的本地开发环境。关于 Astro CLI 的更多信息，您可以访问这篇<a class="ae mn" href="https://www.astronomer.io/blog/astro-cli-the-easiest-way-to-install-apache-airflow/" rel="noopener ugc nofollow" target="_blank">博客文章</a>。</p><p id="1c5b" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">这篇文章不是关于气流的介绍性阅读。因此，我将不涉及诸如<em class="mo">“什么是气流”</em>或<em class="mo">“什么是 DAG”</em>之类的内容。为了了解气流、概念和功能，我强烈推荐你访问这个由天文学家撰写的关于气流的伟大的<a class="ae mn" href="https://docs.astronomer.io/learn" rel="noopener ugc nofollow" target="_blank">指南和教程</a>。</p><p id="fb1f" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated"><strong class="kv io">以下是工作流程概要:</strong></p><figure class="ke kf kg kh gt ki gh gi paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="gh gi mp"><img src="../Images/ce1ab7222e3d2bc3a7a1e29bed8c90a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AI-wtwPjCBVcEpLpPjx4Nw.png"/></div></div><figcaption class="kp kq gj gh gi kr ks bd b be z dk translated">按作者分类的图像:工作流摘要</figcaption></figure><h2 id="26c7" class="lp lq in bd lr ls lt dn lu lv lw dp lx lc ly lz ma lg mb mc md lk me mf mg mh bi translated"><strong class="ak">先决条件:</strong></h2><ol class=""><li id="b509" class="mq mr in kv b kw mi kz mj lc ms lg mt lk mu lo mv mw mx my bi translated">Astro CLI &amp; Docker。按照本指南安装 Astro CLI 。</li><li id="d8b2" class="mq mr in kv b kw mz kz na lc nb lg nc lk nd lo mv mw mx my bi translated">一个谷歌云帐户和项目来访问 GCS &amp; BigQuery。你可以在这里免费试用。</li><li id="3f23" class="mq mr in kv b kw mz kz na lc nb lg nc lk nd lo mv mw mx my bi translated">访问 GCS 和 BigQuery 的服务帐户权限(JSON)。<br/>选择你的项目，进入 IAM &amp;管理，点击 IAM 。点击您的服务帐户上的编辑主体图标，添加这些角色(除了<em class="mo">查看者的角色</em>)；<em class="mo">存储管理员、存储对象管理员和大查询管理员。</em>按照这些说明<a class="ae mn" href="https://cloud.google.com/iam/docs/creating-managing-service-account-keys" rel="noopener ugc nofollow" target="_blank">创建服务账户密钥</a>。</li></ol></div><div class="ab cl ne nf hr ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="ig ih ii ij ik"><h2 id="4547" class="lp lq in bd lr ls lt dn lu lv lw dp lx lc ly lz ma lg mb mc md lk me mf mg mh bi translated"><strong class="ak">数据的简要描述</strong>📝</h2><p id="927f" class="pw-post-body-paragraph kt ku in kv b kw mi jo ky kz mj jr lb lc mk le lf lg ml li lj lk mm lm ln lo ig bi translated">我要提取的数据是<a class="ae mn" href="https://datahub.io/" rel="noopener ugc nofollow" target="_blank"> datahub </a>上欧洲五大足球联赛的足球队统计数据。有 5 个联赛(英超、西甲、德甲、意甲、法甲)和 10 个赛季(2009/2010–2018/2019)的数据以 csv 格式存储。我将从数据源中提取 50 个 csv 文件<a class="ae mn" href="https://datahub.io/collections/football#football-datasets-on-datahub" rel="noopener ugc nofollow" target="_blank">。</a></p><figure class="ke kf kg kh gt ki gh gi paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="gh gi nl"><img src="../Images/ec16f0f4c9cd5e20b5f1ad0d5f938d6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3BSCSE_QrpKf4HDktf8qnQ.png"/></div></div><figcaption class="kp kq gj gh gi kr ks bd b be z dk translated">图片作者:数据集位置--&gt;<a class="ae mn" href="https://datahub.io/collections/football#football-datasets-on-datahub" rel="noopener ugc nofollow" target="_blank">https://data hub . io/collections/football # football-datasets-on-data hub</a></figcaption></figure><p id="6871" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">以下是数据集的简要介绍:</p><figure class="ke kf kg kh gt ki gh gi paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="gh gi nm"><img src="../Images/804a84922353a650e856e84b8ec20de1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BMqiIliP-Icb-ZJ43zKrIA.png"/></div></div><figcaption class="kp kq gj gh gi kr ks bd b be z dk translated">作者图片</figcaption></figure><p id="94d9" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">如上所示，从每个数据集中，我将只提取如上所示的 22 列，并添加 1 列作为季节号的标志(将在数据提取过程中生成)。以下是每列的简短描述和数据类型。</p><figure class="ke kf kg kh gt ki"><div class="bz fp l di"><div class="nn no l"/></div></figure></div><div class="ab cl ne nf hr ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="ig ih ii ij ik"><h2 id="260b" class="lp lq in bd lr ls lt dn lu lv lw dp lx lc ly lz ma lg mb mc md lk me mf mg mh bi translated">开始吧…🚀</h2><p id="b0f6" class="pw-post-body-paragraph kt ku in kv b kw mi jo ky kz mj jr lb lc mk le lf lg ml li lj lk mm lm ln lo ig bi translated">首先，我们需要在一个空的本地目录中初始化一个新的 Astro 项目。在 Astro CLI 中，我们用<code class="fe np nq nr ns b">astro dev init</code>命令创建一个新项目。</p><pre class="ke kf kg kh gt nt ns nu bn nv nw bi"><span id="8d52" class="nx lq in ns b be ny nz l oa ob">$ mkdir airflow_astrocli_demo &amp;&amp; cd airflow_astrocli_demo<br/>$ astro dev init</span></pre><p id="965c" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">它将生成一个基本的项目目录，如下所示。<code class="fe np nq nr ns b">dags</code>文件夹将包含您将要编写的工作流(Dag)。</p><pre class="ke kf kg kh gt nt ns nu bn nv nw bi"><span id="6f03" class="nx lq in ns b be ny nz l oa ob"><br/>├── dags<br/>│   ├── example_dag_advanced.py<br/>│   ├── example_dag_basic.py<br/>├── tests<br/>│   ├── dags<br/>│       ├── test_dag_integrity.py<br/>├── Dockerfile<br/>├── include<br/>├── packages.txt<br/>├── plugins<br/>└── requirements.txt</span></pre><p id="df3f" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">要在您的机器上启动本地版本的气流，请使用<code class="fe np nq nr ns b">astro dev start</code>。这将启动一个气流调度器、webserver 和 postgres(都运行在 docker 容器中)。你可以用<code class="fe np nq nr ns b">astro dev ps</code>进一步验证这一点。</p><p id="aab9" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">接下来，在您的<code class="fe np nq nr ns b">Dockerfile</code>中，您需要指定一些环境变量，Airflow 将使用这些变量来访问您的 GCS 和 BigQuery。</p><pre class="ke kf kg kh gt nt ns nu bn nv nw bi"><span id="6886" class="nx lq in ns b be ny nz l oa ob">FROM quay.io/astronomer/astro-runtime:6.0.3<br/><br/># Insert your gcp project id and gcs bucket name<br/>ENV GCP_PROJECT_ID='your_gcp_project_id'<br/>ENV GCP_GCS_BUCKET='your_gcs_bucket_name'<br/><br/># Create a dataset on your BQ, in my case I name it as european_football_leagues<br/>ENV BIGQUERY_DATASET='your_dataset_name'<br/><br/># Path to the location where you store your JSON service account key<br/>ENV GOOGLE_APPLICATION_CREDENTIALS=/usr/local/airflow/google_credentials.json<br/>ENV AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT='google-cloud-platform://?extra__google_cloud_platform__key_path=/usr/local/airflow/google_credentials.json'</span></pre><p id="b0a0" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">最后，由于我们将使用 BigQuery，我们需要为<code class="fe np nq nr ns b">google</code> provider 安装一个 provider 包，这样 Airflow 就可以与 BigQuery 交互并为我们创建外部表。在<code class="fe np nq nr ns b">requirements.txt</code>中，添加以下内容来安装提供程序:<code class="fe np nq nr ns b">apache-airflow-providers-google==8.4.0</code>。</p><p id="8f42" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">在我们进入下一部分之前，这些是我在这个项目中发现有用的其他 Astro CLI 命令:</p><ul class=""><li id="6683" class="mq mr in kv b kw kx kz la lc oc lg od lk oe lo of mw mx my bi translated"><code class="fe np nq nr ns b">astro dev restart</code>:停止你的 Airflow 环境，把你的 Astro 项目重建成 Docker 镜像，用新的 Docker 镜像重启你的 Airflow 环境。</li><li id="e40c" class="mq mr in kv b kw mz kz na lc nb lg nc lk nd lo of mw mx my bi translated"><code class="fe np nq nr ns b">astro dev stop</code>:暂停所有运行本地气流环境的 Docker 容器。</li><li id="b719" class="mq mr in kv b kw mz kz na lc nb lg nc lk nd lo of mw mx my bi translated"><code class="fe np nq nr ns b">astro dev kill</code>:针对您当地的气流环境，强制停止并移除所有正在运行的容器。</li><li id="612c" class="mq mr in kv b kw mz kz na lc nb lg nc lk nd lo of mw mx my bi translated"><code class="fe np nq nr ns b">astro dev logs</code>:显示本地 Airflow 环境中的 Airflow web 服务器、调度程序和触发器日志。</li></ul><p id="8d5a" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">这个项目我创建了 2 个 Dag，一个用于提取数据并加载到 GCS，另一个用于将数据从 GCS 加载到 BigQuery。</p></div><div class="ab cl ne nf hr ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="ig ih ii ij ik"><h2 id="e69c" class="lp lq in bd lr ls lt dn lu lv lw dp lx lc ly lz ma lg mb mc md lk me mf mg mh bi translated">DAG 1:提取并加载数据到 GCS…⏳</h2><p id="3a65" class="pw-post-body-paragraph kt ku in kv b kw mi jo ky kz mj jr lb lc mk le lf lg ml li lj lk mm lm ln lo ig bi translated">为了开始提取数据，我们首先需要导入我们之前在<code class="fe np nq nr ns b">Dockerfile</code>中定义的环境变量，并创建两个<a class="ae mn" href="https://github.com/Balurc/airflow_astrocli_demo/blob/main/dags/data_ingestion_gcs_dag.py" rel="noopener ugc nofollow" target="_blank"> python 函数，用于下载和上传数据到 GCS </a> ( <code class="fe np nq nr ns b">download_upload_data</code> &amp; <code class="fe np nq nr ns b">upload_to_gcs</code>)。</p><pre class="ke kf kg kh gt nt ns nu bn nv nw bi"><span id="8ddc" class="nx lq in ns b be ny nz l oa ob">PROJECT_ID = os.environ.get("GCP_PROJECT_ID")<br/>BUCKET = os.environ.get("GCP_GCS_BUCKET")<br/>AIRFLOW_HOME = os.environ.get("AIRFLOW_HOME", "/opt/airflow/")<br/>BIGQUERY_DATASET = os.environ.get("BIGQUERY_DATASET", "european_football_leagues")</span></pre><p id="0892" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated"><code class="fe np nq nr ns b">download_upload_data</code>函数包含<strong class="kv io">三个气流任务</strong>，同时使用 bash 操作符和 python 操作符。任务(在气流中)是一个工作单元，它被安排到一个 DAG 中。任务之间设置了上游和下游依赖关系，以表示它们应该运行的顺序。在这种情况下，任务依赖关系和执行顺序如下。</p><figure class="ke kf kg kh gt ki gh gi paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="gh gi og"><img src="../Images/b4d08b222afd10767782c37c32764dce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1IC8PGQN5qAXqxunsaKWkg.png"/></div></div><figcaption class="kp kq gj gh gi kr ks bd b be z dk translated">作者图片:执行顺序</figcaption></figure><h2 id="cb3d" class="lp lq in bd lr ls lt dn lu lv lw dp lx lc ly lz ma lg mb mc md lk me mf mg mh bi translated">任务 1:下载数据→下载数据集任务📥</h2><p id="fb3c" class="pw-post-body-paragraph kt ku in kv b kw mi jo ky kz mj jr lb lc mk le lf lg ml li lj lk mm lm ln lo ig bi translated">该任务将使用<code class="fe np nq nr ns b">BashOperator</code>从数据源中提取数据。<code class="fe np nq nr ns b">BashOperator</code>是一个 Airflow 操作符，它从 Airflow DAG 中执行 bash 命令或 bash 脚本。该命令有 4 个部分，第一部分是用<code class="fe np nq nr ns b">curl -sSLF</code>提取 csv 格式的数据，然后用<code class="fe np nq nr ns b">cut -f 1-22 -d,</code>选择相关字段，用<code class="fe np nq nr ns b">sed</code>添加季节年份，最后将新下载和编辑的文件保存(<code class="fe np nq nr ns b">&gt;</code>)到气流数据库。</p><pre class="ke kf kg kh gt nt ns nu bn nv nw bi"><span id="bb87" class="nx lq in ns b be ny nz l oa ob">download_dataset_task = BashOperator(<br/>         task_id="download_dataset_task",<br/>         bash_command=f"curl -sSLf https://datahub.io/sports-data/english-premier-league/r/season-0910.csv \<br/>                        | cut -f 1-22 -d, \<br/>                        | sed '1s/$/,\season/; 2,$s/$/,\season-0910/' \<br/>                        &gt; usr/local/airflow/english-premier-league_season-0910.csv"<br/>    )</span></pre><h2 id="751d" class="lp lq in bd lr ls lt dn lu lv lw dp lx lc ly lz ma lg mb mc md lk me mf mg mh bi translated">任务 2:将数据摄取到 GCS → local_to_gcs_task💉</h2><p id="efa5" class="pw-post-body-paragraph kt ku in kv b kw mi jo ky kz mj jr lb lc mk le lf lg ml li lj lk mm lm ln lo ig bi translated">一旦提取了数据，下一个任务就是用 python 代码将它们接收到 GCS 中。在 Airflow 中，我们可以使用<code class="fe np nq nr ns b">PythonOperator</code>来执行 python 代码。在本例中，我创建了一个 python 函数，<code class="fe np nq nr ns b">upload_to_gcs</code>，它是一个将数据连接并传输到 GCS 的函数。该功能将通过使用<code class="fe np nq nr ns b">PythonOperator</code>中的<code class="fe np nq nr ns b">python_callable</code>参数传递给气流 DAG。确保在您的<code class="fe np nq nr ns b">ops_kwargs</code>参数中定义您的 GCS 存储桶名称、保存 GCS 存储桶中文件的路径以及文件所在的本地路径。</p><pre class="ke kf kg kh gt nt ns nu bn nv nw bi"><span id="c53c" class="nx lq in ns b be ny nz l oa ob">local_to_gcs_task = PythonOperator(<br/>            task_id="local_to_gcs_task",<br/>            python_callable=upload_to_gcs,<br/>            op_kwargs={<br/>                "bucket": BUCKET,<br/>                "object_name": gcs_path_template,<br/>                "local_file": local_csv_path_template,<br/>            },<br/>    )</span></pre><h2 id="ba54" class="lp lq in bd lr ls lt dn lu lv lw dp lx lc ly lz ma lg mb mc md lk me mf mg mh bi translated">任务 3:从本地清除数据→ rm_task 🧹</h2><p id="5ea9" class="pw-post-body-paragraph kt ku in kv b kw mi jo ky kz mj jr lb lc mk le lf lg ml li lj lk mm lm ln lo ig bi translated">该任务将使用<code class="fe np nq nr ns b">BashOperator</code>和 bash 命令(<code class="fe np nq nr ns b">rm</code>)从 Airflow 数据库中删除所有文件。</p><pre class="ke kf kg kh gt nt ns nu bn nv nw bi"><span id="aabf" class="nx lq in ns b be ny nz l oa ob">rm_task = BashOperator(<br/>            task_id="rm_task",<br/>            bash_command=f"rm usr/local/airflow/english-premier-league_season-0910.csv"<br/>    )</span></pre><p id="62a0" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">定义完任务后，让我们开始创建一个 DAG 来自动化整个过程。有 50 个 csv 文件(来自 5 个足球联赛，每个有 10 个 csv 文件)，我们正在处理。为此，我将循环每个联盟和每个赛季的每个数据，并使用<code class="fe np nq nr ns b">TaskGroup</code>在气流中组织这些任务。<code class="fe np nq nr ns b">TaskGroup</code>帮助您更好地组织您的任务和维护您的 Dag，而没有苛刻的子 Dag。我们总共将有 150 个任务在<code class="fe np nq nr ns b">extract_load_gcs_dag</code>运行。</p><figure class="ke kf kg kh gt ki gh gi paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="gh gi oh"><img src="../Images/ef60bb22bbd86f6667623bb82380d565.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p2pVVvmpHnf7NATQCx1hAA.png"/></div></div><figcaption class="kp kq gj gh gi kr ks bd b be z dk translated">作者图片</figcaption></figure><pre class="ke kf kg kh gt nt ns nu bn nv nw bi"><span id="a155" class="nx lq in ns b be ny nz l oa ob">leagues = ["english-premier-league", "spanish-la-liga", "german-bundesliga", "italian-serie-a", "french-ligue-1"]<br/>seasons = ["season-0910", "season-1011", "season-1112", "season-1213", "season-1314", "season-1415", "season-1516", "season-1617", "season-1718", "season-1819"]<br/><br/>   for league in leagues:<br/>       for season in seasons:<br/>           with TaskGroup(group_id=f'group_{league}_{season}') as tg1:<br/>               URL_TEMPLATE = f"{URL_PREFIX}/{league}/r/{season}.csv"<br/>               CSV_FILE_TEMPLATE = f"{AIRFLOW_HOME}/{league}_{season}.csv"<br/>               GCS_PATH_TEMPLATE = f"football_stats/{league}/{league}_{season}.csv"<br/><br/>               if league == "english-premier-league":<br/>                   download_upload_data(url_template=URL_TEMPLATE,<br/>                                       filter="cut -f 1-10,12-23 -d,",<br/>                                       local_csv_path_template=CSV_FILE_TEMPLATE,<br/>                                       season=season,<br/>                                       gcs_path_template=GCS_PATH_TEMPLATE<br/>                                   )<br/>               else:<br/>                   download_upload_data(url_template=URL_TEMPLATE,<br/>                                       filter="cut -f 1-22 -d,",<br/>                                       local_csv_path_template=CSV_FILE_TEMPLATE,<br/>                                       season=season,<br/>                                       gcs_path_template=GCS_PATH_TEMPLATE<br/>                                   )</span></pre><figure class="ke kf kg kh gt ki gh gi paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="gh gi oi"><img src="../Images/c2a19782c79159ef0c4a77cf244d5aa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8qF9qkwa6srQdpOCavTwPQ.png"/></div></div><figcaption class="kp kq gj gh gi kr ks bd b be z dk translated">作者提供的图片:DAG 运行结束后，所有数据都已被纳入 GCS</figcaption></figure><p id="4f46" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">一旦数据已经在 GCS 内部，下一个任务就是将它们加载到 BigQuery 外部表中。外部表非常类似于标准的 BigQuery 表。它在 BigQuery 存储中存储元数据和模式。然而，主要的区别是，它的数据驻留在外部源中。外部表包含在数据集中，您可以像管理标准的 BigQuery 表一样管理它们。</p><h2 id="cc41" class="lp lq in bd lr ls lt dn lu lv lw dp lx lc ly lz ma lg mb mc md lk me mf mg mh bi translated">DAG 2:将数据加载到 Biguery…⏳</h2><p id="d5c6" class="pw-post-body-paragraph kt ku in kv b kw mi jo ky kz mj jr lb lc mk le lf lg ml li lj lk mm lm ln lo ig bi translated">为了执行这个任务，我创建了一个单独的 DAG ( <code class="fe np nq nr ns b">load_to_bq_dag</code>)，它将数据从 GCS bucket 加载到 BigQuery 外部表，在这个过程中，我使用了<code class="fe np nq nr ns b">BigQueryCreateExternalTableOperator</code>，一个气流操作符。在这个操作符中，您需要在参数中定义源路径(GCS)、目标路径(BigQuery)和模式。总共，我们将有 5 个任务在这个 DAG 中运行，每个足球联赛一个任务。一旦 DAG 运行完成，您应该在 BigQuery 中看到您的数据，并为您的查询做好准备。</p><figure class="ke kf kg kh gt ki gh gi paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="gh gi oj"><img src="../Images/ba8e77ec0cdcce9e5fd81209458b92f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t_s8lmhsMwLFGL5Gd2LHIA.png"/></div></div><figcaption class="kp kq gj gh gi kr ks bd b be z dk translated">作者图片:DAGs 运行成功</figcaption></figure><pre class="ke kf kg kh gt nt ns nu bn nv nw bi"><span id="5b49" class="nx lq in ns b be ny nz l oa ob">for league in leagues:<br/>     with TaskGroup(group_id=f'group_{league}') as tg2:<br/>         bigquery_external_table_task = BigQueryCreateExternalTableOperator(<br/>                    task_id="bigquery_external_table_task",<br/>                    destination_project_dataset_table=f"{BIGQUERY_DATASET}.{league}_external_table",<br/>                    bucket=BUCKET,<br/>                    source_objects=[f"football_stats/{league}/*"],<br/>                    skip_leading_rows=1,<br/>                    schema_fields=[<br/>                        {"name": "Div", "type": "STRING"},<br/>                        {"name": "Date", "type": "STRING"},<br/>                        {"name": "HomeTeam", "type": "STRING"},<br/>                        {"name": "AwayTeam", "type": "STRING"},<br/>                        {"name": "FTHG", "type": "INTEGER"},<br/>                        {"name": "FTAG", "type": "INTEGER"},<br/>                        .......<br/>                      ]</span></pre></div><div class="ab cl ne nf hr ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="ig ih ii ij ik"><p id="0768" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">我们已经达到了帖子的结尾，请<a class="ae mn" href="https://github.com/Balurc/airflow_astrocli_demo" rel="noopener ugc nofollow" target="_blank">检查我的回购</a>项目的全面实施。我希望这篇文章值得你花每一秒钟的时间，如果你有任何反馈，请随时写在评论区。谢谢你。</p><h2 id="c2cd" class="lp lq in bd lr ls lt dn lu lv lw dp lx lc ly lz ma lg mb mc md lk me mf mg mh bi translated">附加资源📚</h2><ul class=""><li id="1054" class="mq mr in kv b kw mi kz mj lc ms lg mt lk mu lo of mw mx my bi translated"><a class="ae mn" href="https://docs.astronomer.io/learn" rel="noopener ugc nofollow" target="_blank">跟天文学家</a>学习气流。</li><li id="81ac" class="mq mr in kv b kw mz kz na lc nb lg nc lk nd lo of mw mx my bi translated"><a class="ae mn" href="https://www.amazon.com/Data-Pipelines-Apache-Airflow-Harenslak/dp/1617296902" rel="noopener ugc nofollow" target="_blank">带阿帕奇气流的数据管道</a>。</li><li id="b278" class="mq mr in kv b kw mz kz na lc nb lg nc lk nd lo of mw mx my bi translated"><a class="ae mn" href="https://github.com/Balurc/data_eng_zoomcamp/tree/main/week2_data_ingestion" rel="noopener ugc nofollow" target="_blank">关于 DataTalksClub 数据摄取的第 2 周课程</a>。</li></ul></div></div>    
</body>
</html>