<html>
<head>
<title>Setup PySpark locally &amp; build your first ETL pipeline with PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在本地设置 PySpark &amp;用 py spark 构建您的第一个 ETL 管道</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/setup-pyspark-locally-build-your-first-etl-pipeline-with-pyspark-91c3060c6133?source=collection_archive---------0-----------------------#2022-10-02">https://blog.devgenius.io/setup-pyspark-locally-build-your-first-etl-pipeline-with-pyspark-91c3060c6133?source=collection_archive---------0-----------------------#2022-10-02</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><p id="0dc2" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">使用 Python，Jupyter 笔记本</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/10d5d8b180e2dc5abe5325e441298950.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oeCxE4ioXhwVNDBXSKTHhQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">PySpark (Python API)</figcaption></figure><p id="bbcb" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/" rel="noopener ugc nofollow" target="_blank"> PySpark </a>是针对<a class="ae ky" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Spark </a>的 Python API。Apache Spark 是一个用于大规模数据处理的分析引擎。它是一个分布式数据处理引擎，这意味着它运行在一个集群上。一个群集由三个或更多节点(或计算机)组成。Spark 是用 Scala 写的，但是它提供了 Java、Python、R 等其他主流语言的 API——py spark 就是 Python API。</p><p id="81d7" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">它还支持其他工具和语言，包括用于 SQL 的<a class="ae ky" href="https://spark.apache.org/docs/latest/sql-programming-guide.html" rel="noopener ugc nofollow" target="_blank"> Spark SQL </a>，用于 pandas 工作负载的<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_ps.html" rel="noopener ugc nofollow" target="_blank"> pandas API，以及用于增量计算和流处理的</a><a class="ae ky" href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html" rel="noopener ugc nofollow" target="_blank">结构化流</a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi kz"><img src="../Images/7ce3876bb2327bb389ef4b8c9424c0ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*24xv6xfBu4hgwoX7NwTg-g.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">PySpark 建筑由<a class="ae ky" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank">https://spark.apache.org/</a></figcaption></figure><p id="cff8" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">完整代码可在<a class="ae ky" href="https://github.com/hnawaz007/pythondataanalysis/tree/main/PySpark" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上获得。随附的视频教程可在<a class="ae ky" href="https://www.youtube.com/watch?v=5S8fEqTfT2I" rel="noopener ugc nofollow" target="_blank"> YouTube 上找到。</a></p><p id="56d8" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">本 PySpark 教程将帮助您:</p><ul class=""><li id="5ec3" class="la lb in jm b jn jo jr js jv lc jz ld kd le kh lf lg lh li bi translated">设置 PySpark 开发环境</li><li id="fea6" class="la lb in jm b jn lj jr lk jv ll jz lm kd ln kh lf lg lh li bi translated">将数据从平面文件读入数据帧</li><li id="e6ad" class="la lb in jm b jn lj jr lk jv ll jz lm kd ln kh lf lg lh li bi translated">在 Spark 上使用 Pandas API 执行数据帧操作</li><li id="7883" class="la lb in jm b jn lj jr lk jv ll jz lm kd ln kh lf lg lh li bi translated">使用 Spark-SQL 查询数据集</li><li id="114a" class="la lb in jm b jn lj jr lk jv ll jz lm kd ln kh lf lg lh li bi translated">将数据帧保存到 PostgreSQL 数据库中。</li></ul><p id="889c" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io"> Pyspark 架构</strong></p><p id="e463" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">Apache Spark 有一个主/从架构，其中主被称为“驱动程序”，从被称为“工人”。</p><p id="2ca9" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">这是一个基于集群的系统，由一个驱动程序和多个工作程序组成。在集群的主节点中，我们有创建<strong class="jm io">火花上下文</strong>的驱动程序。SparkContext 协调集群上的进程。它调度作业的执行并连接到集群管理器。集群管理器在 spark 应用程序中分配资源。一旦连接上，Spark 就会在集群中的 worker 节点上获得<em class="lo">执行器</em>，这些执行器是为我们的应用程序运行计算和存储数据的进程。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lp"><img src="../Images/0169d9935fb204d420b6fa8c8bc44887.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bkcoOMVkuZYrl7G9srGTmQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">https://spark.apache.org/的星火建筑</figcaption></figure><p id="0f32" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">接下来，它将我们的应用程序代码(SparkContext 的 JAR 或 Python 文件)发送给执行器。最后，SparkContext 将<em class="lo">任务</em>发送给执行器运行。</p><p id="a43d" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我们将以独立模式运行 PySpark。Standalone cluster 是 Spark 附带的一个简单的集群管理器，可以轻松设置集群。</p><p id="b6e5" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">先决条件</strong></p><p id="7d02" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我们需要在我们的机器上安装 Java JDK。我在 windows 上，我已经安装了 windows 版本。如果您想要查询或持久化数据库中的数据，那么我们将需要适当数据库的 Jar 文件。因为我有 PostgreSQL 和 SQL Server，所以我将从 maven 存储库站点下载 jar。请记住，Jar 文件必须与您的 Java 版本兼容。最后，我们需要在机器上安装 PySpark。</p><pre class="kj kk kl km gt lq lr ls lt aw lu bi"><span id="1945" class="lv lw in lr b gy lx ly l lz ma">pip install PySpark</span></pre><p id="36dc" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">首款 PySpark App </strong></p><p id="6a26" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">现在我们准备开发我们的第一个 PySpark 应用程序。我们将使用 Jupyter 笔记本作为我们的 IDE。我们在顶部导入所需的库。</p><pre class="kj kk kl km gt lq lr ls lt aw lu bi"><span id="da35" class="lv lw in lr b gy lx ly l lz ma">from pyspark import SparkContext, SparkConf<br/>from pyspark.sql import SparkSession, SQLContext<br/>import os</span></pre><p id="ddf6" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">要运行 PySpark 应用程序，您需要安装 Java。我们需要为 Spark 提供 Java 位置。为此，我们将使用 os dot environ 设置一个 Java home 变量，并提供 Java 安装目录。</p><pre class="kj kk kl km gt lq lr ls lt aw lu bi"><span id="bcdf" class="lv lw in lr b gy lx ly l lz ma">os.environ["JAVA_HOME"] = "C:\Program Files\Java\jdk-18.0.2.1"</span></pre><p id="e844" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">接下来，我们将为 spark 应用程序设置配置。Spark 应用程序只需要很少的配置细节就可以运行。我们可以在从 PySpark 导入的 SparkConf 对象的帮助下提供配置。我们将应用程序名称设置为“Example”。主节点 URL 为 local，并设置 Jar 文件的位置。我们准备创造一个火花环境。我们利用 SparkContext 对象并调用 getOrCreate 函数。我们向该函数提供配置。我们调用 SparkSession 并为其提供 Spark 上下文。我们将会话保存到一个名为 spark 的变量中。</p><pre class="kj kk kl km gt lq lr ls lt aw lu bi"><span id="1e6b" class="lv lw in lr b gy lx ly l lz ma">conf = SparkConf() \<br/>    .setAppName("Example") \<br/>    .setMaster("local") \<br/>    .set("spark.driver.extraClassPath","C:/pyspark/*")</span><span id="65c6" class="lv lw in lr b gy mb ly l lz ma">#<br/>sc = SparkContext.getOrCreate(conf=conf)<br/>spark = SparkSession(sc)</span></pre><p id="d686" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我们可以查看 spark 变量来查看 spark 应用程序的细节。它显示了我们在上面提供的配置细节。此外，我们还有到 Spark UI 的链接。我们点击链接，探索 Spark UI。我们可以在这里看到我们在应用程序中执行的所有操作。创建几个数据帧后，我们将再次访问 SQL/Dataframe 选项卡。</p><p id="2b2c" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我们的 spark 应用程序启动了。让我们用 spark 读取一个 csv 文件。这将把文件读入数据帧。我们使用 show 函数显示数据帧。</p><pre class="kj kk kl km gt lq lr ls lt aw lu bi"><span id="d367" class="lv lw in lr b gy lx ly l lz ma">df=spark.read.options(delimiter=",", header=True).csv(r"C:\Users\haq\OneDrive\Notebooks\data\AdvWorksData.csv")<br/>df.show()</span></pre><p id="e2cb" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">数据帧操作</strong></p><p id="4491" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">现在我们可以在这个数据帧上执行各种操作。你可以在 spark 网站上查看完整列表。我们可以打印 dataframe 的模式来查看列及其数据类型。</p><p id="c870" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我们可以用过滤函数过滤这个数据帧。这里我们只过滤法国的数据集。我们可以过滤出分析所需的数据。</p><pre class="kj kk kl km gt lq lr ls lt aw lu bi"><span id="d2ea" class="lv lw in lr b gy lx ly l lz ma">france = df.filter(df.saleterritory == "France").show(truncate=False)</span></pre><p id="ded0" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">如果我们想关注几列，我们可以通过向数据帧提供一个列列表来缩减数据帧。这个数据帧包括我们指定的列的子集。</p><pre class="kj kk kl km gt lq lr ls lt aw lu bi"><span id="4f6e" class="lv lw in lr b gy lx ly l lz ma">df1 = df[['productcategory','saleterritory','OrderDate','Sales']]<br/>df1.show()</span></pre><p id="aa69" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我们还可以执行聚合函数，例如 group by。这些是使用 Spark dataframe API 进行数据操作的几个例子。</p><pre class="kj kk kl km gt lq lr ls lt aw lu bi"><span id="6c94" class="lv lw in lr b gy lx ly l lz ma">saleterritory = df.groupBy('saleterritory').count()<br/>print(saleterritory.show())</span></pre><p id="fb31" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io"> Spark-SQL </strong></p><p id="06b2" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我们还可以使用 Spark SQL 对这个数据集运行 SQL 查询。我们将数据帧保存为一个临时视图，称为 sales。使用 Spark SQL，我们可以用 SQL 查询它。输出显示为 Caps 子类别过滤的记录。我们可以使用分布式引擎对这个数据集执行标准的 SQL 操作。</p><pre class="kj kk kl km gt lq lr ls lt aw lu bi"><span id="5a7a" class="lv lw in lr b gy lx ly l lz ma">df.createOrReplaceTempView("sales")<br/>output =  spark.sql("SELECT * from sales where productsubcategory='Caps'")<br/>output.show()</span></pre><p id="45c2" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">写入数据库</strong></p><p id="2f28" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">如果我们愿意，我们可以将这个数据集持久化到数据库中。我们已经向 Spark 上下文提供了 PostgreSQL 的 Jar 文件。我们将声明数据库细节和凭证。我们将添加一个名为<em class="lo"> pyspark_sales_table </em>的新表。</p><p id="7431" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我们在 dataframe 上调用 write 函数。我们提供了一些配置，比如模式(如果表存在，我们会覆盖它)格式是 jdbc(我们提供 jdbc URL、目标表名、用户、密码和驱动程序。最后，我们在它上面调用 save 函数。一旦我们执行这个单元，这将把数据帧保存到 PostgreSQL 数据库。</p><pre class="kj kk kl km gt lq lr ls lt aw lu bi"><span id="3864" class="lv lw in lr b gy lx ly l lz ma">dest_tbl = 'public."pyspark_sales_table"'<br/>database = "AdventureWorks"<br/>password = "password"<br/>user = "user"<br/>#<br/>df.write.mode("overwrite") \<br/>    .format("jdbc") \<br/>    .option("url", f"jdbc:postgresql://localhost:5432/{database}") \<br/>    .option("dbtable", dest_tbl) \<br/>    .option("user", user) \<br/>    .option("password", password) \<br/>    .option("driver",  "org.postgresql.Driver") \<br/>    .save()</span></pre><p id="20d3" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我们已经演示了如何在独立模式下设置 PySpark。我们编写了第一个从 CSV 文件读取数据的 PySpark 应用程序。我们在 Spark 上使用 Pandas API 执行数据帧操作。此外，我们使用 Spark-SQL 查询导入的数据集。最后，我们将数据帧持久化到 PostgreSQL 数据库中。</p><p id="6aad" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">结论</strong></p><ul class=""><li id="0194" class="la lb in jm b jn jo jr js jv lc jz ld kd le kh lf lg lh li bi translated">我们描述 PySpark 是什么以及如何在本地配置它。</li><li id="8134" class="la lb in jm b jn lj jr lk jv ll jz lm kd ln kh lf lg lh li bi translated">我们展示了使用 PySpark 导入和操作数据是多么容易。</li><li id="7fb9" class="la lb in jm b jn lj jr lk jv ll jz lm kd ln kh lf lg lh li bi translated">我们使用 PySpark、Pandas API 和 PostgreSQL 实现了一个基本的 ETL 管道。</li><li id="7a78" class="la lb in jm b jn lj jr lk jv ll jz lm kd ln kh lf lg lh li bi translated">完整的代码可以在<a class="ae ky" href="https://github.com/hnawaz007/pythondataanalysis/tree/main/PySpark" rel="noopener ugc nofollow" target="_blank">这里</a>找到</li></ul></div></div>    
</body>
</html>