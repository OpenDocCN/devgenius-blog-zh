<html>
<head>
<title>Scale Vision Transformers (ViT) Beyond Hugging Face | Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超越拥抱脸的视觉变形金刚(ViT)|第 1 部分</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/scale-vision-transformers-vit-beyond-hugging-face-part-1-e09318cab588?source=collection_archive---------13-----------------------#2022-08-29">https://blog.devgenius.io/scale-vision-transformers-vit-beyond-hugging-face-part-1-e09318cab588?source=collection_archive---------13-----------------------#2022-08-29</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><div class=""><h2 id="3e91" class="pw-subtitle-paragraph jk im in bd b jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb dk translated">加快拥抱脸最先进的维生素 t 模型🤗借助 Databricks、Nvidia 和 Spark NLP，速度提升高达 2300%(25 倍)🚀</h2></div><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi kc"><img src="../Images/c7572592ba711def70ee3e626385d598.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q7ZNzdLPjeBLF-04_tfp4A.png"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated"><strong class="bd ks">通过使用<strong class="bd ks">数据模块</strong>、<strong class="bd ks">英伟达</strong>和<strong class="bd ks"> Spark NLP </strong>扩展</strong>基于<strong class="bd ks">变压器</strong>的型号</figcaption></figure><p id="706c" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">我是<a class="ae lp" href="https://github.com/JohnSnowLabs/spark-nlp" rel="noopener ugc nofollow" target="_blank"> Spark NLP </a>开源项目的贡献者之一，最近这个库开始支持端到端<strong class="kv io">视觉变形器(ViT) </strong>模型。我在日常工作中使用 Spark NLP 和其他 ML/DL 开源库，我决定部署一个 ViT 管道来完成最先进的图像分类任务，并提供<strong class="kv io">拥抱脸</strong>和<strong class="kv io"> Spark NLP </strong>之间的深入比较。</p><blockquote class="lq"><p id="66ba" class="lr ls in bd lt lu lv lw lx ly lz lo dk translated">本文的目的是演示如何从 Hugging Face 向外扩展 Vision Transformer (ViT)模型，并将其部署到生产就绪环境中，以实现加速和高性能的推理。最后，我们将通过使用 Databricks、Nvidia 和 Spark NLP，将拥抱脸的 ViT 模型扩展 25 倍(2300%)。</p></blockquote><h2 id="f39e" class="ma mb in bd ks mc md dn me mf mg dp mh lc mi mj mk lg ml mm mn lk mo mp mq mr bi translated">在本文的第 1 部分，我将:</h2><ul class=""><li id="2c55" class="ms mt in kv b kw mu kz mv lc mw lg mx lk my lo mz na nb nc bi translated">视觉转换器(ViT)简介</li><li id="d407" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated">CPU 和 GPU 上戴尔服务器内部拥抱脸的基准测试</li><li id="4109" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated">CPU 和 GPU 上戴尔服务器内部的基准 Spark NLP</li></ul><blockquote class="ni nj nk"><p id="74f9" class="kt ku nl kv b kw kx jo ky kz la jr lb nm ld le lf nn lh li lj no ll lm ln lo ig bi translated"><em class="in">本着完全透明的精神，GitHub </em>  上的 <a class="ae lp" href="https://github.com/JohnSnowLabs/spark-nlp-workshop/tree/master/tutorials/blogposts/medium/scale-vision-transformers-vit-beyond-hugging-face?ref=hackernoon.com" rel="noopener ugc nofollow" target="_blank"> <strong class="kv io"/></a></p></blockquote><div class="np nq gp gr nr ns"><a rel="noopener  ugc nofollow" target="_blank" href="/scale-vision-transformers-vit-beyond-hugging-face-part-1-e09318cab588"><div class="nt ab fo"><div class="nu ab nv cl cj nw"><h2 class="bd io gy z fp nx fr fs ny fu fw im bi translated">超越拥抱脸的视觉变形金刚(ViT)|第 1 部分</h2><div class="nz l"><h3 class="bd b gy z fp nx fr fs ny fu fw dk translated">加快拥抱脸最先进的维生素 t 模型🤗使用 Databricks、Nvidia 和……最高可达 2300%(快 25 倍)</h3></div><div class="oa l"><p class="bd b dl z fp nx fr fs ny fu fw dk translated">blog.devgenius.io</p></div></div><div class="ob l"><div class="oc l od oe of ob og km ns"/></div></div></a></div><div class="np nq gp gr nr ns"><a rel="noopener  ugc nofollow" target="_blank" href="/scale-vision-transformers-vit-beyond-hugging-face-part-2-b7b296d548b7"><div class="nt ab fo"><div class="nu ab nv cl cj nw"><h2 class="bd io gy z fp nx fr fs ny fu fw im bi translated">超越拥抱脸的视觉变形金刚(ViT)|第 2 部分</h2><div class="nz l"><h3 class="bd b gy z fp nx fr fs ny fu fw dk translated">加快拥抱脸最先进的维生素 t 模型🤗使用 Databricks、Nvidia 和……最高可达 2300%(快 25 倍)</h3></div><div class="oa l"><p class="bd b dl z fp nx fr fs ny fu fw dk translated">blog.devgenius.io</p></div></div><div class="ob l"><div class="oc l od oe of ob og km ns"/></div></div></a></div><div class="np nq gp gr nr ns"><a rel="noopener  ugc nofollow" target="_blank" href="/scale-vision-transformers-vit-beyond-hugging-face-part-3-5b8c13ef6477"><div class="nt ab fo"><div class="nu ab nv cl cj nw"><h2 class="bd io gy z fp nx fr fs ny fu fw im bi translated">超越拥抱脸的视觉变形金刚(ViT)|第 3 部分</h2><div class="nz l"><h3 class="bd b gy z fp nx fr fs ny fu fw dk translated">加快拥抱脸最先进的维生素 t 模型🤗使用 Databricks、Nvidia 和……最高可达 2300%(快 25 倍)</h3></div><div class="oa l"><p class="bd b dl z fp nx fr fs ny fu fw dk translated">blog.devgenius.io</p></div></div><div class="ob l"><div class="oc l od oe of ob og km ns"/></div></div></a></div></div><div class="ab cl oh oi hr oj" role="separator"><span class="ok bw bk ol om on"/><span class="ok bw bk ol om on"/><span class="ok bw bk ol om"/></div><div class="ig ih ii ij ik"><h1 id="078f" class="oo mb in bd ks op oq or me os ot ou mh jt ov ju mk jw ow jx mn jz ox ka mq oy bi translated">视觉变压器(ViT)型号介绍</h1><p id="8928" class="pw-post-body-paragraph kt ku in kv b kw mu jo ky kz mv jr lb lc oz le lf lg pa li lj lk pb lm ln lo ig bi translated">早在 2017 年，谷歌人工智能的一组研究人员发表了一篇论文，介绍了一种改变了所有自然语言处理(NLP)标准的 transformer 模型架构。本文描述了一种称为自我注意的新机制，作为一种新的和更有效的语言应用模型。例如，两个最受欢迎的基于变压器的模型系列是 GPT 和伯特。</p><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi pc"><img src="../Images/0b184afb051052e6a5cd1c33bddeafe1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MO_ktD5qzLPlK415aQ1RSQ.png"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">一点变形金刚的历史<a class="ae lp" href="https://huggingface.co/course/chapter1/4" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/course/chapter1/4</a></figcaption></figure><p id="d166" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">有一个很棒的章节是关于“<a class="ae lp" href="https://huggingface.co/course/chapter1/4" rel="noopener ugc nofollow" target="_blank"><strong class="kv io"/></a><strong class="kv io">”</strong>变形金刚是如何工作的，如果你有兴趣，我强烈推荐你阅读。</p><p id="49f7" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">虽然这些新的基于 Transformer 的模型似乎正在彻底改变 NLP 任务，但它们在计算机视觉(CV)中的使用仍然非常有限。计算机视觉领域一直由卷积神经网络(CNN)的使用所主导，并且存在基于 CNN 的流行架构(如 ResNet)。这种情况一直持续到 2021 年 6 月谷歌大脑的另一组研究人员在一篇题为<strong class="kv io"/><a class="ae lp" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank"><strong class="kv io">的论文中介绍了<strong class="kv io">“视觉变形金刚”</strong> (ViT)一幅图像相当于 16x16 个单词:变形金刚用于大规模图像识别</strong></a><strong class="kv io"/></p><p id="a478" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">这篇论文代表了图像识别方面的一个突破，它使用了我们刚刚讨论过的基于变换的模型(如伯特和 GPT)中使用的相同的自我注意机制。在像 BERT 这样的基于转换的语言模型中，输入是一个句子(例如一列单词)。然而，在 ViT 模型中，我们首先将图像分割成子图像片的网格，然后在每个嵌入的片成为标记之前，我们用线性项目嵌入每个片。结果是一系列嵌入补丁，我们将其传递给类似于 BERT 的模型。</p><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi pd"><img src="../Images/940719a29a134532f4f5b384c594e615.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uZ2dRQvKUdr_HYeu.png"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">Google Research 在 2021 年的原始论文中介绍的 ViT 模型结构概述</figcaption></figure><p id="137e" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">Vision Transformer 专注于更高的精度和更少的计算时间。查看论文中公布的基准测试，我们可以看到，针对<a class="ae lp" href="https://arxiv.org/abs/1911.04252v4" rel="noopener ugc nofollow" target="_blank"> <strong class="kv io">嘈杂的学生</strong> </a>数据集(由谷歌于 2020 年 6 月公布)的训练时间减少了 80%，尽管精度状态或多或少相同。如需了解更多关于 ViT 性能的信息，请访问其在<a class="ae lp" href="https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1" rel="noopener ugc nofollow" target="_blank"> <strong class="kv io">论文上的页面，代码为</strong> </a> <strong class="kv io"> : </strong></p><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi pe"><img src="../Images/0ed585ac142c790ff15e6c88fa6d24c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8pcQEvbfzhNU5ZYZvY4aqw.png"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">流行图像分类基准的比较。(<a class="ae lp" href="https://arxiv.org/pdf/2010.11929.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2010.11929.pdf</a>)</figcaption></figure><p id="add4" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">值得一提的是，一旦您通过 ViT 架构训练了一个模型，您就可以像在 NLP 中一样预训练和微调您的转换器。(实际上这很酷！)</p><p id="a7b8" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">如果我们将 ViT 模型与 CNN 进行比较，我们可以看到，它们具有更高的精度，而计算成本却低得多。您可以将 ViT 模型用于计算机视觉中的各种下游任务，如图像分类、检测对象和图像分割。这也可以是医疗保健领域特有的，你可以针对<a class="ae lp" href="https://towardsdatascience.com/vision-transformers-for-femur-fracture-classification-480d62f87252" rel="noopener" target="_blank">股骨骨折</a>、<a class="ae lp" href="https://iopscience.iop.org/article/10.1088/1361-6560/ac3dc8/meta" rel="noopener ugc nofollow" target="_blank">肺气肿</a>、<a class="ae lp" href="https://arxiv.org/abs/2110.14731" rel="noopener ugc nofollow" target="_blank">乳腺癌</a>、<a class="ae lp" href="https://www.mdpi.com/1660-4601/18/21/11086/pdf" rel="noopener ugc nofollow" target="_blank">新冠肺炎</a>和<a class="ae lp" href="https://www.biorxiv.org/content/10.1101/2021.11.27.470184v2.full" rel="noopener ugc nofollow" target="_blank">老年痴呆症</a>对你的 ViT 模型进行预训练/微调。</p><p id="e946" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">我将在本文末尾留下参考资料，以防您想更深入地了解 ViT 模型是如何工作的。</p><p id="d2c6" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">[1]:深潜:拥抱脸上的视觉变形金刚最佳图芯<a class="ae lp" href="https://huggingface.co/blog/vision-transformers" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/blog/vision-transformers</a></p><h2 id="bfbb" class="ma mb in bd ks mc pf dn me mf pg dp mh lc ph mj mk lg pi mm mn lk pj mp mq mr bi translated">一些正在使用的 ViT 型号</h2><p id="2121" class="pw-post-body-paragraph kt ku in kv b kw mu jo ky kz mv jr lb lc oz le lf lg pa li lj lk pb lm ln lo ig bi translated">Vision Transformer (ViT)模型(<a class="ae lp" href="https://huggingface.co/google/vit-base-patch16-224" rel="noopener ugc nofollow" target="_blank">ViT-base-patch 16–224</a>)在分辨率为 224x224 的 ImageNet-21k(1400 万张图像，21，843 个类别)上进行了预训练，并在分辨率为 224x224 的 ImageNet 2012(100 万张图像，1，000 个类别)上进行了微调:</p><div class="kd ke kf kg gt ab cb"><figure class="pk kh pl pm pn po pp paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><img src="../Images/5022db77c74538799997cd4705d19028.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*po9mkAjFzwnsj7RGG5ZePg.png"/></div></figure><figure class="pk kh pq pm pn po pp paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><img src="../Images/b04d10d50b993fc8480d32bd234dc437.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*WvhVnm0Bb-1LWW8AUwkQoQ.png"/></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk pr di ps pt translated"><a class="ae lp" href="https://huggingface.co/google/vit-base-patch16-224" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/google/vit-base-patch16-224</a></figcaption></figure></div><p id="055a" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">用于食品分类的微调 ViT 模型:</p><div class="kd ke kf kg gt ab cb"><figure class="pk kh pu pm pn po pp paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><img src="../Images/659ca69932ced659364a03795b87d3e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*yBm4zdkjIkcyf3phXZU3-Q.png"/></div></figure><figure class="pk kh pv pm pn po pp paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><img src="../Images/abcf2087fb6e8303d6b86c32787f6b27.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*QU9VVAH7lRaEGZU3dyk71w.png"/></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk pw di px pt translated"><a class="ae lp" href="https://huggingface.co/nateraw/food" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/nateraw/food</a>——<a class="ae lp" href="https://huggingface.co/julien-c/hotdog-not-hotdog" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/julien-c/hotdog-not-hotdog</a></figcaption></figure></div><p id="5af2" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">然而，当涉及到预测时，任何 DL/ML 模型都有局限性和限制。没有 100%准确的模型，因此当您将它们用于医疗保健等重要领域时，请记住:</p><div class="kd ke kf kg gt ab cb"><figure class="pk kh py pm pn po pp paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><img src="../Images/b43a9ddc546f066682f564e0b99ce252.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*aAMKODkvgeYMwZVMyQea3g.png"/></div></figure><figure class="pk kh pz pm pn po pp paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><img src="../Images/5f883876b1923726d960396c7dc1acac.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*TmM5pc6-YO-p6H7hjA-OIQ.png"/></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk qa di qb pt translated"><strong class="bd ks">图片摘自:</strong><a class="ae lp" href="https://www.akc.org/expert-advice/lifestyle/do-you-live-in-dog-state-or-cat-state/" rel="noopener ugc nofollow" target="_blank">https://www . AKC . org/expert-advice/life style/do-you-live-in-dog-state-or-cat-state/</a>—<strong class="bd ks">ViT model</strong>:<a class="ae lp" href="https://huggingface.co/julien-c/hotdog-not-hotdog" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/julien-c/hotdog-not-hotdog</a></figcaption></figure></div><p id="c7a5" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">我们是否可以使用拥抱脸的这些模型或微调新的 ViT 模型，并将其用于实际生产中的推断？我们如何通过使用 AWS EMR、Azure Insight、GCP Dataproc 或 Databricks 等分布式计算托管服务来扩展它们？</p><p id="56de" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">希望在本文结束时，这些问题中的一些能够得到解答。</p></div><div class="ab cl oh oi hr oj" role="separator"><span class="ok bw bk ol om on"/><span class="ok bw bk ol om on"/><span class="ok bw bk ol om"/></div><div class="ig ih ii ij ik"><h1 id="63c5" class="oo mb in bd ks op oq or me os ot ou mh jt ov ju mk jw ow jx mn jz ox ka mq oy bi translated">让基准测试开始吧！</h1><h2 id="ec28" class="ma mb in bd ks mc pf dn me mf pg dp mh lc ph mj mk lg pi mm mn lk pj mp mq mr bi translated">关于我们基准的一些细节:</h2><p id="992e" class="pw-post-body-paragraph kt ku in kv b kw mu jo ky kz mv jr lb lc oz le lf lg pa li lj lk pb lm ln lo ig bi translated"><strong class="kv io"> 1-数据集:</strong> ImageNet mini: <strong class="kv io">样本</strong> ( &gt; 3K) — <strong class="kv io">完整</strong> ( &gt; 34K)</p><p id="6d37" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">我已经从 https://www.kaggle.com/datasets/ifigotin/imagenetmini-1000<a class="ae lp" href="https://www.kaggle.com/datasets/ifigotin/imagenetmini-1000" rel="noopener ugc nofollow" target="_blank">的 ka ggle</a>下载了 ImageNet 1000(迷你)数据集</p><p id="2865" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">我选择了包含超过 34K 图像的目录，并将其命名为 imagenet-mini，因为我所需要的是足够多的图像来进行耗时较长的基准测试。此外，我随机选择了不到 10%的完整数据集，并将其命名为<strong class="kv io"> imagenet-mini-sample </strong>，其中有<strong class="kv io"> 3544 张图像</strong>，用于我的较小基准测试，并微调合适的参数，如批量大小。</p><p id="3f06" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated"><strong class="kv io"> 2-型号:</strong>谷歌的<a class="ae lp" href="https://huggingface.co/google/vit-base-patch16-224" rel="noopener ugc nofollow" target="_blank">vit-base-patch 16–224</a></p><p id="c66f" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">我们将使用这个来自谷歌的拥抱脸模型:<a class="ae lp" href="https://huggingface.co/google/vit-base-patch16-224" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/google/vit-base-patch16-224</a></p><p id="cbfc" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated"><strong class="kv io"> 3-库:</strong> <a class="ae lp" href="https://github.com/huggingface/transformers/" rel="noopener ugc nofollow" target="_blank"> <strong class="kv io">变形金刚</strong> </a>🤗&amp; <a class="ae lp" href="https://github.com/JohnSnowLabs/spark-nlp" rel="noopener ugc nofollow" target="_blank"> <strong class="kv io">火花 NLP </strong> </a>🚀</p><h2 id="ee39" class="ma mb in bd ks mc pf dn me mf pg dp mh lc ph mj mk lg pi mm mn lk pj mp mq mr bi translated">裸机服务器上的基准测试拥抱脸</h2><p id="33c1" class="pw-post-body-paragraph kt ku in kv b kw mu jo ky kz mv jr lb lc oz le lf lg pa li lj lk pb lm ln lo ig bi translated"><strong class="kv io">Dell PowerEdge c 4130 上的 ViT 型号</strong></p><p id="28c5" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">什么是裸机服务器？一台<strong class="kv io">裸机</strong> - <strong class="kv io">金属服务器</strong>只是一台只由一个用户使用的物理计算机。这台机器上没有安装虚拟机管理程序，没有虚拟化，一切都直接在主操作系统(Linux-Ubuntu)上执行——这台机器的 CPU、GPU 和内存的详细规格都在笔记本电脑中。</p><blockquote class="ni nj nk"><p id="3077" class="kt ku nl kv b kw kx jo ky kz la jr lb nm ld le lf nn lh li lj no ll lm ln lo ig bi translated">正如我的初始测试以及拥抱脸工程团队撰写的比较 DL 引擎推理速度的几乎每篇博客文章所揭示的那样，拥抱脸库(Transformer)中推理的最佳性能是通过使用 PyTorch 而不是 TensorFlow 实现的。我不确定这是否是因为 TensorFlow 在拥抱脸方面是二等公民，因为支持的功能更少，支持的模型更少，示例更少，过时的教程，以及过去 2 年用户回答的年度调查更多地询问 TensorFlow 或 PyTorch 在 CPU 和 GPU 上的推理延迟更低。</p></blockquote><blockquote class="lq"><p id="ad49" class="lr ls in bd lt lu qg qh qi qj qk lo dk translated">TensorFlow 仍然是最常用的深度学习框架<a class="ae lp" href="https://twitter.com/fchollet/status/1478404084881190912?lang=en" rel="noopener ugc nofollow" target="_blank"/></p></blockquote><p id="9933" class="pw-post-body-paragraph kt ku in kv b kw ql jo ky kz qm jr lb lc qn le lf lg qo li lj lk qp lm ln lo ig bi translated">不管什么原因，我在拥抱人脸库中选择了 PyTorch，以获得我们图像分类基准的最佳结果。这是在拥抱脸中使用 ViT 模型(当然是 PyTorch)的简单代码片段:</p><figure class="kd ke kf kg gt kh"><div class="bz fp l di"><div class="qq qr l"/></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated"><a class="ae lp" href="https://huggingface.co/google/vit-base-patch16-224#how-to-use" rel="noopener ugc nofollow" target="_blank">https://hugging face . co/Google/vit-base-patch 16-224 #使用方法</a></figcaption></figure><p id="0d2b" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">这可能看起来直接预测图像作为输入，但它不适合大量的图像，尤其是在 GPU 上。为了避免连续预测图像，并利用 GPU 等加速硬件，最好通过<a class="ae lp" href="https://huggingface.co/docs/transformers/main_classes/pipelines#pipelines" rel="noopener ugc nofollow" target="_blank"> <strong class="kv io">管道</strong> </a>向模型提供批量图像，这在拥抱面部中是可能的。不用说，您可以通过扩展 Hugging Face 的管道或者自己实现批处理技术。</p><p id="c76d" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">用于<strong class="kv io">图像分类</strong>的一个简单管道将如下所示:</p><figure class="kd ke kf kg gt kh"><div class="bz fp l di"><div class="qq qr l"/></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated"><a class="ae lp" href="https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.ImageClassificationPipeline" rel="noopener ugc nofollow" target="_blank">https://hugging face . co/docs/transformers/main _ classes/pipelines # transformers。图像分类管道</a></figcaption></figure><p id="79a1" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">根据文档，我已经为特征提取器和模型(当然是 PyTorch 检查点)下载/加载了<strong class="kv io">Google/vit-base-patch 16–224</strong>,以便在图像分类任务中使用它们。对于我们的基准测试，有三件事非常重要:</p><ul class=""><li id="b87b" class="ms mt in kv b kw kx kz la lc qs lg qt lk qu lo mz na nb nc bi translated"><strong class="kv io">设备</strong>:如果是<code class="fe qc qd qe qf b">-1</code>(默认)，它将只使用 CPU，而如果是正整数，它将在相关联的 CUDA 设备 id 上运行模型。(最好隐藏 GPU，强制 PyTorch 使用 CPU，不要只依赖这里的这个数字)。</li><li id="11bc" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated"><strong class="kv io"> batch_size: </strong>当管道将使用<em class="nl"> DataLoader </em>(在 Pytorch 模型的 GPU 上传递数据集时)时，要使用的批处理的大小对于推断并不总是有利的。</li><li id="827d" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated">您必须使用 DataLoader 或 PyTorch 数据集来充分利用 GPU 上拥抱面管道中的批处理。</li></ul><p id="e22d" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">在我们继续进行基准测试之前，您需要知道一件关于拥抱面部管道批处理的事情，它并不总是有效的。正如 Hugging Face 的文档中所述，设置<strong class="kv io"> batch_size </strong>可能根本不会提高管道的性能。这可能会降低您的销售进度:</p><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi qv"><img src="../Images/c874180342ebb7d114037734f1bb4ff9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gQfJvStnNz2wlF2vJPPNUA.png"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated"><a class="ae lp" href="https://huggingface.co/docs/transformers/main_classes/pipelines#pipeline-batching" rel="noopener ugc nofollow" target="_blank">https://hugging face . co/docs/transformers/main _ classes/pipelines # pipeline-batching</a></figcaption></figure><p id="f9a4" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">公平地说，在我的基准测试中，我使用了一个从 1 开始的批量范围，以确保我能从中找到最佳结果。这是我如何在 CPU 上对拥抱脸管道进行基准测试的:</p><figure class="kd ke kf kg gt kh"><div class="bz fp l di"><div class="qq qr l"/></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">基准抱面管道</figcaption></figure><p id="f1a7" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">让我们来看看我们的第一个在 CPU 上的拥抱脸图像分类管道的基准测试在样本(3K) ImageNet 数据集上的结果:</p><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi qw"><img src="../Images/97093bbf145cffab054a755bf166dea1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6FWYy2MsYugV6DrN7XeAww.png"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">CPU 上的拥抱人脸图像分类流水线——预测 3544 幅图像</figcaption></figure><p id="8136" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">可以看出，完成对样本数据集中大约<strong class="kv io"> 3544 幅图像</strong>的处理花费了大约 3 分钟(<strong class="kv io"> 188 秒)</strong>。现在我知道了哪个批处理大小(8)最适合我的管道/数据集/硬件，我可以在更大的数据集(<strong class="kv io"> 34K 图像</strong>)上使用相同的管道，批处理大小为:</p><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi qx"><img src="../Images/e80b8f0eb290e869e0b7f182f7ad5c31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YM44UR5L4Q7SMe322peq6A.png"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">CPU 上的拥抱人脸图像分类流水线—预测 34745 幅图像</figcaption></figure><p id="3fce" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">这一次花了大约 31 分钟(<strong class="kv io"> 1，879 秒</strong>)在 CPU 上完成了对<strong class="kv io"> 34745 幅图像</strong>的分类预测。</p><p id="18af" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">要改进大多数深度学习模型，尤其是这些基于 transformer 的新模型，应该使用 GPU 等加速硬件。让我们看看如何在完全相同的数据集上对完全相同的流水线进行基准测试，但这次是在一个<strong class="kv io"> GPU </strong>设备上。如前所述，我们需要将<strong class="kv io">设备</strong>改为类似<code class="fe qc qd qe qf b">0</code>(第一个 GPU)的 CUDA 设备 id:</p><figure class="kd ke kf kg gt kh"><div class="bz fp l di"><div class="qq qr l"/></div></figure><p id="11fd" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">除了设置<code class="fe qc qd qe qf b">device=0</code>，我还按照推荐的方式通过<code class="fe qc qd qe qf b">.to(device)</code>在 GPU 设备上运行 PyTorch 模型。由于我们使用加速硬件(GPU ),我还将测试的最大批量增加到 1024，以找到最佳结果。</p><p id="1fc4" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">让我们通过示例 ImageNet 数据集(3K)来看看我们在 GPU 设备上的拥抱面部图像分类管道:</p><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi qy"><img src="../Images/5894c2d05bcb2a7345e433ff989c3985.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s0u6EZN7vT3-0j453a6EJQ.png"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">GPU 上的拥抱人脸图像分类流水线—预测 3544 幅图像</figcaption></figure><p id="7034" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">可以看出，在一个<strong class="kv io"> GPU 设备</strong>上，我们花了大约<strong class="kv io"> 50 秒</strong>来完成对来自 imagenet-mini-sample 数据集的大约<strong class="kv io"> 3544 张图像</strong>的处理。批处理提高了速度，特别是与来自 CPU 的结果相比，但是，在批处理大小为 32 左右时，这种提高停止了。尽管在批量大小为 32 之后结果是相同的，但我还是选择了批量大小为<strong class="kv io"> 256 </strong>作为我的更大的基准，以便利用足够的 GPU 内存。</p><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi qz"><img src="../Images/67fc69608adad4315253071950018d00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w9XjDWtPgXWjZEeDyEezFg.png"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">在 GPU 上拥抱人脸图像分类流水线——预测 34745 张图像</figcaption></figure><p id="1e04" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">这一次，我们的基准测试用了大约 8:17 分钟(<strong class="kv io"> 497 秒</strong>)在一个<strong class="kv io"> GPU </strong>设备上完成了对<strong class="kv io"> 34745 幅图像</strong>的预测。如果我们比较我们在 CPU 和 GPU 设备上的基准测试结果，我们可以看到 GPU 是赢家:</p><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi ra"><img src="../Images/7c5d3c3fabe7b49b2650ff69f59182f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HIuXuVVg7Rvm8ABzEbBn-A.png"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">拥抱脸(PyTorch)在 GPU 上比 CPU 快 3.9 倍</figcaption></figure><blockquote class="ni nj nk"><p id="57e1" class="kt ku nl kv b kw kx jo ky kz la jr lb nm ld le lf nn lh li lj no ll lm ln lo ig bi translated">我使用 Hugging Face 管道来加载 ViT PyTorch 检查点，将我的数据加载到 Torch 数据集中，并在 CPU 和 GPU 上使用现成的批处理。与在 CPU 上运行相同的流水线相比，<strong class="kv io"> GPU </strong>的速度高达<strong class="kv io">到 3.9 倍</strong>。</p></blockquote><p id="863e" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">我们已经改进了我们的 ViT 管道，通过使用一个<strong class="kv io"> GPU 设备</strong>而不是 CPU 来执行图像分类，但是在将其扩展到多台机器之前，我们能否在单台机器中的两个<strong class="kv io"> CPU </strong> &amp; <strong class="kv io"> GPU </strong>上进一步改进我们的管道？我们来看看 Spark NLP 库。</p><h1 id="4637" class="oo mb in bd ks op rb or me os rc ou mh jt rd ju mk jw re jx mn jz rf ka mq oy bi translated">Spark NLP:最先进的自然语言处理</h1><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi rg"><img src="../Images/09d6d3794c338e42b6264c4f6be52771.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*NttOf77CvBxPYJ7e.png"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">Spark NLP 是一个开源的最先进的自然语言处理库(<a class="ae lp" href="https://github.com/JohnSnowLabs/spark-nlp" rel="noopener ugc nofollow" target="_blank">https://github.com/JohnSnowLabs/spark-nlp</a>)</figcaption></figure><p id="c9e0" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">Spark NLP 是一个最先进的自然语言处理库，构建于 Apache Spark 之上。它为机器学习管道提供了简单、高性能和准确的 NLP 注释，可以在分布式环境中轻松扩展。Spark NLP 配有<strong class="kv io"> 7000+ </strong>经过预处理的<strong class="kv io">管道</strong>和<strong class="kv io">型号</strong>，支持超过<strong class="kv io"> 200+种语言</strong>。它还提供诸如标记化、分词、词性标注、单词和句子嵌入、命名实体识别、依存解析、拼写检查、文本分类、情感分析、标记分类、机器翻译(+180 种语言)、摘要&amp;问答、文本生成、图像分类(ViT)等任务，以及更多<a class="ae lp" href="https://github.com/JohnSnowLabs/spark-nlp#features" rel="noopener ugc nofollow" target="_blank"> NLP 任务</a>。</p><blockquote class="ni nj nk"><p id="5806" class="kt ku nl kv b kw kx jo ky kz la jr lb nm ld le lf nn lh li lj no ll lm ln lo ig bi translated">Spark NLP 是唯一一个正在生产的开源 NLP 库，提供最先进的变压器，如<strong class="kv io">伯特</strong>、<strong class="kv io">卡门伯特</strong>、<strong class="kv io">艾伯特</strong>、<strong class="kv io">伊莱克特拉</strong>、<strong class="kv io"> XLNet </strong>、<strong class="kv io">蒸馏伯特</strong>、<strong class="kv io">罗伯塔</strong>、<strong class="kv io">德伯塔</strong>、<strong class="kv io">XLM-罗伯塔</strong>、<strong class="kv io"> 而 Vision Transformer ( <strong class="kv io"> ViT </strong>)不仅可以扩展到<strong class="kv io"> Python </strong>和<strong class="kv io"> R </strong>，还可以通过原生扩展 Apache Spark 大规模扩展到 JVM 生态系统(<strong class="kv io"> Java </strong>、<strong class="kv io"> Scala </strong>和<strong class="kv io"> Kotlin </strong>)。</strong></p></blockquote><h2 id="5d8b" class="ma mb in bd ks mc pf dn me mf pg dp mh lc ph mj mk lg pi mm mn lk pj mp mq mr bi translated">裸机服务器上的 Spark NLP 基准测试</h2><p id="1bd7" class="pw-post-body-paragraph kt ku in kv b kw mu jo ky kz mv jr lb lc oz le lf lg pa li lj lk pb lm ln lo ig bi translated"><strong class="kv io">Dell PowerEdge c 4130 上的 ViT 型号</strong></p><p id="c852" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">Spark NLP 在最近的<strong class="kv io"> 4.1.0 </strong>版本中添加了与拥抱脸相同的<strong class="kv io">图像分类</strong>ViT 功能。这个特性叫做<strong class="kv io"><em class="nl">ViTForImageClassification，</em> </strong> <em class="nl">它拥有超过</em> <strong class="kv io"> <em class="nl"> 240 个预先训练好的模型</em> </strong> <em class="nl">准备就绪</em> <strong class="kv io"> <em class="nl">，</em> </strong>，在 Spark NLP 中使用这个特性的简单代码如下:</p><figure class="kd ke kf kg gt kh"><div class="bz fp l di"><div class="qq qr l"/></div></figure><p id="a2bb" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">如果我们并排比较 Spark NLP 和 Hugging Face 下载和加载预训练的 ViT 模型以进行图像分类预测，除了加载图像和在 Hugging Face 库外使用类似<code class="fe qc qd qe qf b">argmax</code>的后期计算，它们都非常简单。此外，它们都可以被保存，并在以后用作管道，以将这些行减少到只有一行代码:</p><div class="kd ke kf kg gt ab cb"><figure class="pk kh rh pm pn po pp paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><img src="../Images/f378f4da704633f668afc73d1e7d5838.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*zwX-srDeeQ21vEB7arvG-Q.png"/></div></figure><figure class="pk kh ri pm pn po pp paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><img src="../Images/eef7013034e58d74e74822ca92d82bc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*xgQK3yr6jFu6h5lieVNAfg.png"/></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk rj di rk pt translated">在 Spark NLP(左)和拥抱脸(右)中加载和使用 ViT 模型进行图像分类</figcaption></figure></div><p id="d95d" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">因为 Apache Spark 有一个叫做<strong class="kv io">惰性评估</strong>的概念，所以直到调用了<strong class="kv io">动作</strong>它才开始执行流程。Apache Spark 中的操作可以是<code class="fe qc qd qe qf b">.count()</code>或<code class="fe qc qd qe qf b">.show()</code>或<code class="fe qc qd qe qf b">.write()</code>以及许多其他基于 RDD 的操作，我现在不会深入讨论这些操作，对于本文，您不需要了解它们。我通常选择<code class="fe qc qd qe qf b">count()</code>目标列或者<code class="fe qc qd qe qf b">write()</code>磁盘上的结果来触发执行数据帧中的所有行。此外，像拥抱脸基准一样，我将循环选择批量大小，以确保我可以得到所有可能的结果，而不会错过最佳结果。</p><p id="2f15" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">现在，我们知道了如何在 Spark NLP 中加载 ViT 模型，我们也知道了如何触发一个动作来强制对数据帧中的所有行进行计算以进行基准测试，剩下要学习的就是来自<a class="ae lp" href="https://github.com/oneapi-src/oneDNN" rel="noopener ugc nofollow" target="_blank"> oneAPI 深度神经网络库(oneDNN) </a>的 oneDNN。由于 Spark NLP 中的 DL 引擎是 TensorFlow，您还可以启用 oneDNN 来提高 CPU 上的速度(像其他任何事情一样，您需要测试这一点，以确保它提高了速度，而不是相反)。除了没有启用 oneDNN 的普通 CPU 之外，我也将使用这个标志</p><p id="0631" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">现在，我们知道了 Hugging Face 的所有 ViT 模型也可用于 Spark NLP，以及如何在管道中使用它们，我们将在裸机戴尔服务器上重复之前的基准测试，以比较 CPU 和 GPU。让我们看看 Spark NLP 在 CPU 上的图像分类管道在我们的样本(3K) ImageNet 数据集上的结果:</p><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi rl"><img src="../Images/c0334eef45a4a34fa8a917df6c0ab4f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5jxRquCP0eWXzRnj6zBjFg.png"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">没有 oneDNN 的 CPU 上的 spark nli image-分类流水线—预测 3544 个图像</figcaption></figure><p id="7a61" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">大约花了 2.1 分钟(<strong class="kv io"> 130 秒)</strong>来完成对来自我们样本数据集的大约<strong class="kv io"> 3544 张图像</strong>的处理。使用较小的数据集来尝试不同的批处理大小有助于为您的任务、数据集和计算机选择正确的批处理大小。很明显<strong class="kv io">批次大小 16 </strong>是我们的管道交付最佳结果的最佳大小。</p><p id="db10" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">我还想启用<strong class="kv io"> oneDNN </strong>，看看在这种特定情况下，与没有 oneDNN 的 CPU 相比，它是否提高了我的性能指标评测。通过将<strong class="kv io"> TF_ENABLE_ONEDNN_OPTS </strong>的环境变量设置为<strong class="kv io"> 1，可以在 Spark NLP 中启用 oneDNN。</strong>让我们看看，如果我启用此标志并在 CPU 上重新运行之前的基准测试以找到最佳批量，会发生什么情况:</p><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi rm"><img src="../Images/887bfaba4391dc65e5b8b9a6d3a4271a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V_8I0FRzircn_OGrHEJW_Q.png"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">在具有 oneDNN 的 CPU 上的 spark nli image-分类流水线—预测 3544 幅图像</figcaption></figure><p id="a54b" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">很明显，在这种特定情况下，为 TensorFlow 启用 oneDNN 将我们的结果提高了至少 14%。因为我们不需要做/改变任何事情，只需要说<code class="fe qc qd qe qf b">export TF_ENABLE_ONEDNN_OPTS=1</code>我将把它用于具有更大数据集的基准测试，以查看差异。这大约快了几秒钟，但是在较大的数据集上快 14%会减少几分钟的结果。</p><p id="42a5" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">现在，我知道了不使用 oneDNN 的 CPU 的批量大小为 16，启用 oneDNN 的 CPU 的批量大小为 2，可以获得最佳结果，我可以继续在更大的数据集上使用相同的管道(<strong class="kv io"> 34K 图像</strong>):</p><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi rn"><img src="../Images/9672d2180ba4d1453f82c4fb0bae2345.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wX2uayKk7vzjuOXOOGYzag.png"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">无 oneDNN 的 CPU 上的 Spark NLP 图像分类流水线—预测 34745 幅图像</figcaption></figure><p id="0ca2" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">这一次，我们的基准测试用了大约 24 分钟(<strong class="kv io"> 1423 秒</strong>)在没有启用 oneDNN 的<strong class="kv io"> CPU </strong>设备上完成了对<strong class="kv io"> 34745 个图像</strong>的分类预测。现在让我们看看，如果我为 TensorFlow 启用 oneDNN 并使用批量大小 2(最佳结果)，会发生什么情况:</p><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi ro"><img src="../Images/c9549b764c84fb22e9ba539e539f7abe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DO_iC4ewBXYI5HXGueMZwQ.png"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">具有 oneDNN 的 CPU 上的 Spark NLP 图像分类流水线—预测 34745 幅图像</figcaption></figure><p id="fcf7" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">这次用了大约 21 分钟(<strong class="kv io"> 1278 秒</strong>)。正如我们的样本性能指标评测所预期的那样，我们可以在结果中看到大约<strong class="kv io"> 11%的改进</strong>，与没有启用 oneDNN 的情况相比，这确实节省了时间。</p><p id="29c8" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">让我们看看如何在 GPU 设备上对完全相同的流水线进行基准测试。在 Spark NLP 中，使用 GPU 所需要的只是在启动 Spark NLP 会话时用<code class="fe qc qd qe qf b">gpu=True</code>启动它:</p><pre class="kd ke kf kg gt rp qf rq rr aw rs bi"><span id="45ee" class="ma mb in qf b gy rt ru l rv rw">spark = sparknlp.start(gpu=True)<br/># you can set the memory here as well<br/>spark = sparknlp.start(gpu=True, memory="16g")</span></pre><p id="a597" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">就是这样！如果您的管道中有可以在 GPU 上运行的东西，它会自动完成，而不需要显式地做任何事情。</p><p id="236a" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">让我们通过示例 ImageNet 数据集(3K)来看看我们在 GPU 设备上的 Spark NLP 图像分类管道:</p><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi rx"><img src="../Images/a49a420818a026da1a9147d357f700a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CUTCbLVzyLgfWG1iMNSvFg.png"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">GPU 上的 Spark 图像分类流水线—预测 3544 幅图像</figcaption></figure><p id="637f" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">出于好奇，我想知道我在较小的数据集上寻找一个好的批量大小的努力是否正确，我在较大的数据集上用 GPU 运行了相同的管道，以查看批量大小 32 是否会有最好的结果:</p><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi ry"><img src="../Images/d49457988e0516a660cb061bf70f0ebb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EOj7yjW4x91VmZzbuBfYRA.png"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">GPU 上的 Spark NLP 图像分类流水线—预测 34745 幅图像</figcaption></figure><p id="920f" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">幸运的是，批量 32 产生了最好的时间。所以用了 4 分半左右(<strong class="kv io"> 277 秒)。</strong></p><p id="368e" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">我将从具有 oneDNN 的<strong class="kv io">CPU 中挑选结果，因为它们更快，我将把它们与<strong class="kv io"> GPU </strong>结果进行比较:</strong></p><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi rz"><img src="../Images/81bc1314b0ac3591e9a8429c6c763f4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*39jbyAuqUvEvw6E8GF2zXA.png"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">Spark NLP (TensorFlow)在 GPU 上比 CPU (oneDNN)快 4.6 倍</figcaption></figure><blockquote class="ni nj nk"><p id="c1ec" class="kt ku nl kv b kw kx jo ky kz la jr lb nm ld le lf nn lh li lj no ll lm ln lo ig bi translated">这太棒了！我们可以看到，即使启用了 oneDNN，GPU 上的 Spark NLP 也比 CPU 快了 4.6 倍<strong class="kv io"/>。</p></blockquote><p id="d915" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">让我们来看看这些结果是如何与拥抱脸基准进行比较的:</p><div class="kd ke kf kg gt ab cb"><figure class="pk kh sa pm pn po pp paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><img src="../Images/14eb465c158e80d261ca64d50e2d706b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*iQPds7hZ3Im7f8b-LyI6XQ.png"/></div></figure><figure class="pk kh sb pm pn po pp paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><img src="../Images/73c1c4bbed546b3b147c8ce3039ffe8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*goHScndoPtXlWuF-9lHV2Q.png"/></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk sc di sd pt translated"><strong class="bd ks"> Spark NLP </strong>在<strong class="bd ks"> CPU </strong>和<strong class="bd ks"> GPU </strong>上<strong class="bd ks"> ViT </strong>型号推断<strong class="bd ks">比<strong class="bd ks">抱脸</strong>快</strong></figcaption></figure></div><blockquote class="ni nj nk"><p id="dfeb" class="kt ku nl kv b kw kx jo ky kz la jr lb nm ld le lf nn lh li lj no ll lm ln lo ig bi translated"><strong class="kv io"> Spark NLP </strong>在预测具有 3K 图像的样本数据集的图像类时，比<strong class="kv io">CPU</strong>上的拥抱脸快<strong class="kv io"> 65%,在具有 34K 图像的较大数据集上快 47%。<strong class="kv io"> Spark NLP </strong>在单个<strong class="kv io"> GPU </strong>推理 34K 图像的较大数据集上比拥抱脸</strong>快<strong class="kv io"> 79%，在较小数据集上快 35%。</strong></p></blockquote><div class="kd ke kf kg gt ab cb"><figure class="pk kh se pm pn po pp paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><img src="../Images/9df2664c63115a258989bc1bc4365b81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*teCFtqAJURQYPCdbtKasNA.png"/></div></figure><figure class="pk kh sf pm pn po pp paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><img src="../Images/75b99f1a337573c2e1f86814cf8c5d08.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*b1hfD_0COkN2mE-mLs0HSw.png"/></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk pr di ps pt translated">与拥抱脸相比，<strong class="bd ks"> Spark NLP </strong>在<strong class="bd ks"> CPU </strong>上快了 65% ，在<strong class="bd ks"> GPU </strong>上快了 79% </figcaption></figure></div><blockquote class="lq"><p id="820c" class="lr ls in bd lt lu qg qh qi qj qk lo dk translated">通过使用 CPU 或 GPU，Spark NLP 比单机中的拥抱脸更快——通过使用视觉转换器(ViT)进行图像分类</p></blockquote></div><div class="ab cl oh oi hr oj" role="separator"><span class="ok bw bk ol om on"/><span class="ok bw bk ol om on"/><span class="ok bw bk ol om"/></div><div class="ig ih ii ij ik"><p id="7335" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">在<a class="ae lp" href="https://medium.com/@maziyar/scale-vision-transformers-vit-beyond-hugging-face-part-2-b7b296d548b7" rel="noopener"> <strong class="kv io">第 2 部分</strong> </a>中，我将在 Databricks 单节点(CPU &amp; GPU)上运行相同的基准测试，以比较 Spark NLP 与 Hugging Face。</p><h1 id="26a5" class="oo mb in bd ks op rb or me os rc ou mh jt rd ju mk jw re jx mn jz rf ka mq oy bi translated">参考</h1><p id="cedc" class="pw-post-body-paragraph kt ku in kv b kw mu jo ky kz mv jr lb lc oz le lf lg pa li lj lk pb lm ln lo ig bi translated"><strong class="kv io"> ViT </strong></p><ul class=""><li id="7d2c" class="ms mt in kv b kw kx kz la lc qs lg qt lk qu lo mz na nb nc bi translated"><a class="ae lp" href="https://arxiv.org/pdf/2010.11929.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2010.11929.pdf</a></li><li id="b56a" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated">【https://github.com/google-research/vision_transformer T42】</li><li id="0d5a" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated"><a class="ae lp" href="https://viso.ai/deep-learning/vision-transformer-vit/" rel="noopener ugc nofollow" target="_blank">图像识别中的视觉变压器(ViT)——2022 指南</a></li><li id="71aa" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated"><a class="ae lp" href="https://github.com/lucidrains/vit-pytorch" rel="noopener ugc nofollow" target="_blank">https://github.com/lucidrains/vit-pytorch</a></li><li id="f236" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated"><a class="ae lp" href="https://medium.com/mlearning-ai/an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale-51f3561a9f96" rel="noopener">https://medium . com/mlearning-ai/an-image-is-worth-16x 16-words-transformers-for-image-recognition-at-scale-51f 3561 a9f 96</a></li><li id="3efb" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated"><a class="ae lp" href="https://medium.com/nerd-for-tech/an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale-paper-summary-3a387e71880a" rel="noopener">https://medium . com/nerd-for-tech/an-image-worth-16x 16-words-transformers-for-image-recognition-at-scale-paper-summary-3a 387 e 71880 a</a></li><li id="df17" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated"><a class="ae lp" href="https://gareemadhingra11.medium.com/summary-of-paper-an-image-is-worth-16x16-words-3f7f3aca941" rel="noopener">https://gareemadhingra 11 . medium . com/summary-of-paper-an-image-worth-16x 16-words-3f 7 F3 ACA 941</a></li><li id="8176" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated"><a class="ae lp" href="https://medium.com/analytics-vidhya/vision-transformers-bye-bye-convolutions-e929d022e4ab" rel="noopener">https://medium . com/analytics-vid hya/vision-transformers-bye-bye-convolutions-e 929d 022 E4 ab</a></li><li id="f68c" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated"><a class="ae lp" href="https://medium.com/syncedreview/google-brain-uncovers-representation-structure-differences-between-cnns-and-vision-transformers-83b6835dbbac" rel="noopener">https://medium . com/synced review/Google-brain-uncovers-re presentation-structure-differences-between-CNN-and-vision-transformers-83b 6835 db BAC</a></li></ul><p id="37a4" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated"><strong class="kv io">抱紧脸</strong></p><ul class=""><li id="734b" class="ms mt in kv b kw kx kz la lc qs lg qt lk qu lo mz na nb nc bi translated"><a class="ae lp" href="https://huggingface.co/docs/transformers/main_classes/pipelines" rel="noopener ugc nofollow" target="_blank">https://hugging face . co/docs/transformers/main _ classes/pipelines</a></li><li id="a212" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated"><a class="ae lp" href="https://huggingface.co/blog/fine-tune-vit" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/blog/fine-tune-vit</a></li><li id="6a7a" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated"><a class="ae lp" href="https://huggingface.co/blog/vision-transformers" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/blog/vision-transformers</a></li><li id="5b51" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated"><a class="ae lp" href="https://huggingface.co/blog/tf-serving-vision" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/blog/tf-serving-vision</a></li><li id="a69b" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated"><a class="ae lp" href="https://huggingface.co/blog/deploy-tfserving-kubernetes" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/blog/deploy-tfserving-kubernetes</a></li><li id="eb7f" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated"><a class="ae lp" href="https://huggingface.co/google/vit-base-patch16-224" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/google/vit-base-patch16-224</a></li><li id="398e" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated"><a class="ae lp" href="https://huggingface.co/blog/deploy-vertex-ai" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/blog/deploy-vertex-ai</a></li><li id="decb" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated"><a class="ae lp" href="https://huggingface.co/models?other=vit" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/models?other=vit</a></li></ul><p id="c6e2" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated"><strong class="kv io">数据块</strong></p><ul class=""><li id="7cda" class="ms mt in kv b kw kx kz la lc qs lg qt lk qu lo mz na nb nc bi translated"><a class="ae lp" href="https://www.databricks.com/spark/getting-started-with-apache-spark" rel="noopener ugc nofollow" target="_blank">https://www . data bricks . com/spark/getting-started-with-Apache-spark</a></li><li id="7f24" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated"><a class="ae lp" href="https://docs.databricks.com/getting-started/index.html" rel="noopener ugc nofollow" target="_blank">https://docs.databricks.com/getting-started/index.html</a></li><li id="53ad" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated"><a class="ae lp" href="https://docs.databricks.com/getting-started/quick-start.html" rel="noopener ugc nofollow" target="_blank">https://docs . data bricks . com/getting-started/quick-start . html</a></li><li id="5d01" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated">看尽<a class="ae lp" href="https://www.databricks.com/dataaisummit/" rel="noopener ugc nofollow" target="_blank">数据+AI 峰会 2022 </a></li><li id="b422" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated"><a class="ae lp" href="https://www.databricks.com/blog/2020/05/15/shrink-training-time-and-cost-using-nvidia-gpu-accelerated-xgboost-and-apache-spark-on-databricks.html" rel="noopener ugc nofollow" target="_blank">https://www . data bricks . com/blog/2020/05/15/shrink-training-time-and-cost-using-NVIDIA-GPU-accelerated-xgboost-and-Apache-spark-on-data bricks . html</a></li></ul><p id="1950" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated"><strong class="kv io"> Spark NLP </strong></p><ul class=""><li id="2e9c" class="ms mt in kv b kw kx kz la lc qs lg qt lk qu lo mz na nb nc bi translated"><a class="ae lp" href="https://github.com/JohnSnowLabs/spark-nlp" rel="noopener ugc nofollow" target="_blank"> Spark NLP GitHub </a></li><li id="c1bc" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated"><a class="ae lp" href="https://github.com/JohnSnowLabs/spark-nlp-workshop" rel="noopener ugc nofollow" target="_blank"> Spark NLP 研讨会</a> (Spark NLP 示例)</li><li id="a4af" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated"><a class="ae lp" href="https://nlp.johnsnowlabs.com/docs/en/transformers" rel="noopener ugc nofollow" target="_blank">火花 NLP 变压器</a></li><li id="ec4d" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated"><a class="ae lp" href="https://nlp.johnsnowlabs.com/models?edition=Spark+NLP" rel="noopener ugc nofollow" target="_blank"> Spark NLP 车型轮毂</a></li><li id="62f1" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated"><a class="ae lp" href="https://www.johnsnowlabs.com/watch-webinar-speed-optimization-benchmarks-in-spark-nlp-3-making-the-most-of-modern-hardware/" rel="noopener ugc nofollow" target="_blank">Spark NLP 3 中的速度优化&amp;基准测试:充分利用现代硬件</a></li><li id="e1a8" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated"><a class="ae lp" href="https://nlp.johnsnowlabs.com/docs/en/hardware_acceleration" rel="noopener ugc nofollow" target="_blank">Spark NLP 中的硬件加速</a></li><li id="0469" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated"><a class="ae lp" href="https://medium.com/spark-nlp/serving-spark-nlp-via-api-spring-and-lightpipelines-64d2e6413327" rel="noopener">通过 API 服务 Spark NLP:Spring 和 LightPipelines </a></li><li id="27bb" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated"><a class="ae lp" href="https://medium.com/spark-nlp/serving-spark-nlp-via-api-1-3-microsoft-synapse-ml-2c77a3f61f9d" rel="noopener">通过 API 服务 Spark NLP(1/3):微软的 Synapse ML </a></li><li id="eb33" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated"><a class="ae lp" href="https://medium.com/spark-nlp/serving-spark-nlp-via-api-2-3-fastapi-and-lightpipelines-218d1980c9fc" rel="noopener">通过 API 服务 Spark NLP(2/3):FastAPI 和 LightPipelines </a></li><li id="13c6" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated"><a class="ae lp" href="https://medium.com/spark-nlp/serving-spark-nlp-via-api-3-3-databricks-and-mlflow-serve-apis-4ef113e7fac4" rel="noopener">通过 API (3/3)服务 Spark NLP:数据块作业和 MLFlow 服务 API</a></li><li id="31d7" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated"><a class="ae lp" href="https://aws.amazon.com/blogs/opensource/leverage-deep-learning-in-scala-with-gpu-on-spark-3-0/" rel="noopener ugc nofollow" target="_blank">利用 Scala 中的深度学习和 Spark 3.0 上的 GPU</a></li><li id="0ea4" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated"><a class="ae lp" href="https://www.nvidia.com/en-us/ai-data-science/spark-ebook/getting-started-spark-3/" rel="noopener ugc nofollow" target="_blank">开始使用 GPU 加速的 Apache Spark 3 </a></li><li id="9039" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated"><a class="ae lp" href="https://spark.apache.org/docs/latest/sql-performance-tuning.html" rel="noopener ugc nofollow" target="_blank">阿帕奇 Spark 性能调优</a></li><li id="5d7f" class="ms mt in kv b kw nd kz ne lc nf lg ng lk nh lo mz na nb nc bi translated">GPU 上可能的额外优化:<a class="ae lp" href="https://nvidia.github.io/spark-rapids/docs/configs.html" rel="noopener ugc nofollow" target="_blank">Apache Spark 配置的 RAPIDS 加速器</a></li></ul></div></div>    
</body>
</html>