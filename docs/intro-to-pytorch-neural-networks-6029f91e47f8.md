# PyTorch 神经网络简介

> 原文：<https://blog.devgenius.io/intro-to-pytorch-neural-networks-6029f91e47f8?source=collection_archive---------8----------------------->

![](img/c7bed63d069b90b7cef7388e0ae6a823.png)

深度学习重新唤起了人们对人工智能的兴趣。道理很简单:深度学习就是管用。它给了我们创造以前无法创造的技术的力量。它开辟了新的商业前景，总体上提高了技术水平。

要进行深度学习，你需要会编码，尤其是用 Python。在那里，您可以从广泛的深度学习库中进行选择，包括 TensorFlow、Keras、MXNet、MatConvNet 以及最近的 Pytorch！

Pytorch 在首次发布后迅速流行起来。它被称为 TensorFlow 杀手，因为它更加用户友好，使用简单。事实上，您将了解到 Pytorch 和深度学习的入门是多么简单。

# 从 PyTorch 开始

Pytorch 开发的目标是使其尽可能与 Python 的 Numpy 相媲美。这将在标准 Python 代码、Numpy 和 Pytorch 之间实现简单无缝的接口，使开发更快更容易。

首先，我们可以通过 Pip 安装 PyTorch:

```
pip install torch torchvision
```

## 张量

张量是每个深学习包最基本的组成部分。张量是类似矩阵的数据结构，其执行和行为类似于 Numpy 数组。事实上，在大多数情况下，您可以将它们视为 Numpy 数组。两者之间最显著的区别是张量可能在当代深度学习包(非常快)的 CPU 或 GPU 上实现。

可以使用基本张量对象在 PyTorch 中声明张量:

```
import torch 
a = torch.Tensor(3, 3)
```

前面的代码生成一个大小为(3，3)的张量，即三行三列浮点零:

```
0\.  0\.  0.
0\.  0\.  0.
0\.  0\.  0.
[torch.FloatTensor of size 3x3]
```

您也可以创建一个张量填充的随机浮点值:

```
x = torch.rand(3, 3)
print(x)"""
Prints out:
tensor([[0.3211, 0.1423, 0.6453],
        [0.0343, 0.7986, 0.3213],
        [0.0767, 0.5432, 0.3456]])
"""
```

使用 Pytorch，张量相乘、相加和其他基本数学运算轻而易举:

```
a = torch.ones(3, 3)
b = torch.ones(3, 3) * 4
c = a + b
print(c)"""
Prints out:
tensor([[5., 5., 5.],
        [5., 5., 5.],
        [5., 5., 5.]])
"""
```

Pytorch 张量甚至支持 Numpy 风格的切片功能！

```
a = torch.ones(3, 3) * 5
b = x[:, :2]
print(b)
"""
Prints out:
tensor([[5., 5.],
        [5., 5.],
        [5., 5.]])
"""
```

因此，Pytorch 张量的使用和操作方式与 Numpy 数组非常相似。现在我们来看看如何使用这些简单的 Pytorch 张量来构建深层网络！

# 制造神经网络

在 Pytorch 中，神经网络被定义为 Python 类。该网络是通过扩展 Torch 库的 torch.nn.Module 的类来定义的。让我们创建一个卷积神经网络(CNN)类，我们可以在 MNIST 数据集上使用它。

看看下面定义我们网络的代码！

__init__()和 forward()过程是 Pytorch 网络类中最关键的函数。__init__()函数用于设置模型将采用的任何网络层。forward()函数是将所有层堆叠在一起以创建模型的地方。

在 init 函数中，我们为模型定义了两个卷积层，其中一个我们将多次重用(conv2)。最后，我们将应用最大池层和全局平均池层。为了得到最终的输出概率，我们使用全连接(FC)层和 softmax。

我们指定我们的层如何堆叠在一起以在正向函数中产生整个模型。该网络是传统网络，具有堆叠的 conv、池和 FC 层。Pytorch 的美妙之处在于，我们可以在 forward()方法中的任何地方用一个简单的 print 语句打印中间层中任何张量的形式和结果！

# 培训、测试，还有储蓄！

## 加载数据

是时候准备我们训练的数据了！我们将开始设置适当的导入，初始化设置，并确保 Pytorch 配置为使用 GPU。下面的代码行利用 torch.device()来验证 Pytorch 是否支持 CUDA，如果支持，它就使用 GPU！

MNIST 数据集可以直接从 Pytorch 中检索。我们将下载数据并将其分成两个张量:一个用于训练，一个用于测试。我们将把数据输入到 torch DataLoader 中，它将为传递给模型做好准备，并设置批量大小和可选的洗牌。

## 培养

其他深度学习框架如 TensorFlow、Keras 和 MXNet 都有类似的优化器(我们将使用 Adam)和损失函数(我们将使用交叉熵)。

在 Pytorch 中，所有网络模型和数据集都必须手动从 CPU 移动到 GPU。将. to()方法应用到下面的模型中可以做到这一点。稍后，我们将对我们的图片数据执行同样的操作。

最后，我们可以将我们的训练循环写在纸上。要了解它是如何工作的，请看下面的代码。

1.  在训练数据加载器中，所有 Pytorch 训练循环将遍历每个时期和批次。
2.  在每次循环迭代中，图片数据和标签被传输到 GPU。
3.  正向传递、反向传递和优化阶段都明确应用于每个训练循环。
4.  当将该模型应用于该批中的照片时，估计该批的损失。
5.  网络的梯度被计算并反向传输。

# 测试和保存

在 Pytorch 中，测试网络性能会创建一个类似于训练阶段的循环。主要区别在于梯度不需要向后传播。我们将继续向前传递，只在网络输出中寻找概率最高的标签。

在这个场景中，经过 10 个时期后，我们的网络的测试集准确率达到了 99.06%。

只需使用 torch.save()将模型保存到磁盘供以后使用。这就是全部了！