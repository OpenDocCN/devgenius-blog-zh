<html>
<head>
<title>Join streaming Kafka topics using Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Spark 加入流式 Kafka 主题</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/join-streaming-kafka-topics-using-spark-5072bce6cd45?source=collection_archive---------8-----------------------#2022-03-03">https://blog.devgenius.io/join-streaming-kafka-topics-using-spark-5072bce6cd45?source=collection_archive---------8-----------------------#2022-03-03</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><h2 id="b96f" class="il im in bd b dl io ip iq ir is it dk iu translated" aria-label="kicker paragraph">达达工程</h2><div class=""/><div class=""><h2 id="e789" class="pw-subtitle-paragraph jt iw in bd b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk dk translated">基于事件的系统中最常用的功能之一的实际操作体验。</h2></div><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/466c16e62dad843aa282157bcbd3cd14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qEMR15Xr2JI27pW5cn3uTA.png"/></div></div></figure><p id="e154" class="pw-post-body-paragraph kx ky in kz b la lb jx lc ld le ka lf lg lh li lj lk ll lm ln lo lp lq lr ls ig bi translated">本实用技术指南讲述了如何使用 Spark 引擎连接来自 Kafka 的两个数据流。我们在事件处理系统中使用它，我们希望它们是实时的。</p><p id="efba" class="pw-post-body-paragraph kx ky in kz b la lb jx lc ld le ka lf lg lh li lj lk ll lm ln lo lp lq lr ls ig bi translated">连接流可能很棘手，因为数据没有边界。但是，在本文中，我不会谈论窗口。这是另一篇文章的主题。</p></div><div class="ab cl lt lu hr lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ig ih ii ij ik"><h1 id="a8d6" class="ma mb in bd mc md me mf mg mh mi mj mk kc ml kd mm kf mn kg mo ki mp kj mq mr bi translated">前方是什么？</h1><p id="6006" class="pw-post-body-paragraph kx ky in kz b la ms jx lc ld mt ka lf lg mu li lj lk mv lm ln lo mw lq lr ls ig bi translated">这个小型项目的先决条件:</p><ul class=""><li id="a282" class="mx my in kz b la lb ld le lg mz lk na lo nb ls nc nd ne nf bi translated">码头工人</li><li id="e79d" class="mx my in kz b la ng ld nh lg ni lk nj lo nk ls nc nd ne nf bi translated">Python 3</li><li id="5aaa" class="mx my in kz b la ng ld nh lg ni lk nj lo nk ls nc nd ne nf bi translated">IntelliJ Idea[用于 Scala IDE]</li><li id="c9f1" class="mx my in kz b la ng ld nh lg ni lk nj lo nk ls nc nd ne nf bi translated">互联网连接</li><li id="218e" class="mx my in kz b la ng ld nh lg ni lk nj lo nk ls nc nd ne nf bi translated">【大概是写 Atom，Sublime 等 Python 脚本的 IDE。]</li></ul><p id="a509" class="pw-post-body-paragraph kx ky in kz b la lb jx lc ld le ka lf lg lh li lj lk ll lm ln lo lp lq lr ls ig bi translated">这是一个初学者的动手项目，所以我没有为它使用专用的 Spark 集群。我们将只使用 Intellij 模拟器的火花。我们将在 Docker 中打开 Kafka 和 Zookeeper，并使用 Scala 的 Spark 仿真器读取 Kafka 中的数据。我们还将使用 Python 脚本来生成一个样本数据流，以便我们可以测试我们的管道。</p></div><div class="ab cl lt lu hr lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ig ih ii ij ik"><h1 id="9b62" class="ma mb in bd mc md me mf mg mh mi mj mk kc ml kd mm kf mn kg mo ki mp kj mq mr bi translated">开始大吃</h1><p id="9aad" class="pw-post-body-paragraph kx ky in kz b la ms jx lc ld mt ka lf lg mu li lj lk mv lm ln lo mw lq lr ls ig bi translated">首先，您需要为 Zookeeper 和 Kafka 提取 Docker 图像——Zookeeper 需要作为 Kafka 的管弦乐手。我更喜欢 Bitnami 图像，因为它们更紧凑，记录更好。</p><pre class="km kn ko kp gt nl nm nn no aw np bi"><span id="481e" class="nq mb in nm b gy nr ns l nt nu">docker pull bitnami/zookeeper:latest<br/>docker pull bitnami/kafka:latest</span></pre><p id="6d1e" class="pw-post-body-paragraph kx ky in kz b la lb jx lc ld le ka lf lg lh li lj lk ll lm ln lo lp lq lr ls ig bi translated">你还必须为 Scala 和 Docker 安装 IntelliJ 插件。进入 IntelliJ，按照下图所示安装插件。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi nv"><img src="../Images/a125a22576f0c74256c55f27447dfaa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lUAFUiQnmRUlPiuay-a5nA.png"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk translated">如何搜索和找到 Scala 插件</figcaption></figure><p id="ab24" class="pw-post-body-paragraph kx ky in kz b la lb jx lc ld le ka lf lg lh li lj lk ll lm ln lo lp lq lr ls ig bi translated">现在让我们创建一个新项目。我们将使用 Scala 插件和 SBT 构建工具。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi oa"><img src="../Images/3a38a785b68da24a566c9a9ce765e596.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S3ChVztAWXn-slD8fVs9CA.png"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk translated">如何创建新项目</figcaption></figure><p id="3566" class="pw-post-body-paragraph kx ky in kz b la lb jx lc ld le ka lf lg lh li lj lk ll lm ln lo lp lq lr ls ig bi translated">在下一个窗口中选择名称、位置、JDK、SBT 版本和 Scala 版本。现在，让我们为我们的项目添加 SBT 需求。打开<code class="fe ob oc od nm b">build.sbt</code>并将这些行添加到<code class="fe ob oc od nm b">libraryDependencies</code>:</p><pre class="km kn ko kp gt nl nm nn no aw np bi"><span id="9e44" class="nq mb in nm b gy nr ns l nt nu">"org.apache.spark" %% "spark-core" % "3.2.0"<br/>"org.apache.spark" %% "spark-sql" % "3.2.0"<br/>"org.apache.spark" %% "spark-sql-kafka-0-10" % "3.2.0"<br/>"org.apache.spark" %% "spark-streaming-kafka-0-10" % "3.2.0"<br/>"org.apache.spark" %% "spark-streaming-kafka-0-10-assembly" %"3.2.0"</span></pre><p id="2759" class="pw-post-body-paragraph kx ky in kz b la lb jx lc ld le ka lf lg lh li lj lk ll lm ln lo lp lq lr ls ig bi translated">然后按窗口右上角的刷新按钮来解决依赖性。一旦解决了这些依赖关系，我们就可以开始编码了——这是最激动人心的部分。</p><p id="d6c9" class="pw-post-body-paragraph kx ky in kz b la lb jx lc ld le ka lf lg lh li lj lk ll lm ln lo lp lq lr ls ig bi translated">在项目的根目录下创建一个目录，并将其命名为<code class="fe ob oc od nm b">docker-clusters</code>。在里面，创建另一个名为<code class="fe ob oc od nm b">data</code>的目录。除此之外，创建一个名为<code class="fe ob oc od nm b">docker-compose.yml</code>的文件并打开它(IntelliJ 会自动打开它)。你需要在里面写下这些代码:</p><pre class="km kn ko kp gt nl nm nn no aw np bi"><span id="e18b" class="nq mb in nm b gy nr ns l nt nu">version: "1.0"<br/>services:<br/>  zookeeper:<br/>    image: bitnami/zookeeper:3<br/>    ports:<br/>      - '2181:2181'<br/>    environment:<br/>      - ALLOW_ANONYMOUS_LOGIN=yes<br/>  kafka:<br/>    image: bitnami/kafka:3<br/>    ports:<br/>      - '9092:9092'<br/>    environment:<br/>      - KAFKA_BROKER_ID=1<br/>      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181<br/>      - ALLOW_PLAINTEXT_LISTENER=yes<br/>      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://127.0.0.1:9092<br/>      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092<br/>    depends_on:<br/>      - zookeeper<br/>    volumes:<br/>      - ./data:/opt/spark-data</span></pre><p id="8282" class="pw-post-body-paragraph kx ky in kz b la lb jx lc ld le ka lf lg lh li lj lk ll lm ln lo lp lq lr ls ig bi translated">我们将使用这个脚本在 Docker 中创建我们需要的容器。</p><p id="446e" class="pw-post-body-paragraph kx ky in kz b la lb jx lc ld le ka lf lg lh li lj lk ll lm ln lo lp lq lr ls ig bi translated">是时候创建一个 Python 脚本了，这样我们就可以将样本流数据分成两个主题进行连接。</p><pre class="km kn ko kp gt nl nm nn no aw np bi"><span id="1aaa" class="nq mb in nm b gy nr ns l nt nu">from kafka import KafkaProducer<br/>from datetime import datetime<br/>import time, random<br/><br/>topics = input("Enter topic name: ").strip().split(" ")<br/>producer = KafkaProducer('localhost:9092', api_version=(0, 10, 2))<br/><br/><br/>for i in range(1,100):<br/>   time_ = datetime.now().strftime("%H:%M:%S")<br/>   for topic in topics:<br/>      ran = random.randint(5,15)<br/>      text = f"{ran} topic_{topic}_row_{i} {time_}"<br/>      producer.send(topic, bytes(text, "utf-8"))<br/>   time.sleep(1)</span></pre><p id="f3fd" class="pw-post-body-paragraph kx ky in kz b la lb jx lc ld le ka lf lg lh li lj lk ll lm ln lo lp lq lr ls ig bi translated">该脚本将首先从控制台获取主题名称，然后每秒向每个主题推送一条记录。该记录包括一个作为 ID 的随机数、一个文本和一个时间。我们将在 Spark 中进一步解析这个结构。</p><p id="e3ab" class="pw-post-body-paragraph kx ky in kz b la lb jx lc ld le ka lf lg lh li lj lk ll lm ln lo lp lq lr ls ig bi translated">现在让我们在卡夫卡中创建 2 个主题。分别命名为<code class="fe ob oc od nm b">stream-1</code>和<code class="fe ob oc od nm b">stream-2</code>:</p><pre class="km kn ko kp gt nl nm nn no aw np bi"><span id="2632" class="nq mb in nm b gy nr ns l nt nu">&gt;  kafka-topics.sh --create --topic stream-1 --bootstrap-server localhost:9092<br/>&gt;  kafka-topics.sh --create --topic stream-2 --bootstrap-server localhost:9092</span></pre><p id="aaf7" class="pw-post-body-paragraph kx ky in kz b la lb jx lc ld le ka lf lg lh li lj lk ll lm ln lo lp lq lr ls ig bi translated">让我们开始编写 Scala 代码。首先，在<code class="fe ob oc od nm b">src/main/scala/</code>中你的项目内部创建一个文件，命名为<code class="fe ob oc od nm b">TwoKafkaStreams</code>。然后在这个文件里面，创建一个<code class="fe ob oc od nm b">SparkSession</code>:</p><pre class="km kn ko kp gt nl nm nn no aw np bi"><span id="188f" class="nq mb in nm b gy nr ns l nt nu">val <em class="oe">spark</em>: SparkSession = SparkSession.<em class="oe">builder</em>()<br/>  .appName("KafkaStreamsJoin")<br/>  .config("spark.master", "local")<br/>  .getOrCreate()</span></pre><p id="bbb4" class="pw-post-body-paragraph kx ky in kz b la lb jx lc ld le ka lf lg lh li lj lk ll lm ln lo lp lq lr ls ig bi translated">还要为它设置日志级别，因为 Spark 会为流加入操作生成大量日志:</p><pre class="km kn ko kp gt nl nm nn no aw np bi"><span id="5b69" class="nq mb in nm b gy nr ns l nt nu"><em class="oe">spark</em>.sparkContext.setLogLevel("WARN")</span></pre><p id="05aa" class="pw-post-body-paragraph kx ky in kz b la lb jx lc ld le ka lf lg lh li lj lk ll lm ln lo lp lq lr ls ig bi translated">我们将使用<code class="fe ob oc od nm b">spark</code>来访问 Spark API。然后，我们可以使用火花引擎进行读取、写入和处理。我们该从题目中读取数据了。我将为此创建一个函数，以便我们可以重用它。首款进口 Spark 隐式转换器:</p><pre class="km kn ko kp gt nl nm nn no aw np bi"><span id="cae5" class="nq mb in nm b gy nr ns l nt nu">import <em class="oe">spark</em>.implicits._</span><span id="9d19" class="nq mb in nm b gy of ns l nt nu">def readFromKafka(topic: String): DataFrame = <em class="oe">spark</em>.readStream<br/>  .format("kafka")<br/>  .option("kafka.bootstrap.servers", "localhost:9092")<br/>  .option("subscribe", topic)<br/>  .option("startingOffsets", "earliest")<br/>  .load()<br/>  .selectExpr("CAST(value AS STRING)").as[String]<br/>  .splitKafkaValue()</span></pre><p id="89e5" class="pw-post-body-paragraph kx ky in kz b la lb jx lc ld le ka lf lg lh li lj lk ll lm ln lo lp lq lr ls ig bi translated">这个管道将 Kafka 数据作为一个流读取，并且每次运行时将从主题的开头开始。它还将记录类型转换为字符串，因此函数的返回类型变成了<code class="fe ob oc od nm b">Dataset[String]</code>。最后一个方法<code class="fe ob oc od nm b">splitKafkaValue()</code>现在还没有定义，我们来定义一下:</p><pre class="km kn ko kp gt nl nm nn no aw np bi"><span id="a857" class="nq mb in nm b gy nr ns l nt nu">implicit class dsUtils[T](ds: Dataset[T]) {<br/>  def consolePrint(mode: String): Unit = ds<br/>    .writeStream<br/>    .format("console")<br/>    .outputMode(mode)<br/>    .start()<br/>    .awaitTermination()<br/><br/>  def splitKafkaValue(): DataFrame = ds<br/>    .withColumn("split_", <em class="oe">split</em>(<em class="oe">col</em>("value"), " "))<br/>    .select(<br/>      <em class="oe">col</em>("split_").getItem(0).as("id"),<br/>      <em class="oe">col</em>("split_").getItem(1).as("name"),<br/>      <em class="oe">col</em>("split_").getItem(2).as("time")<br/>    )<br/>}</span></pre><p id="1169" class="pw-post-body-paragraph kx ky in kz b la lb jx lc ld le ka lf lg lh li lj lk ll lm ln lo lp lq lr ls ig bi translated">这是一个隐式类，所以每个<code class="fe ob oc od nm b">DataFrame</code>都可以使用它的方法。它丰富了<code class="fe ob oc od nm b">dataframe</code>并增加了它的功能。</p><p id="f290" class="pw-post-body-paragraph kx ky in kz b la lb jx lc ld le ka lf lg lh li lj lk ll lm ln lo lp lq lr ls ig bi translated"><code class="fe ob oc od nm b">consolePrint</code>方法将把流数据打印到控制台中，因此我们可以看到它们。它从用户那里获得写模式。另一个<code class="fe ob oc od nm b">splitKafkaValue</code>方法拆分并解析 Kafka 记录，使其成为一个具有 3 列的<code class="fe ob oc od nm b">dataframe</code>:<code class="fe ob oc od nm b">id</code>、<code class="fe ob oc od nm b">name</code>和<code class="fe ob oc od nm b">time</code>。如您所见，我们执行了一次拆分，并从该拆分中派生出所有列。</p><p id="b4e4" class="pw-post-body-paragraph kx ky in kz b la lb jx lc ld le ka lf lg lh li lj lk ll lm ln lo lp lq lr ls ig bi translated">现在我们已经有了所有这些函数，让我们进入<code class="fe ob oc od nm b">main</code>程序。我们需要一个管道来读取每个主题，连接它们，然后将结果打印到控制台。</p><pre class="km kn ko kp gt nl nm nn no aw np bi"><span id="299d" class="nq mb in nm b gy nr ns l nt nu">val kafkaStream1 = <em class="oe">readFromKafka</em>("stream-1")<br/>val kafkaStream2 = <em class="oe">readFromKafka</em>("stream-2")</span><span id="1b1a" class="nq mb in nm b gy of ns l nt nu">val predicate = kafkaStream1.col("id") === kafkaStream2.col("id")<br/>kafkaStream1.join(kafkaStream2, predicate).consolePrint("append")<br/>kafkaStream1.consolePrint("append")</span></pre><p id="3a88" class="pw-post-body-paragraph kx ky in kz b la lb jx lc ld le ka lf lg lh li lj lk ll lm ln lo lp lq lr ls ig bi translated">最后，打开 bash 终端，将目录切换到您的<code class="fe ob oc od nm b">docker-clusters</code>文件夹，然后执行:</p><pre class="km kn ko kp gt nl nm nn no aw np bi"><span id="afcf" class="nq mb in nm b gy nr ns l nt nu">docker-compose up</span></pre><p id="4ad8" class="pw-post-body-paragraph kx ky in kz b la lb jx lc ld le ka lf lg lh li lj lk ll lm ln lo lp lq lr ls ig bi translated">这将运行您的容器并为您打开 Kafka 集群。到那时，你就可以运行你的 Scala 程序了。接下来，运行 Python 脚本将数据推入 Kafka 主题。</p><p id="a1a1" class="pw-post-body-paragraph kx ky in kz b la lb jx lc ld le ka lf lg lh li lj lk ll lm ln lo lp lq lr ls ig bi translated">它会读取你推送到 Kafka 主题中的任何内容，通过 ID 加入它们，并在控制台中打印出来。</p></div><div class="ab cl lt lu hr lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ig ih ii ij ik"><h1 id="310c" class="ma mb in bd mc md me mf mg mh mi mj mk kc ml kd mm kf mn kg mo ki mp kj mq mr bi translated">结尾部分</h1><p id="3aab" class="pw-post-body-paragraph kx ky in kz b la ms jx lc ld mt ka lf lg mu li lj lk mv lm ln lo mw lq lr ls ig bi translated">我们使用 Kafka 和 Spark 来模拟事件处理系统中的一个常见用例。我们构建了两个流源，并加入它们来检查它们的公共 id。你可以用你自己的创新去发现更复杂的用例。</p><p id="8b38" class="pw-post-body-paragraph kx ky in kz b la lb jx lc ld le ka lf lg lh li lj lk ll lm ln lo lp lq lr ls ig bi translated">我是一名数据工程师，有电信行业的工作背景。处理大量数据和学习大数据系统的热情将我带到了这个仙境。随时给我写反馈，联系我找到更复杂的挑战<a class="ae og" href="http://www.linkedin.com/in/ali-t-asl/" rel="noopener ugc nofollow" target="_blank">这里</a>。</p></div></div>    
</body>
</html>