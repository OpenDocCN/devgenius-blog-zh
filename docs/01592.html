<html>
<head>
<title>Visualizations of Machine Learning Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习模型的可视化</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/visualizations-of-machine-learning-models-bec2f0bfa496?source=collection_archive---------20-----------------------#2020-07-07">https://blog.devgenius.io/visualizations-of-machine-learning-models-bec2f0bfa496?source=collection_archive---------20-----------------------#2020-07-07</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><div class=""><h2 id="da6f" class="pw-subtitle-paragraph jk im in bd b jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb dk translated">机器学习没那么难</h2></div><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi kc"><img src="../Images/79d0028a3f9ec117032e0b40c0201905.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4TlEpBBJCJyfSyzcb3RENA.png"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">不然我怎么能抓住你的注意力呢？</figcaption></figure><p id="0f55" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">现在我可能已经疏远了我的所有观众，因为没有一个头脑正常的人认为机器学习是容易的，让我来解释一下。</p><p id="1a9d" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">机器学习这个术语是一个令人生畏的术语——几乎每次我在 ML 项目中工作时，我都会查找关于如何执行特定模型的代码。但是我要和你分享一个秘密:</p><blockquote class="lo lp lq"><p id="a621" class="ks kt lr ku b kv kw jo kx ky kz jr la ls lc ld le lt lg lh li lu lk ll lm ln ig bi translated">“大多数你向其展示你的发现的人不知道线性和逻辑之间的区别。他们也不在乎。”</p></blockquote><p id="7e92" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">因此，在你创建一个新的分类器“<strong class="ku io">我放弃机器学习</strong>”之前，一个可能相当准确的模型，取决于用例(笑话)，尝试一下本文中概述的例子！</p><h1 id="8402" class="lv lw in bd lx ly lz ma mb mc md me mf jt mg ju mh jw mi jx mj jz mk ka ml mm bi translated">假设您已经有了数据并准备好了</h1><p id="d614" class="pw-post-body-paragraph ks kt in ku b kv mn jo kx ky mo jr la lb mp ld le lf mq lh li lj mr ll lm ln ig bi translated">您是否已经完成了设置工作——通过任何必要的方式获取数据、格式化数据、添加列、删除空值、转换数据类型、执行一些初步分析(散点图、热图、面积图等)。)?</p><p id="1545" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">我将使用欺诈数据集。它来自 ka ggle——我会使用一些我自己从现在/以前的公司得到的数据，但是……关于 PII 的一些东西，它可能会让我被解雇，所以是的。我们将使用公共数据集，没什么大不了的。</p><pre class="kd ke kf kg gt ms mt mu mv aw mw bi"><span id="bdd3" class="mx lw in mt b gy my mz l na nb"># Print the shape of the data<br/>data = pd.read_csv('creditcard.csv')<br/>data = data.sample(frac=0.1, random_state = 1)<br/>print(data.columns)<br/>print(data.shape)<br/>print(data.describe())</span><span id="d595" class="mx lw in mt b gy nc mz l na nb"># V1 - V28 are the results of a PCA Dimensionality reduction to protect user identities and sensitive features</span></pre><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/0b036444da0958a0a2001b9c3768c069.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*zDJ_bX-QlpRM3YsMTZpxkQ.png"/></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">请打印出来</figcaption></figure><h1 id="4fb4" class="lv lw in bd lx ly lz ma mb mc md me mf jt mg ju mh jw mi jx mj jz mk ka ml mm bi translated">太好了，现在你的数据是什么样的？</h1><p id="1db0" class="pw-post-body-paragraph ks kt in ku b kv mn jo kx ky mo jr la lb mp ld le lf mq lh li lj mr ll lm ln ig bi translated">一些表格可以让你感受一下你的数据框架:</p><pre class="kd ke kf kg gt ms mt mu mv aw mw bi"><span id="78f1" class="mx lw in mt b gy my mz l na nb">df = data<br/># df.head()</span><span id="f049" class="mx lw in mt b gy nc mz l na nb">table_bar = df.head(10).style.bar(color='lightblue')<br/>table_bar</span></pre><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi ne"><img src="../Images/0653e368f191bf8823f224edf506dcf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M6idw4ZHCVb9iRcL5qYkoQ.png"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">病床。生病的</figcaption></figure><pre class="kd ke kf kg gt ms mt mu mv aw mw bi"><span id="aa2a" class="mx lw in mt b gy my mz l na nb">gradient = df.head(10).style.background_gradient(cmap='Blues')<br/>gradient</span></pre><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi nf"><img src="../Images/b17b5edbee599a4cbc28edd29031ff91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dc8A8dlZLnrK-vrBUZqCwg.png"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">蓝色的！</figcaption></figure><h1 id="205b" class="lv lw in bd lx ly lz ma mb mc md me mf jt mg ju mh jw mi jx mj jz mk ka ml mm bi translated">初步分析:</h1><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi ng"><img src="../Images/ae2e8f21a8a17bc4297357ac4ceceb9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fefBAfyBvz0VAmLz2SE1aw.png"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">柱状图</figcaption></figure><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/b992bcec0f9e72cc282e6cf7a24e3b6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/1*2LNzeMTkXJP8nsQpVK22gA.png"/></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">热图</figcaption></figure><pre class="kd ke kf kg gt ms mt mu mv aw mw bi"><span id="d2ce" class="mx lw in mt b gy my mz l na nb"># Plot histograms of each parameter </span><span id="074d" class="mx lw in mt b gy nc mz l na nb">data.hist(figsize = (20, 20))</span><span id="9c65" class="mx lw in mt b gy nc mz l na nb">plt.show()</span><span id="4a21" class="mx lw in mt b gy nc mz l na nb"># Correlation matrix</span><span id="891e" class="mx lw in mt b gy nc mz l na nb">corrmat = data.corr()<br/>fig = plt.figure(figsize = (12, 9))</span><span id="0db4" class="mx lw in mt b gy nc mz l na nb">sns.heatmap(corrmat, vmax = .8, square = True)</span><span id="a87f" class="mx lw in mt b gy nc mz l na nb">plt.show()</span></pre><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi ni"><img src="../Images/54ae6dcf227da8d1d4208569d2ce8ccf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VraCB2cJoDfCCR5Rp80VFQ.png"/></div></div></figure><pre class="kd ke kf kg gt ms mt mu mv aw mw bi"><span id="86d0" class="mx lw in mt b gy my mz l na nb">sns.pairplot(data)</span></pre><h1 id="0066" class="lv lw in bd lx ly lz ma mb mc md me mf jt mg ju mh jw mi jx mj jz mk ka ml mm bi translated">又一个直观的初步视觉——树状图</h1><p id="5cd3" class="pw-post-body-paragraph ks kt in ku b kv mn jo kx ky mo jr la lb mp ld le lf mq lh li lj mr ll lm ln ig bi translated">我是出了名的懒，在学习到完美的时候。一开始我看起来非常有动力，一旦我发现了足够的信息让我到达终点，也许更远一点，我就出局了。这些模型大部分是我在网上找到的，并插入了我自己的数据。我只是不明白你为什么要从头开始写？我甚至不相信我自己写的 ML 模型，看来这是唯一的出路。</p><p id="94a5" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">*是的，这就是人们在创造一些视觉效果时大肆宣传的同一个虹膜数据集。别担心，我用下面不同的数据重新创建。</p><pre class="kd ke kf kg gt ms mt mu mv aw mw bi"><span id="af90" class="mx lw in mt b gy my mz l na nb">#dendrogram<br/>from scipy.cluster.hierarchy import dendrogram<br/>from sklearn.datasets import load_iris<br/>from sklearn.cluster import AgglomerativeClustering</span><span id="0b9b" class="mx lw in mt b gy nc mz l na nb">def plot_dendrogram(model, **kwargs):<br/>    # Create linkage matrix and then plot the dendrogram</span><span id="10cd" class="mx lw in mt b gy nc mz l na nb"># create the counts of samples under each node<br/>    counts = np.zeros(model.children_.shape[0])<br/>    n_samples = len(model.labels_)<br/>    for i, merge in enumerate(model.children_):<br/>        current_count = 0<br/>        for child_idx in merge:<br/>            if child_idx &lt; n_samples:<br/>                current_count += 1  # leaf node<br/>            else:<br/>                current_count += counts[child_idx - n_samples]<br/>        counts[i] = current_count</span><span id="7451" class="mx lw in mt b gy nc mz l na nb">linkage_matrix = np.column_stack([model.children_, model.distances_,<br/>                                      counts]).astype(float)</span><span id="fc9a" class="mx lw in mt b gy nc mz l na nb"># Plot the corresponding dendrogram<br/>    dendrogram(linkage_matrix, **kwargs)</span><span id="924a" class="mx lw in mt b gy nc mz l na nb">iris = load_iris()<br/>X = iris.data</span><span id="97e9" class="mx lw in mt b gy nc mz l na nb"># setting distance_threshold=0 ensures we compute the full tree.<br/>model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)</span><span id="f519" class="mx lw in mt b gy nc mz l na nb">model = model.fit(X)<br/>plt.title('Hierarchical Clustering Dendrogram')<br/># plot the top three levels of the dendrogram<br/>plot_dendrogram(model, truncate_mode='level', p=3)<br/>plt.xlabel("Number of points in node (or index of point if no parenthesis).")<br/>plt.show()</span></pre><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/9e3b8e7750409024c6049d8eb097a647.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*OWfmTgzM5wJwdM_ANhbB_A.png"/></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">没错。这一切都是为了这个。</figcaption></figure><h1 id="c391" class="lv lw in bd lx ly lz ma mb mc md me mf jt mg ju mh jw mi jx mj jz mk ka ml mm bi translated">下一步——准备好你的数据 ML(Xs，Ys，Trains，Tests，Targets 等)。)</h1><pre class="kd ke kf kg gt ms mt mu mv aw mw bi"><span id="cdda" class="mx lw in mt b gy my mz l na nb"># Determine number of fraud cases in dataset<br/>Fraud = data[data['Class'] == 1]<br/>Valid = data[data['Class'] == 0]</span><span id="d7a7" class="mx lw in mt b gy nc mz l na nb">outlier_fraction = len(Fraud)/float(len(Valid))<br/>print(outlier_fraction)</span><span id="63d9" class="mx lw in mt b gy nc mz l na nb">print('Fraud Cases: {}'.format(len(data[data['Class'] == 1])))<br/>print('Valid Transactions: {}'.format(len(data[data['Class'] == 0])))</span><span id="2703" class="mx lw in mt b gy nc mz l na nb">print(Fraud.head())<br/>print(Valid.head())</span><span id="5326" class="mx lw in mt b gy nc mz l na nb"># Get all the columns from the dataFrame<br/>columns = data.columns.tolist()</span><span id="e7e8" class="mx lw in mt b gy nc mz l na nb"># Filter the columns to remove data we do not want<br/>columns = [c for c in columns if c not in ["Class"]]</span><span id="e787" class="mx lw in mt b gy nc mz l na nb"># Store the variable we'll be predicting on<br/>target = "Class"</span><span id="bdee" class="mx lw in mt b gy nc mz l na nb">X = data[columns]<br/>Y = data[target]</span><span id="e7a1" class="mx lw in mt b gy nc mz l na nb"># Print shapes<br/>print(X.shape)<br/>print(Y.shape)<br/>print(columns)<br/>print(X.head())<br/>print(Y.head())</span></pre><h1 id="b824" class="lv lw in bd lx ly lz ma mb mc md me mf jt mg ju mh jw mi jx mj jz mk ka ml mm bi translated">下一个即插即用—我的最爱</h1><p id="839e" class="pw-post-body-paragraph ks kt in ku b kv mn jo kx ky mo jr la lb mp ld le lf mq lh li lj mr ll lm ln ig bi translated"><strong class="ku io">决策树分类器</strong></p><pre class="kd ke kf kg gt ms mt mu mv aw mw bi"><span id="4de9" class="mx lw in mt b gy my mz l na nb">#decisiontreeclassifier <br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.model_selection import train_test_split</span><span id="7454" class="mx lw in mt b gy nc mz l na nb"># Create the training and test sets<br/>X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=123)</span><span id="5c63" class="mx lw in mt b gy nc mz l na nb"># Instantiate the classifier: dt_clf_4<br/>dt_clf_4 = DecisionTreeClassifier(random_state=123, max_depth=4)</span><span id="361a" class="mx lw in mt b gy nc mz l na nb"># Fit the classifier to the training set<br/>dt_clf_4.fit(X_train,y_train)</span><span id="dbad" class="mx lw in mt b gy nc mz l na nb"># Predict the labels of the test set: y_pred_4<br/>y_pred_4 = dt_clf_4.predict(X_test)</span><span id="8c1a" class="mx lw in mt b gy nc mz l na nb"># Compute the accuracy of the predictions: accuracy<br/>accuracy = float(np.sum(y_pred_4==y_test))/y_test.shape[0]<br/>print("accuracy:", accuracy)</span></pre><blockquote class="lo lp lq"><p id="fc9e" class="ks kt lr ku b kv kw jo kx ky kz jr la ls lc ld le lt lg lh li lu lk ll lm ln ig bi translated">精确度:0.9984612609</p></blockquote><p id="6aa3" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated"><strong class="ku io">极端梯度提升分类器(可以说是分类的最佳算法)</strong></p><pre class="kd ke kf kg gt ms mt mu mv aw mw bi"><span id="d240" class="mx lw in mt b gy my mz l na nb">#xgboost and decisiontreeclassifier<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.tree import DecisionTreeClassifier<br/>import xgboost as xgb</span><span id="6757" class="mx lw in mt b gy nc mz l na nb">state=1</span><span id="798b" class="mx lw in mt b gy nc mz l na nb">#xgboost<br/>X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)<br/>xg_cl = xgb.XGBClassifier(objective='binary:logistic',n_estimators=10,seed=state)<br/>xg_cl.fit(X_train, y_train)</span><span id="6405" class="mx lw in mt b gy nc mz l na nb">#decisiontreeclassifier<br/>dt_clf_4 = DecisionTreeClassifier(random_state=123, max_depth=4)<br/>dt_clf_4.fit(X_train,y_train)</span><span id="ff2e" class="mx lw in mt b gy nc mz l na nb"># Predict the labels of the test set: y_pred_4<br/>preds = xg_cl.predict(X_test)<br/>y_pred_4 = dt_clf_4.predict(X_test)<br/>accuracy_xg_cl = float(np.sum(preds==y_test))/y_test.shape[0]<br/>accuracy_dt_clf_4 = float(np.sum(y_pred_4==y_test))/y_test.shape[0]</span><span id="c397" class="mx lw in mt b gy nc mz l na nb">#print it<br/>print("accuracy_dt_clf_4:", accuracy)<br/>print("accuracy_xg_cl: %f" % (accuracy))</span></pre><blockquote class="lo lp lq"><p id="8320" class="ks kt lr ku b kv kw jo kx ky kz jr la ls lc ld le lt lg lh li lu lk ll lm ln ig bi translated">accuracy _ dt _ clf _ 4:0.9989468141126909<br/>accuracy _ XG _ cl:0.998947</p></blockquote><p id="2e59" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated"><strong class="ku io">xgboost 的更多例子</strong></p><pre class="kd ke kf kg gt ms mt mu mv aw mw bi"><span id="553e" class="mx lw in mt b gy my mz l na nb">#xgboost</span><span id="de95" class="mx lw in mt b gy nc mz l na nb"># import xgboost as xgb<br/># import pandas as pd</span><span id="2929" class="mx lw in mt b gy nc mz l na nb"># Create arrays for the features and the target: X, y<br/>#X, y = df.iloc[:,:-1], df.iloc[:,-1]</span><span id="a022" class="mx lw in mt b gy nc mz l na nb"># Create the DMatrix from X and y: churn_dmatrix<br/>churn_dmatrix = xgb.DMatrix(data=data.iloc[:,:-1], label=data.Class)<br/>data.iloc[1]<br/>X.iloc[1]</span><span id="6f7b" class="mx lw in mt b gy nc mz l na nb"># Create the parameter dictionary: params<br/>params = {"objective":"reg:logistic", "max_depth":3}</span><span id="9e35" class="mx lw in mt b gy nc mz l na nb"># Perform cross-validation: cv_results<br/>cv_results_error = xgb.cv(dtrain=churn_dmatrix,params=params,\<br/>                          nfold=3,num_boost_round=5,metrics="error",as_pandas=True,seed=123)<br/>cv_results_auc = xgb.cv(dtrain=churn_dmatrix,params=params,\<br/>                        nfold=3,num_boost_round=5,metrics="auc", as_pandas=True, seed=123)</span><span id="ba9a" class="mx lw in mt b gy nc mz l na nb"># Print cv_results<br/>print(cv_results_error)<br/>print(cv_results_auc)</span><span id="4caf" class="mx lw in mt b gy nc mz l na nb"># Print the %<br/>print(((1-cv_results_error["test-error-mean"]).iloc[-1]))<br/>print((cv_results_auc["test-auc-mean"]).iloc[-1])</span></pre><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/be8edfe11aea125d0cc8846e610b4b6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*LK6Tdb9xtjGyd6VtAgTdmg.png"/></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">大话等于大文。好吧，没有。</figcaption></figure><p id="e663" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated"><strong class="ku io">隔离林和局部离群因子</strong></p><pre class="kd ke kf kg gt ms mt mu mv aw mw bi"><span id="3b99" class="mx lw in mt b gy my mz l na nb">#isolationforest and localoutlierfactor<br/>from sklearn.metrics import classification_report, accuracy_score<br/>from sklearn.ensemble import IsolationForest<br/>from sklearn.neighbors import LocalOutlierFactor</span><span id="e5ca" class="mx lw in mt b gy nc mz l na nb"># define random states<br/>state = 1</span><span id="514e" class="mx lw in mt b gy nc mz l na nb"># define outlier detection tools to be compared<br/>classifiers = {<br/>    "Isolation Forest": IsolationForest(max_samples=len(X),<br/>                                        contamination=outlier_fraction,<br/>                                        random_state=state),<br/>    "Local Outlier Factor": LocalOutlierFactor(n_neighbors=20,<br/>                                               contamination=outlier_fraction)<br/>}</span><span id="7f1b" class="mx lw in mt b gy nc mz l na nb">n_outliers = len(Fraud)<br/>for i, (clf_name,clf) in enumerate(classifiers.items()):<br/>    #Fit the data and tag outliers<br/>    if clf_name == "Local Outlier Factor":<br/>        y_pred = clf.fit_predict(X)<br/>        scores_prediction = clf.negative_outlier_factor_<br/>    elif clf_name == "Support Vector Machine":<br/>        clf.fit(X)<br/>        y_pred = clf.predict(X)<br/>    else:    <br/>        clf.fit(X)<br/>        scores_prediction = clf.decision_function(X)<br/>        y_pred = clf.predict(X)<br/>    #Reshape the prediction values to 0 for Valid transactions , 1 for Fraud transactions<br/>    y_pred[y_pred == 1] = 0<br/>    y_pred[y_pred == -1] = 1<br/>    n_errors = (y_pred != Y).sum()<br/>    # Run Classification Metrics<br/>    print("{}: {}".format(clf_name,n_errors))<br/>    print("Accuracy Score :")<br/>    print(accuracy_score(Y,y_pred))<br/>    print("Classification Report :")<br/>    print(classification_report(Y,y_pred))</span></pre><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/4c7fd5b489bff574117c99136de1e4c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*b40BMaGcywMaI-ArU2gPXg.png"/></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">使用 f1 分数—精确度和召回率的组合(分别为 28%和 2%)</figcaption></figure><p id="0976" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">克内堡分级机</p><pre class="kd ke kf kg gt ms mt mu mv aw mw bi"><span id="d083" class="mx lw in mt b gy my mz l na nb">#KNeighborsClassifier predicting labels<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.model_selection import train_test_split</span><span id="f5cf" class="mx lw in mt b gy nc mz l na nb">#train test<br/>X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)</span><span id="3574" class="mx lw in mt b gy nc mz l na nb"># Create arrays for the features and the response variable<br/>y = data['Class'].values<br/>X = data.drop('Class', axis=1).values</span><span id="dc29" class="mx lw in mt b gy nc mz l na nb">y<br/>X</span><span id="80cc" class="mx lw in mt b gy nc mz l na nb"># Create a k-NN classifier with 6 neighbors<br/>knn = KNeighborsClassifier(n_neighbors=6)</span><span id="dd0c" class="mx lw in mt b gy nc mz l na nb"># Fit the classifier to the data<br/>knn.fit(X,y)</span><span id="0b87" class="mx lw in mt b gy nc mz l na nb"># Predict the labels for the training data X<br/>y_pred = knn.predict(X)<br/>y_pred</span><span id="aa1d" class="mx lw in mt b gy nc mz l na nb">print("Prediction: {}".format(y_pred))<br/>print(knn.score(X_test, y_test))</span></pre><blockquote class="lo lp lq"><p id="9650" class="ks kt lr ku b kv kw jo kx ky kz jr la ls lc ld le lt lg lh li lu lk ll lm ln ig bi translated">预测:[0 0 0...0 0 0]<br/>0 . 54860 . 48868888661</p></blockquote><h1 id="9832" class="lv lw in bd lx ly lz ma mb mc md me mf jt mg ju mh jw mi jx mj jz mk ka ml mm bi translated">好的，但是你怎么知道使用哪种模型呢？</h1><p id="7972" class="pw-post-body-paragraph ks kt in ku b kv mn jo kx ky mo jr la lb mp ld le lf mq lh li lj mr ll lm ln ig bi translated">很高兴你这么问，<code class="fe nm nn no mt b">Sklearn</code>人们已经为测试你的模型编写了一些非常酷的代码。</p><pre class="kd ke kf kg gt ms mt mu mv aw mw bi"><span id="7673" class="mx lw in mt b gy my mz l na nb">import numpy as np<br/>import matplotlib.pyplot as plt<br/>from matplotlib.colors import ListedColormap<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.datasets import make_moons, make_circles, make_classification<br/>from sklearn.neural_network import MLPClassifier<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.svm import SVC<br/>from sklearn.gaussian_process import GaussianProcessClassifier<br/>from sklearn.gaussian_process.kernels import RBF<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier<br/>from sklearn.naive_bayes import GaussianNB<br/>from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis</span><span id="9397" class="mx lw in mt b gy nc mz l na nb">h = .02  # step size in the mesh</span><span id="b737" class="mx lw in mt b gy nc mz l na nb">names = ["Nearest Neighbors", "Linear SVM", "RBF SVM", "Gaussian Process",<br/>         "Decision Tree", "Random Forest", "Neural Net", "AdaBoost",<br/>         "Naive Bayes", "QDA"]</span><span id="2701" class="mx lw in mt b gy nc mz l na nb">classifiers = [<br/>    KNeighborsClassifier(3),<br/>    SVC(kernel="linear", C=0.025),<br/>    SVC(gamma=2, C=1),<br/>    GaussianProcessClassifier(1.0 * RBF(1.0)),<br/>    DecisionTreeClassifier(max_depth=5),<br/>    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),<br/>    MLPClassifier(alpha=1, max_iter=1000),<br/>    AdaBoostClassifier(),<br/>    GaussianNB(),<br/>    QuadraticDiscriminantAnalysis()]</span><span id="903a" class="mx lw in mt b gy nc mz l na nb">X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,<br/>                           random_state=1, n_clusters_per_class=1)<br/>rng = np.random.RandomState(2)<br/>X += 2 * rng.uniform(size=X.shape)<br/>linearly_separable = (X, y)</span><span id="c449" class="mx lw in mt b gy nc mz l na nb">datasets = [make_moons(noise=0.3, random_state=0),<br/>            make_circles(noise=0.2, factor=0.5, random_state=1),<br/>            linearly_separable<br/>            ]</span><span id="fb1b" class="mx lw in mt b gy nc mz l na nb">figure = plt.figure(figsize=(30, 12))</span><span id="91c0" class="mx lw in mt b gy nc mz l na nb">i = 1<br/># iterate over datasets<br/>for ds_cnt, ds in enumerate(datasets):<br/>    # preprocess dataset, split into training and test part<br/>    X, y = ds<br/>    X = StandardScaler().fit_transform(X)<br/>    X_train, X_test, y_train, y_test = \<br/>        train_test_split(X, y, test_size=.4, random_state=42)</span><span id="0401" class="mx lw in mt b gy nc mz l na nb">x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5<br/>    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5<br/>    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),<br/>                         np.arange(y_min, y_max, h))</span><span id="b1c8" class="mx lw in mt b gy nc mz l na nb"># just plot the dataset first<br/>    cm = plt.cm.winter<br/>    cm_bright = ListedColormap(['#3B5ADC', '#67CDD9'])<br/>    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)<br/>    if ds_cnt == 0:<br/>        ax.set_title("Input data")<br/>    # Plot the training points<br/>    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,<br/>               edgecolors='k')<br/>    # Plot the testing points<br/>    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,<br/>               edgecolors='k')<br/>    ax.set_xlim(xx.min(), xx.max())<br/>    ax.set_ylim(yy.min(), yy.max())<br/>    ax.set_xticks(())<br/>    ax.set_yticks(())<br/>    i += 1</span><span id="ae88" class="mx lw in mt b gy nc mz l na nb"># iterate over classifiers<br/>    for name, clf in zip(names, classifiers):<br/>        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)<br/>        clf.fit(X_train, y_train)<br/>        score = clf.score(X_test, y_test)</span><span id="a1e0" class="mx lw in mt b gy nc mz l na nb"># Plot the decision boundary. For that, we will assign a color to each<br/>        # point in the mesh [x_min, x_max]x[y_min, y_max].<br/>        if hasattr(clf, "decision_function"):<br/>            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])<br/>        else:<br/>            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]</span><span id="597d" class="mx lw in mt b gy nc mz l na nb"># Put the result into a color plot<br/>        Z = Z.reshape(xx.shape)<br/>        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)</span><span id="de1e" class="mx lw in mt b gy nc mz l na nb"># Plot the training points<br/>        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,<br/>                   edgecolors='k')<br/>        # Plot the testing points<br/>        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,<br/>                   edgecolors='k', alpha=0.6)</span><span id="4a07" class="mx lw in mt b gy nc mz l na nb">ax.set_xlim(xx.min(), xx.max())<br/>        ax.set_ylim(yy.min(), yy.max())<br/>        ax.set_xticks(())<br/>        ax.set_yticks(())<br/>        if ds_cnt == 0:<br/>            ax.set_title(name)<br/>        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),<br/>                size=15, horizontalalignment='right')<br/>        i += 1</span><span id="f9c2" class="mx lw in mt b gy nc mz l na nb"># plt.tight_layout()<br/>plt.show()</span></pre><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi kc"><img src="../Images/79d0028a3f9ec117032e0b40c0201905.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4TlEpBBJCJyfSyzcb3RENA.png"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">我自己选的颜色</figcaption></figure><h1 id="6ec3" class="lv lw in bd lx ly lz ma mb mc md me mf jt mg ju mh jw mi jx mj jz mk ka ml mm bi translated">好吧，但是我怎么知道这些聚类算法实际上在做什么呢？</h1><p id="1353" class="pw-post-body-paragraph ks kt in ku b kv mn jo kx ky mo jr la lb mp ld le lf mq lh li lj mr ll lm ln ig bi translated">很高兴你也问了这个问题。sklearn 的人制作了一个非常酷的脚本来展示一些更流行的 ml 模型在分组和分类方面所做的事情。</p><pre class="kd ke kf kg gt ms mt mu mv aw mw bi"><span id="c182" class="mx lw in mt b gy my mz l na nb">import time<br/>import warnings</span><span id="0fef" class="mx lw in mt b gy nc mz l na nb">import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="adfe" class="mx lw in mt b gy nc mz l na nb">from sklearn import cluster, datasets, mixture<br/>from sklearn.neighbors import kneighbors_graph<br/>from sklearn.preprocessing import StandardScaler<br/>from itertools import cycle, islice</span><span id="09e3" class="mx lw in mt b gy nc mz l na nb">np.random.seed(0)</span><span id="b8c9" class="mx lw in mt b gy nc mz l na nb"># ============<br/># Generate datasets. We choose the size big enough to see the scalability<br/># of the algorithms, but not too big to avoid too long running times<br/># ============<br/>n_samples = 1500<br/>noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,<br/>                                      noise=.05)<br/>noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)<br/>blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)<br/>no_structure = np.random.rand(n_samples, 2), None</span><span id="9087" class="mx lw in mt b gy nc mz l na nb"># Anisotropicly distributed data<br/>random_state = 170<br/>X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)<br/>transformation = [[0.6, -0.6], [-0.4, 0.8]]<br/>X_aniso = np.dot(X, transformation)<br/>aniso = (X_aniso, y)</span><span id="d904" class="mx lw in mt b gy nc mz l na nb"># blobs with varied variances<br/>varied = datasets.make_blobs(n_samples=n_samples,<br/>                             cluster_std=[1.0, 2.5, 0.5],<br/>                             random_state=random_state)</span><span id="db22" class="mx lw in mt b gy nc mz l na nb"># ============<br/># Set up cluster parameters<br/># ============<br/>plt.figure(figsize=(9 * 2 + 3, 12.5))<br/>plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,<br/>                    hspace=.01)</span><span id="809e" class="mx lw in mt b gy nc mz l na nb">plot_num = 1</span><span id="bfd6" class="mx lw in mt b gy nc mz l na nb">default_base = {'quantile': .3,<br/>                'eps': .3,<br/>                'damping': .9,<br/>                'preference': -200,<br/>                'n_neighbors': 10,<br/>                'n_clusters': 3,<br/>                'min_samples': 20,<br/>                'xi': 0.05,<br/>                'min_cluster_size': 0.1}</span><span id="2271" class="mx lw in mt b gy nc mz l na nb">datasets = [<br/>    (noisy_circles, {'damping': .77, 'preference': -240,<br/>                     'quantile': .2, 'n_clusters': 2,<br/>                     'min_samples': 20, 'xi': 0.25}),<br/>    (noisy_moons, {'damping': .75, 'preference': -220, 'n_clusters': 2}),<br/>    (varied, {'eps': .18, 'n_neighbors': 2,<br/>              'min_samples': 5, 'xi': 0.035, 'min_cluster_size': .2}),<br/>    (aniso, {'eps': .15, 'n_neighbors': 2,<br/>             'min_samples': 20, 'xi': 0.1, 'min_cluster_size': .2}),<br/>    (blobs, {}),<br/>    (no_structure, {})]</span><span id="a02b" class="mx lw in mt b gy nc mz l na nb">for i_dataset, (dataset, algo_params) in enumerate(datasets):<br/>    # update parameters with dataset-specific values<br/>    params = default_base.copy()<br/>    params.update(algo_params)</span><span id="e5e5" class="mx lw in mt b gy nc mz l na nb">X, y = dataset</span><span id="8094" class="mx lw in mt b gy nc mz l na nb"># normalize dataset for easier parameter selection<br/>    X = StandardScaler().fit_transform(X)</span><span id="7dc4" class="mx lw in mt b gy nc mz l na nb"># estimate bandwidth for mean shift<br/>    bandwidth = cluster.estimate_bandwidth(X, quantile=params['quantile'])</span><span id="8c69" class="mx lw in mt b gy nc mz l na nb"># connectivity matrix for structured Ward<br/>    connectivity = kneighbors_graph(<br/>        X, n_neighbors=params['n_neighbors'], include_self=False)<br/>    # make connectivity symmetric<br/>    connectivity = 0.5 * (connectivity + connectivity.T)</span><span id="3215" class="mx lw in mt b gy nc mz l na nb"># ============<br/>    # Create cluster objects<br/>    # ============<br/>    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)<br/>    two_means = cluster.MiniBatchKMeans(n_clusters=params['n_clusters'])<br/>    ward = cluster.AgglomerativeClustering(<br/>        n_clusters=params['n_clusters'], linkage='ward',<br/>        connectivity=connectivity)<br/>    spectral = cluster.SpectralClustering(<br/>        n_clusters=params['n_clusters'], eigen_solver='arpack',<br/>        affinity="nearest_neighbors")<br/>    dbscan = cluster.DBSCAN(eps=params['eps'])<br/>    optics = cluster.OPTICS(min_samples=params['min_samples'],<br/>                            xi=params['xi'],<br/>                            min_cluster_size=params['min_cluster_size'])<br/>    affinity_propagation = cluster.AffinityPropagation(<br/>        damping=params['damping'], preference=params['preference'])<br/>    average_linkage = cluster.AgglomerativeClustering(<br/>        linkage="average", affinity="cityblock",<br/>        n_clusters=params['n_clusters'], connectivity=connectivity)<br/>    birch = cluster.Birch(n_clusters=params['n_clusters'])<br/>    gmm = mixture.GaussianMixture(<br/>        n_components=params['n_clusters'], covariance_type='full')</span><span id="d0c5" class="mx lw in mt b gy nc mz l na nb">clustering_algorithms = (<br/>        ('MiniBatchKMeans', two_means),<br/>        ('AffinityPropagation', affinity_propagation),<br/>        ('MeanShift', ms),<br/>        ('SpectralClustering', spectral),<br/>        ('Ward', ward),<br/>        ('AgglomerativeClustering', average_linkage),<br/>        ('DBSCAN', dbscan),<br/>        ('OPTICS', optics),<br/>        ('Birch', birch),<br/>        ('GaussianMixture', gmm)<br/>    )</span><span id="2bd1" class="mx lw in mt b gy nc mz l na nb">for name, algorithm in clustering_algorithms:<br/>        t0 = time.time()</span><span id="11a1" class="mx lw in mt b gy nc mz l na nb"># catch warnings related to kneighbors_graph<br/>        with warnings.catch_warnings():<br/>            warnings.filterwarnings(<br/>                "ignore",<br/>                message="the number of connected components of the " +<br/>                "connectivity matrix is [0-9]{1,2}" +<br/>                " &gt; 1. Completing it to avoid stopping the tree early.",<br/>                category=UserWarning)<br/>            warnings.filterwarnings(<br/>                "ignore",<br/>                message="Graph is not fully connected, spectral embedding" +<br/>                " may not work as expected.",<br/>                category=UserWarning)<br/>            algorithm.fit(X)</span><span id="c5b1" class="mx lw in mt b gy nc mz l na nb">t1 = time.time()<br/>        if hasattr(algorithm, 'labels_'):<br/>            y_pred = algorithm.labels_.astype(np.int)<br/>        else:<br/>            y_pred = algorithm.predict(X)</span><span id="350b" class="mx lw in mt b gy nc mz l na nb">plt.subplot(len(datasets), len(clustering_algorithms), plot_num)<br/>        if i_dataset == 0:<br/>            plt.title(name, size=18)</span><span id="def7" class="mx lw in mt b gy nc mz l na nb">colors = np.array(list(islice(cycle(['#2C629B', '#24C8AA', '#9D7BD5',<br/>                                             '#20CF6B', '#B548D8', '#341D98',<br/>                                             '#77F2FC', '#FB5A36', '#2C6CFF']),<br/>                                      int(max(y_pred) + 1))))<br/>        # add purple color for outliers (if any)<br/>        colors = np.append(colors, ["#DA7FFE"])<br/>        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])</span><span id="c9b3" class="mx lw in mt b gy nc mz l na nb">plt.xlim(-2.5, 2.5)<br/>        plt.ylim(-2.5, 2.5)<br/>        plt.xticks(())<br/>        plt.yticks(())<br/>        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),<br/>                 transform=plt.gca().transAxes, size=14,<br/>                 horizontalalignment='right', bbox=dict(facecolor='white', alpha=0.5),<br/>                color='#B548D8')<br/>        plot_num += 1</span><span id="604d" class="mx lw in mt b gy nc mz l na nb">plt.show()</span></pre><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi np"><img src="../Images/31b1dee49bcc53ed9c0bdf9815d124ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RBe712J8rORKzJpzuLK2fQ.png"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">这些颜色也是我自己选的。你。是。欢迎光临。</figcaption></figure><h1 id="69fe" class="lv lw in bd lx ly lz ma mb mc md me mf jt mg ju mh jw mi jx mj jz mk ka ml mm bi translated">好的，最后一个——结合热图和树状图，赢得胜利</h1><p id="5f60" class="pw-post-body-paragraph ks kt in ku b kv mn jo kx ky mo jr la lb mp ld le lf mq lh li lj mr ll lm ln ig bi translated">热图很累，树状图有太多的问题。因此，当你向任何人展示你的结果时，当他们问你问题时，这是一个视觉效果，让他们感到哑口无言——因为这就像聚类一样清晰。如果你问我，就回答:</p><blockquote class="lo lp lq"><p id="8532" class="ks kt lr ku b kv kw jo kx ky kz jr la ls lc ld le lt lg lh li lu lk ll lm ln ig bi translated">伙计，看一下图表。</p></blockquote><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi nq"><img src="../Images/eb0fe23302c9b9d7d59066c72c48d841.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tg-Cb1cUfnfwMiAtl8g7QQ.png"/></div></div></figure><h1 id="8ffe" class="lv lw in bd lx ly lz ma mb mc md me mf jt mg ju mh jw mi jx mj jz mk ka ml mm bi translated">使用哪种模型的良好经验法则</h1><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi nr"><img src="../Images/5dd1e8bbb17fe035f3bad5f71165e990.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*49mWwfqR_B70xZUWGrvvcA.png"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">香槟酒</figcaption></figure><h1 id="a51b" class="lv lw in bd lx ly lz ma mb mc md me mf jt mg ju mh jw mi jx mj jz mk ka ml mm bi translated">综上</h1><p id="672a" class="pw-post-body-paragraph ks kt in ku b kv mn jo kx ky mo jr la lb mp ld le lf mq lh li lj mr ll lm ln ig bi translated">如果你是自由职业者，没有大公司给你买机器学习自动化产品，那么就找一些听起来很酷的模型，然后插入你自己的数据。没有必要重新发明轮子！在决定使用哪种型号时，请尝试上图以获得帮助。</p><p id="c547" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">如果你在一家大公司工作，能够负担得起一款新的人工智能产品，我会选择<a class="ae ns" href="https://www.datarobot.com/lp/ai-ethics/?utm_medium=search&amp;utm_source=google&amp;utm_campaign=2019aiethicsBRANDED2020paidadsGPS&amp;utm_term=message2&amp;utm_content=variation1&amp;campaignid=9733876715&amp;adgroupid=103232814641&amp;adid=428351901572&amp;gclid=EAIaIQobChMIv4P84ei56gIVqtSzCh2lawNcEAAYASAAEgLT3PD_BwE" rel="noopener ugc nofollow" target="_blank">数据机器人</a>——这可能是我见过的最酷、最简单的人工智能产品。我目前没有权限访问它，但我过去有，而且它肯定有我的批准。数据机器人如果你要招人，来找我吧。</p><p id="3bd4" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">如果你觉得这篇文章很有帮助，欢迎鼓掌或提出问题。我喜欢反馈。在一天内完成这件事也让我非常头疼。请随意捐赠一些咖啡或止痛药。</p><blockquote class="lo lp lq"><p id="2e72" class="ks kt lr ku b kv kw jo kx ky kz jr la ls lc ld le lt lg lh li lu lk ll lm ln ig bi translated"><strong class="ku io">欢呼</strong>，</p><p id="84a6" class="ks kt lr ku b kv kw jo kx ky kz jr la ls lc ld le lt lg lh li lu lk ll lm ln ig bi translated"><strong class="ku io">最大</strong></p></blockquote><figure class="kd ke kf kg gt kh gh gi paragraph-image"><a href="https://www.buymeacoffee.com/31yearoldmoron"><div class="gh gi nv"><img src="../Images/5768eced23f6e47f18c56ad2b7691ed5.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*l5CYO--RK4n0piEGDAvxqw.png"/></div></a></figure><p id="4501" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated"><strong class="ku io">Github</strong>:<a class="ae ns" href="https://github.com/maxwellbade/machine_learning_fraud" rel="noopener ugc nofollow" target="_blank">https://github.com/maxwellbade/machine_learning_fraud</a></p><p id="d8f6" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated"><strong class="ku io"> Instagram </strong> : @maxbade</p><p id="0f44" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated"><strong class="ku io"> linkedin </strong> : @maxbade</p></div></div>    
</body>
</html>