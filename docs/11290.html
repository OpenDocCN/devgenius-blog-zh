<html>
<head>
<title>Big Data And Computing: A Beginners Approach</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">大数据和计算:初学者方法</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/big-data-and-computing-a-beginners-approach-63a10d259cd8?source=collection_archive---------8-----------------------#2022-12-30">https://blog.devgenius.io/big-data-and-computing-a-beginners-approach-63a10d259cd8?source=collection_archive---------8-----------------------#2022-12-30</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><figure class="im in gp gr io ip gh gi paragraph-image"><div role="button" tabindex="0" class="iq ir di is bf it"><div class="gh gi il"><img src="../Images/39317d2c9e3f658377a29f31be0111a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ry8cWx-82xmWJyrgPZq18g.jpeg"/></div></div><figcaption class="iw ix gj gh gi iy iz bd b be z dk translated">大卫·迪伯特的照片:<a class="ae ja" href="https://www.pexels.com/photo/time-lapse-photography-of-vehicles-passing-near-building-635609/" rel="noopener ugc nofollow" target="_blank">https://www . pexels . com/photo/time-lapse-photography-of-vehicles-passing-near-building-635609/</a></figcaption></figure><div class=""/><div class=""><h2 id="e775" class="pw-subtitle-paragraph ka jc jd bd b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr dk translated">地图简化算法</h2></div><p id="6fa5" class="pw-post-body-paragraph ks kt jd ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">作为大数据和分布式计算领域的初学者，我们经常会遇到一种叫做 Map Reduce 的东西。在我的<a class="ae ja" href="https://medium.com/@myac.abhijit/big-data-and-computing-a-beginners-approach-73461d389aba" rel="noopener">上一篇文章</a>中，我们也谈到了 Map Reduce 作为框架 Hadoop 的基本组件之一，用于处理和处理大数据。那么，什么是地图简化，为什么它如此重要？这是新进入这个领域的人们心中的疑问。在这篇文章中，让我们试着找到这个问题的答案。</p><h2 id="e96d" class="lo lp jd bd lq lr ls dn lt lu lv dp lw lb lx ly lz lf ma mb mc lj md me mf mg bi translated">什么是 Map Reduce？</h2><p id="f1b0" class="pw-post-body-paragraph ks kt jd ku b kv mh ke kx ky mi kh la lb mj ld le lf mk lh li lj ml ll lm ln ig bi translated">Map Reduce 基本上是为了处理或存储大量数据而实现的编程范例，因为它实现了并行计算，从而加快了处理过程。基于 Java 的算法需要创建两个最关键的组件，其中一个被称为映射器和归约器，顾名思义，它们执行两个主要任务映射和归约。</p><p id="0385" class="pw-post-body-paragraph ks kt jd ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">输入数据被分解成一组文档，文档集中的每个文档被发送到一个单独的映射器，该映射器使用用户定义的映射函数执行映射，以生成一组<strong class="ku je">中间键值对。</strong>一旦我们从所有映射器中获得了所有的键值对，它们就会被称为<strong class="ku je">组合器、洗牌器和分割器</strong>的 map-reduce 算法的其他组件基于某种标准重新分组和洗牌，以创建中间键值对的新集合。最后，它们被喂给减速器。每个集合都被提供给一个缩减器，该缩减器再次使用用户定义的 Reduce 函数来缩减该集合，以生成最终的键值对。</p><figure class="mn mo mp mq gt ip gh gi paragraph-image"><div role="button" tabindex="0" class="iq ir di is bf it"><div class="gh gi mm"><img src="../Images/3d111edb199c4952b11607842d2d3a4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HlMvk0QNKDqoy-8JL2vxjg.png"/></div></div><figcaption class="iw ix gj gh gi iy iz bd b be z dk translated">地图简化工作</figcaption></figure><p id="701c" class="pw-post-body-paragraph ks kt jd ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">一旦所有的映射器产生了它们各自的中间对，组合器就开始组合所有的对，并把输出提供给洗牌机。洗牌机根据某种定义的基础，比如说，可能按字母顺序，对所有的对子进行排序或排列。一旦洗牌机完成，分割器接管并根据一些用户逻辑将洗牌后的配对分成组，并将它们交给归约器。</p><h2 id="6a4e" class="lo lp jd bd lq lr ls dn lt lu lv dp lw lb lx ly lz lf ma mb mc lj md me mf mg bi translated">Map 如何减少工作和 Hadoop，为什么？</h2><p id="c51a" class="pw-post-body-paragraph ks kt jd ku b kv mh ke kx ky mi kh la lb mj ld le lf mk lh li lj ml ll lm ln ig bi translated">众所周知，Hadoop 集群系统部署了多个节点或低成本硬件，从而提供了水平扩展能力。</p><p id="8c0a" class="pw-post-body-paragraph ks kt jd ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">现在，考虑一下，有一本 1000 页的书，我们需要按字母顺序计算每个单词在书中出现的次数。如果我们把这项工作交给一台没有并行处理能力的特定的单机，它将花费大量的时间和处理能力，因为它将是一个连续的过程。但是想象一下，如果集群的一些节点被编程来执行映射工作，并且类似地，一些节点被分配减少工作，那么我们可以将 1000 页的书分成 100 个集合，每个集合大约 10 页，并且将每个集合提供给映射器。一切都是并行发生的，也就是说，在处理 10 页的时间里，我们可以处理所有 1000 页。我们还划分了转换任务，因此对于第二部分，并行化也是通过并行运行的减速器来实现的。由于 Hadoop 集群可以非常顺利地扩展，我们可以通过部署更多资源来更轻松地处理任何数量的数据。</p><p id="4936" class="pw-post-body-paragraph ks kt jd ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">为了运行这些组件，Hadoop 上有一个驱动程序类，它设置环境、映射器和还原器并启动进程。</p><h2 id="fe32" class="lo lp jd bd lq lr ls dn lt lu lv dp lw lb lx ly lz lf ma mb mc lj md me mf mg bi translated">来自 Hadoop 的架构支持</h2><p id="15b8" class="pw-post-body-paragraph ks kt jd ku b kv mh ke kx ky mi kh la lb mj ld le lf mk lh li lj ml ll lm ln ig bi translated">为了支持 Map Reduce 任务，Hadoop 再次使用了主从架构。它创建并使用以下组件:</p><ol class=""><li id="04c0" class="mr ms jd ku b kv kw ky kz lb mt lf mu lj mv ln mw mx my mz bi translated"><strong class="ku je">一个主作业跟踪器</strong>:可以作为主节点处理。它负责资源管理、监控和任务调度。它从集群中选择节点来执行 map 或 reduce 任务并分配它们。</li><li id="30e5" class="mr ms jd ku b kv na ky nb lb nc lf nd lj ne ln mw mx my mz bi translated"><strong class="ku je">许多从任务跟踪器:</strong>它们是从节点，由主作业跟踪器分配任务。它们执行任务并将结果返回给主节点。如果任务失败，它会将状态通知主服务器。为了保持可用性和可达性，从设备也向主设备发送心跳信号。没有信号表示从节点停机，主节点在其上分配新的资源。</li></ol><figure class="mn mo mp mq gt ip gh gi paragraph-image"><div role="button" tabindex="0" class="iq ir di is bf it"><div class="gh gi nf"><img src="../Images/f1f160e9b0dd3ca938123aa5e729bbb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hEGQPdzVU65HVVlOjIJMwg.png"/></div></div><figcaption class="iw ix gj gh gi iy iz bd b be z dk translated">地图简化架构</figcaption></figure><h2 id="4a18" class="lo lp jd bd lq lr ls dn lt lu lv dp lw lb lx ly lz lf ma mb mc lj md me mf mg bi translated">示例工作流程</h2><p id="c8fc" class="pw-post-body-paragraph ks kt jd ku b kv mh ke kx ky mi kh la lb mj ld le lf mk lh li lj ml ll lm ln ig bi translated">让我们考虑同样的例子，我们有一本 1000 页的书，然后我们需要按字母顺序统计每个单词的出现次数。</p><p id="5f67" class="pw-post-body-paragraph ks kt jd ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">步骤 1:我们决定分成 100 个集合，每个集合有 10 个连续的页面。现在，映射函数也接受键值对，所以我们为书中的每个单词创建键值对为<index word="">，即以单词的索引为键，单词为值，作为预处理。</index></p><p id="9c6c" class="pw-post-body-paragraph ks kt jd ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">第二步:将 100 套中的每一套送入 100 台绘图机。制图者基本上计算每个单词在分配给它的 10 页中出现的次数。</p><p id="cb5c" class="pw-post-body-paragraph ks kt jd ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">步骤 3:一旦映射器完成，我们从所有 100 个映射器中获得一组键值对，因此有 100 组键值对。组合器接受所有这些，并创建一个键-值对的组合列表，其中单词是键，出现的次数是值。</p><p id="63ed" class="pw-post-body-paragraph ks kt jd ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">第四步:根据要求，洗牌者接受组合列表，并按照字母顺序对键进行排序。</p><p id="b654" class="pw-post-body-paragraph ks kt jd ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">步骤 5:一旦混洗，我们使用分区器来创建 26 个分区，即 26 个分区，每个字母表一个分区，使得以该字母表开始的所有单词都进入该分区。</p><p id="0af6" class="pw-post-body-paragraph ks kt jd ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">第 6 步:每个分区都被发送到一个缩减器，该缩减器只添加每个特定单词的出现值。</p><p id="7152" class="pw-post-body-paragraph ks kt jd ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">步骤 7:一旦 26 个归约者产生了他们各自的结果，一个合并器被用来得到最终的结果</p><h2 id="ec37" class="lo lp jd bd lq lr ls dn lt lu lv dp lw lb lx ly lz lf ma mb mc lj md me mf mg bi translated"><strong class="ak">如何编写 Map-Reduce 算法</strong></h2><p id="3725" class="pw-post-body-paragraph ks kt jd ku b kv mh ke kx ky mi kh la lb mj ld le lf mk lh li lj ml ll lm ln ig bi translated">为了编写 Map-Reduce 算法，我们需要为 Mapper、Reducer 和 Driver 扩展预定义的 Java 类。</p><pre class="mn mo mp mq gt ng nh ni bn nj nk bi"><span id="039e" class="nl lp jd nh b be nm nn l no np">public static class Map extends Mapper&lt;LongWritable,Text,Text,IntWritable&gt; <br/>{<br/>}</span></pre><p id="89cb" class="pw-post-body-paragraph ks kt jd ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">这是我们从 java 的 mapper 抽象类扩展而来的 Map 类的格式。尖括号中的前两种类型给出了输入<key value="">对的格式或数据类型，接下来的两种类型给出了输出<key value="">对的数据类型。</key></key></p><p id="ab81" class="pw-post-body-paragraph ks kt jd ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">在我们的例子中，映射器的输入键-值对，其中单词的索引是长数据类型的键，值是文本类型的单词。类似地，映射器返回一个<key value="">对，其中键是文本类型的单词，计数是 int 类型的值。一旦定义了这一点，我们就可以在 mapper 类中的 map 方法下编写我们的功能定义</key></p><pre class="mn mo mp mq gt ng nh ni bn nj nk bi"><span id="e558" class="nl lp jd nh b be nm nn l no np">public static class Map extends MapReduceBase implements Mapper&lt;LongWritable,Text,Text,IntWritable&gt; <br/>{<br/><br/>   public void map(LongWritable key, Text value,<br/>   OutputCollector &lt;Text, IntWritable&gt; output, Context context) <br/>   throws IOException {<br/>    <br/>    /// Logic <br/>  <br/>   }<br/>}</span></pre><p id="9083" class="pw-post-body-paragraph ks kt jd ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">类似地，为了编写 reducer 代码，我们必须扩展 Reducer 类。对于缩减器，输入对数据类型将与映射器的输出类型相同，因为该输出被提供给缩减器<text intwritable="">。而 Reducer 的输出将是作为键的单词和整个 1000 页文档中单词的总计数，所以键-值对的数据类型将是<text intwritable="">。</text></text></p><pre class="mn mo mp mq gt ng nh ni bn nj nk bi"><span id="36f1" class="nl lp jd nh b be nm nn l no np">public static class Reduce extends MapReduceBase implements Reducer&lt;Text,IntWritable,Text,IntWritable&gt; <br/>{<br/> <br/>  public void reduce(Text key, Iterable&lt;IntWritable&gt; values,<br/>  OutputCollector &lt;Text, IntWritable&gt; output,Context context)<br/>  throws IOException,InterruptedException {<br/><br/>    ///logic<br/><br/>  }<br/>}</span></pre><p id="5583" class="pw-post-body-paragraph ks kt jd ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">我们可以将我们的逻辑放在 reduce 类的 reduce 函数中。</p><p id="95bd" class="pw-post-body-paragraph ks kt jd ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">接下来，我们需要定义驱动程序代码。这是将被调用的类，它将调用映射器和缩减器类。在这种情况下，我们需要使用 setter 功能定义 Mapper 和 Reducer 类名。我们还需要定义键的数据类型和输出对的值。</p><pre class="mn mo mp mq gt ng nh ni bn nj nk bi"><span id="b7e9" class="nl lp jd nh b be nm nn l no np">public static void main(String args[])<br/>throws Exception {<br/> <br/>      JobConf conf = new JobConf(WordCount.class); <br/>      <br/>      conf.setJobName("WordCount");<br/>      conf.setJarByClass(WordCount.class);    <br/>      // the jar will be created from this class on extraction of executable<br/>      conf.setMapperClass(Map.class);<br/>      conf.setReducerClass(Reduce.class); <br/>      conf.setOutputKeyClass(Text.class);<br/>      conf.setOutputValueClass(IntWritable.class);<br/>      conf.setInputFormat(TextInputFormat.class); <br/>      // It sets the input type, as we are sending 1 line of text.<br/>      conf.setOutputFormat(TextOutputFormat.class); <br/>      <br/>      JobClient.runJob(conf); <br/>}</span></pre><p id="b2e5" class="pw-post-body-paragraph ks kt jd ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">所有这些静态类都必须在包装类 WordCount 下声明，以使它成为一个完整的 MapReduce 类。</p><h2 id="b2d3" class="lo lp jd bd lq lr ls dn lt lu lv dp lw lb lx ly lz lf ma mb mc lj md me mf mg bi translated">结论</h2><p id="2284" class="pw-post-body-paragraph ks kt jd ku b kv mh ke kx ky mi kh la lb mj ld le lf mk lh li lj ml ll lm ln ig bi translated">在本文中，我们学习了 MapReduce 算法及其工作原理。在<a class="ae ja" href="https://medium.com/@myac.abhijit/big-data-and-computing-a-beginners-approach-1b5c8ca612d6" rel="noopener">即将到来的</a>中，我们将了解大数据发展道路上的其他重要主题。</p><p id="70ef" class="pw-post-body-paragraph ks kt jd ku b kv kw ke kx ky kz kh la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">快乐阅读！！！</p></div></div>    
</body>
</html>