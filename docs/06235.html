<html>
<head>
<title>All you need to know about Transfer Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于迁移学习，你需要知道的是</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/all-you-need-to-know-about-transfer-learning-4a7e3cbaf1dd?source=collection_archive---------7-----------------------#2021-12-23">https://blog.devgenius.io/all-you-need-to-know-about-transfer-learning-4a7e3cbaf1dd?source=collection_archive---------7-----------------------#2021-12-23</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><figure class="gl gn jl jm jn jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi jk"><img src="../Images/8815b1a9a85772bbc7b4388f3965c319.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9xgMViGHMKUX6Ir4_ZcoKg.png"/></div></div></figure><h1 id="4ff1" class="jv jw in bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">目录</h1><ul class=""><li id="3927" class="kt ku in kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">1.介绍</li><li id="a7d6" class="kt ku in kv b kw ll ky lm la ln lc lo le lp lg lh li lj lk bi translated">1.1.什么是迁移学习？</li><li id="13b6" class="kt ku in kv b kw ll ky lm la ln lc lo le lp lg lh li lj lk bi translated">1.2.迁移学习的需要</li><li id="558d" class="kt ku in kv b kw ll ky lm la ln lc lo le lp lg lh li lj lk bi translated">1.3.限制</li><li id="73ae" class="kt ku in kv b kw ll ky lm la ln lc lo le lp lg lh li lj lk bi translated">2.使用预训练模型</li><li id="af20" class="kt ku in kv b kw ll ky lm la ln lc lo le lp lg lh li lj lk bi translated">3.微调最后一层</li><li id="b8f6" class="kt ku in kv b kw ll ky lm la ln lc lo le lp lg lh li lj lk bi translated">4.冻结一些层和微调模型层的其余部分</li><li id="308d" class="kt ku in kv b kw ll ky lm la ln lc lo le lp lg lh li lj lk bi translated">5.仅使用模型架构，从头开始学习权重</li></ul><h1 id="838c" class="jv jw in bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">1.介绍</h1><h2 id="c2ed" class="lq jw in bd jx lr ls dn kb lt lu dp kf la lv lw kj lc lx ly kn le lz ma kr mb bi translated">1.1什么是迁移学习？</h2><p id="318b" class="pw-post-body-paragraph mc md in kv b kw kx me mf ky kz mg mh la mi mj mk lc ml mm mn le mo mp mq lg ig bi translated"><em class="mr">迁移学习就像站在巨人的肩膀上</em></p><p id="af1a" class="pw-post-body-paragraph mc md in kv b kw ms me mf ky mt mg mh la mu mj mk lc mv mm mn le mw mp mq lg ig bi translated">在迁移学习中，我们使用预先训练的知识或权重，并在数据集上调整或微调模型</p><p id="7bd5" class="pw-post-body-paragraph mc md in kv b kw ms me mf ky mt mg mh la mu mj mk lc mv mm mn le mw mp mq lg ig bi translated">转移学习是一种实际上把已经完成的工作转移到不同领域的方法<br/>例如- <strong class="kv io"> Imagenet </strong>包含猫、狗、公共汽车和许多日常物品的常规图像。我们在这些图像上训练模型，并在疟原虫检测或任何其他任务中使用该知识</p><p id="7656" class="pw-post-body-paragraph mc md in kv b kw ms me mf ky mt mg mh la mu mj mk lc mv mm mn le mw mp mq lg ig bi translated"><em class="mr">迁移学习在计算机视觉和NLP任务中非常流行。在自然语言处理中，我们可以使用基于变压器的模型，如BERT <br/>在本教程中，我们将重点关注计算机视觉</em></p><p id="28e8" class="pw-post-body-paragraph mc md in kv b kw ms me mf ky mt mg mh la mu mj mk lc mv mm mn le mw mp mq lg ig bi translated">ImageNet是一个图像分类问题，包含超过1400万张图像和1000个类别</p><h2 id="4f7f" class="lq jw in bd jx lr ls dn kb lt lu dp kf la lv lw kj lc lx ly kn le lz ma kr mb bi translated">1.2.迁移学习的需要？</h2><ol class=""><li id="8888" class="kt ku in kv b kw kx ky kz la lb lc ld le lf lg mx li lj lk bi translated">如果我们的数据集真的很小</li><li id="fa2d" class="kt ku in kv b kw ll ky lm la ln lc lo le lp lg mx li lj lk bi translated">低计算能力</li><li id="d562" class="kt ku in kv b kw ll ky lm la ln lc lo le lp lg mx li lj lk bi translated">如果我们的数据集与预训练数据相似，那么我们只需微调我们的模型，这将节省大量时间</li></ol><h2 id="a37a" class="lq jw in bd jx lr ls dn kb lt lu dp kf la lv lw kj lc lx ly kn le lz ma kr mb bi translated">1.3.限制</h2><ol class=""><li id="d5ee" class="kt ku in kv b kw kx ky kz la lb lc ld le lf lg mx li lj lk bi translated">数据集与训练前的数据完全不同</li></ol><h1 id="992c" class="jv jw in bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">2.使用预训练模型</h1><p id="2e7f" class="pw-post-body-paragraph mc md in kv b kw kx me mf ky kz mg mh la mi mj mk lc ml mm mn le mo mp mq lg ig bi translated">在预训练模型中，我们完全使用图像净重，而没有对模型进行任何微调。<strong class="kv io">这种方法的问题</strong>是我们只能根据预先训练的模型权重被训练的类别来分类或执行任务<br/> imagenet 1000类别-<a class="ae my" href="http://In pretrained model we use imagenet weight completely without any fine tuning of model.Problem with this approach is that we can only classify or perform task based upon the classes in which pretrained model weights are train imagenet 1000 classes-https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a" rel="noopener ugc nofollow" target="_blank">https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a</a></p><h2 id="6d5a" class="lq jw in bd jx lr ls dn kb lt lu dp kf la lv lw kj lc lx ly kn le lz ma kr mb bi translated">从维基百科下载图片进行分类</h2><p id="de13" class="pw-post-body-paragraph mc md in kv b kw kx me mf ky kz mg mh la mi mj mk lc ml mm mn le mo mp mq lg ig bi translated">1.微型货车<br/> 2。滑行<br/> 3。哈巴狗</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="28bd" class="lq jw in ne b gy ni nj l nk nl"><em class="mr">#mini-van</em><br/>!wget https://upload.wikimedia.org/wikipedia/commons/b/b4/Bloody_Bridge_car_park%2C_May_2010_%2806%29.JPG<br/><em class="mr">#Taxi</em><br/>!wget https://upload.wikimedia.org/wikipedia/commons/e/ef/NYC_Taxi_Ford_Crown_Victoria.jpg<br/><em class="mr">#pug dog</em><br/>!wget https://upload.wikimedia.org/wikipedia/commons/f/f0/Mops_oct09_cropped2.jpg</span></pre><h2 id="a9ee" class="lq jw in bd jx lr ls dn kb lt lu dp kf la lv lw kj lc lx ly kn le lz ma kr mb bi translated">重要的模型参数</h2><ol class=""><li id="af8c" class="kt ku in kv b kw kx ky kz la lb lc ld le lf lg mx li lj lk bi translated"><strong class="kv io">权重</strong>-由于我们在这里使用预训练模型，我们设置了weight='imagenet'。如果我们设置weight='None ',那么我们就不能使用预训练权重。</li><li id="6814" class="kt ku in kv b kw ll ky lm la ln lc lo le lp lg mx li lj lk bi translated"><strong class="kv io">include _ top</strong>-是否包含网络顶部的3个全连通层。因为我们想在没有训练的情况下进行分类，所以我们必须设置include_top='True '。如果我们设置include_top='False ',那么我们必须为预训练模型添加密集层和输出层，这在预训练中是不需要的，我们将在微调最后一层&amp;中看到include_top='False ',冻结一些层，微调其余的层。</li><li id="c577" class="kt ku in kv b kw ll ky lm la ln lc lo le lp lg mx li lj lk bi translated"><strong class="kv io">input_shape</strong>-图像的输入大小，因为我们设置了include_top='True '和weights= 'imagenet '，那么input _ shape必须是'(224，244，3)'。如果weights='imagenet '和include_top='True ',则我们无法对自定义图像形状进行训练或分类。如果您想根据自定义图像大小进行训练，请确保weights不等于“imagenet ”, include _ top不等于“True”。权重必须为零或include_top='False '</li><li id="0d3a" class="kt ku in kv b kw ll ky lm la ln lc lo le lp lg mx li lj lk bi translated">当我们设置include_top='True '时，那么classes='1000 '和classifier_activation='softmax '</li></ol><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="a16d" class="lq jw in ne b gy ni nj l nk nl">from keras.applications.vgg19 import VGG19<br/><em class="mr"># load the model</em><br/>model = VGG19(weights= 'imagenet',include_top=True,input_shape=(224,224,3))<br/>model.summary()</span><span id="ab50" class="lq jw in ne b gy nm nj l nk nl">from keras.preprocessing.image import img_to_array<br/>from keras.preprocessing.image import load_img<br/>import numpy as np<br/>from keras.applications.vgg16 import preprocess_input<br/>from tensorflow.keras.applications.mobilenet import decode_predictions<br/>def make_classification(image):<br/>    pred = model.predict(image)<br/>    pred_classes = decode_predictions(pred, top=5)<br/>    for i <strong class="ne io">in</strong> pred_classes[0]:<br/>        print(i)<br/><br/><br/>def preprocess(image_name):<br/>    image = load_img(image_name, target_size=(224, 224))<br/>    image = img_to_array(image)<br/>    image = np.expand_dims(image, axis=0)<br/>    image = preprocess_input(image)<br/>    return image</span><span id="7a0a" class="lq jw in ne b gy nm nj l nk nl">mini_bus='Bloody_Bridge_car_park,_May_2010_(06).JPG'<br/>pubg_dogs='Mops_oct09_cropped2.jpg'<br/>taxi='NYC_Taxi_Ford_Crown_Victoria.jpg'<br/><br/><em class="mr">#classifing mini_van </em><br/>print("for mini bus")<br/>image=preprocess(mini_bus)<br/>make_classification(image)<br/>print("<strong class="ne io">\n</strong>")<br/>print("<strong class="ne io">\n</strong>")<br/><em class="mr">#classifing pubg_dogs </em><br/>print("for pubg dogs")<br/>image=preprocess(pubg_dogs)<br/>make_classification(image)<br/>print("<strong class="ne io">\n</strong>")<br/>print("<strong class="ne io">\n</strong>")<br/><em class="mr">#classifing taxi </em><br/>print("for taxi")<br/>image=preprocess(taxi)<br/>make_classification(image)<br/>print("<strong class="ne io">\n</strong>")<br/>print("<strong class="ne io">\n</strong>")</span></pre><figure class="mz na nb nc gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi nn"><img src="../Images/fad9da6ea2c942ac44dca1f9f7c3f9b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*o6seAFs8bYtLuUbbpYKQtw.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk translated">输出</figcaption></figure><p id="bfb9" class="pw-post-body-paragraph mc md in kv b kw ms me mf ky mt mg mh la mu mj mk lc mv mm mn le mw mp mq lg ig bi translated">为了解释<code class="fe ns nt nu ne b">3.Fine tuning Last layer,4.Freezing some layer and Fine tuning rest of model’s layer </code> &amp; <code class="fe ns nt nu ne b">5.Use only model architecture and learn weight from scratch</code>的概念，我将使用来自kaggle- <a class="ae my" href="https://www.kaggle.com/c/aerial-cactus-identification" rel="noopener ugc nofollow" target="_blank">链接</a>的空中仙人掌数据</p><h2 id="759b" class="lq jw in bd jx lr ls dn kb lt lu dp kf la lv lw kj lc lx ly kn le lz ma kr mb bi translated">数据准备</h2><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="5887" class="lq jw in ne b gy ni nj l nk nl">import pandas as pd<br/>train=pd.read_csv("/kaggle/input/aerial-cactus-identification/train.csv")</span><span id="6eda" class="lq jw in ne b gy nm nj l nk nl">train.head()</span><span id="1733" class="lq jw in ne b gy nm nj l nk nl">has_cactus=train['has_cactus'].value_counts()<br/>print(has_cactus)</span><span id="5aea" class="lq jw in ne b gy nm nj l nk nl">import tensorflow as tf<br/>from tensorflow.keras import Sequential<br/>from tensorflow.keras.layers import Flatten,Dense,Conv2D,MaxPooling2D,ZeroPadding2D,Dropout<br/>from tensorflow.keras.preprocessing.image import ImageDataGenerator</span><span id="6c4d" class="lq jw in ne b gy nm nj l nk nl">datagen=ImageDataGenerator(rescale=1/255,validation_split=0.30)</span><span id="5a75" class="lq jw in ne b gy nm nj l nk nl">train["has_cactus"]= train["has_cactus"].apply(str)</span><span id="bf6b" class="lq jw in ne b gy nm nj l nk nl">bs = 64<br/>train_generator = datagen.flow_from_dataframe(<br/>    dataframe = train,<br/>    x_col = "id",<br/>    y_col = "has_cactus",<br/>    subset = "training",<br/>    batch_size = bs,<br/>    target_size=(32,32),<br/>    <em class="mr">#seed = 1,</em><br/>    shuffle = True,<br/>    class_mode = "categorical")<br/><br/>valid_generator = datagen.flow_from_dataframe(<br/>    dataframe = train,<br/>    x_col = "id",<br/>    y_col = "has_cactus",<br/>    subset = "validation",<br/>    target_size=(32,32),<br/>    batch_size = bs,<br/>    shuffle = True,<br/>    class_mode = "categorical")</span></pre><h1 id="feae" class="jv jw in bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">3.微调最后一层</h1><p id="d5a7" class="pw-post-body-paragraph mc md in kv b kw kx me mf ky kz mg mh la mi mj mk lc ml mm mn le mo mp mq lg ig bi translated">在这一点上，我们将使用所有卷积层的imagenet权重，并添加输出层，然后我们将训练最后一层</p><h1 id="b285" class="jv jw in bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">重要的模型参数</h1><ol class=""><li id="8c28" class="kt ku in kv b kw kx ky kz la lb lc ld le lf lg mx li lj lk bi translated"><strong class="kv io">weights</strong>-由于我们正在微调我们的模型，我们将设置weight='imagenet'。如果我们设置weight='None ',那么我们就不能使用在imagenet上训练的weight。</li><li id="248f" class="kt ku in kv b kw ll ky lm la ln lc lo le lp lg mx li lj lk bi translated"><strong class="kv io">include _ top</strong>-是否包括网络顶部的3个全连接层不包括。这里，我们在自定义数据集上微调模型的最后一层，因此我们需要根据数据集中的类数量添加输出层。这里，我们将设置include_top='False '</li><li id="7578" class="kt ku in kv b kw ll ky lm la ln lc lo le lp lg mx li lj lk bi translated"><strong class="kv io">input _ shape</strong>-由于我们设置了include_top='False ',因此我们对vgg19的任何图像大小进行训练，图像大小不应小于32x32。</li><li id="53ac" class="kt ku in kv b kw ll ky lm la ln lc lo le lp lg mx li lj lk bi translated">因为我们已经设置了include_top='False '，所以不需要在这里传递类的数量，我们将在密集层中传递类的数量，对于classifier_activation也是如此</li></ol><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="87be" class="lq jw in ne b gy ni nj l nk nl">#importing model<br/>from tensorflow.keras.applications.vgg19 import VGG19</span><span id="e540" class="lq jw in ne b gy nm nj l nk nl">#downloading the model with imagenet weights</span><span id="193c" class="lq jw in ne b gy nm nj l nk nl">model = VGG19(weights= 'imagenet',include_top=False,input_shape=(32,32,3))</span></pre><h2 id="b054" class="lq jw in bd jx lr ls dn kb lt lu dp kf la lv lw kj lc lx ly kn le lz ma kr mb bi translated">使层可训练=假或冻结卷积层</h2><p id="219e" class="pw-post-body-paragraph mc md in kv b kw kx me mf ky kz mg mh la mi mj mk lc ml mm mn le mo mp mq lg ig bi translated">这将使每一层不可训练</p><figure class="mz na nb nc gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi nv"><img src="../Images/0bb1a7fbcba468fa3d6ab6d7e9bcf8b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*DKakKq1e--Llmj-tVBkL6A.png"/></div></div></figure><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="11e6" class="lq jw in ne b gy ni nj l nk nl"><em class="mr">#to freeze some layer</em><br/>for layer <strong class="ne io">in</strong> model.layers[:]:<br/>    layer.trainable = False</span></pre><p id="dafa" class="pw-post-body-paragraph mc md in kv b kw ms me mf ky mt mg mh la mu mj mk lc mv mm mn le mw mp mq lg ig bi translated">这里layer_n='False '意味着你不想训练那个层，你也可以说我们已经冻结了那个层<br/>。当layer_n='True '意味着你想训练那个层<br/>为include_top= 'False ',那么我们只有卷积层，我们已经冻结了那些层</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="82bb" class="lq jw in ne b gy ni nj l nk nl">for layer <strong class="ne io">in</strong> model.layers:<br/>    sp=' '[len(layer.name):]<br/>    print(layer.name,sp,layer.trainable)</span></pre><figure class="mz na nb nc gt jo gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/08d83357aa22210f50189c32ccc23d35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*CjIjRGH8IKzBMEuuJWztDw.png"/></div><figcaption class="no np gj gh gi nq nr bd b be z dk translated">输出</figcaption></figure><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="c7ff" class="lq jw in ne b gy ni nj l nk nl">from tensorflow.keras.models import Model<br/>x = Flatten()(model.output)<br/><br/>prediction = Dense(2, activation='softmax')(x)<br/><br/><em class="mr"># create a model object</em><br/>model = Model(inputs=model.input, outputs=prediction)</span><span id="26d7" class="lq jw in ne b gy nm nj l nk nl">model.summary()</span></pre><p id="dc09" class="pw-post-body-paragraph mc md in kv b kw ms me mf ky mt mg mh la mu mj mk lc mv mm mn le mw mp mq lg ig bi translated">编译模型</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="eb65" class="lq jw in ne b gy ni nj l nk nl">model.compile(loss='categorical_crossentropy', optimizer="adam", metrics=['accuracy'])</span></pre><p id="8667" class="pw-post-body-paragraph mc md in kv b kw ms me mf ky mt mg mh la mu mj mk lc mv mm mn le mw mp mq lg ig bi translated">模型训练</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="e958" class="lq jw in ne b gy ni nj l nk nl">history = model.fit_generator(train_generator,<br/>                         steps_per_epoch = len(train_generator),<br/>                         epochs = 5,<br/>                         validation_data = valid_generator,<br/>                         validation_steps = len(valid_generator),<br/>                         verbose=1)</span></pre><p id="6aae" class="pw-post-body-paragraph mc md in kv b kw ms me mf ky mt mg mh la mu mj mk lc mv mm mn le mw mp mq lg ig bi translated">绘制训练和验证曲线</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="fe30" class="lq jw in ne b gy ni nj l nk nl">import matplotlib.pyplot as plt<br/>def plot_learningCurve(history, epoch):<br/>    <em class="mr"># Plot training &amp; validation accuracy values</em><br/>    epoch_range = range(1, epoch+1)<br/>    plt.plot(epoch_range, history.history['accuracy'])<br/>    plt.plot(epoch_range, history.history['val_accuracy'])<br/>    plt.title('Model accuracy')<br/>    plt.ylabel('Accuracy')<br/>    plt.xlabel('Epoch')<br/>    plt.legend(['Train', 'Val'], loc='upper left')<br/>    plt.show()<br/><br/>    <em class="mr"># Plot training &amp; validation loss values</em><br/>    plt.plot(epoch_range, history.history['loss'])<br/>    plt.plot(epoch_range, history.history['val_loss'])<br/>    plt.title('Model loss')<br/>    plt.ylabel('Loss')<br/>    plt.xlabel('Epoch')<br/>    plt.legend(['Train', 'Val'], loc='upper left')<br/>    plt.show()<br/><br/>plot_learningCurve(history, 5)</span></pre><figure class="mz na nb nc gt jo gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/6f6c08f54b5cfe8edfe75bec92a7a4d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*nFDMPL35fMKbdq_SkDdHIw.png"/></div><figcaption class="no np gj gh gi nq nr bd b be z dk translated">输出</figcaption></figure><h1 id="0963" class="jv jw in bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">4.冻结一些层和微调模型层的其余部分</h1><p id="f39c" class="pw-post-body-paragraph mc md in kv b kw kx me mf ky kz mg mh la mi mj mk lc ml mm mn le mo mp mq lg ig bi translated">在这种情况下，我们将冻结一些层和微调其余的层。在开始层模型了解边缘和圆度。</p><h2 id="19ea" class="lq jw in bd jx lr ls dn kb lt lu dp kf la lv lw kj lc lx ly kn le lz ma kr mb bi translated">重要的模型参数</h2><ol class=""><li id="48d7" class="kt ku in kv b kw kx ky kz la lb lc ld le lf lg mx li lj lk bi translated"><strong class="kv io">权重</strong>-由于我们正在微调我们的模型，我们将设置weight='imagenet'。如果我们设置weight='None ',那么我们就不能使用在imagenet上训练的权重。</li><li id="24ec" class="kt ku in kv b kw ll ky lm la ln lc lo le lp lg mx li lj lk bi translated"><strong class="kv io">include _ top</strong>-是否包括网络顶部的3个完全连接的层不包括。这里，我们在自定义数据集上微调模型的最后一层，因此我们需要根据数据集中的类数量添加输出层。这里，我们将设置include_top='False '</li><li id="8ae5" class="kt ku in kv b kw ll ky lm la ln lc lo le lp lg mx li lj lk bi translated"><strong class="kv io">input _ shape</strong>-由于我们已经设置include_top='False '那么我们对vgg19的任何图像大小进行训练，图像大小不应小于32x32。</li><li id="5d1a" class="kt ku in kv b kw ll ky lm la ln lc lo le lp lg mx li lj lk bi translated">因为我们已经设置了include_top='False '，所以不需要在这里传递类的数量，我们将在密集层中传递类的数量，对于classifier_activation也是如此</li></ol><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="d792" class="lq jw in ne b gy ni nj l nk nl">from tensorflow.keras.applications.vgg19 import VGG19<br/>#downloading the model with imagenet weights</span><span id="1d91" class="lq jw in ne b gy nm nj l nk nl">model = VGG19(weights= 'imagenet',include_top=False,input_shape=(32,32,3))</span></pre><h1 id="4c19" class="jv jw in bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">使layer.trainable = False或冻结一些层</h1><p id="5e69" class="pw-post-body-paragraph mc md in kv b kw kx me mf ky kz mg mh la mi mj mk lc ml mm mn le mo mp mq lg ig bi translated">在这里，我们包括_top=False，所以我们只有一个卷积层，我们通过制作层来冻结开始的8层。可训练= 'False' <br/> <strong class="kv io">代码:</strong></p><figure class="mz na nb nc gt jo gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/7b189bf94f1859634a040858a9f7dfb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*WWvWI40vVdT8bppnKYI6OQ.png"/></div></figure><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="c656" class="lq jw in ne b gy ni nj l nk nl"><em class="mr">#to freeze some layer</em><br/>for layer <strong class="ne io">in</strong> model.layers[:8]:<br/>    layer.trainable = False</span></pre><p id="310f" class="pw-post-body-paragraph mc md in kv b kw ms me mf ky mt mg mh la mu mj mk lc mv mm mn le mw mp mq lg ig bi translated">这里layer_n='False '表示您不想训练该层，您也可以说我们已经冻结了该层<br/>。当layer_n='True '表示您想要将该层<br/>训练为include_top= 'False '时，我们只有卷积层，并且我们已经冻结了开始8层</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="a389" class="lq jw in ne b gy ni nj l nk nl">for layer <strong class="ne io">in</strong> model.layers:<br/>    sp=' '[len(layer.name):]<br/>    print(layer.name,sp,layer.trainable)</span></pre><figure class="mz na nb nc gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi nz"><img src="../Images/9f25a18a873e9cbd7eaead5750a0db6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ET1byTvUFWXzLfEn0p1vxw.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk translated">输出</figcaption></figure><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="c6c9" class="lq jw in ne b gy ni nj l nk nl">from tensorflow.keras.models import Model<br/>x = Flatten()(model.output)<br/><br/>prediction = Dense(2, activation='softmax')(x)<br/><br/><em class="mr"># create a model object</em><br/>model = Model(inputs=model.input, outputs=prediction)<br/>model.summary()</span></pre><p id="ac4f" class="pw-post-body-paragraph mc md in kv b kw ms me mf ky mt mg mh la mu mj mk lc mv mm mn le mw mp mq lg ig bi translated">编译模型</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="8bdf" class="lq jw in ne b gy ni nj l nk nl">model.compile(loss='categorical_crossentropy', optimizer="adam", metrics=['accuracy'])</span></pre><p id="907f" class="pw-post-body-paragraph mc md in kv b kw ms me mf ky mt mg mh la mu mj mk lc mv mm mn le mw mp mq lg ig bi translated">模型训练</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="2e21" class="lq jw in ne b gy ni nj l nk nl">history = model.fit_generator(train_generator,<br/>                         steps_per_epoch = len(train_generator),<br/>                         epochs = 5,<br/>                         validation_data = valid_generator,<br/>                         validation_steps = len(valid_generator),<br/>                         verbose=1)</span></pre><p id="897b" class="pw-post-body-paragraph mc md in kv b kw ms me mf ky mt mg mh la mu mj mk lc mv mm mn le mw mp mq lg ig bi translated">绘图训练和测试精度</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="d6e9" class="lq jw in ne b gy ni nj l nk nl">import matplotlib.pyplot as plt<br/>def plot_learningCurve(history, epoch):<br/>    <em class="mr"># Plot training &amp; validation accuracy values</em><br/>    epoch_range = range(1, epoch+1)<br/>    plt.plot(epoch_range, history.history['accuracy'])<br/>    plt.plot(epoch_range, history.history['val_accuracy'])<br/>    plt.title('Model accuracy')<br/>    plt.ylabel('Accuracy')<br/>    plt.xlabel('Epoch')<br/>    plt.legend(['Train', 'Val'], loc='upper left')<br/>    plt.show()<br/><br/>    <em class="mr"># Plot training &amp; validation loss values</em><br/>    plt.plot(epoch_range, history.history['loss'])<br/>    plt.plot(epoch_range, history.history['val_loss'])<br/>    plt.title('Model loss')<br/>    plt.ylabel('Loss')<br/>    plt.xlabel('Epoch')<br/>    plt.legend(['Train', 'Val'], loc='upper left')<br/>    plt.show()<br/><br/>plot_learningCurve(history, 5)</span></pre><figure class="mz na nb nc gt jo gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/7f857f13c1c26695002cff3f6d572d41.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*jzlK-JIPOE8NdEFzSPMZnw.png"/></div><figcaption class="no np gj gh gi nq nr bd b be z dk translated">输出</figcaption></figure><h2 id="82ac" class="lq jw in bd jx lr ls dn kb lt lu dp kf la lv lw kj lc lx ly kn le lz ma kr mb bi translated">5.仅从头开始使用模型架构和培训</h2><h2 id="de0e" class="lq jw in bd jx lr ls dn kb lt lu dp kf la lv lw kj lc lx ly kn le lz ma kr mb bi translated">重要的模型参数</h2><ol class=""><li id="662b" class="kt ku in kv b kw kx ky kz la lb lc ld le lf lg mx li lj lk bi translated"><strong class="kv io">权重</strong>-因为我们从头开始训练我们的模型，所以我们将设置权重=‘无’。</li><li id="834d" class="kt ku in kv b kw ll ky lm la ln lc lo le lp lg mx li lj lk bi translated"><strong class="kv io">include _ top</strong>-是否包含网络顶部的3个全连通层。这里我们设置了include_top=True，它将添加完全连接的层。如果你愿意，你可以设置include_top=False，然后你必须定义输出层和密集层</li><li id="f66a" class="kt ku in kv b kw ll ky lm la ln lc lo le lp lg mx li lj lk bi translated"><strong class="kv io">Input _ shape</strong>-当我们设置include_top='False '或weights= 'None '时，输入图像的大小，然后我们对vgg19的任何图像大小进行训练，图像大小不应小于32x32。</li><li id="20ec" class="kt ku in kv b kw ll ky lm la ln lc lo le lp lg mx li lj lk bi translated"><strong class="kv io">类</strong> =2</li><li id="2113" class="kt ku in kv b kw ll ky lm la ln lc lo le lp lg mx li lj lk bi translated"><strong class="kv io">分类器_激活</strong> =softmax</li></ol><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="f834" class="lq jw in ne b gy ni nj l nk nl">from tensorflow.keras.applications.vgg19 import VGG19</span><span id="175a" class="lq jw in ne b gy nm nj l nk nl">#here weight is None<br/>model = VGG19(weights= None,include_top=True,input_shape=(32,32,3),classes=2,classifier_activation="softmax")</span></pre><p id="f2f8" class="pw-post-body-paragraph mc md in kv b kw ms me mf ky mt mg mh la mu mj mk lc mv mm mn le mw mp mq lg ig bi translated">使所有层可训练</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="3d84" class="lq jw in ne b gy ni nj l nk nl">for layer <strong class="ne io">in</strong> model.layers:<br/>    sp=' '[len(layer.name):]<br/>    print(layer.name,sp,layer.trainable)</span></pre><figure class="mz na nb nc gt jo gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/61a3e7ecf58973f9748e1f6095ad14f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*t2O7MR_VbCvWncYq9Fh6Cw.png"/></div><figcaption class="no np gj gh gi nq nr bd b be z dk translated">输出</figcaption></figure><p id="df41" class="pw-post-body-paragraph mc md in kv b kw ms me mf ky mt mg mh la mu mj mk lc mv mm mn le mw mp mq lg ig bi translated">编译模型</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="ca90" class="lq jw in ne b gy ni nj l nk nl">model.compile(loss='categorical_crossentropy', optimizer="adam", metrics=['accuracy'])</span></pre><p id="6e76" class="pw-post-body-paragraph mc md in kv b kw ms me mf ky mt mg mh la mu mj mk lc mv mm mn le mw mp mq lg ig bi translated">模型训练</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="829d" class="lq jw in ne b gy ni nj l nk nl">history = model.fit_generator(train_generator,<br/>                         steps_per_epoch = len(train_generator),<br/>                         epochs = 5,<br/>                         validation_data = valid_generator,<br/>                         validation_steps = len(valid_generator),<br/>                         verbose=1</span></pre><p id="0491" class="pw-post-body-paragraph mc md in kv b kw ms me mf ky mt mg mh la mu mj mk lc mv mm mn le mw mp mq lg ig bi translated">绘制训练和测试曲线</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="a955" class="lq jw in ne b gy ni nj l nk nl">import matplotlib.pyplot as plt<br/>def plot_learningCurve(history, epoch):<br/>    <em class="mr"># Plot training &amp; validation accuracy values</em><br/>    epoch_range = range(1, epoch+1)<br/>    plt.plot(epoch_range, history.history['accuracy'])<br/>    plt.plot(epoch_range, history.history['val_accuracy'])<br/>    plt.title('Model accuracy')<br/>    plt.ylabel('Accuracy')<br/>    plt.xlabel('Epoch')<br/>    plt.legend(['Train', 'Val'], loc='upper left')<br/>    plt.show()<br/><br/>    <em class="mr"># Plot training &amp; validation loss values</em><br/>    plt.plot(epoch_range, history.history['loss'])<br/>    plt.plot(epoch_range, history.history['val_loss'])<br/>    plt.title('Model loss')<br/>    plt.ylabel('Loss')<br/>    plt.xlabel('Epoch')<br/>    plt.legend(['Train', 'Val'], loc='upper left')<br/>    plt.show()<br/><br/>plot_learningCurve(history, 5)</span></pre><figure class="mz na nb nc gt jo gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/79317ac019d2023e647c436b271ae518.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*OX4pZlMBiqJrJ4tDOM0bWA.png"/></div><figcaption class="no np gj gh gi nq nr bd b be z dk translated">输出</figcaption></figure><h1 id="5444" class="jv jw in bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">结束注释</h1><ol class=""><li id="f71f" class="kt ku in kv b kw kx ky kz la lb lc ld le lf lg mx li lj lk bi translated">在本笔记本中，我们使用了VGG19，但是您可以使用任何适用于所有型号<a class="ae my" href="https://keras.io/api/applications/" rel="noopener ugc nofollow" target="_blank">https://keras.io/api/applications/</a>的模型程序</li><li id="8be5" class="kt ku in kv b kw ll ky lm la ln lc lo le lp lg mx li lj lk bi translated"><strong class="kv io">input_shape</strong>-输入图像的大小，我们设置include_top='True '和weights= 'imagenet '，那么input _ shape必须是'(224，244，3)'。如果weights= 'imagenet '和include_top='True ',则我们无法训练自定义图像形状。如果您想根据自定义图像大小进行训练，请确保weights不等于“imagenet ”, include _ top不等于“True”。权重必须为零或include_top='False</li></ol><figure class="mz na nb nc gt jo gh gi paragraph-image"><div class="gh gi od"><img src="../Images/9deacaf219086d7eb7d146a10949c884.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*zUUpjIekdbgDVTKPvE3U7A.png"/></div></figure><p id="c5c0" class="pw-post-body-paragraph mc md in kv b kw ms me mf ky mt mg mh la mu mj mk lc mv mm mn le mw mp mq lg ig bi translated">3.如果layer_n='False '表示您不想训练该层，当layer_n='True '表示您我们想要训练该层时，您也可以说我们已经冻结了这些层</p><p id="642c" class="pw-post-body-paragraph mc md in kv b kw ms me mf ky mt mg mh la mu mj mk lc mv mm mn le mw mp mq lg ig bi translated"><strong class="kv io">代码</strong></p><figure class="mz na nb nc gt jo gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/7b189bf94f1859634a040858a9f7dfb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*WWvWI40vVdT8bppnKYI6OQ.png"/></div></figure><figure class="mz na nb nc gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi nz"><img src="../Images/9f25a18a873e9cbd7eaead5750a0db6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ET1byTvUFWXzLfEn0p1vxw.png"/></div></div></figure><p id="c23c" class="pw-post-body-paragraph mc md in kv b kw ms me mf ky mt mg mh la mu mj mk lc mv mm mn le mw mp mq lg ig bi translated">Kaggle笔记本-<a class="ae my" href="https://www.kaggle.com/rahulanand0070/all-you-need-to-know-about-transfer-learning-cnn/notebook" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/rahulanand 0070/all-you-need-know-about-transfer-learning-CNN/Notebook</a></p></div></div>    
</body>
</html>