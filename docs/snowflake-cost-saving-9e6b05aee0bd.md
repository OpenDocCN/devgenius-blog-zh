# 雪花—节约成本

> 原文：<https://blog.devgenius.io/snowflake-cost-saving-9e6b05aee0bd?source=collection_archive---------3----------------------->

![](img/99edc0dabcbfc61498e14137f5e1d77c.png)

在 [Unsplash](https://unsplash.com/s/photos/saving?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上由 [micheile dot com](https://unsplash.com/@micheile?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 拍摄的照片

降低雪花成本的技巧

> 我们的环境运行高效吗？

这是我们用雪花运行企业数据平台时经常遇到的一个问题。好消息是，雪花为我们提供了几个杠杆来最小化成本和最大化性能。

雪花费用分为以下几大类:

*   云服务(不太重要)
*   仓库
*   保管费用
*   区域内数据传输费用(外部表)
*   雪管
*   数据库复制和保留
*   自动聚类
*   物化视图维护
*   搜索优化服务
*   自动刷新外部表元数据

在这篇博客中，我们将讨论一些关于雪花节约成本的策略。

## 正确加载数据

*   我们应该将大文件分成较小的文件，然后使用 copy 命令加载，这样可以利用并行性，从而缩短完成时间。估计的文件大小应该在 90-100 MB 范围内。
*   我们应该按照我们希望使用的顺序加载数据。数据按照自然摄取顺序在雪花中自动分区。此外，从 s3 存储桶加载排序后的文件应该比使用 ORDER BY 子句插入要快得多。
*   我们应该使用 COPY INTO 而不是 INSERT，因为它利用了更有效的大容量装载过程。
*   对于外部表，我们应该加强与雪花帐户相同区域的数据文件。

## 合理确定仓库规模

*   我们应该将相似的工作负载分组到同一个虚拟仓库中。
*   我们应该为 ETL 和查询操作建立独立的专用仓库，以优化它们的性能。
*   我们应该监控查询性能，如果我们在查询概要文件中发现溢出，那么我们应该考虑更大的仓库。
*   如果我们观察查询排队，这应该是一个指标，要么增加大小，要么有更多的集群(首选)。
*   我们应该明确将仓库配置的控制权交给最终用户。他们应该没有更新 WH 配置的能力。

正如我提到的，扩展计算能力(更多的并发性)和扩展(更多的性能)我们需要在迭代函数中尝试这些来达到最佳效果。

## 暂停虚拟仓库

雪花旨在对我们的计算需求做出动态反应，因此我们应该利用它。当没有长时间执行的查询时，我们不应该让虚拟仓库闲置，例如批处理作业。最好在空闲几分钟后自动挂起它们。

```
alter warehouse <warehouse_name> set auto_suspend = <num_in_sec>;
```

或者，对于报告应用程序，我们可以采用多集群方法，根据并发运行的查询数量自动添加和删除仓库。

## 利用缓存

虚拟仓库缓存表示为一些较旧的查询而复制到该仓库 VM 的微分区。这在 QA 环境中非常有用。我们可以只加载一次数据，然后对数据重复运行规则。

## 为查询超时设置设置适当的非默认值

暂停正在运行的查询的默认值是 2 天。我们应该根据工作量将其设置为更保守的值。对于大多数情况，4-6 小时应该足够了。

```
STATEMENT_TIMEOUT_IN_SECONDS
```

## 在共享仓库中运行短期查询

如果我们有快速完成的查询(不到 1 分钟)，我们应该在共享仓库中运行这些查询，以提高仓库利用率。

## 监控**读者账号**

读者账户中共享数据的查询费用由提供商账户支付。因此，我们有必要监控共享数据的使用情况，并设置防护栏，以便它们不会因成本而失控。

## **在生产、开发和 UAT 环境之间使用零拷贝克隆**

如果我们有进行数据质量/回归测试的用例，那么我们可以利用零拷贝特性跨 env 拷贝表并节省成本。

*注意:当我们删除父/原始表时，存储所有权转移到克隆的表。因此，在删除父表时，要确保克隆的表在不使用时也被删除。*

## 巧妙使用临时、暂时和外部表格来节省成本

这些非永久表省略了故障安全和时间旅行，因此节省了存储成本。所以我们应该将它们用于管道中的中间表。

## 弃用未使用的表/对象

这是显而易见的。如果我们长时间不使用某个表，那么将该表保存在雪花中是没有意义的。我们可以将其卸载到外部位置，然后删除该表。要找出未被支持的表，我们可以查看`account_usage.access_history`。

此外，我们应该从内部阶段(用户、表和命名)中删除不再需要的文件。`remove @mystage/path1/subpath;`

如果我们使用 spark 加载数据到雪花，我们应该使用外部阶段(S3)。

## 拥有正确的桌子设计

我们应该尝试和测试表的性能，如果必要的话，在表的大小超过 1 TB 或者有大约 10 亿行的表中的字段上创建聚集键。

## 微调我们的 SQL

在这篇博客中，我想把讨论更多地放在节约成本的唾手可得的成果上。优化 SQL 是一个很大的话题，也很难控制。尽管如此，我们可以有一些明显的护栏。

*   避免选择*
*   使用 ANSI 连接
*   避免联合，排序依据
*   字符串上的日期或时间戳数据类型

## 设置每月信用消费阈值和警报

我们应该经常检查当前的成本。每月预算的阈值和警报将有助于我们更快地做出成本决策，并防止成本进一步增加。使用资源监视器，我们还可以看到使用中的异常情况。我们应该使用查询配置文件、信息模式、帐户用法和读者帐户用法来了解性能和成本。在查询历史记录中查找扫描的列。类似地，我们应该检查频繁使用的列的聚类深度。

我们可以开发一个仪表板，将性能和成本管理交给关键决策者。当成本突然增加时，它会发出警报。它还应该自动推荐补救方法。在这方面，有许多第三方解决方案值得关注。

不断重复这些过程，并将您自己的发现添加到您使用的知识库中。

仓储快乐！！