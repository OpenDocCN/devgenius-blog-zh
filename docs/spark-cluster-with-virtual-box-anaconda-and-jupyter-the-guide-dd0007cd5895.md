# ç«èŠ±é›†ç¾¤ä¸è™šæ‹Ÿç›’ï¼ŒèŸ’è›‡å’Œ Jupyter æŒ‡å—

> åŸæ–‡ï¼š<https://blog.devgenius.io/spark-cluster-with-virtual-box-anaconda-and-jupyter-the-guide-dd0007cd5895?source=collection_archive---------3----------------------->

![](img/957436b606a00c261a8b470543c348af.png)

â€”ç«èŠ±â€”

# æ”¾å¼ƒ

> æˆ‘ä¸ä¼šæ›´æ–°è¿™ä¸ªåšå®¢ï¼Œç›¸åï¼Œæˆ‘çš„ github ä¸Šçš„æ–‡æ¡£ä¼šéšç€æˆ‘çš„ä»»ä½•æ”¹å˜è€Œæ›´æ–°

[Github](https://github.com/produdez/sparkimental) = >è½¬è‡³`docs/spark-cluster-setup.md`ğŸ¡

***(æˆ‘ä¹Ÿè®¤ä¸ºåœ¨ markdown ä¸Šé˜…è¯»æ›´å¥½ï¼Œå› ä¸º medium ç¼ºå°‘è¿™ä¹ˆå¤šåŸºæœ¬çš„ç¼–è¾‘å¸ƒå±€åŠŸèƒ½)***

# åº

å¦‚æœæ‚¨åœ¨è¿›å…¥æ•°æ®ç§‘å­¦çš„æ—…ç¨‹ä¸­é‡åˆ°è¿‡`Spark`ï¼Œé‚£ä¹ˆå½“æ‚¨ç¬¬ä¸€æ¬¡é‡åˆ°å®ƒæ—¶ï¼Œå¯èƒ½ä¼šæœ‰ä¸€äº›åŸºæœ¬é—®é¢˜æŒ¥ä¹‹ä¸å»ï¼Œä¾‹å¦‚:

*   å¦‚ä½•è®¾ç½®`Spark`(æœ¬åœ°æœºè¿˜æ˜¯è™šæ‹Ÿæœºï¼Ÿ)(é›†ç¾¤è¿˜æ˜¯åªæ˜¯æœ¬åœ°è¿›ç¨‹ï¼Ÿ)
*   å¦‚ä½•åœ¨ä¸Šé¢è¿è¡Œä»£ç ï¼Ÿç›´æ¥ï¼Ÿé€šè¿‡ python è„šæœ¬ï¼Ÿ`jupyter`ï¼Ÿ
*   å“ªä¸ªç‰ˆæœ¬é€‚ç”¨äºå“ªä¸ªç‰ˆæœ¬ï¼Ÿ`Java` `Python` `Spark` `Scala`
*   è¿™æ•´ä»¶äº‹ä¸ºä»€ä¹ˆæ²¡æœ‰ä¸€ä¸ªè¶…çº§è¯¦ç»†çš„æ•™ç¨‹ï¼Ÿå¤§å¤šæ•°æ•™ç¨‹

1.  åªè¦ç¡®ä¿ Spark èƒ½å¤Ÿè¿è¡Œ(æœ¬åœ°æˆ–åˆ†å¸ƒå¼éƒ½æ²¡å…³ç³»)
2.  å®‰è£… Spark é›†ç¾¤ï¼Œä½†æ²¡æœ‰å±•ç¤ºå¦‚ä½•åœ¨å…¶ä¸Šè¿è¡Œä»£ç 
3.  ä¸åŒ…æ‹¬å¦‚ä½•ä¸ Spark ç”¨æˆ·ç•Œé¢äº¤äº’(å†å²ã€ä¸»ç”¨æˆ·ç•Œé¢ã€ä»ç”¨æˆ·ç•Œé¢ã€ä½œä¸šç›‘æ§ç”¨æˆ·ç•Œé¢ç­‰)
4.  æ²¡æœ‰ python è™šæ‹Ÿç¯å¢ƒè®¾ç½®æ¥ç®¡ç† Spark çš„ python åŒ…
5.  æ²¡æœ‰æåˆ°å¦‚ä½•å……åˆ†åˆ©ç”¨è™šæ‹Ÿæœº(å¦‚`VirtualBox`è™šæ‹Ÿæœº)æ¥å¸®åŠ©æˆ‘ä»¬çš„å¼€å‘è¿‡ç¨‹

æ‰€ä»¥åœ¨è¿™ç¯‡åšæ–‡ä¸­ï¼Œåœ¨ä¸ºæˆ‘çš„åˆ†å¸ƒå¼æ•°æ®å¤„ç†ä»»åŠ¡è¿›è¡Œç ”ç©¶æ—¶ï¼Œæˆ‘å·²ç»*(å°½æˆ‘æœ€å¤§çš„èƒ½åŠ›)*è®°å½•äº†æˆ‘æ˜¯å¦‚ä½•

1.  åœ¨ç”±è™šæ‹Ÿæœºç®±ç®¡ç†çš„è™šæ‹Ÿæœºä¸Šè®¾ç½®æˆ‘çš„ Spark é›†ç¾¤
2.  ä½¿ç”¨ Anaconda è®¾ç½® python ç¯å¢ƒ(è™šæ‹Ÿç¯å¢ƒ)æ¥ç®¡ç†è¿™äº›è™šæ‹Ÿæœºä¸Šçš„ python åŒ…
3.  è¿è¡Œ/æäº¤ python ä»£ç åˆ°æˆ‘æ­£åœ¨è¿è¡Œ spark é›†ç¾¤*(æˆ‘ä»¬ä¸æƒ³è¦ä»»ä½•æœ¬åœ°è¿›ç¨‹ Spark)*
4.  è§£å†³æˆ‘ä¸€è·¯ä¸Šé‡åˆ°çš„æ‰€æœ‰å°é—®é¢˜/é™·é˜±

> *æœ¬æŒ‡å—æ—¨åœ¨æ±‡é›†æˆ‘å¯¹è¯¥ä¸»é¢˜çš„æ‰€æœ‰ç ”ç©¶å‚è€ƒèµ„æ–™ï¼ŒåŒæ—¶ä¹Ÿæ—¨åœ¨å®Œå…¨é¿å…è¢«æ„šå¼„ğŸªåƒæˆ‘è¿™æ ·çš„äººğŸ™ˆ*

*æ‰€ä»¥è¯·åŸè°…æˆ‘çš„å†—é•¿çš„å¸–å­ğŸ“£æˆ‘ä»¬èµ°å§ğŸ‘ŸğŸªœ*

# ç›®å½•

[å‰è¨€](#165c)
[è™šæ¡†](#8aba)
[åˆ›å»ºåŸºç¡€ Linux](#b09e)
[Linux è®¾ç½®](#0761)
âˆ˜ [è§„æ ¼](#6e30)
âˆ˜ [æ­¥éª¤](#5ece)
âˆ˜ [æˆ‘ä»¬åœ¨å“ªé‡Œ](#3b05)
âˆ˜ [ä¸ºä»€ä¹ˆï¼Ÿ](#fdc7)
[å®‰è£…ç«èŠ±](#cef6)
âˆ˜ [æ­¥éª¤](#34b3)
âˆ˜ [è¿›åº¦å¦‚ä½•](#0ea6)
[æ–°èŠ‚ç‚¹](#827c)
âˆ˜ [å…‹éš†](#024a)
âˆ˜ [é‡å‘½åæœºå™¨](#aad7)
âˆ˜ [è”ç½‘](#cee9)
[ä»è®¾ç½®](#90e4)
[ä¸»è®¾ç½®](#58d0)

# è™šæ‹Ÿç›’å­

> *è™šæ‹Ÿæœºç®¡ç†å™¨*

*   6.1.38(æœ€ç¨³å®šçš„ç‰ˆæœ¬)(â‰ï¸ä¸è¦å®‰è£… 7.0 ä»¥ä¸Šï¼Œå› ä¸ºå®ƒæœ‰å¾ˆå¤šé”™è¯¯å’Œä¸ç¨³å®š)

[å‚è€ƒ](https://download.virtualbox.org/virtualbox/6.1.38/VirtualBox-6.1.38-153438-Win.exe)ï¼Œ[ä¸‹è½½](https://download.virtualbox.org/virtualbox/6.1.38/VirtualBox-6.1.38-153438-Win.exe)

# åˆ›å»ºåŸºæœ¬ Linux

> *è¿™å°†æ˜¯æˆ‘ä»¬çš„* `*building block*` *è™šæ‹Ÿæœºï¼Œæˆ‘ä»¬å°†å¯¹å…¶è¿›è¡Œå…‹éš†ï¼Œç„¶åè¿›ä¸€æ­¥è°ƒæ•´ä¸ºæˆ‘ä»¬çš„ä¸»èŠ‚ç‚¹å’Œå·¥ä½œèŠ‚ç‚¹*

# Linux è®¾ç½®

## è§„èŒƒ

*   Ubuntu 18.04 LTS ( [Ref](https://releases.ubuntu.com/18.04/) ï¼Œ[ä¸‹è½½ 64 ä½](https://releases.ubuntu.com/18.04/ubuntu-18.04.6-desktop-amd64.iso))
*   å»ºè®®çš„è™šæ‹Ÿæœºå­˜å‚¨å¤§å°:20GB (â—You'll å·²ç»ä¸ºæ‰€æœ‰åŸºæœ¬è½¯ä»¶åŒ…å®‰è£…ä½¿ç”¨äº†å¤§çº¦ 10GB)
*   RAM/CPU â†’ä½ çš„é€‰æ‹©ï¼Œæˆ‘çš„é€‰æ‹©:
    -ä¸»èŠ‚ç‚¹ 2 ä¸ª CPUï¼Œ4GB RAM
    -ä»èŠ‚ç‚¹é»˜è®¤(workers)

> Ram/Cpu å¯ä»¥å¾ˆå®¹æ˜“åœ°æ”¹å˜ä»¥åï¼Œæ‰€ä»¥æ²¡ä»€ä¹ˆå¤§ä¸äº†çš„ï¼

## æ­¥ä¼

> *ä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œæˆ‘å°†æˆ‘çš„ ubuntu æœºå™¨åå‘½åä¸ºæˆ‘çš„è™šæ‹Ÿæœºå™¨å*

1.  åœ¨ virtual box ä¸Šè¿è¡Œå®‰è£…ç¨‹åºï¼Œåˆ›å»ºä¸€ä¸ªç¬¦åˆä¸Šè¿°è§„æ ¼çš„æ–°çš„ç©ºè™šæ‹Ÿæœº

> æ³¨æ„ğŸ—’ï¸:æˆ‘ç»™è¿™å°æœºå™¨å–å`base-clean`

[è¯¦ç»†æ•™ç¨‹](https://medium.com/dfclub/create-a-virtual-machine-on-virtualbox-47e7ce10b21)(è¿™ä¸ªå¯¹æˆ‘æ¥è¯´å¤ªç®€å•äº†ï¼Œæ¶µç›–ä¸äº†)

> *è¯·æ³¨æ„ï¼Œæ‚¨åº”è¯¥ä½¿ç”¨åŠ¨æ€åˆ†é…*

2.å°†è™šæ‹Ÿæœºçš„ç½‘ç»œè®¾ç½®åˆ°æœ¬åœ°ä¸»æœº

*   å³é”®å•å‡»æœºå™¨>è®¾ç½®>ç½‘ç»œ
*   ç¡®ä¿`Adapter 2`æœ‰è¿™äº›é…ç½®(ä¸ºä»€ä¹ˆï¼Ÿ[å‚è€ƒå·](https://serverfault.com/questions/225155/virtualbox-how-to-set-up-networking-so-both-host-and-guest-can-access-internet)

![](img/7e4d34c3aa1775322fc55a5d47d82bc7.png)

3.è¿è¡Œæ–°çš„è™šæ‹Ÿæœºï¼Œé€‰æ‹©æˆ‘ä»¬çš„`Ubuntu` ISO å¹¶å¼€å§‹å®‰è£…(è¿™ä¸€æ­¥è€—æ—¶æœ€é•¿ğŸ˜·)

*   è¯·è¾“å…¥ç®€å•çš„ç”¨æˆ·åå’Œå¯†ç ğŸ™
*   ä¸éœ€è¦å…³å¿ƒæœºå™¨åï¼Œæˆ‘ä»¬æ€»æ˜¯å¯ä»¥æ”¹å˜å®ƒ

> æ³¨æ„ğŸ—’ï¸:æˆ‘çš„è®¾ç½®çš„ç”¨æˆ·åæ˜¯`prod`

4.ï¼ˆâ°å¯é€‰)æ›´æ–°å†…æ ¸/ç³»ç»Ÿ(è¿™å¯èƒ½æ˜¯ä¸€ä¸ªå±å¹• UI æç¤º)

5.ï¼ˆâ°å¯é€‰)å®‰è£…`Guest Addition CD Image`

*   è¿™æ ·æˆ‘ä»¬å°±å¯ä»¥åœ¨ä¸»æœº(*æ‚¨çš„ pc)* å’Œè™šæ‹Ÿæœºä¹‹é—´ä½¿ç”¨åŒå‘å‰ªè´´æ¿

*ä¸‹é¢ç¬¬ 5 æ­¥çš„è¯¦ç»†ä¿¡æ¯*ğŸ”½

a)å®‰è£…å…‰ç›˜

![](img/44cc69bcb9c4bdc35f93a647ea7d14ee.png)

b)ç­‰å¾…å®‰è£… CD+å¼¹å‡ºå®‰è£…çª—å£

![](img/ca30d5cdc6156b8352e3df6089dd71b5.png)

c)ç‚¹å‡»è¿è¡Œï¼Œç­‰åˆ°å®Œæˆï¼Œå¦‚æœä½ æ˜¯ç»…å£«ï¼Œå¼¹å‡ºå…‰ç›˜ğŸ©

d)é‡æ–°å¯åŠ¨è™šæ‹Ÿæœºå¹¶ç¡®ä¿å…±äº«å‰ªè´´æ¿æ­£å¸¸å·¥ä½œ

> æµ‹è¯•å‰ä¸è¦å¿˜è®°é€‰æ‹©å…±äº«å‰ªè´´æ¿é€‰é¡¹ğŸ˜‰

![](img/53999e14efbcb27fdf9cc0a217b175d0.png)

*å®Œæˆç¬¬äº”æ­¥*

6.ï¼ˆâ°å¯é€‰)æ›´æ–°åŒ…ç®¡ç†å™¨`apt-get`

```
sudo apt-get upgrade
```

7.è·å–è™šæ‹Ÿæœºçš„`ip address`

```
ip addr
```

> å‰ä¸¤ä¸ª IP ç”¨äºäº’è”ç½‘è¿æ¥ï¼Œåº”è¯¥æœ‰ç¬¬ä¸‰ä¸ª IP æŒ‡ç¤ºæˆ‘ä»¬çš„ä¸»æœºæœ¬åœ°ç½‘ç»œä¸­çš„è™šæ‹Ÿæœº IP(å¦‚æœç¼ºå°‘ä¸€ä¸ªæ¡ç›®ï¼Œè¯·é‡æ–°æ£€æŸ¥é€‚é…å™¨ 2 ç½‘ç»œè®¾ç½®)

![](img/ba2f646cff3829dfcb303788847b76a2.png)

æœ¬åœ°ç½‘ç»œä¸Šè™šæ‹Ÿæœºçš„ IP

è®°ä¸‹æœ¬åœ°ç½‘ç»œä¸Šè™šæ‹Ÿæœºçš„ IPï¼Œä»¥ä¾¿ä»¥åè¿›è¡Œé…ç½®

> è¯·æ³¨æ„ï¼ŒğŸ—’ï¸:æˆ‘çš„åŸºæœ¬æ¸…ç†è™šæ‹Ÿæœºçš„ IP åœ°å€æ˜¯ 192.168.56.105

8.å®‰è£…`SSH`

*   [è®¡] ä¸‹è½½

```
sudo apt-get install openssh-server openssh-client
```

*   ç”¨`ssh-keygen`ç”Ÿæˆå¯†é’¥

> æˆ–è€…ä½¿ç”¨`mkdir ~/.ssh`åˆ›å»º`.ssh`æ–‡ä»¶å¤¹ï¼Œå¦‚æœæ‚¨ä¸å¸Œæœ›æ‚¨çš„è™šæ‹Ÿæœºåœ¨å…‹éš†æ—¶æœ‰é‡å¤çš„ SSH å¯†é’¥

*   ï¼ˆâ°å¯é€‰)å°†ä¸»æœºçš„ SSH å¯†é’¥å¤åˆ¶åˆ°è™šæ‹Ÿæœºï¼Œä»¥ä¾¿ä»¥åå¿«é€Ÿè¿›è¡Œ SSH è®¿é—®ï¼Œè€Œæ— éœ€æ¯æ¬¡éƒ½é‡æ–°é”®å…¥å¯†ç 

```
# assuming you already have you ssh key generated on your host machine 
cat C:\Users\USER\.ssh\id_rsa.pub | ssh prod@192.168.56.105 â€œcat >> .ssh/authorized_keysâ€
```

*   ç”¨`ssh <username>@<ip-addr>`æµ‹è¯•(ä¸åº”è¯¥è¯¢é—®å¯†ç )

9.å®‰è£…`curl`(ç”¨äºä»¥åä¸‹è½½æ–‡ä»¶)

```
sudo apt-get install curl
```

10.å…³é—­æœºå™¨ç”µæºï¼ŒğŸ¥‚

11.å…‹éš†å½“å‰æœºå™¨(`base-clean`)ä½œä¸ºå¤‡ä»½ï¼Œå‘½åä¸º`base-installed`(ç”¨äºä¸‹ä¸€æ­¥)

> *âš¡æ€»æ˜¯å…‹éš† MAC åœ°å€ç­–ç•¥è®¾ç½®ä¸º* `*Generate new MAC address for all network adapters*` *(é¿å… IP é‡å¤)*

## æˆ‘ä»¬åœ¨å“ªé‡Œ

æˆ‘ä»¬ç°åœ¨æœ‰äº†ä¸€ä¸ª`base-clean` Linux è™šæ‹Ÿæœº

*   å…·æœ‰åŸºæœ¬çš„ç½‘ç»œè®¾ç½®
*   å®‰è£…å¹¶æ›´æ–°äº† Ubuntu
*   æ²¡æœ‰é¢å¤–åŒ…è£…çš„å®Œå…¨æ¸…æ´çŠ¶æ€
*   SSH è®¾ç½®ä¾¿äºä»æœ¬åœ°æœºå™¨çš„å¤–å£³è®¿é—®

## ä¸ºä»€ä¹ˆï¼Ÿ

*   è¿™ç§å¹²å‡€çš„çŠ¶æ€æœ‰åŠ©äºå›æ»šï¼Œä»¥é˜²æˆ‘ä»¬åé¢çš„`Spark/Python/Java/Scala/â€¦`åŒ…å®‰è£…å¤±è´¥ğŸ’€
*   SSH è®¾ç½®å…è®¸æˆ‘ä»¬ç¨ååœ¨`headless`æ¨¡å¼ä¸‹è¿è¡Œè¿™äº›æœºå™¨(æ²¡æœ‰ UI ),å¹¶ä½¿ç”¨`ssh`ä»æœ¬åœ°æœºå™¨ shell ä¸­è½»æ¾è®¿é—®å®ƒä»¬

# å®‰è£… Spark

> *ä¸‹ä¸€æ­¥æˆ‘ä»¬å°†ç»§ç»­åœ¨* `*base-installed*` *è™šæ‹Ÿæœº*ä¸Šå®‰è£…æ‰€éœ€çš„è½¯ä»¶åŒ…

## æ­¥ä¼

âš ï¸:è¿™äº›æ­¥éª¤éœ€è¦èŠ±è´¹ç›¸å½“å¤šçš„æ—¶é—´ğŸ‘¿

> *âš¡åº”è¯¥åœ¨æ­£å¸¸å¯åŠ¨(GUI)æ¨¡å¼ä¸‹è¿è¡Œæ‚¨æ–°å…‹éš†çš„è™šæ‹Ÿæœºä¸€æ¬¡ï¼Œä»¥è·å–å…¶ IPï¼Œç„¶åä½¿ç”¨ SSH shell è¿›è¡Œæ— å¤´è¿è¡Œï¼Œä»¥ä¾¿äºç®¡ç†*

1.  è¿æ¥åˆ°æœºå™¨`ssh <username>:<base-installed-VM-IP>`

> æ³¨æ„ğŸ—’ï¸:æˆ‘çš„`base-installed`è™šæ‹Ÿæœºçš„ IP æ˜¯`192.168.56.106`

2.Javaï¼ŒScala

```
sudo apt install default-jdk scala # Verify 
java -version; javac -version; scala -version;
```

å®‰è£…çš„ç‰ˆæœ¬åº”è¯¥æ˜¯

```
openjdk version â€œ11.0.16â€ 2022â€“07â€“19 OpenJDK Runtime Environment (build 11.0.16+8-post-Ubuntu-0ubuntu118.04) OpenJDK 64-Bit Server VM (build 11.0.16+8-post-Ubuntu-0ubuntu118.04, mixed mode, sharing) javac 11.0.16 Scala code runner version 2.11.12 â€” Copyright 2002â€“2017, LAMP/EPFL
```

3.ï¼ˆâ°å¯é€‰)Git

```
sudo apt-get install git git â€” version
```

> å®‰è£…çš„âš¡ Git åº”è¯¥æ˜¯ 2.17.1ï¼Œæ¯” 2.23 è¦æ—§ã€‚æ‰€ä»¥ç”¨`checkout`ä»£æ›¿`switch` ( [å‚è€ƒ](https://stackoverflow.com/questions/60754571/why-does-git-switch-checkout-not-switch-branch))

4.èŸ’è›‡

```
# download 
curl -O [https://repo.anaconda.com/archive/Anaconda3-2022.10-Linux-x86_64.sh](https://repo.anaconda.com/archive/Anaconda3-2022.10-Linux-x86_64.sh) # validate download 
sha256sum Anaconda3â€“2022.10-Linux-x86_64.sh # install (follow the instructions) 
bash Anaconda3â€“2022.10-Linux-x86_64.sh
```

å¦‚æœæ˜¾ç¤ºä»¥ä¸‹é€‰é¡¹ï¼Œæ¥å—å®ƒ

```
Do you wish the installer to initialize Anaconda3 by running conda init? # Answer yes :v
```

å®Œæˆåï¼Œç”¨`source ~/.bashrc`é‡æ–°åŠ è½½ bashï¼Œå¹¶ç¡®ä¿å¯ä»¥è¿è¡Œ`conda`å’Œ`python`

```
which python #should return 
/home/prod/anaconda3/bin/python
```

5.Spark ( [é¦–é¡µ](https://www.apache.org/dyn/closer.lua/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz)ï¼Œ[ä¸‹è½½ç«™ç‚¹](https://www.apache.org/dyn/closer.lua/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz)ï¼Œ[ä¸‹è½½é“¾æ¥](https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz))

```
curl -O [https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz](https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz) sha256sum spark-3.3.1-bin-hadoop3.tgz sudo tar xvf spark-3.3.1-bin-hadoop3.tgz
```

> æ³¨æ„ğŸ—’ï¸:è¿˜è®°å¾— SPARK çš„å®‰è£…æ–‡ä»¶å¤¹ï¼Œç›®å‰`~/spark-3.3.1-bin-hadoop3/`

6.åˆ›å»º Anaconda ç¯å¢ƒ(ç®¡ç†æˆ‘ä»¬çš„ python åŒ…)

> å‚è€ƒé¢å¤–éƒ¨åˆ†ä¸­çš„â€œåˆ›å»º`conda`ç¯å¢ƒâ€éƒ¨åˆ†
> 
> æ³¨æ„ğŸ—’ï¸:æˆ‘çš„`conda`ç¯å¢ƒè¢«å‘½åä¸º`sparkimental`ï¼Œå®ƒçš„ python è·¯å¾„æ˜¯`/home/prod/anaconda3/envs/sparkimental/bin/python`

7.ç«èŠ±â˜£ï¸çš„é…ç½®ç¯å¢ƒ

*   æ‰“å¼€`bashrc`

```
sudo nano ~/.bashrc
```

*   å°†è¿™äº›é™„åŠ åˆ°æ–‡ä»¶çš„æœ«å°¾

```
# 1 
export SPARK_HOME=~/spark-3.3.1-bin-hadoop3 
# 2 
export PATH=$PATH:$SPARK_HOME/bin 
export PATH=$PATH:$SPARK_HOME/sbin 
# 3 
export PATH=$PATH:~/anaconda3/bin 
# 4 
export PATH=$PATH:$JAVA_HOME/jre/bin 
# 5 
export PYTHONPATH=<path-to-python-binary-in-your-conda-env> #ex: /home/prod/anaconda3/envs/sparkimental/bin/python 
export PYSPARK_PYTHON=$PYTHONPATH 
export PYSPARK_DRIVER_PYTHON=$PYTHONPATH 
# 6 optional 
conda activate <your-conda-env-name> #ex: sparkimental
```

*   é‡è£…çŒ›å‡»`source ~/.bashrc`

> **ğŸ¤”ä¸ºä»€ä¹ˆæ˜¯è¿™äº›é…ç½®ï¼Ÿ** `**Explanation**`

ä¸‹é¢æ˜¯æˆ‘å¯¹ä¸Šé¢æ¯ä¸€è¡Œçš„ç®€å•è§£é‡Š

1.  è¿™å°±æ˜¯æˆ‘ä»¬å®‰è£… Spark çš„åœ°æ–¹
2.  æ·»åŠ  Spark çš„äºŒè¿›åˆ¶æ–‡ä»¶ä»¥ä¾¿äºè®¿é—®
3.  Anaconda äºŒè¿›åˆ¶æ–‡ä»¶
4.  JRE äºŒè¿›åˆ¶
5.  ä¸º`PySpark`é©±åŠ¨ç¨‹åº(ä¸»)å’Œå·¥ä½œç¨‹åº(ä»)è®¾ç½® python

> â—Very:é‡è¦çš„æ˜¯ï¼Œé©±åŠ¨ç¨‹åºçš„ python ç‰ˆæœ¬ä¸å·¥ä½œç¨‹åºçš„ç‰ˆæœ¬ç›¸åŒ

6.è¿™æ˜¯å¯é€‰çš„ï¼Œä»¥ç¡®ä¿æ‚¨æƒ³è¦çš„`conda`è™šæ‹Ÿç¯å¢ƒæ€»æ˜¯åœ¨ shell ä¸­è¢«æ¿€æ´»

*å®Œæˆç¬¬ 7 æ­¥*

8.æµ‹è¯•ç«èŠ±

è¿è¡Œä¸‹é¢çš„ä¸€äº›å‘½ä»¤ï¼Œç¡®ä¿æ²¡æœ‰é‡åˆ°é”™è¯¯
*ä½†æ˜¯å¯èƒ½ä¼šæœ‰ä¸€äº›è­¦å‘Š~*

*   `pyspark` â†’ç¡®ä¿`pyspark`åœ¨`sparkimental`(ä¹Ÿå°±æ˜¯æˆ‘ä»¬é¦–é€‰çš„`conda`ç¯å¢ƒ)ä¸­ä½¿ç”¨ python çš„åŒ¹é…ç‰ˆæœ¬

åº”è¯¥ä½¿ç”¨`python=3.10`(æ­£å¦‚æˆ‘ä»¬åœ¨åˆ›å»º python ç¯å¢ƒæ—¶é…ç½®çš„é‚£æ ·- `sparkimental`)

```
Using Python version 3.10.6 (main, Oct 24 2022 16:07:47)
```

âš ï¸ Ubuntu 18.04 é»˜è®¤é¢„è£…çš„ python ç‰ˆæœ¬æ˜¯ 3.6 Anaconda base python ç‰ˆæœ¬æ˜¯ 3.9
â€”â€”æ‰€ä»¥å¦‚æœè¿™ä¸¤ä¸ªç‰ˆæœ¬ä¸­çš„ä»»ä½•ä¸€ä¸ªå‡ºç°ï¼Œä½ çš„`.bashrc`å¾ˆå¯èƒ½é…ç½®ä¸è‰¯
â€”â€”*å‚è€ƒæœ€åçš„é™·é˜±éƒ¨åˆ†*

*   `spark-shell` â†’åº”è¯¥æ²¡æœ‰é”™è¯¯
*   è¿è¡Œé›†ç¾¤æµ‹è¯•
    â€” `start-all.sh` (âš ï¸å¦‚æœæœ‰æƒé™é”™è¯¯åªéœ€æ›´æ”¹æ–‡ä»¶å¤¹çš„æƒé™)
    â€”è¿è¡Œ`jps`å¹¶ç¡®ä¿ worker + master å¯ç”¨

```
# Expected output 
3907 Jps 
3723 Master 
3851 Worker
```

*   è¿è¡Œç¤ºä¾‹ python ä»£ç ( [Ref](https://stackoverflow.com/questions/25585194/standalone-apache-spark-what-to-put-as-slave-ip-and-port) )

```
# run from anywhere 
spark-submit - master spark://base-clean:7077 /home/prod/spark-3.3.1-bin-hadoop3/examples/src/main/python/pi.py 10
```

è¾“å‡ºé¢„æœŸä¸º:
â€” *è¿è¡Œ 10 ä¸ªä»»åŠ¡(æˆ‘ä»¬éœ€è¦ 10 ä¸ªä½œä¸š)*

```
â€¦â€¦â€¦â€¦ 
22/11/01 23:13:57 INFO TaskSchedulerImpl: Adding task set 0.0 with 10 tasks resource profile 0 
22/11/01 23:14:00 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.2.15:35918) with ID 0, ResourceProfileId 0 
22/11/01 23:14:00 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:36705 with 413.9 MiB RAM, BlockManagerId(0, 10.0.2.15, 36705, None) 
22/11/01 23:14:01 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.0.2.15, executor 0, partition 0, PROCESS_LOCAL, 4437 bytes) taskResourceAssignments Map() 
22/11/01 23:14:01 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:36705 (size: 8.6 KiB, free: 413.9 MiB) 
22/11/01 23:14:03 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (10.0.2.15, executor 0, partition 1, PROCESS_LOCAL, 4437 bytes) taskResourceAssignments Map() 
â€¦â€¦â€¦â€¦
```

â€”æ²¡æœ‰ Java å¼‚å¸¸
â€”è·å– Pi çš„ç»“æœ

```
â€¦â€¦â€¦â€¦ 
22/11/01 23:14:04 INFO DAGScheduler: Job 0 finished: reduce at /home/prod/spark-3.3.1-bin-hadoop3/examples/src/main/python/pi.py:42, took 8.446990 s Pi is roughly 3.143960 
â€¦â€¦â€¦â€¦
```

> æ³¨æ„ğŸ—’ï¸: `base-clean`è¿™é‡Œæ˜¯å½“å‰æœºå™¨çš„åç§°(å¯ä»¥ä½¿ç”¨`hostname`å‘½ä»¤æ£€æŸ¥)

*å®Œæˆç¬¬ 8 æ­¥*

9.ï¼ˆâ°å¯é€‰)`gparted`(ä»¥é˜²ä»¥åéœ€è¦è°ƒæ•´ç£ç›˜å¤§å°) [Ref](https://askubuntu.com/questions/101715/resizing-virtual-drive)

```
sudo apt-get install gparted
```

10.å…³æœºğŸŠ

## æˆ‘ä»¬åœ¨å“ªé‡Œ

*   åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œæˆ‘ä»¬åº”è¯¥æœ‰ä¸€ä¸ªæ”¯æŒ spark çš„ VM ( `spark-installed`)ï¼Œå®ƒå¯ä»¥:
    -è¿è¡Œ`pyspark`
    -è¿è¡Œ`spark-submit`ç¤ºä¾‹
    -å…·æœ‰å…¼å®¹çš„`python/ java/ scala/ spark`ç‰ˆæœ¬
    -ä»¥åŠé…ç½®å¥½çš„`conda`ç¯å¢ƒï¼Œå¦‚æœå‡ºç°ä»»ä½•é—®é¢˜ï¼Œå¯ä»¥è½»æ¾è¿›è¡ŒåŒ…ç®¡ç†
*   æˆ‘ä»¬ç°åœ¨éœ€è¦åšçš„å°±æ˜¯å‘ spark ç½‘ç»œæ·»åŠ ä¸»/ä»èŠ‚ç‚¹ï¼Œå¹¶å®Œå–„æˆ‘ä»¬çš„ä¸»/ä»è®¾ç½®

# æ–°èŠ‚ç‚¹

> *å¯¹äºæ·»åŠ çš„æ¯ä¸ªæ–°èŠ‚ç‚¹ï¼Œæ— è®ºæ˜¯ä»èŠ‚ç‚¹è¿˜æ˜¯ä¸»èŠ‚ç‚¹ï¼Œéƒ½å¿…é¡»é…ç½®è¿™äº›æ­¥éª¤*

1.  ä»`base-installed`å…‹éš†æ–°çš„è™šæ‹Ÿæœº
2.  ï¼ˆâ°å¯é€‰)é‡å‘½åæœºå™¨
3.  ç½‘ç»œå®‰è£…ç¨‹åº

*ä¸‹é¢è¯¦ç»†ä»‹ç»* â¬‡ï¸

## å…‹éš†

> *â—Every æ—¶é—´æ‚¨éœ€è¦å‘æˆ‘ä»¬çš„é›†ç¾¤æ·»åŠ ä¸€å°æ–°æœºå™¨(å³ä¸€ä¸ªæ–°çš„ä»æœº)åªéœ€å…‹éš†æˆ‘ä»¬å…¨åŠŸèƒ½çš„* `*base-install*` *è™šæ‹Ÿæœºå¹¶ç»§ç»­*

1.  å†³å®šæ˜¯è¦åœ¨`full`æ¨¡å¼è¿˜æ˜¯`linked`æ¨¡å¼ä¸‹å…‹éš†

> âš¡:å°±æˆ‘ä¸ªäººè€Œè¨€ï¼Œæˆ‘ä¼šæ¨èé“¾æ¥å…‹éš†ï¼Œåªè¦ç¡®ä¿ä½ é“¾æ¥çš„åŸå§‹è™šæ‹Ÿå­˜å‚¨ç£ç›˜æœ‰è¶³å¤Ÿçš„ç©ºé—´

2.å…‹éš†å®ƒ(å¦‚æœé€‰æ‹©äº†`full clone`ï¼Œå‡†å¤‡ç­‰å¾…ä¸€æ®µæ—¶é—´)

3.åœ¨ GUI æ¨¡å¼ä¸‹å¯åŠ¨æœºå™¨ï¼Œå¹¶è®°ä¸‹æœºå™¨çš„ IP/åç§°(`ip addr`)

> è¯·æ³¨æ„ï¼ŒğŸ—’ï¸:ï¼Œä»ç°åœ¨å¼€å§‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ IP å¯¹æœºå™¨è¿›è¡Œæ“ä½œ

4.ï¼ˆâ°å¯é€‰)å¦‚æœéœ€è¦ï¼Œè°ƒæ•´å­˜å‚¨å¤§å°*(å‚è€ƒæœ€åçš„ç¼ºé™·éƒ¨åˆ†)*

## é‡å‘½åæœºå™¨

é‡å‘½åæœºå™¨ä»¥ä¾¿ä¸`cli`åŒºåˆ†å¼€æ¥(å› ä¸ºæˆ‘ä»¬å°†ä»`shell/bash`é€šè¿‡`ssh`è®¿é—®å®ƒä»¬)

[re f1](https://www.cyberciti.biz/faq/ubuntu-change-hostname-command/)re F2

> *æ˜¾ç¤ºåœ¨ bash å’Œç½‘ç»œé€šè®¯ä¸Šçš„åå­—(ä¸æ˜¯ Virtua Box UI ä¸Šçš„åå­—*
> 
> *æ³¨æ„ğŸ—’ï¸:æˆ‘å°†ä½¿ç”¨* `*spark-master*` *ä½œä¸ºæˆ‘çš„ä¸»èŠ‚ç‚¹ï¼Œä½¿ç”¨* `*spark-slave-1 (2,3, ..)*` *ä½œä¸ºæˆ‘çš„ä»èŠ‚ç‚¹*

1.  `sudo nano /etc/hostname` â†’åˆ é™¤`<old-name>`ï¼Œæ¢æˆä½ çš„`<new-name>`
2.  `sudo nano /etc/hosts` â†’å°†`127.0.1.1 <old-name>`æ”¹ä¸º`127.0.1.1 <new-name>`
3.  `sudo hostname <new-name>`
4.  ç”¨`hostnamectl`å‘½ä»¤å’Œ`hostname`å‘½ä»¤éªŒè¯
5.  é‡è£…æœºå™¨
6.  ç°åœ¨ï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿä½¿ç”¨æœºå™¨çš„åç§°å¯¹å…¶è¿›è¡Œ`ssh`æ“ä½œ

```
# Ex (from local machine) 
ssh prod@spark-master # Instead of 
ssh prod@<a-long-ip-addr>
```

`ip addr`è¿˜åº”è¾“å‡ºä¸æœºå™¨åç§°åŒ¹é…çš„åœ°å€

## å»ºç«‹å·¥ä½œå…³ç³»ç½‘

1.  é…ç½®ç½‘ç»œ/IP åˆ—è¡¨ğŸ“µ

*   `sudo nano /etc/hosts` æ–‡ä»¶åº”åŒ…å«

```
127.0.0.1 localhost # Remove this entry below 
127.0.1.1 <this-current-machine-name> # ex: spark-master
```

*   åˆ é™¤ IP ä¸º`127.0.1.1`çš„æ¡ç›®ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªå›ç¯ï¼Œä¼šå¹²æ‰°ä»¥åä»æµè§ˆå™¨å¯¹ Spark UI çš„è®¿é—®*(å‚è€ƒæœ€åçš„é™·é˜±éƒ¨åˆ†)*
*   å°†ç½‘ç»œä¸­æ‰€æœ‰æœºå™¨çš„ IP å’Œåç§°æ·»åŠ åˆ°è¯¥æ–‡ä»¶ä¸­(åŒ…æ‹¬å½“å‰æœºå™¨çš„ IP æœ¬èº«)

```
# example 
192.168.56.107 spark-master 
192.168.56.108 spark-slave-1
```

> â€¼ï¸è®°å¾—åœ¨æ·»åŠ æ–°èŠ‚ç‚¹æ—¶æ›´æ–°è¿™ä¸ªåˆ—è¡¨

2.ï¼ˆâ°å¯é€‰)æ‚¨å¯ä»¥`ssh`ä»å½“å‰æœºå™¨åˆ°å…¶ä»–æœºå™¨ä½¿ç”¨ä»–ä»¬çš„åå­—ä»¥ç¡®ä¿ä»–ä»¬åœ¨ç½‘ç»œä¸Šç›¸äº’è¯†åˆ«

```
# from the spark-master's shell 
ssh prod@spark-slave-1 # should prompt for access and password
```

3.ç»§ç»­ä¸‹ä¸€æ­¥ï¼Œé…ç½®ä¸»/ä»ç»†èŠ‚

# ä»å±è®¾ç½®

> *æ­¤æ—¶ï¼Œä»æœºå·²ç»å®Œæˆé…ç½®ğŸ‘·â€â™‚ï¸ï¼Œä¸‹é¢çš„ä¸¤ä¸ªæ­¥éª¤å¯¹äºä¸€ä¸ªæ­£å¸¸å·¥ä½œçš„ä»èŠ‚ç‚¹*(æˆ‘ä»¬å·²ç»ç»å†è¿‡äº†)å·²ç»è¶³å¤Ÿäº†

*   ç«èŠ±å®‰è£…
*   IP é…ç½®

åªéœ€è½¬åˆ°ä¸»èŠ‚ç‚¹*(ä¸‹ä¸€æ­¥)*å¹¶åœ¨é‚£é‡Œæ›´æ–° spark çš„ç½‘ç»œé…ç½®(`spark-master`)

# ä¸»è®¾ç½®

> *åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åœ¨ä¸»è®¾å¤‡ä¸Šå®Œå–„äº†æˆ‘ä»¬çš„ spark è®¾ç½®ğŸ‘‘*

1.  ç¡®ä¿ä¸»äººçš„`/etc/hosts`ä¸­æ‰€æœ‰çš„å¥´éš¶éƒ½å¯ç”¨
2.  è®¾ç½® spark master ç¯å¢ƒ

```
cd $SPARK_HOME/conf 
cp spark-env.sh.template spark-env.sh 
sudo nano spark-env.sh # Add 
export SPARK_MASTER_HOST= <master-ip-addr> # Ex: 192.168.56.107 
export JAVA_HOME='/usr'
```

â“å¦‚ä½•æ‰¾åˆ°çˆªå“‡ pathâ“`which java`å¤åˆ¶è·¯å¾„å‰`/bin`

3.å°†ä»è®¾å¤‡æ·»åŠ åˆ° spark é…ç½®

```
cd $SPARK_HOME/conf 
sudo nano slaves #notice the 's' # add slave name from our network settings here spark-master 
spark-slave-1
```

> âš¡æˆ‘ä¹Ÿåœ¨æˆ‘çš„ä¸»èŠ‚ç‚¹ä¸Šå¯åŠ¨äº†ä¸€ä¸ªå·¥ä½œè¿›ç¨‹ï¼(ä¸è¦æ··æ·†)

4.å¦‚æœè¿˜æ²¡æœ‰ç”Ÿæˆ`ssh`é”®

```
ssh-keygen
```

5.å°†`ssh`é’¥åŒ™å¤åˆ¶ç»™æ‰€æœ‰å…¶ä»–å·¥äºº

```
ssh-copy-id prod@spark-master 
ssh-copy-id prod@spark-slave-1
```

> â€¼ï¸è®°å¾—åœ¨ä½ æ·»åŠ æ–°å¥´éš¶æ—¶æ›´æ–°`$SPARK_HOME/conf/slaves`ã€‚ç„¶åæŠŠä¸»äººçš„`ssh`é’¥åŒ™æŠ„è¿‡æ¥

*åŒæ ·ï¼Œå¯ä»¥ä»ä¸»æœºåˆ°ä»æœºæµ‹è¯•* `*ssh*` *ä»¥ç¡®ä¿æ²¡æœ‰ç»™å‡ºå¯†ç æç¤º*

6.ï¼ˆâ°å¯é€‰)è®¾ç½®`history-server`æ¥ç®¡ç†æ‰€æœ‰å®Œæˆçš„åº”ç”¨ç¨‹åºçš„æ—¥å¿—

*   åˆ›å»ºä¸€ä¸ªåŒ…å«æ—¥å¿—çš„æ–‡ä»¶å¤¹

```
mkdir ~/spark-logs
```

*   é…ç½®

```
cd $SPARK_HOME/conf 
cp spark-defaults.conf.template spark-defaults.conf sudo nano spark-defaults.conf # add these lines 
spark.eventLog.enabled true 
spark.eventLog.dir file://~/spark-logs 
spark.history.fs.logDirectory file://~/spark-logs
```

*   è¿è¡Œ`start-history-server.sh`
*   `jps`åº”æ˜¾ç¤º`HistoryServer`ä½œä¸ºè¾“å…¥
*   é€šè¿‡è½¬è‡³`spark-master:18080`æˆ–`<master-ip>:18080`è¿›è¡ŒéªŒè¯

![](img/52c640aa8a80be1647903b8ff3d240b7.png)

7.æµ‹è¯• Spark çš„é›†ç¾¤è®¾ç½®

*   è¿è¡Œé›†ç¾¤(ä¸»+æ‰€æœ‰ä»)

```
start-all.sh
```

*   `jps`è¾“å‡ºåº”è¯¥æœ‰`Worker`å’Œ`Master`è¿‡ç¨‹

```
3645 Worker 
3533 Master
```

*   åœ¨æµè§ˆå™¨ä¸ŠéªŒè¯é“¾æ¥`spark-master:8080`

![](img/6a040b29a2a3b033a6facb8ec13766fd.png)

è¯·æ³¨æ„æˆ‘ä»¬çš„å‘˜å·¥å„è‡ªçš„ IP åœ°å€

æ²¡æœ‰åº”ç”¨ç¨‹åºï¼Œå› ä¸ºç°åœ¨è¿˜æ²¡æœ‰æµ‹è¯•ä»»ä½•ä¸œè¥¿

8.è¿è¡Œç¤ºä¾‹å¹¶æŸ¥çœ‹ç»“æœ

> æˆ‘ä»¬å°†è¿è¡Œ spark å®‰è£…ä¸­å¯ç”¨çš„ç®€å• Pi è®¡ç®—ç¤ºä¾‹ï¼Œä»¥éªŒè¯æˆ‘ä»¬çš„é›†ç¾¤æ˜¯å¦æ­£å¸¸è¿è¡Œ

*   ç¡®ä¿ä¸»æœåŠ¡å™¨/ä»æœåŠ¡å™¨æ­£åœ¨è¿è¡Œ(å¦‚æœæ‚¨é…ç½®äº†å†å²æœåŠ¡å™¨ï¼Œå®ƒä¹Ÿåœ¨è¿è¡Œ)

```
jps # expected output 
4658 Jps 
4396 HistoryServer 
3886 Master 
3999 Worker
```

*   è¿è¡Œ`spark-submit`åˆ°æˆ‘ä»¬æ­£åœ¨è¿è¡Œçš„é›†ç¾¤

```
cd $SPARK_HOME 
spark-submit - master spark://spark-master:7077 ./examples/src/main/python/pi.py 10
```

*   (å¦‚æœæœ‰å†å²æœåŠ¡å™¨)è½¬åˆ°å†å²æœåŠ¡å™¨(`spark-master:18080` )
    -è¯¥æ—¥å¿—åº”è¯¥å­˜åœ¨

![](img/5417d591b421c3c7abc0c164bc45c5f3.png)

æˆ‘ä»¬çš„ç¤ºä¾‹ PythonPi è¿è¡Œæ—¥å¿—

æ·±å…¥äº†è§£å·¥ä½œçš„æ›´å¤šç»†èŠ‚

![](img/3d0102180314daf914cf91ff08ea5308.png)

ç‚¹å‡» AppId å

![](img/be55c7adb48ce19c93c7f846a468dc16.png)

ç‚¹å‡»èŒä½æè¿°å

ä¸€è·¯èµ°åˆ°ç»†èŠ‚ï¼Œä½ ä¼šçœ‹åˆ°æˆ‘ä»¬çš„å·¥ä½œä¸ 10 ä¸ªä»»åŠ¡åˆ†å¸ƒåœ¨æˆ‘ä»¬çš„ä¸¤ä¸ªé…ç½®å·¥äºº

![](img/503030ce555e6e00ae1a3f9ec87db6ea.png)

ä»»åŠ¡æ—¥ç¨‹è¡¨

![](img/d739670ecdacf1a5613b1009007868e2.png)

ä»»åŠ¡åˆ—è¡¨åŠå…¶è¿è¡Œè¿›ç¨‹ä½ç½®(ä»å± ips)

æ‚¨å¯ä»¥åˆ°å…¶ä»–é€‰é¡¹å¡äº†è§£æ›´å¤šè¯¦æƒ…
æ‰§è¡Œç¨‹åºé€‰é¡¹å¡æ˜¾ç¤º 1 ä¸ªé©±åŠ¨ç¨‹åºå’Œ 2 ä¸ªå·¥äººæ‰§è¡Œç¨‹åº

![](img/a7c50247410d050c87fc5a0906ff6929.png)

ğŸ‰è¿™å°±æ˜¯æˆ‘ä»¬éšæ—¶å¯ä»¥ä½¿ç”¨çš„ç«èŠ±ç°‡ğŸ‰

> *ğŸ–é¢å¤–:å¦‚æœæ‚¨ä¸æƒ³æ¯æ¬¡è¿è¡Œ* `*spark-submit*` *æ—¶éƒ½å¿…é¡»æŒ‡å®šä¸»èŠ‚ç‚¹é€‰é¡¹* `*--master spark://...*` *ï¼Œåªéœ€åœ¨* `*$SPARK_HOME/conf/spark-defaults.conf*`ä¸­è¿½åŠ 
> `*spark.master spark://spark-master:7077*` *æ¥è®¾ç½®ä¸»èŠ‚ç‚¹å³å¯*

# å¦‚ä½•ä¸ºé›†ç¾¤ç¼–å†™ä»£ç 

## ä½¿ç”¨ spark-æäº¤

ç®€å•ï¼Œåªæ˜¯

```
import pyspark# do your code
```

å¹¶ä¸`spark-submit <path-to-py-file>`ä¸€èµ·æäº¤

## ä½¿ç”¨ python

> `*python*` *äºŒè¿›åˆ¶ä¸çŸ¥é“æˆ‘ä»¬æœ¬åœ°å®‰è£…äº†* `*pyspark*` *æ‰€ä»¥æˆ‘ä»¬éœ€è¦ä¸€ä¸ªåä¸º* `*findspark*`çš„æ”¯æŒåŒ…

```
# this first
import findspark
findspark.init()
findspark.find()# then import pyspark
import pyspark# your codes
```

ç”¨`python <path-to-py-file>`è¿è¡Œ

â—Note:è®°ä½å°†ä¸»èŠ‚ç‚¹è®¾ç½®ä¸ºé›†ç¾¤ä¸­å½“å‰è¿è¡Œçš„ä¸»èŠ‚ç‚¹ï¼Œæ–¹æ³•æ˜¯

*   åœ¨`$SPARK_HOME/config/spark-defaults.conf`ä¸­å°†`spark.master`è®¾ç½®ä¸ºé»˜è®¤å€¼
*   ä½¿ç”¨`SparkConf` ( [Ref](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.SparkConf.html#pyspark.SparkConf) )åœ¨æ‚¨çš„è„šæœ¬(`.py`)ä¸­è®¾ç½®å®ƒ

```
conf = SparkConf() 
conf.setMaster("spark://spark-master:7077") 
sc = SparkContext.getOrCreate(conf)
```

âš ï¸å¦‚æœä½ ä¸è¿™æ ·é…ç½®ï¼Œä½ çš„ä»£ç å°†åœ¨ä¸»èŠ‚ç‚¹ä¸Šæ–°åˆ›å»ºçš„(ä¸´æ—¶çš„)è¿›ç¨‹ä¸Šè¿è¡Œï¼Œè€Œä¸æ˜¯è¢«æ·»åŠ åˆ°ä½ å½“å‰è¿è¡Œçš„é›†ç¾¤ä¸­

## ä½¿ç”¨ç¬”è®°æœ¬

> `*Jupyter*` *åº”ä»…ç”¨äºå¼€å‘/è°ƒè¯•ç›®çš„ï¼Œå®é™…ç”Ÿäº§ä»£ç åº”é€šè¿‡* `*spark-submit*`æ±‡æ€»è‡³é›†ç¾¤

1.  ç¡®ä¿å®‰è£…äº†`jupyter`
2.  ä»ä¸»èŠ‚ç‚¹(`spark-master`)ç”¨`jupyter notebook`å¯åŠ¨`jupyter`ç¬”è®°æœ¬è¿›ç¨‹
3.  æ‰“å¼€ä»æœ¬åœ°æœºå™¨åˆ°`spark-master`çš„`ssh`éš§é“

```
ssh -N -L 8888:localhost:8888 prod@spark-master 
# this makes anyone accessing localhost:8888 be tunneled to prod@spark-master:8888
```

ğŸŒ€æˆ–è€…åªéœ€é€šè¿‡`jupyter notebook --ip 0.0.0.0`å¯åŠ¨`0.0.0.0`ä¸Šçš„`jupyter`ï¼Œå¹¶åœ¨`spark-master:8888`ä»æœ¬åœ°æœºå™¨çš„æµè§ˆå™¨ç›´æ¥è®¿é—®å®ƒ

4.åœ¨æ‚¨çš„æœ¬åœ°æœºå™¨ä¸Šï¼Œè½¬åˆ°`localhost:8888`å¹¶è¾“å…¥è®¿é—®ä»¤ç‰Œ/å¯†ç å¼€å§‹å·¥ä½œ

5.(ğŸ—’ï¸æ³¨)ç¡®ä¿åœ¨ç¬”è®°æœ¬ä¸­é€‰æ‹©æ‚¨æƒ³è¦çš„ python ç¯å¢ƒä½œä¸ºå†…æ ¸( [Ref](https://towardsdatascience.com/get-your-conda-environment-to-show-in-jupyter-notebooks-the-easy-way-17010b76e874) )

```
conda activate spakimental # if not already 
ipython kernel install - user - name=sparkimental# then select "sparkimental" in notebook
```

6.ç±»ä¼¼äºç›´æ¥ä½¿ç”¨`python` bin è¿è¡Œï¼Œæˆ‘ä»¬çš„ä»£ç éœ€è¦è¿™äº›æ­¥éª¤

*   å¯»æ‰¾ç«èŠ±

```
import findspark 
findspark.init() 
findspark.find()
```

*   å®‰è£…ËŒä½¿æˆå½¢

```
from pyspark import SparkConf 
conf = SparkConf() 
# if not already configured in spark-defaults.conf 
conf.setMaster('spark://spark-master:7077') 
# if you dont set app name, our jupyter job will be named `spark-shell` 
conf.setAppName('jupyter job');
```

*   åˆ›é€ ç«èŠ±æƒ…å¢ƒï¼Œç–¯ç‹‚

```
sc = SparkContext.getOrCreate(conf)
```

åœ¨æ‰§è¡Œå®Œä¸Šé¢ä¸€è¡Œä¹‹åï¼Œ`spark-master:8080`çš„`spark-master` web UI åº”è¯¥ä¼šæ˜¾ç¤ºä¸€ä¸ªæ–°çš„æ­£åœ¨è¿è¡Œçš„åº”ç”¨ç¨‹åº

![](img/048f4fa3eae5599a12c7517ed7dac22f.png)

# ä¸´æ—¶æ¼”å‘˜

## åˆ›é€ `conda`ç¯å¢ƒ

> *æ¥è‡ªä¸€ä¸ª* `*config.yml*` *æ–‡ä»¶*

```
# example conda.env.yml config file content
name: sparkimental # this is environment name
dependencies: # and the packages we need
	# must have these 2
	- python=3.10
	- conda-forge::findspark
	# NOTE! don't install pyspark and java here since we already installed them
	# ones below are optional
	- jupyter
	- ipython
	- nltk
	- ipykernel
	- numpy
```

*   ä»é…ç½®æ–‡ä»¶
    åˆ›å»ºç¯å¢ƒ`conda env create -f conda.env.yml` â—Careful å…³äº`yml`æ–‡ä»¶( [ref](https://stackoverflow.com/questions/57381678/how-to-create-conda-environment-with-yml-file-without-this-error) )
*   è¯·åœ¨ä»»ä½•å…¶ä»–æ­¥éª¤ä¹‹å‰æ¿€æ´»ç¯å¢ƒï¼Œå§‹ç»ˆç¡®ä¿æ‚¨æ­£åœ¨æ­£ç¡®çš„`conda`ç¯å¢ƒ
    `conda activate sparkimental`ä¸­æ‰§è¡Œä»£ç 
*   å¦‚æœè¦æ¸…é™¤ç¯å¢ƒ
    `conda activate base
    conda env remove -n sparkimental -y`

# é™·é˜±

*   ç”¨å°½å­˜å‚¨ç©ºé—´â†’ [(å¦‚ä½•è°ƒæ•´å¤§å°)](https://askubuntu.com/questions/101715/resizing-virtual-drive)
*   è¿è¡Œ spark æ—¶ä¸å…è®¸åˆ›å»ºæ—¥å¿—â†’åªæ›´æ”¹è¢«æ‹’ç»è®¿é—®çš„æ–‡ä»¶å¤¹çš„æƒé™`chmod -R 777 dirname`
*   spark master ç›‘å¬çš„é»˜è®¤æœåŠ¡ç«¯å£æ˜¯ä»€ä¹ˆï¼Ÿ`7077` ( [å‚è€ƒ 1](https://spark.apache.org/docs/latest/spark-standalone.html) ï¼Œ[å‚è€ƒ 2](https://stackoverflow.com/questions/25585194/standalone-apache-spark-what-to-put-as-slave-ip-and-port) )
*   worker å’Œ driver ä¹‹é—´çš„ Python ä¸åŒ¹é…â†’åªéœ€ç¡®ä¿ä¸¤è€…éƒ½è®¾ç½®ä¸ºç›¸åŒçš„ python(å»ºè®®ä½¿ç”¨ç»å¯¹è·¯å¾„)
    - [ä¸åŒ¹é… Ref](https://stackoverflow.com/questions/54115290/mismatch-between-python-version-in-spark-worker-and-spark-driver)-[å¦‚ä½•åœ¨ Linux ä¸Šæ£€æŸ¥æ‰€æœ‰ python ç‰ˆæœ¬](https://stackoverflow.com/questions/30464980/how-to-check-all-versions-of-python-installed-on-osx-and-centos)
*   ç«èŠ±å›é€ IP é—®é¢˜([é—®é¢˜æè¿°](https://support.datastax.com/s/article/Spark-hostname-resolving-to-loopback-address-warning-in-spark-worker-logs))

```
Your hostname, â€¦ resolves to a loopback address: 127.0.0.1; using 10.1.2.1 instead
```

é€šè¿‡ä»`/etc/hosts`ä¸­åˆ é™¤`127.0.1.1`æ¡ç›®æ¥è§£å†³

# å‚è€ƒ

> *æœ¬æ–‡ä¸»è¦åŸºäºä»¥ä¸‹è§‚ç‚¹/æŒ‡å¯¼ğŸ™‡ï¼Œæˆ‘æ„Ÿè°¢æ‰€æœ‰å…¶ä»–ä½œè€…â£ï¸*

*   åŸºç¡€ã€https://www.guru99.com/pyspark-tutorial.html#4ã€‘æ•™ç¨‹:[åŸºç¡€](https://www.guru99.com/pyspark-tutorial.html#4)
*   å¦‚ä½•è®¾ç½®`Spark`é›†ç¾¤:[https://medium . com/@ joo Torres _ 11979/how-to-install-and-setup-an-Apache-spark-cluster-on-Hadoop-18-04-b4d 70650 ed 42](https://medium.com/@jootorres_11979/how-to-install-and-set-up-an-apache-spark-cluster-on-hadoop-18-04-b4d70650ed42)
*   å¸¦ç€`Jupyter`ç¬”è®°æœ¬è·‘`PySpark`:[https://blog . dev genius . io/a-convenient-way-to-run-py spark-4e 84 a 32 f 00 b 7](/a-convenient-way-to-run-pyspark-4e84a32f00b7)
*   Spark å†å²æœåŠ¡å™¨è®¾ç½®:[https://sparkbyexamples . com/spark/spark-History-Server-to-monitor-applications/](https://sparkbyexamples.com/spark/spark-history-server-to-monitor-applications/)

# ç»“è®º

æˆ‘ä»¬æœ‰ it ğŸ¥³ï¼Œä¸€ä¸ªè¿è¡Œ spark çš„å…¨åŠŸèƒ½è™šæ‹Ÿæœºé›†ç¾¤ï¼Œæ‚¨å¯ä»¥ä»æœ¬åœ°è®¡ç®—æœºç®¡ç†å®ƒï¼Œå¹¶ä»¥ä»»ä½•æ–¹å¼è¿è¡Œä»£ç ã€‚

è¯·å¯¹æˆ‘çš„è¯é¢˜å‘è¡¨ä½ çš„çœ‹æ³•ï¼Œå¦‚æœæœ‰ä»»ä½•å¸®åŠ©ï¼Œè¯·é¼“æŒã€‚å¾ˆä¹æ„æ”¶åˆ°ä½ çš„æ¥ä¿¡ğŸ˜¸

æ„Ÿè°¢é˜…è¯»ğŸ–¤ï¼Œç»§ç»­é—ªè€€ğŸ˜‰ğŸ’«