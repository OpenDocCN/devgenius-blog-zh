<html>
<head>
<title>Apache Spark : AQE(Adaptive Query Execution)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Apache Spark : AQE(自适应查询执行)</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/apache-spark-aqe-adaptive-query-execution-d754afff96ba?source=collection_archive---------11-----------------------#2022-12-06">https://blog.devgenius.io/apache-spark-aqe-adaptive-query-execution-d754afff96ba?source=collection_archive---------11-----------------------#2022-12-06</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><h1 id="1644" class="jk jl in bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">Spark 自适应查询执行</h1><p id="0f33" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">自适应查询执行(AQE)是 Spark 3.0 最重要的特性之一，它根据在查询执行期间收集的运行时统计信息来重新优化和调整查询计划。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi lg"><img src="../Images/b23aec2bc3c8adbfe103099f19d637b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jhivdDiEYfOaViR0M-OC7g@2x.png"/></div></div></figure><p id="f0a1" class="pw-post-body-paragraph ki kj in kk b kl ls kn ko kp lt kr ks kt lu kv kw kx lv kz la lb lw ld le lf ig bi translated">在 spark 3.0 中，AQE 受到以下支持:</p><p id="36c9" class="pw-post-body-paragraph ki kj in kk b kl ls kn ko kp lt kr ks kt lu kv kw kx lv kz la lb lw ld le lf ig bi translated"><strong class="kk io">🎯动态切换加入策略</strong></p><p id="fb68" class="pw-post-body-paragraph ki kj in kk b kl ls kn ko kp lt kr ks kt lu kv kw kx lv kz la lb lw ld le lf ig bi translated"><strong class="kk io">🎯动态合并洗牌分区</strong></p><p id="2edf" class="pw-post-body-paragraph ki kj in kk b kl ls kn ko kp lt kr ks kt lu kv kw kx lv kz la lb lw ld le lf ig bi translated"><strong class="kk io">🎯动态处理偏斜连接</strong></p><h2 id="06f0" class="lx jl in bd jm ly lz dn jq ma mb dp ju kt mc md jy kx me mf kc lb mg mh kg mi bi translated"><strong class="ak">🎯动态切换加入策略:</strong></h2><p id="f33b" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">当任何连接端的运行时统计数据小于自适应广播散列连接阈值时，Spark 会将排序-合并连接切换到广播散列连接。通过这样做，我们可以节省两个表上的排序操作，并在本地读取 shuffle 文件以节省网络。</p><p id="cdb5" class="pw-post-body-paragraph ki kj in kk b kl ls kn ko kp lt kr ks kt lu kv kw kx lv kz la lb lw ld le lf ig bi translated"><strong class="kk io">重要调整变量:</strong></p><p id="feb3" class="pw-post-body-paragraph ki kj in kk b kl ls kn ko kp lt kr ks kt lu kv kw kx lv kz la lb lw ld le lf ig bi translated"><strong class="kk io">如果洗牌必须在本地完成，traffic . spark . SQL . adaptive . localshufflereader . enabled</strong>需要为真。</p><p id="11de" class="pw-post-body-paragraph ki kj in kk b kl ls kn ko kp lt kr ks kt lu kv kw kx lv kz la lb lw ld le lf ig bi translated"><code class="fe mj mk ml mm b">spark.sql.adaptive.autoBroadcastJoinThreshold</code> <strong class="kk io"> 10Mb </strong>，如果该值设置为-1，则广播将被禁用</p><h2 id="5980" class="lx jl in bd jm ly lz dn jq ma mb dp ju kt mc md jy kx me mf kc lb mg mh kg mi bi translated"><strong class="ak">🎯动态合并混洗分区</strong></h2><p id="d21d" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">混洗操作在 spark 应用程序的性能中起着关键作用。Shuffle 的一个关键属性是分区的数量。如果我们设置几个分区，那么每个分区的数据大小可能会非常大，处理这些大分区的任务可能需要将数据溢出到磁盘，因此会降低查询速度。如果分区太多，那么每个分区的数据大小可能会非常小，并且会有很多小的网络数据读取来读取 shuffle 块，这也会降低查询的速度。为了解决这个问题，我们在开始时设置了一个相对较大的分区，spark 和 AQE 将这些小分区合并成一个。因此 spark 总是保持最佳分区来执行洗牌</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi mn"><img src="../Images/114f289f964b52bbd274409fb1ab0cfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VG4CdzRAMUaBu7Jg4YAJlw.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk translated">图片来源:<a class="ae ms" href="https://www.databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html" rel="noopener ugc nofollow" target="_blank"> Databricks 博客</a></figcaption></figure><p id="adfb" class="pw-post-body-paragraph ki kj in kk b kl ls kn ko kp lt kr ks kt lu kv kw kx lv kz la lb lw ld le lf ig bi translated">在上面的例子中，我们需要在所有 5 个分区上执行 Shuffle 操作。现在，如果启用了 AQE，spark 会加入较小的分区，并在较少的分区上执行 Shuffle。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/adaf7159ea5a80e15f683a5c58dd8d20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*zAi-F_Y54T7565KR.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk translated">图片来源:<a class="ae ms" href="https://www.databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html" rel="noopener ugc nofollow" target="_blank"> Databricks 博客</a></figcaption></figure><p id="5f28" class="pw-post-body-paragraph ki kj in kk b kl ls kn ko kp lt kr ks kt lu kv kw kx lv kz la lb lw ld le lf ig bi translated"><strong class="kk io">重要调整变量:</strong></p><p id="8425" class="pw-post-body-paragraph ki kj in kk b kl ls kn ko kp lt kr ks kt lu kv kw kx lv kz la lb lw ld le lf ig bi translated"><code class="fe mj mk ml mm b">spark.sql.adaptive.coalescePartitions.enabled</code> <strong class="kk io">真</strong></p><p id="bb69" class="pw-post-body-paragraph ki kj in kk b kl ls kn ko kp lt kr ks kt lu kv kw kx lv kz la lb lw ld le lf ig bi translated"><code class="fe mj mk ml mm b">spark.sql.adaptive.coalescePartitions.minPartitionSize</code> <strong class="kk io"> 1MB。</strong>合并后混洗分区的最小大小。</p><h2 id="999c" class="lx jl in bd jm ly lz dn jq ma mb dp ju kt mc md jy kx me mf kc lb mg mh kg mi bi translated"><strong class="ak">🎯动态处理偏斜连接</strong></h2><p id="e8d7" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">这个特性与合并分区正好相反。在我们的大多数应用中，我们需要处理数据偏斜。严重的不对称会显著降低查询性能，尤其是对于连接。因此 Spark 有能力识别这种偏斜。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi mu"><img src="../Images/b91444c1a7fe525e6c6c7a590cb6e106.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gqnYQFXrwZ4iM5Ug.png"/></div></div></figure><p id="a6de" class="pw-post-body-paragraph ki kj in kk b kl ls kn ko kp lt kr ks kt lu kv kw kx lv kz la lb lw ld le lf ig bi translated">在这个例子中，你可以注意到<strong class="kk io"> A0 </strong>分区与其他分区相比相当大，因此 A0 需要被分成两个分区。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi mu"><img src="../Images/7b13e2dc1ddc0f690cf2a183f3c36dc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2-5PQ9WpUG3iv6B7.png"/></div></div></figure><p id="936c" class="pw-post-body-paragraph ki kj in kk b kl ls kn ko kp lt kr ks kt lu kv kw kx lv kz la lb lw ld le lf ig bi translated"><strong class="kk io">重要的调谐变量:</strong></p><p id="7baf" class="pw-post-body-paragraph ki kj in kk b kl ls kn ko kp lt kr ks kt lu kv kw kx lv kz la lb lw ld le lf ig bi translated">通过将<strong class="kk io">spark . SQL . adaptive . skew join . enabled</strong>设置为 true，可以在 spark 中启用该特性，默认值为 True。</p><p id="0a05" class="pw-post-body-paragraph ki kj in kk b kl ls kn ko kp lt kr ks kt lu kv kw kx lv kz la lb lw ld le lf ig bi translated"><code class="fe mj mk ml mm b">spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes</code>T10】256 MB。如果一个分区的大小(以字节为单位)大于该阈值，则该分区被认为是倾斜的。</p><p id="bf3a" class="pw-post-body-paragraph ki kj in kk b kl ls kn ko kp lt kr ks kt lu kv kw kx lv kz la lb lw ld le lf ig bi translated"><code class="fe mj mk ml mm b">spark.sql.adaptive.skewJoin.skewedPartitionFactor</code> <strong class="kk io"> 5 </strong>。如果一个分区的大小大于该因子乘以分区大小的中值，则该分区被认为是倾斜的</p><h2 id="10c0" class="lx jl in bd jm ly lz dn jq ma mb dp ju kt mc md jy kx me mf kc lb mg mh kg mi bi translated">参考:</h2><ul class=""><li id="74ba" class="mv mw in kk b kl km kp kq kt mx kx my lb mz lf na nb nc nd bi translated"><a class="ae ms" href="https://spark.apache.org/docs/latest/sql-performance-tuning.html#adaptive-query-execution" rel="noopener ugc nofollow" target="_blank">https://spark . Apache . org/docs/latest/SQL-performance-tuning . html # adaptive-query-execution</a></li><li id="6211" class="mv mw in kk b kl ne kp nf kt ng kx nh lb ni lf na nb nc nd bi translated"><a class="ae ms" href="https://www.databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html" rel="noopener ugc nofollow" target="_blank">https://www . databricks . com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-SQL-at-runtime . html</a></li></ul></div></div>    
</body>
</html>