<html>
<head>
<title>Solving Sudoku in real-time using a Convolutional Neural Network and OpenCV</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用卷积神经网络和 OpenCV 实时求解数独</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/solving-sudoku-in-real-time-using-a-convolutional-neural-network-and-opencv-e47a92478dce?source=collection_archive---------1-----------------------#2022-05-27">https://blog.devgenius.io/solving-sudoku-in-real-time-using-a-convolutional-neural-network-and-opencv-e47a92478dce?source=collection_archive---------1-----------------------#2022-05-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="c13c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有一次，我决定用机器学习和图像处理来检验是否有可能实时解决数独问题。我最初认为这可以在 2-3 小时内完成，因为可以找到现成的库来满足各种需求。剧透:我的估计大错特错，但它最终成为了一个有趣的教程。</p><p id="2d33" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">数独是一种基于逻辑的游戏。这个游戏的想法是将数字放在 9x9 的网格上，这样每一列、每一行和每一个 3x3 的子网格都包含从 1 到 9 的所有数字。实时寻找数独解需要几个步骤，本文将对此进行描述:</p><ul class=""><li id="2b36" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">从网络摄像头实时获取图像帧。我希望通过将摄像机对准数独板找到解决方案。这也提出了一些性能要求，例如，视频帧速率不应低于 15 fps。</li><li id="f186" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">在每一帧上寻找数独板的轮廓。有现成的算法可以检测轮廓，但图像可能包含许多额外的细节，其中一些是不需要的，我们需要过滤它们。</li><li id="3cde" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">从棋盘上提取单个数字图像并识别它们。神经网络可以识别图像上的数字，但我们需要为它准备数据。</li><li id="4429" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">如果可能的话，为董事会寻找解决方案。</li><li id="581b" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">将数字放回显示的图像上。我们在屏幕上看到的是一个网络摄像头。为了使这个过程更有趣，我们将把丢失的数字放在原来板上它们应该在的地方。</li></ul><h1 id="56b2" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">寻找数独板</h1><p id="b497" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">在第一次测试中，我简单地从维基媒体的网页上截取了随机数独板的截图:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi md"><img src="../Images/7b4269532324f87e849838ebf9e8d41b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yacgs7fxPh19S3MFGj8r-w.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">数独板截图(图片由作者提供)</figcaption></figure><p id="4458" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们使用 OpenCV 加载图像:</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="aef7" class="my la iq mu b gy mz na l nb nc">import cv2</span><span id="9c35" class="my la iq mu b gy nd na l nb nc">img = cv2.imread("sudoku.png")</span></pre><p id="f50e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">预处理有两个步骤。首先，我需要<strong class="jp ir">得到数独板的轮廓</strong>。为此，我使用 OpenCV 的<a class="ae mc" href="https://docs.opencv.org/4.x/da/d22/tutorial_py_canny.html" rel="noopener ugc nofollow" target="_blank"> Canny 边缘检测</a>和<em class="ne"> findContours </em>方法:</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="b5dc" class="my la iq mu b gy mz na l nb nc">img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<br/>edged = cv2.Canny(img_gray, 170, 490)<br/>contours, _ = cv2.findContours(edged, cv2.RETR_EXTERNAL, <br/>                               cv2.CHAIN_APPROX_SIMPLE)</span></pre><p id="beda" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">结果将是这样的:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nf"><img src="../Images/19c63f154d928df7b8a7fcb512505729.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gmO-y5zdJNqjR1t-hvO8jQ.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">边缘检测后的图像(图片由作者提供)</figcaption></figure><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nf"><img src="../Images/d30a21e4207a28009b63375810b7842a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*up1qycTEICnIQrD9JKqXCg.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">检测轮廓后的图像(图片由作者提供)</figcaption></figure><p id="e945" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">轮廓将用于寻找数独板。但是正如我们所看到的，有许多小轮廓我们必须忽略，我们稍后将在代码中添加一些规则。</p><p id="6da4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其次，我创建了图像的<strong class="jp ir">单色副本</strong>，用于 OCR:</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="e261" class="my la iq mu b gy mz na l nb nc">blurred = cv2.GaussianBlur(img_gray, (11, 11), 0)<br/>img_bw = cv2.adaptiveThreshold(blurred, 255, <br/>                               cv2.ADAPTIVE_THRESH_GAUSSIAN_C,<br/>                               cv2.THRESH_BINARY, 11, 2)</span></pre><p id="d171" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这对于截图来说并不重要，但对于处理实时流摄像机图像却很重要——使用<em class="ne">自适应阈值</em>方法，我们可以将图像转换为单色，这些图像具有更高的对比度，更适合 OCR:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi ng"><img src="../Images/efbd507a28e6544658f9b2d85d869782.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OR8R91MHJMIZ8cRCe4Kcog.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">自适应阈值前后的图像(图片由作者提供)</figcaption></figure><p id="392f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们准备从图像中检测轮廓。数独板是矩形的，所以我们只检查不太小、不太窄且有 4 个点的轮廓:</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="8b78" class="my la iq mu b gy mz na l nb nc">img_out = img.copy()</span><span id="64fc" class="my la iq mu b gy nd na l nb nc">w, h = img.shape[1], img.shape[0]<br/>for cntr in contours:<br/>  imgx, imgy, imgw, imgh = cv2.boundingRect(cntr)<br/>  if imgw &lt; w/5 or imgw &lt; h/5 or \<br/>     imgw/imgh &lt; 0.25 or imgw/imgh &gt; 1.5:<br/>        continue</span><span id="ffca" class="my la iq mu b gy nd na l nb nc"><em class="ne">  # Approximate the contour with 4 points<br/>  </em>peri = cv2.arcLength(cntr, True)<br/>  frm = cv2.approxPolyDP(cntr, 0.1*peri, True)<br/>  if len(frm) != 4:<br/>    continue<br/><br/>  <em class="ne"># Converted image should fit into the original size<br/>  </em>board_size = max(imgw, imgh)<br/>  if imgx + board_size &gt;= w or imgy + board_size &gt;= h:<br/>    continue<br/>  <em class="ne"># Points should not be too close to each other <br/>  # (use euclidian distance)<br/>  </em>if cv2.norm(frm[0][0] - frm[1][0], cv2.NORM_L2) &lt; 0.1*peri or \<br/>     cv2.norm(frm[2][0] - frm[1][0], cv2.NORM_L2) &lt; 0.1*peri or \<br/>     cv2.norm(frm[3][0] - frm[1][0], cv2.NORM_L2) &lt; 0.1*peri or \<br/>     cv2.norm(frm[3][0] - frm[2][0], cv2.NORM_L2) &lt; 0.1*peri:<br/>      continue</span></pre><p id="d6d5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">还发现 OpenCV 返回的一些轮廓是三角形的，但有 4 个点，其中 2 个点彼此靠得太近。添加了额外的欧几里德<em class="ne"> </em>距离检查来避免这种情况。</p><p id="41e5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">出于调试目的，在输出图像上绘制轮廓并检查结果很方便:</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="5a69" class="my la iq mu b gy mz na l nb nc"><em class="ne">  # Draw sudoku contour using lines and points<br/>  </em>cv2.line(img_out, frm[0][0], frm[1][0], (0, 200, 0), thickness=3)<br/>  cv2.line(img_out, frm[1][0], frm[2][0], (0, 200, 0), thickness=3)<br/>  cv2.line(img_out, frm[2][0], frm[3][0], (0, 200, 0), thickness=3)<br/>  cv2.line(img_out, frm[0][0], frm[3][0], (0, 200, 0), thickness=3)<br/>  cv2.drawContours(img_out, frm, -1, (0, 255, 255), 10)</span><span id="a1a7" class="my la iq mu b gy nd na l nb nc">  cv2.imshow('image', img_out)<br/>  cv2.waitKey(0)</span></pre><p id="1934" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最终结果应该是这样的:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nh"><img src="../Images/a25771e1805d6a9ce49a20acc70a8035.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hDXNyWezkpYlFc7PjO1doQ.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">最后的数独板检测(图片由作者提供)</figcaption></figure><p id="e0ba" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果需要，也可以将图像保存到临时文件中:</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="37a4" class="my la iq mu b gy mz na l nb nc">  cv2.imwrite("img_out.png", img_out)</span></pre><p id="705a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下一个重要的步骤是应用<strong class="jp ir">透视变换</strong>。它是可选的，但对于像这样的扭曲或旋转的图像，它可以使识别过程更好:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nf"><img src="../Images/69664871b3534659e60055249274a09d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b5-85EOIIj9Xu25VRBBtBg.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">透视校正结果(图片由作者提供)</figcaption></figure><p id="72d1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这段代码中，我们创建了一个从原始轮廓坐标到矩形框架的透视变换:</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="df9e" class="my la iq mu b gy mz na l nb nc">  def normalize_points(pts):<br/>    rect = np.zeros((4, 2), dtype="float32")<br/>    s = pts.sum(axis=1)<br/>    rect[0] = pts[np.argmin(s)]<br/>    rect[2] = pts[np.argmax(s)]<br/>    diff = np.diff(pts, axis=1)<br/>    rect[1] = pts[np.argmin(diff)]<br/>    rect[3] = pts[np.argmax(diff)]<br/>    return rect</span><span id="53ef" class="my la iq mu b gy nd na l nb nc"><em class="ne">  # Source and destination points for the perspective transform</em><br/>  src_pts = normalize_points(frm.reshape((4, 2)))<br/>  dst_pts = np.array([[0, 0], [board_size, 0], <br/>                      [board_size, board_size], [0, board_size]],<br/>                     dtype=np.float32)<br/>  t_matrix = cv2.getPerspectiveTransform(approx__, dst_pts)<br/>  _, t_matrix_inv = cv2.invert(t_matrix)</span><span id="f544" class="my la iq mu b gy nd na l nb nc"><em class="ne">  # Convert images, colored and monochrome<br/>  </em>warped_disp = cv2.warpPerspective(img, t_matrix, <br/>                                    (board_size, board_size))<br/>  warped_bw = cv2.warpPerspective(img_bw, t_matrix, <br/>                                  (board_size, board_size))</span></pre><p id="2c8a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过使用<em class="ne"> getPerspectiveTransform </em>方法进行转换，这需要两个输入和输出点数组。原来，轮廓中的点有时从左到右保存，有时从右到左保存，添加了<em class="ne"> normalize_points </em>方法来解决这个问题——没有这个方法，输出图像可以被镜像。使用这种变换，我转换了两幅图像——彩色的用于显示，单色的用于光学识别过程。</p><h1 id="ef22" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">提取数字</h1><p id="c05d" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">上一步结束时，我们有一个正方形大小的数独板，现在我们可以将它分成 9 个单元，并提取所有数字。方法大体上是相同的——我得到每个数字的轮廓并检查它们的大小，以便只得到数字并跳过不想要的纹理或噪声:</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="377e" class="my la iq mu b gy mz na l nb nc">  images = []<br/>  cell_w, cell_h = board_size//9, board_size//9<br/>  for x in range(9):<br/>    for y in range(9):<br/>      x1, y1 = x*cell_w, y*cell_h <br/>      x2, y2 = (x + 1)*cell_w, (y + 1)*cell_h<br/>      cx, cy = (x1 + x2)//2, (y1 + y2)//2 <br/>      w2, h2 = cell_w, cell_h<br/><br/>      <em class="ne"># Find the contour of the digit<br/>      </em>crop = warped_bw[y1:y2, x1:x2]<br/>      cntrs, _ = cv2.findContours(crop, cv2.RETR_LIST, <br/>                                  cv2.CHAIN_APPROX_SIMPLE)<br/>      for dc in cntrs:<br/>          <em class="ne"># w2, h2 = x2 - x1, y2 - y1<br/>          </em>imgx2, imgy2, imgw2, imgh2 = cv2.boundingRect(dc)<br/>          if 0.2*w2 &lt; imgw2 &lt; 0.8*w2 and 0.4*h2 &lt; imgh2 &lt; 0.8*h2:<br/>            digit_img = crop[imgy2:imgy2 + imgh2, <br/>                             imgx2:imgx2 + imgw2]<br/>            images.append((x, y, cx, cy, digit_img))<br/>            break</span></pre><p id="61b5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">结果应该是一组分离的图像，如下所示:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/5e365734efa8123d02ff425a2940bcf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*5Byt68Nnd4z4Dq8xSwJkqA.png"/></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">提取的数字样本(图片由作者提供)</figcaption></figure><p id="24e7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们得到了所有的图像，我们已经为下一步做好了准备——光学识别。</p><h1 id="fe7d" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">使用宇宙魔方的 OCR</h1><p id="9033" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">我的第一个方法是使用<a class="ae mc" href="https://github.com/tesseract-ocr/tesseract" rel="noopener ugc nofollow" target="_blank"> Tesseract </a>，一个用于文本识别的开源库。使用<a class="ae mc" href="https://pypi.org/project/pytesseract/" rel="noopener ugc nofollow" target="_blank">pyTesseract</a>库将 tesserac 与 Python 绑定很容易:</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="600a" class="my la iq mu b gy mz na l nb nc">def predict_tesseract(images):<br/>  results = []<br/>  for x, y, img_x, img_y, digit_img in images:<br/>    value = predict_digit_tesseract(digit_img, x, y)<br/>    results.append(value)<br/>  return results<br/><br/><br/>def predict_digit_tesseract(digit_img, x, y):<br/>  w, h = digit_img.shape<br/>  if w &gt; h:  <em class="ne"># Convert image to square size<br/>      </em>digit_img = cv2.copyMakeBorder(digit_img, 0, 0, <br/>                       (w - h)//2, w - h - (w - h)//2,<br/>                       cv2.BORDER_CONSTANT, value=(255,))<br/>  digit_img = cv2.copyMakeBorder(digit_img, <br/>                       w//10, w//10, w//10, w//10, <br/>                       cv2.BORDER_CONSTANT, value=(255,))<br/><em class="ne">  # Run OCR<br/>  cf</em>='-l eng --psm 8 --dpi 70 -c tessedit_char_whitelist=0123456789'<em class="ne">    <br/>  </em>res = pytesseract.image_to_string(digit_img, <br/>                                    config=cf).strip()<br/>  return int(res[0:1]) if len(res) &gt; 0 else None</span></pre><p id="746f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">将识别出的数字放在数独板上会很有趣。为此，我使用透视变换的逆矩阵:</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="2283" class="my la iq mu b gy mz na l nb nc">  res = predict_tesseract(images)</span><span id="e555" class="my la iq mu b gy nd na l nb nc">  board = [0]*(9*9)<br/>  for (x, y, img_x, img_y, digit_img), result in zip(images, res):<br/>    if result:<br/>      board[9*x + y] = result<br/><br/>      <em class="ne"># Calculate coordinates on the original image<br/>      </em>orig = cv2.perspectiveTransform(np.array([[[img_x, img_y]]], <br/>                                               dtype=np.float32),<br/>                    t_matrix_inv).reshape((2,)).astype(np.int32)<br/>      cv2.putText(img_out, str(result), orig,<br/>                    cv2.FONT_HERSHEY_SIMPLEX, 1, <br/>                    (128, 0, 0), 2, cv2.LINE_AA, False)</span></pre><p id="e99f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">运行代码后，我们得到以下结果:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nj"><img src="../Images/5f6dd24432b897399a82951a25a4956d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EpXPwFJyeQQXluRO67n-DA.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">使用 Tesseract 进行 OCR 的结果(图片由作者提供)</figcaption></figure><p id="594d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">嗯，有点效果。结果还不错，虽然计算真的很慢。首先，Tesseract 是一个相当“笨重”的库，为识别扫描图像进行了优化。其次，pyteserract 中没有批处理，所以无法将 OCR 的所有数字作为一个批处理发送。结果，在锐龙 9 CPU 上识别数独棋盘上的所有数字需要 2.1 秒。最后但同样重要的是，Teserract 无法识别第三行中的数字“9”。好了，是时候找点乐子自己做 OCR 了。</p><h1 id="c1cb" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">使用 PyTorch 和 CNN 的 OCR</h1><h2 id="22c1" class="my la iq bd lb nk nl dn lf nm nn dp lj jy no np ln kc nq nr lr kg ns nt lv nu bi translated">神经网络</h2><p id="d06a" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">我将使用卷积神经网络(CNN)，正如我们所知，CNN 非常适合识别图像。那些不熟悉 CNN 架构的人可以读一篇关于这个的好文章。</p><p id="057e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们创建一个神经网络模型:</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="0f27" class="my la iq mu b gy mz na l nb nc">import torch<br/>import torch.nn as nn<br/>import torch.nn.functional as F<br/>import torch.optim as optim<br/></span><span id="8f81" class="my la iq mu b gy nd na l nb nc">IMG_SIZE = 32</span><span id="54bf" class="my la iq mu b gy nd na l nb nc">class Model(nn.Module):<br/>    def __init__(self):<br/>        super(Model, self).__init__()<br/>        kernel_size = 5<br/>        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32,<br/>                               kernel_size=kernel_size, stride=1,<br/>                               padding=0)<br/>        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64,<br/>                               kernel_size=kernel_size, stride=1,<br/>                               padding=0)<br/>        out_layer_size = ((IMG_SIZE-kernel_size+1)-kernel_size+1)//2<br/>        self.dropout1 = nn.Dropout(0.25)<br/>        self.dropout2 = nn.Dropout(0.5)<br/>        self.fc1 = nn.Linear(64*out_layer_size*out_layer_size, 256)<br/>        self.fc2 = nn.Linear(256, 10)<br/><br/>    def forward(self, x):<br/>        x = self.conv1(x) <em class="ne"><br/>        </em>x = F.relu(x)<br/>        x = self.conv2(x) <em class="ne"><br/>        </em>x = F.relu(x)<br/>        x = F.max_pool2d(x, 2)  <em class="ne"><br/>        </em>x = self.dropout1(x)<br/>        x = torch.flatten(x, 1)<br/>        x = self.fc1(x)<br/>        x = F.relu(x)<br/>        x = self.dropout2(x)<br/>        x = self.fc2(x)<br/>        return F.log_softmax(x, dim=1)</span></pre><p id="57f0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">显然，在使用一个模型之前，我们必须训练它，为此，我们需要一个数据集。</p><h2 id="03e3" class="my la iq bd lb nk nl dn lf nm nn dp lj jy no np ln kc nq nr lr kg ns nt lv nu bi translated">资料组</h2><p id="acd0" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">对于深度学习来说，识别数字是一项简单的任务，有很多关于这方面的教程。但是大多数作者使用的是<a class="ae mc" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank"> MNIST </a>手写数字数据集:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nj"><img src="../Images/8ea3c40815cc0968867f393b282b8355.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MvCT3ubwZCOl1UVuGIraow.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">MNIST 数据集渲染(图片由作者提供)</figcaption></figure><p id="aac4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这对自我教育来说是不错的，但我从未见过数独板，用这样的字体印刷。因此，我们可以使用这个数据集来训练神经网络，但很容易预测结果将不是最好的。</p><p id="a57e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">幸运的是，我们只需要位于图片中心的 0 到 9 的数字。创建数据集类很容易，它将生成不同的数字:</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="a2b9" class="my la iq mu b gy mz na l nb nc">class DigitsDataset(torch.utils.data.Dataset):<br/>    def __init__(self):<br/>        # TTF files should be placed in the 'fonts' folder<br/>        self.fonts = glob.glob("fonts/*.ttf")<br/>        self.fonts_dict = {}<br/>        self.digits_ = [None] * self.__len__()<br/><em class="ne">        </em>self.generate_all()<br/><br/>    def __len__(self):<br/>        return 60000<br/><br/>    def __getitem__(self, index):<br/>        return self.digits_[index]<br/><br/>    def generate_all(self):<br/>        print("Generating the digits dataset...")<br/>        t_start = time.monotonic()<br/>        for p in range(self.__len__()):<br/>            if p % 10000 == 0:<br/>                print(f"  {p} of {self.__len__()}...")<br/>            self.digits_[p] = self.generate_digit()<br/>        print(f"Done, dT={time.monotonic() - t_start}s\n")<br/><br/>    def generate_digit(self):<br/>        digit = random.randint(0, 9)<br/>        data = self.generate_digit_pil(digit)<em class="ne"><br/>        </em>return data, digit<br/><br/>    def generate_digit_pil(self, digit: int):<br/>        text = str(digit)<br/>        area_size = 2*IMG_SIZE<br/>        img = Image.new("L", (area_size, area_size), (0,))<br/>        draw = ImageDraw.Draw(img)<br/>        f_name, f_size = random.choice(self.fonts), <br/>                         random.randint(48, 64)<br/>        key = f"{f_name}-{f_size}"<br/>        if font_key not in self.fonts_dict:<br/>            self.fonts_dict[key] = ImageFont.truetype(f_name,<br/>                                                      f_size)<br/>        font = self.fonts_dict[key]<br/>        text_x = area_size//2 + random.randint(-2, 2)<br/>        text_y = area_size//2 - random.randint(-1, 1)<br/>        draw.text((text_x, text_y), text, (255,), <br/>                  font=font, anchor="mm")<br/>        transform = transforms.Compose([transforms.Resize([IMG_SIZE,<br/>                                                         IMG_SIZE]),<br/>                                     transforms.ToTensor(), <br/>                                     transforms.Normalize((0.1307,), <br/>                                                       (0.3081,))])<br/>        resized = transform(img)[0].unsqueeze(0)<br/>        return resized</span></pre><p id="86a5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从代码中可以看出，我使用不同的字体、不同的大小和位置生成数字，这有助于更好地训练网络。生成数字是一个缓慢的过程，缓存图像可以提高训练的速度。</p><p id="e5b5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以通过显示数字来轻松验证生成:</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="0770" class="my la iq mu b gy mz na l nb nc">ds = DigitsDataset()<br/>images = []<br/>for r in range(10):<br/>  hor_images = []<br/>  for d in range(10):<br/>    img = ds[10*r+d][0].reshape(IMG_SIZE, IMG_SIZE).detach().numpy()<br/>    digit_img = cv2.copyMakeBorder(img, 2, 2, 2, 2, <br/>                                cv2.BORDER_CONSTANT, value=(128,))<br/>    hor_images.append(digit_img)<br/>  images.append(np.concatenate(hor_images, axis=1))</span><span id="4fe3" class="my la iq mu b gy nd na l nb nc">cv2.imshow("Dataset", np.concatenate(images, axis=0))<br/>cv2.waitKey(0)</span></pre><p id="df40" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">很明显，结果看起来比手写数字更好:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nj"><img src="../Images/028339975510706cd61963acf2712561.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jyJW4rWQhrT2APQ-ouxdjg.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">由 DigitsDataset 类生成的图像(图片由作者提供)</figcaption></figure><h2 id="73d5" class="my la iq bd lb nk nl dn lf nm nn dp lj jy no np ln kc nq nr lr kg ns nt lv nu bi translated">训练模型</h2><p id="bc4b" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">现在我们有了一个数据集，我们可以训练这个模型:</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="9a81" class="my la iq mu b gy mz na l nb nc">import time<br/></span><span id="560e" class="my la iq mu b gy nd na l nb nc">max_epochs = 12<br/>batch_size_train = 10<br/>log_interval = 600<br/>model_file_name = "ocr_model.pt"<br/>use_cuda = torch.cuda.is_available()<br/>device = torch.device("cuda" if use_cuda else "cpu")</span><span id="ef8e" class="my la iq mu b gy nd na l nb nc"># Dataset instance<br/>dataset = DigitsDataset()<br/>train_loader = torch.utils.data.DataLoader(dataset,<br/>                         batch_size=batch_size_train, shuffle=True)</span><span id="4612" class="my la iq mu b gy nd na l nb nc"><em class="ne"># Create the model<br/></em>model = Model().to(device)<br/><em class="ne"><br/># Train<br/></em>model.train()<br/>print("Training the model, use CUDA:", use_cuda)<br/>optimizer = optim.Adadelta(model.parameters(), lr=0.01)<br/>for epoch in range(max_epochs):<br/>    print(f"Epoch: {epoch + 1} of {max_epochs}")<br/>    t1 = time.monotonic()<br/>    total_loss = 0<br/>    for batch_idx, (data, target) in enumerate(train_loader):<br/>        data, target = data.to(device), target.to(device)<br/>        optimizer.zero_grad()<br/>        output = model(data)<br/>        loss = F.nll_loss(output, target)<br/>        loss.backward()<br/>        optimizer.step()<br/>        if batch_idx % log_interval == 0:<br/>            print('  Train [{}/{} ({:.0f}%)] Loss: {:.6f}'.format(batch_idx * len(data), len(train_loader.dataset),                                                                  100 * batch_idx / len(train_loader), loss.item()))<br/>        total_loss += loss.item()<br/><em class="ne">    </em>print(f"  Total loss: {total_loss}, dT={time.monotonic() - t1}s")<br/><br/>torch.save(model.state_dict(), model_file_name)<br/>print("Model saved to", model_file_name)</span></pre><p id="227c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">每个训练历元使用 CUDA 需要 12s 左右，使用 CPU 需要 80s，所以强烈推荐使用好的显卡。如果没有支持 CUDA 的 GPU 可用(这里我对 Mac 用户说“你好”；)，一个<a class="ae mc" href="https://colab.research.google.com" rel="noopener ugc nofollow" target="_blank">谷歌 Colab </a>是一个不错的免费替代品，它可以支持 GPU，运行速度相当快。要在 Colab 笔记本中使用外部文件，我们可以将文件(Python 源代码和 TTF 字体)放到 Google Drive 中，并使用以下代码挂载这个驱动器:</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="54b4" class="my la iq mu b gy mz na l nb nc">from google.colab import drive</span><span id="ff6a" class="my la iq mu b gy nd na l nb nc">drive.mount('/content/drive')</span></pre><p id="6c90" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">之后，Google Drive 文件夹将可以从代码中访问(文件路径也应该更改为类似于<em class="ne">/content/Drive/my Drive/Colab Notebooks/…)</em>。这很有效——在 Google Colab 设置中启用 GPU 后，训练时间花了 21 秒。它比我的 16 核 CPU 快 4 倍，只比 GPU 慢 2 倍，考虑到谷歌 Colab 服务是免费的，我为我的 GPU 支付了大约 1200 美元，这令人惊讶地不算差。</p><h2 id="7d2e" class="my la iq bd lb nk nl dn lf nm nn dp lj jy no np ln kc nq nr lr kg ns nt lv nu bi translated">承认</h2><p id="6b01" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">在之前的步骤中，创建了“ocr_model.pt ”,现在我们可以使用经过训练的模型来识别数字。首先，让我们加载模型:</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="08d4" class="my la iq mu b gy mz na l nb nc">model_file_name = "ocr_model.pt"</span><span id="8919" class="my la iq mu b gy nd na l nb nc">model = Model()<br/>device = "cpu"<br/>model.load_state_dict(torch.load(model_file_name, <br/>                                 map_location=torch.device(device)))</span></pre><p id="009f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以看到，设备被设置为“CPU”——识别过程很快，我这里不需要 GPU。实际上，所有 GPU 库的初始化和数据交换可能比基于 CPU 的识别花费更多的时间。</p><p id="7c66" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我已经有了一个<em class="ne">预测 _ 宇宙魔方</em>方法，我将创建一个类似的方法来使用我们的 PyTorch 模型预测数字:</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="7f91" class="my la iq mu b gy mz na l nb nc">def predict_pytorch(model: Model, images: List):<br/>    transform = transforms.Compose([transforms.ToPILImage(),<br/>                  transforms.Resize([IMG_SIZE, IMG_SIZE]),<br/>                  transforms.ToTensor(),<br/>                  transforms.Normalize((0.1307,), (0.3081,))])</span><span id="bcb1" class="my la iq mu b gy nd na l nb nc">    <em class="ne"># Prepare images for the recognition<br/>    </em>images_ = []<br/>    for x, y, img_x, img_y, digit_img in images:<br/>      w, h = digit_img.shape<br/>      <em class="ne"># Convert image to square size<br/>      </em>if w &gt; h:<br/>        img_square = cv2.copyMakeBorder(digit_img, <br/>               10, 10, 10 + (w - h)//2, 10 + w - h - (w - h)//2,<br/>               cv2.BORDER_CONSTANT, value=(255,))<br/>      else:<br/>        img_square = cv2.copyMakeBorder(digit_img, <br/>               10 + (h - w)//2, 10 + h - w - (h - w)//2, 10, 10,<br/>               cv2.BORDER_CONSTANT, value=(255,))<br/><em class="ne">      </em>data = transform(~img_square).unsqueeze(0)<br/>      images_.append(data)</span><span id="cfdd" class="my la iq mu b gy nd na l nb nc">    if len(images_) == 0:<br/>      return []<br/>    <em class="ne"># Convert separated images to the single Pytorch tensor</em>    <br/>    data = torch.cat(images_)<br/>    <em class="ne"># Run OCR model<br/>    </em>model.eval()<br/>    with torch.no_grad():<br/>      out = model(data.to(device))<br/>      p = out.data.max(1, keepdim=True)[1].reshape((len(images_), ))<br/>      return p.tolist()</span></pre><p id="33d1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们只需要用新的方法替换旧的方法，不再需要修改代码。之后我们可以得到结果:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nj"><img src="../Images/f8e11da8f15386aa08a1f8740ec8e9b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_R-2QNdX636gHeW6djtzzw.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">使用训练好的神经网络进行 OCR 的结果(图片由作者提供)</figcaption></figure><p id="ed9e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">非常好——所有的数字都被识别，总时间只有 0.01 秒，而不是宇宙魔方的 2.1 秒！</p><p id="f9d7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">顺便说一下，对于那些想要测试<strong class="jp ir"> MNIST 数据集</strong>进行识别的人来说，这很容易做到。只需要用<em class="ne">数据集替换训练代码中的<em class="ne"> DigitsDataset() </em>实例即可。MNIST </em>类:</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="f65a" class="my la iq mu b gy mz na l nb nc">dataset = datasets.MNIST(mnist_folder, train=True, download=True,   <br/>                         transform=transforms.Compose(<br/>                           [transforms.Resize([IMG_SIZE, IMG_SIZE]),<br/>                            transforms.ToTensor(),<br/>                            transforms.Normalize((0.1307,),<br/>                                                 (0.3081,))]))</span></pre><p id="9398" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以以同样的方式可视化和使用数据集，不需要修改代码。有趣的是，结果出人意料地并不太糟糕，大多数数字仍然可以被正确识别。这是由 MNIST 训练的同一个神经网络的结果:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nv"><img src="../Images/df03c8006aa2fa0f2ea8892e44408980.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n8yC0YPfTPTluSLm7poepw.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">使用经过 MNIST 训练的神经网络进行 OCR 的结果(图片由作者提供)</figcaption></figure><p id="0fce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如我们所看到的，只有一个数字没有被正确识别，一个数字“9”被识别，而不是“6”。</p><h1 id="a23c" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">解决数独游戏</h1><p id="63c6" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">我们几乎准备好完成挑战了——我们已经识别了所有的数字，现在我们可以找到数独板的解决方案了。我不是数独专家，所以我在一个<a class="ae mc" href="https://github.com/techwithtim/Sudoku-GUI-Solver" rel="noopener ugc nofollow" target="_blank">数独 GUI-Solver </a>项目中发现了这个算法的想法。有点效果。代码正在使用一种<a class="ae mc" href="https://en.wikipedia.org/wiki/Sudoku_solving_algorithms" rel="noopener ugc nofollow" target="_blank">回溯算法</a>来寻找解决方案，但问题是 Python 擅长数据的高级操作，但它不太擅长处理数字——每块板的处理时间约为 10 秒，这绝对不是实时的！简单的解决方案是使用<a class="ae mc" href="https://numba.pydata.org" rel="noopener ugc nofollow" target="_blank">Numba</a>——一个 Python 编译器。从技术上来说，这是一个简单的修复方法，在大多数情况下，只需要将“<em class="ne"> @njit </em>”指令添加到方法定义中。这很有效，处理时间从 10 秒减少到了 0.8 秒，这本身很好，但对于我们的任务来说仍然不够。</p><p id="fcab" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">更有效的方法是用 C 语言重写算法，我确实这么做了，代码本身只有大约 50 行代码。可以使用<em class="ne"> ctypes </em>从 Python 中调用 c 函数:</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="0e0d" class="my la iq mu b gy mz na l nb nc">import ctypes</span><span id="0904" class="my la iq mu b gy nd na l nb nc">lib = None</span><span id="8980" class="my la iq mu b gy nd na l nb nc">def solve_c(bo: List) -&gt; bool:<br/><em class="ne">    </em>global lib<br/>    if lib is None:<br/>        lib = ctypes.CDLL('solver_lib.so')<br/>        lib.solve.argtypes = [ctypes.POINTER(ctypes.c_int)]<br/><br/>    board_data = (ctypes.c_int * len(bo))(*bo)<br/>    res = lib.solve(board_data)<br/>    if res:<br/>        bo[:] = list(board_data)<br/>    return res</span><span id="47a2" class="my la iq mu b gy nd na l nb nc">board = [1, 6, 0, 0, .... ]<br/>res = solve_c(board)<br/>print("Solution found:", res)</span></pre><p id="fe37" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如我们所见，我使用<em class="ne"> ctypes </em>将 Python 列表转换为 C 数组，并在计算后取回数据。C 文件包含了所有的“魔法”:</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="3cbc" class="my la iq mu b gy mz na l nb nc">DLL_EXPORT int solve(int *bo) {<br/> ...<br/>}</span></pre><p id="dcdb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用 C-binding 的唯一缺点是，在使用程序之前，应该为所需的平台编译库:</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="a8b8" class="my la iq mu b gy mz na l nb nc"><em class="ne"># Linux:<br/></em>gcc -shared -Wl,-soname,solver_lib -o solver_lib.so -fPIC solver_lib.c</span><span id="0d20" class="my la iq mu b gy nd na l nb nc"># OSX:<br/>gcc -shared -Wl,-install_name,solver_lib.so -o solver_lib.so -fPIC solver_lib.c</span><span id="7a14" class="my la iq mu b gy nd na l nb nc"># Windows: should be doable via the Visual Studio DLL Project</span></pre><p id="ead2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">而且现在速度还好——计算时间大概 0.01s，我们准备实时出发。</p><p id="6a61" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在测试过程中，增加了对算法的另一个改变。原来，原始算法在输入时期望得到正确的值<em class="ne">和</em>，但 OCR 并不总是这样。有时，数字 7 可能会被错误地识别为 1，或者 8 被识别为 0，如果输入值错误，算法就会陷入无限循环。添加了一个简单的检查，它验证输入数据中没有重复的数字，如果发现了这样的数字，那么显然不能解决棋盘问题。</p><h1 id="ccbc" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">实时运行</h1><p id="3d38" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">我们的最后一步是把所有的片段组合在一起，用一个摄像机流运行这个代码。使用 OpenCV 很容易做到这一点:</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="359b" class="my la iq mu b gy mz na l nb nc">model = Model()<br/>device = "cpu"<br/>model_file_name = "ocr_model.pt"<br/>model.load_state_dict(torch.load(model_file_name, <br/>                                 map_location=torch.device(device)))<br/>model.eval()</span><span id="1f49" class="my la iq mu b gy nd na l nb nc">cap = cv2.VideoCapture(0)<br/>width, height, fps = 1280, 720, 15<br/>cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)<br/>cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)<br/>cap.set(cv2.CAP_PROP_FPS, fps)<br/>print(f"Starting the video stream: {width}x{height}")<br/><br/>while True:<br/>    <em class="ne"># Capture images frame-by-frame<br/>    </em>ret, frame = cap.read()<br/>    frame_orig = frame.copy()<br/>    try:<br/>        <em class="ne"># Process<br/>        </em>res, img_out = process_image(model, frame)<br/>        <em class="ne"># Display<br/>        </em>cv2.imshow('Frame', img_out)<br/>    except Exception as e:<br/>        # If something was wrong, save frame for debugging<br/>        cv2.imwrite("crash.png", frame_orig)<br/>        break<br/><br/>    <em class="ne"># Process key codes<br/>    </em>key_code = cv2.waitKey(1)<br/>    if key_code &amp; 0xFF == ESC_KEY:<br/>        break<br/><br/>cap.release()<br/>cv2.destroyAllWindows()</span></pre><p id="c664" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如我们所看到的，代码相对较少，我们从网络摄像头获取帧，并将它们发送到包含所有逻辑的<em class="ne"> process_image </em>方法，这在前面已经描述过了。我还添加了数独板的缩略图预览，这有助于评估透视变换和数字提取的质量。动画显示最终结果相当准确:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nw"><img src="../Images/02d4cca21b8cfbc8f04ebdb2136f432a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*fc5rG6GhoKHUD-EJc0I8gA.gif"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">使用网络摄像头流实时解决数独棋盘(图片由作者提供)</figcaption></figure><h1 id="e67b" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">结论</h1><p id="f39e" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">与数据和神经网络打交道可能会很有趣。正如我们所看到的，数字识别本身是一个相对容易和众所周知的问题，但将所有东西结合在一起并将其“投入生产”可能需要许多额外的步骤、优化和改进。无论如何，我最初估计这样的任务可以在 2-4 小时内完成，这是非常错误的，但它使挑战更加有趣。</p><p id="93c3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">希望自己做实验的人可以从 GitHub 下载<a class="ae mc" href="https://github.com/dmitryelj/Sudoku-RealTime" rel="noopener ugc nofollow" target="_blank">代码。</a></p><p id="c5d9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">感谢阅读。</p></div></div>    
</body>
</html>