<html>
<head>
<title>Hands-on Hudi with Athena and EMR</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">与雅典娜和 EMR 一起实践胡迪</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/hands-on-hudi-with-athena-and-emr-35de9edc123f?source=collection_archive---------6-----------------------#2022-11-16">https://blog.devgenius.io/hands-on-hudi-with-athena-and-emr-35de9edc123f?source=collection_archive---------6-----------------------#2022-11-16</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><p id="a062" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">许多数据工程师需要将数据加载到他们的数据湖中，并让公司能够访问这些数据。它要求在添加新数据的同时调整模式更改并与 Metastore 同步，同时对其进行重复数据删除、分区，并将其作为分区注册到 Metastore。胡迪通过设计解决了一些问题。在这篇文章中，我想分享我们在 Athena 和 EMR 中使用胡迪的第一步，以及我们是如何设置的。我们在这里关注原始数据存储格式，因为现代数据仓库也将提供一些功能来解决这个问题。</p><p id="d7a5" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我们调查了一下</p><ul class=""><li id="88e1" class="ki kj in jm b jn jo jr js jv kk jz kl kd km kh kn ko kp kq bi translated"><a class="ae kr" href="https://delta.io/" rel="noopener ugc nofollow" target="_blank">三角洲湖</a></li><li id="9d15" class="ki kj in jm b jn ks jr kt jv ku jz kv kd kw kh kn ko kp kq bi translated"><a class="ae kr" href="https://iceberg.apache.org/" rel="noopener ugc nofollow" target="_blank">阿帕奇冰山</a></li><li id="9ceb" class="ki kj in jm b jn ks jr kt jv ku jz kv kd kw kh kn ko kp kq bi translated"><a class="ae kr" href="https://hudi.apache.org/" rel="noopener ugc nofollow" target="_blank">阿帕奇胡迪</a></li></ul><p id="4e11" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">关于德尔塔湖的论文描述了内部概念。<br/>他们听起来很强大，而我们的印象是 Delta Lake 正在技术之上建立一个生态系统，例如<a class="ae kr" href="https://docs.databricks.com/workflows/delta-live-tables/index.html" rel="noopener ugc nofollow" target="_blank"> Delta Live Tables </a>。</p><p id="bd32" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">然而，我们检查了将它与 S3、Glue 和 Athena 集成的难易程度。我们发现胡迪拥有 AWS 的一流支持:Athena 可以读取它，EMR 预装了胡迪，所以我们可以使用 Spark 来编写 S3 文件。要更深入地了解胡迪，请查看这个<a class="ae kr" href="https://hudi.apache.org/blog/2021/07/21/streaming-data-lake-platform/" rel="noopener ugc nofollow" target="_blank">链接</a>。对于 Apache Iceberg 和 Delta Lake 来说，相比较而言，粘合同步要么是不可能的(还没有)，要么不像胡迪那样容易。另一个问题是与<a class="ae kr" href="https://aws.amazon.com/de/lake-formation/" rel="noopener ugc nofollow" target="_blank">湖形成</a>的兼容性，这似乎也是胡迪提供的。</p><p id="290c" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我们想测试一下</p><ul class=""><li id="dd28" class="ki kj in jm b jn jo jr js jv kk jz kl kd km kh kn ko kp kq bi translated">在 Athena 中创建表格</li><li id="8614" class="ki kj in jm b jn ks jr kt jv ku jz kv kd kw kh kn ko kp kq bi translated">设置一个带有 PySpark 引擎的 EMR 笔记本电脑来写入数据</li><li id="e08e" class="ki kj in jm b jn ks jr kt jv ku jz kv kd kw kh kn ko kp kq bi translated">通过插入带有附加字段的新行来写入第二个数据集</li><li id="f2a7" class="ki kj in jm b jn ks jr kt jv ku jz kv kd kw kh kn ko kp kq bi translated">删除一些条目</li></ul><p id="5fb1" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">→对于每一步:我们期望 S3 文件被分区，数据(分区)+模式被同步到 Glue</p><h2 id="49ff" class="kx ky in bd kz la lb dn lc ld le dp lf jv lg lh li jz lj lk ll kd lm ln lo lp bi translated">创建 Athena 表</h2><pre class="lq lr ls lt gt lu lv lw bn lx ly bi"><span id="1f0d" class="lz ky in lv b be ma mb l mc md">CREATE EXTERNAL TABLE `hudi_partition_cow`( <br/>    `_hoodie_commit_time` string, <br/>    `_hoodie_commit_seqno` string, <br/>    `_hoodie_record_key` string, <br/>    `_hoodie_partition_path` string, <br/>    `_hoodie_file_name` string, <br/>    `event_id` string, <br/>    `event_time` string, <br/>    `event_name` string<br/>    )<br/>PARTITIONED BY ( <br/>    `partition_date` string<br/>)<br/>ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' <br/>STORED AS INPUTFORMAT 'org.apache.hudi.hadoop.HoodieParquetInputFormat' <br/>OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' <br/>LOCATION 's3://&lt;YOUR_BUCKET&gt;/&lt;YOUR_FOLDER&gt;/hudi_events/'</span></pre><h2 id="a707" class="kx ky in bd kz la lb dn lc ld le dp lf jv lg lh li jz lj lk ll kd lm ln lo lp bi translated">设置 EMR Jupyter 笔记本</h2><p id="f2f7" class="pw-post-body-paragraph jk jl in jm b jn me jp jq jr mf jt ju jv mg jx jy jz mh kb kc kd mi kf kg kh ig bi translated">用笔记本开始你的电子病历。</p><pre class="lq lr ls lt gt lu lv lw bn lx ly bi"><span id="1a5e" class="lz ky in lv b be ma mb l mc md">#ssh into your emr master and copy the Hudi JAR to a S3 Location you can access<br/>aws s3 cp /usr/lib/hudi/hudi-spark-bundle.jar s3://&lt;YOUR_BUCKET&gt;/&lt;YOUR_PATH&gt;.jar</span></pre><p id="94a9" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">在笔记本的第一个单元格中，你必须配置它使用正确的胡迪罐</p><pre class="lq lr ls lt gt lu lv lw bn lx ly bi"><span id="c392" class="lz ky in lv b be ma mb l mc md">%%configure -f<br/>{"conf":{<br/>    "spark.jars": "s3://&lt;YOUR_BUCKET&gt;/&lt;YOUR_PATH&gt;.jar",<br/>    "spark.serializer":"org.apache.spark.serializer.KryoSerializer",<br/>    "spark.sql.hive.convertMetastoreParquet":"false"<br/>}}</span></pre><h2 id="2884" class="kx ky in bd kz la lb dn lc ld le dp lf jv lg lh li jz lj lk ll kd lm ln lo lp bi translated">PySpark 写入和胶水同步</h2><pre class="lq lr ls lt gt lu lv lw bn lx ly bi"><span id="12b0" class="lz ky in lv b be ma mb l mc md">from pyspark.sql.functions import sha2, concat_ws<br/><br/>data = [<br/>    ("1", "2022-11-15T11:30:00Z", "offer_created", "2022-11-15"),<br/>    ("1", "2022-11-15T12:30:00Z", "offer_updated", "2022-11-15"),<br/>    ("1", "2022-11-16T10:30:00Z", "offer_updated_again", "2022-11-16"),<br/>    ("1", "2022-11-16T10:31:00Z", "offer_reverted", "2022-11-16"),<br/>    ("2", "2022-11-16T10:32:00Z", "offer_created", "2022-11-16"),<br/>]<br/>columns = ["event_id","event_time","event_name", "partition_date"]<br/><br/>df = spark.createDataFrame(data=data, schema = columns)<br/>df = df.withColumn("uuid", sha2(concat_ws("||", *df.columns), 256))<br/><br/>hudiOptions = {<br/>'hoodie.table.name': 'hudi_partition_cow',<br/>'hoodie.datasource.write.recordkey.field': 'uuid',<br/>'hoodie.datasource.write.partitionpath.field': 'partition_date',<br/>'hoodie.datasource.write.precombine.field': 'event_time',<br/>'hoodie.datasource.hive_sync.enable': 'true',<br/>'hoodie.datasource.hive_sync.database': 'test_tmp',<br/>'hoodie.datasource.hive_sync.table': 'hudi_partition_cow',<br/>'hoodie.datasource.hive_sync.partition_fields': 'partition_date',<br/>'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor',<br/>'hoodie.datasource.hive_sync.jdbcurl': 'jdbc:hive2://&lt;INTERNAL_EMR_MASTER_DNS&gt;:10000/'<br/>}<br/><br/># Write a DataFrame as a Hudi dataset<br/>df.write \<br/>.format('org.apache.hudi') \<br/>.option('hoodie.datasource.write.operation', 'insert') \<br/>.options(**hudiOptions) \<br/>.mode('overwrite') \<br/>.save('s3://&lt;YOUR_BUCKET&gt;/&lt;YOUR_FOLDER&gt;/hudi_events/')</span></pre><p id="e197" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">别忘了设置 INTERNAL_EMR_MASTER_DNS。你可以在 EMR 管理控制台中查找。还必须设置定义表位置的路径。<br/>执行此操作后，我们可以看到我们的数据按照预期进行了分区，并且与 Glue 进行了同步。</p><figure class="lq lr ls lt gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi mj"><img src="../Images/ab271ffa5f659ab93e04baca7991363c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3xaBslqx8FUUXmWJjFhLxg.png"/></div></div></figure><h2 id="df58" class="kx ky in bd kz la lb dn lc ld le dp lf jv lg lh li jz lj lk ll kd lm ln lo lp bi translated">通过向模式中添加新字段来添加新数据</h2><pre class="lq lr ls lt gt lu lv lw bn lx ly bi"><span id="30bb" class="lz ky in lv b be ma mb l mc md">from pyspark.sql.functions import sha2, concat_ws<br/><br/>data2 = [<br/>    ("1", "2022-11-16T11:30:00Z", "offer_updated_again", "brochure_child", "2022-11-16"),<br/>    ("2", "2022-11-16T11:31:00Z", "offer_reverted", "search_page", "2022-11-16"),<br/>    ("2", "2022-11-16T11:32:00Z", "offer_created", "brochure_child", "2022-11-16"),<br/>    ("3", "2022-11-16T11:30:00Z", "offer_updated_again", "brochure_child", "2022-11-16"),<br/>    ("4", "2022-11-16T11:31:00Z", "offer_reverted", "search_page", "2022-11-16"),<br/>    ("5", "2022-11-16T11:32:00Z", "offer_created", "brochure_child", "2022-11-16")<br/>]<br/><br/>columns2 = ["event_id","event_time", "event_name", "platform", "partition_date"]<br/><br/>df2 = spark.createDataFrame(data=data2, schema = columns2)<br/>df2 = df2.withColumn("uuid", sha2(concat_ws("||", *df2.columns), 256))<br/><br/># Write a DataFrame as a Hudi dataset<br/>df2.write \<br/>.format('org.apache.hudi') \<br/>.option('hoodie.datasource.write.operation', 'insert') \<br/>.options(**hudiOptions) \ # hudi options from previous cell will be used<br/>.mode('append') \<br/>.save('s3://&lt;YOUR_BUCKET&gt;/&lt;YOUR_FOLDER&gt;/hudi_events/')</span></pre><p id="0cf2" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">请注意，我们使用的是 spark”。模式(追加)”来追加新数据，而不是“覆盖”。覆盖将删除整个数据集。<br/>我们也使用“insert”作为查找，因为没有发生更新，因为您看到查找的记录关键字被计算为整个条目的散列，并被放置为“UUID”。如果整个条目相等，我们可以测试重复数据删除，但是如果您想要进行更新，您也可以使用真实 ID 字段作为记录关键字，并使用“upsert”作为“hoodie . data source . write . operation”来进行适当的查找。</p><p id="a23d" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">在执行这个单元之后，我们再次检查 Athena 表，可以看到新的条目进入，新的字段被添加到表中。</p><figure class="lq lr ls lt gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi mr"><img src="../Images/7596ab5b8bc2c5eefd113f96f97ac61c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*99LT7Qf8s3pGfcaJ3s7sew.png"/></div></div></figure><h2 id="4f07" class="kx ky in bd kz la lb dn lc ld le dp lf jv lg lh li jz lj lk ll kd lm ln lo lp bi translated">删除条目</h2><pre class="lq lr ls lt gt lu lv lw bn lx ly bi"><span id="448d" class="lz ky in lv b be ma mb l mc md">df3 = spark.sql("SELECT uuid, partition_date FROM test_tmp.hudi_partition_cow where event_id = '1'")<br/><br/>df3.write.format("org.apache.hudi")\<br/>.option('hoodie.datasource.write.operation', 'delete') \<br/>.options(**hudiOptions) \ # hudi options from previous cell will be used<br/>.mode('append') \<br/>.save('s3://&lt;YOUR_BUCKET&gt;/&lt;YOUR_FOLDER&gt;/hudi_events/')</span></pre><p id="ba87" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">执行这个笔记本单元格从我们的数据集中删除了不需要的条目。</p><figure class="lq lr ls lt gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi mr"><img src="../Images/22ea9f903a421be64c3696b39451b16f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MXvlVumBjXRtax-pSAAjrQ.png"/></div></div></figure><h2 id="d464" class="kx ky in bd kz la lb dn lc ld le dp lf jv lg lh li jz lj lk ll kd lm ln lo lp bi translated">我们没有实现什么？</h2><p id="0d37" class="pw-post-body-paragraph jk jl in jm b jn me jp jq jr mf jt ju jv mg jx jy jz mh kb kc kd mi kf kg kh ig bi translated">我们不能删除一列并将其作为数据集写入，这样 Glue 就可以接受它，Athena 就可以查询该表。这也是调查的终点，因为我们可以得到不同的具有可选字段的 JSON 类型。</p><p id="90f7" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">尽管如此:胡迪，以及我们所读到的 Iceberg 和 Delta Lake，都是有前途的输出格式，它们带来了数据库功能，如“upsert”和分区数据+注册以粘合，同时对数据湖进行增量模式更改。这是一个巨大的胜利！</p><p id="2574" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">非常感谢我的同事 Paul Kristian 与我一起结对工作。</p><h2 id="7f40" class="kx ky in bd kz la lb dn lc ld le dp lf jv lg lh li jz lj lk ll kd lm ln lo lp bi translated">我们在调查时收集的一些链接</h2><ul class=""><li id="6f26" class="ki kj in jm b jn me jr mf jv ms jz mt kd mu kh kn ko kp kq bi translated"><a class="ae kr" href="https://docs.aws.amazon.com/athena/latest/ug/querying-hudi.html" rel="noopener ugc nofollow" target="_blank">https://docs . AWS . Amazon . com/Athena/latest/ug/query-hudi . html</a></li><li id="bf13" class="ki kj in jm b jn ks jr kt jv ku jz kv kd kw kh kn ko kp kq bi translated"><a class="ae kr" href="https://hudi.apache.org/docs/schema_evolution/" rel="noopener ugc nofollow" target="_blank">https://hudi.apache.org/docs/schema_evolution/</a></li><li id="0ad2" class="ki kj in jm b jn ks jr kt jv ku jz kv kd kw kh kn ko kp kq bi translated"><a class="ae kr" href="https://medium.com/datalex/5-reasons-to-use-delta-lake-format-on-databricks-d9e76cf3e77d" rel="noopener">https://medium . com/datalex/5-reasons-to-use-delta-lake-format-on-data bricks-d 9 e 76 cf 3 e 77d</a></li><li id="4657" class="ki kj in jm b jn ks jr kt jv ku jz kv kd kw kh kn ko kp kq bi translated"><a class="ae kr" href="https://selectfrom.dev/delta-live-table-dlt-framework-ac0aba005571" rel="noopener ugc nofollow" target="_blank">https://select from . dev/delta-live-table-DLT-framework-AC 0 ABA 005571</a></li><li id="1e55" class="ki kj in jm b jn ks jr kt jv ku jz kv kd kw kh kn ko kp kq bi translated"><a class="ae kr" href="https://docs.delta.io/latest/presto-integration.html" rel="noopener ugc nofollow" target="_blank">https://docs.delta.io/latest/presto-integration.html</a></li><li id="928a" class="ki kj in jm b jn ks jr kt jv ku jz kv kd kw kh kn ko kp kq bi translated"><a class="ae kr" href="https://medium.datadriveninvestor.com/why-is-databricks-delta-table-aws-athena-are-not-a-good-combo-aaf1138b3d56" rel="noopener ugc nofollow" target="_blank">https://medium . datadriveninvestor . com/why-is-data bricks-delta-table-AWS-Athena-are-not-a-good-combo-AAF 1138 B3 d 56</a></li></ul></div></div>    
</body>
</html>