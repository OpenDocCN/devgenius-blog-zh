<html>
<head>
<title>‘Divergence’ — where measures converge!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">‘发散’—度量收敛的地方！</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/divergence-where-measures-converge-5efc26f0ed58?source=collection_archive---------11-----------------------#2021-12-20">https://blog.devgenius.io/divergence-where-measures-converge-5efc26f0ed58?source=collection_archive---------11-----------------------#2021-12-20</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><p id="b794" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">好吧，我可能在标题上有点夸张，但“分歧”应该得到更多的信任，尤其是在数据科学时代，我们很多人都在以不同的名义使用它。我们日常分析工作中使用的相当多的非常流行的度量标准只不过是所谓的<em class="ki">kull back-lei bler散度的延伸。</em>今天，我们将更深入地探讨这些指标——信息价值、熵、群体稳定性指数(PSI)和证据权重(WoE)。我们将研究所有这些是如何通过一个共同的散度概念联系起来的。</p><h1 id="e755" class="kj kk in bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated"><strong class="ak">直觉</strong></h1><p id="224d" class="pw-post-body-paragraph jk jl in jm b jn lh jp jq jr li jt ju jv lj jx jy jz lk kb kc kd ll kf kg kh ig bi translated">假设有两个长度相同的概率向量，它们是从同一支持面上的两个离散分布生成的。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lm"><img src="../Images/cc783031fbdb4350cc6015e1566a5799.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5Xeg-CeIM7_g_JaAvDV_cg.png"/></div></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk translated">p和Q——两个离散分布</figcaption></figure><p id="86e3" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">让我们试着想一想，我们如何定义一个度量，以某种方式展示P和Q之间的整体“距离”或“差异”或“发散”感:</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mc"><img src="../Images/b0caeab78cc9b616c942b9f80922e710.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mXo2_VhDK4z_yl8NWYXcGw.png"/></div></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk translated">或许我们可以用上面的方法来测量距离:)</figcaption></figure><p id="a2bc" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">所有这些都可能反映了两个分布之间的某种距离感。你能认出第五个吗？这是<em class="ki"> Kolmogorov-Smirnov </em>统计。对第三个的一点修改将导致<em class="ki">卡方</em>测试统计。上述第二个测量值称为<em class="ki">总变化距离。</em>有一类距离叫做<a class="ae md" href="https://en.wikipedia.org/wiki/F-divergence" rel="noopener ugc nofollow" target="_blank"><em class="ki">f-散度</em> </a> <em class="ki"> </em>涵盖了其中的大部分。既然我已经用很多距离烦你了，让我们看看我们感兴趣的一个。</p><h1 id="e7a8" class="kj kk in bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated"><strong class="ak"> KL发散</strong></h1><p id="5127" class="pw-post-body-paragraph jk jl in jm b jn lh jp jq jr li jt ju jv lj jx jy jz lk kb kc kd ll kf kg kh ig bi translated">它被简单地定义为:</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi me"><img src="../Images/19374d73fe6b99664f28018839b69fb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_T-sbQf3xb8LGudHwELuZg.png"/></div></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk translated">KL散度的定义</figcaption></figure><p id="6f54" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">请快速注意，这个度量是不对称的，即<em class="ki"> D(P | Q) </em>不同于<em class="ki"> D(Q|P) </em>。那么这个公式捕捉到了什么呢？我们首先测量差异(因此发散！)的概率值之间的差值，然后取关于P的期望值(平均值)。因此，对于任何<em class="ki"> i，</em>如果<em class="ki"> p_i </em>大而q_i小，则存在大的散度。如果p_i小，q_i大，那么散度也大，但没有第一个重要。定义中的不对称性使得能够对第一种情况赋予较大的权重，而对第二种情况赋予较小的权重。</p><p id="88a7" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">注意，每当p &lt; q but at overall level, it can be proved that KLD measure is non-negative (<a class="ae md" href="https://en.wikipedia.org/wiki/Gibbs%27_inequality" rel="noopener ugc nofollow" target="_blank">见</a>时，公式中的log(p/q)可以取负值。</p><h1 id="0bee" class="kj kk in bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">香农熵</h1><p id="733c" class="pw-post-body-paragraph jk jl in jm b jn lh jp jq jr li jt ju jv lj jx jy jz lk kb kc kd ll kf kg kh ig bi translated">变量的熵测量其可能结果固有的“混乱”或“不确定性”的平均水平。</p><p id="81dc" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">有最大“混乱”的一个分布是什么？让我们掷硬币决定。如果这是一枚公平的硬币，那么下一个结果是正面还是反面同样可能使它变得最“不确定”。另一方面，如果硬币偏向正面，那么“惊喜”就会减少，因为结果往往只偏向正面。因此，最混乱的将是所有结果都有同等可能性的分布，即均匀分布。所以要捕捉一个给定分布的熵，从均匀分布中找出它的散度是合乎逻辑的。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mf"><img src="../Images/b5666e68f658d4ace6fae801a908ee2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UP16W1P6AT5JdMvic3NGTQ.png"/></div></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk translated">所以熵是与均匀性的“背离”</figcaption></figure><p id="2e16" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">简单地说，熵的定义是:</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/d040a0e65abe612ed8a9b66eace0d453.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*ovSq9xq1Ijd-y-wWFsLicw.png"/></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk translated">熵的定义</figcaption></figure><p id="0c2e" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">这就是为什么它只是一个“分歧”。为了刷新你的高中微积分，你能验证当所有的P(xi)相等时，上面的H(x)是最大的吗？</p><h1 id="047e" class="kj kk in bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">人口稳定指数</h1><p id="3848" class="pw-post-body-paragraph jk jl in jm b jn lh jp jq jr li jt ju jv lj jx jy jz lk kb kc kd ll kf kg kh ig bi translated">前面我们讲过KL散度公式的不对称性。让它对称并不难:</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mh"><img src="../Images/8af97ba724fc9404d8773b791006d617.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gUxV59VZT_yTaR9gI-BY2w.png"/></div></div></figure><p id="a13a" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">所以，PSI只不过是KL散度的对称形式。但重要的是要清楚地理解在PSI的上下文中P和Q可能是什么。p和Q在两个不同的时间点可能是相同的群体，我们正在检查分布是否随时间而改变。p和Q可以是在两种不同人口统计学上测量的相同变量。例如，不同的州大米消费量是否不同。</p><h1 id="311b" class="kj kk in bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">信息价值(四)和权重</h1><p id="74ec" class="pw-post-body-paragraph jk jl in jm b jn lh jp jq jr li jt ju jv lj jx jy jz lk kb kc kd ll kf kg kh ig bi translated">到现在为止，您一定已经猜到了IV。IV是一种通常用于二元分类问题的度量，在二元分类问题中，我们有<em class="ki">好的</em>和<em class="ki">坏的。</em>二元分类设置中变量X的信息值定义为:</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mi"><img src="../Images/522a155d271904a654ff336bdd8c7e06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vdoR2Ah4EVIMKAMUr-T4eQ.png"/></div></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk translated">四定义</figcaption></figure><p id="5ba9" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">看与PSI公式的相似性。也是KL散度的对称版本。但这里的重要问题(以及它与PSI的不同之处)是——它测量的两种概率分布是什么？</p><p id="c2bc" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">它是测量穿过X 的<em class="ki">好</em> <strong class="jm io">的概率分布和穿过X</strong>的<em class="ki">坏</em> <strong class="jm io">的概率分布之间的差异</strong></p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mj"><img src="../Images/b962b251977a2ce94bde8ba09a5ecb06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5MaEx3nkRdpvzS72DsM8Jw.png"/></div></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk translated">IV计算框</figcaption></figure><p id="47e9" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">根据之前KLD的对称公式，P是<em class="ki"> good的分布。问</em>是<em class="ki">坏</em>(反之亦然；).如果<em class="ki"> pi </em>和<em class="ki"> qi </em>对于X的所有箱都是相同的，这意味着X与分类<em class="ki">好-坏</em>无关，因此在这种情况下IV变成0。直觉上，如果所有的<em class="ki"> pi </em>和<em class="ki"> qi </em>都相同，那么-</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mk"><img src="../Images/f3df7f9762100d532a493ad4b7add1bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*849iDzeFO6lSk0MAYjFUUw.png"/></div></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk translated">如果pi和qi都是相同的，那么它总是50-50的预测，一样好(或不好！)作为随机分类器。</figcaption></figure><p id="49b7" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">现在，在我结束之前，不要忘记注意到，悲哀只不过是KLD公式中的对数分量。但是悲哀本身应该是一个独立的故事。</p><p id="849c" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我现在必须停下来。离圣诞节只有4天了，让你做计算已经够糟了:)祝你节日快乐。新年见。在那之前…</p></div></div>    
</body>
</html>