<html>
<head>
<title>Understanding PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解 PySpark</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/understanding-pyspark-fadd9588636f?source=collection_archive---------6-----------------------#2022-09-20">https://blog.devgenius.io/understanding-pyspark-fadd9588636f?source=collection_archive---------6-----------------------#2022-09-20</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><figure class="gl gn jl jm jn jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi jk"><img src="../Images/d8fd89437da62be4fbca765b9a9aab4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ATeXeSSpRX2FQLZqgXzosg.png"/></div></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">来源:-<a class="ae jz" href="https://spark.apache.org/docs/latest/api/python/" rel="noopener ugc nofollow" target="_blank">https://spark.apache.org/docs/latest/api/python/</a></figcaption></figure><p id="e821" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">本文将涵盖以下内容</p><ul class=""><li id="2b3d" class="ky kz in kc b kd ke kh ki kl la kp lb kt lc kx ld le lf lg bi translated">什么是火花？</li><li id="8b99" class="ky kz in kc b kd lh kh li kl lj kp lk kt ll kx ld le lf lg bi translated">什么是 RDD？</li><li id="4de3" class="ky kz in kc b kd lh kh li kl lj kp lk kt ll kx ld le lf lg bi translated">Apache Spark 的组件</li><li id="55d1" class="ky kz in kc b kd lh kh li kl lj kp lk kt ll kx ld le lf lg bi translated">什么是 PySpark</li><li id="0332" class="ky kz in kc b kd lh kh li kl lj kp lk kt ll kx ld le lf lg bi translated">PySpark 中常见的数据争论函数</li><li id="12b7" class="ky kz in kc b kd lh kh li kl lj kp lk kt ll kx ld le lf lg bi translated">使用 PySpark 机器学习库构建机器学习模型</li></ul><p id="52bd" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">如果你以前用过熊猫，并且对 PySpark 很好奇，那么这篇文章就是为你准备的。你可以在这里找到本教程<a class="ae jz" href="https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Coimbra" rel="noopener ugc nofollow" target="_blank">使用的数据集。请注意，本文将只包含一些选定函数的输出。</a></p><h2 id="ab6d" class="lm ln in bd lo lp lq dn lr ls lt dp lu kl lv lw lx kp ly lz ma kt mb mc md me bi translated">阿帕奇火花</h2><p id="cb2b" class="pw-post-body-paragraph ka kb in kc b kd mf kf kg kh mg kj kk kl mh kn ko kp mi kr ks kt mj kv kw kx ig bi translated">Apache Spark 是一个开源的集群计算框架。它是用于并行大规模数据处理的统一分析引擎。Spark 构建于 Scala 之上，但也可以用于 Python，这就是 PySpark 发挥作用的地方。Spark 支持 SQL、机器学习、图形处理和流的高级工具。让我们详细看看这些工具。</p><h2 id="fa23" class="lm ln in bd lo lp lq dn lr ls lt dp lu kl lv lw lx kp ly lz ma kt mb mc md me bi translated">Apache Spark 的优势？</h2><ul class=""><li id="6602" class="ky kz in kc b kd mf kh mg kl mk kp ml kt mm kx ld le lf lg bi translated">Apache Spark 是一个统一的引擎，支持流数据、SQL 查询、机器学习和图形处理模块。</li><li id="a0de" class="ky kz in kc b kd lh kh li kl lj kp lk kt ll kx ld le lf lg bi translated">Spark 使用弹性分布式数据集(RDD)的概念处理 RAM 中的数据，比使用<a class="ae jz" href="https://www.tutorialspoint.com/hadoop/hadoop_mapreduce.htm" rel="noopener ugc nofollow" target="_blank"> MapReduce </a>的<a class="ae jz" href="https://hadoop.apache.org/" rel="noopener ugc nofollow" target="_blank"> Hadoop </a>更快。</li><li id="a8ce" class="ky kz in kc b kd lh kh li kl lj kp lk kt ll kx ld le lf lg bi translated">它很容易使用</li><li id="28bc" class="ky kz in kc b kd lh kh li kl lj kp lk kt ll kx ld le lf lg bi translated">它可以用于各种编程语言，如 Python、Java 和 r。</li><li id="32d1" class="ky kz in kc b kd lh kh li kl lj kp lk kt ll kx ld le lf lg bi translated">它是免费的</li><li id="5068" class="ky kz in kc b kd lh kh li kl lj kp lk kt ll kx ld le lf lg bi translated">它提供分布式计算</li></ul><h2 id="f7ef" class="lm ln in bd lo lp lq dn lr ls lt dp lu kl lv lw lx kp ly lz ma kt mb mc md me bi translated">什么是 RDD？</h2><p id="422a" class="pw-post-body-paragraph ka kb in kc b kd mf kf kg kh mg kj kk kl mh kn ko kp mi kr ks kt mj kv kw kx ig bi translated">RDD 完全是弹性分布式数据集。这是 Spark 的基本数据结构。rdd 被称为<em class="mn">弹性</em>是因为它们是不可变的，<em class="mn">分布式</em>是因为它们被划分到可以并行操作的集群节点上，而<em class="mn">数据集</em>是因为它们保存数据。需要注意的是，rdd 不是表格，也没有像 Spark 数据框那样的数据模式，我们将在本教程中使用 Spark 数据框。</p><h2 id="5775" class="lm ln in bd lo lp lq dn lr ls lt dp lu kl lv lw lx kp ly lz ma kt mb mc md me bi translated">Apache Spark 的组件</h2><p id="db20" class="pw-post-body-paragraph ka kb in kc b kd mf kf kg kh mg kj kk kl mh kn ko kp mi kr ks kt mj kv kw kx ig bi translated">以下是构成 Spark 的组件</p><ul class=""><li id="b9ae" class="ky kz in kc b kd ke kh ki kl la kp lb kt lc kx ld le lf lg bi translated"><strong class="kc io"> Spark Core — </strong>是 Spark 平台的<strong class="kc io"> </strong>底层通用执行引擎，所有其他功能都构建在该引擎之上。它提供内置的内存计算，并协调输入和输出(I/O)操作。</li><li id="048d" class="ky kz in kc b kd lh kh li kl lj kp lk kt ll kx ld le lf lg bi translated"><strong class="kc io">机器学习库(ML)——</strong>建立在 Spark 之上，其目的是让机器学习变得可扩展和简单。它提供机器学习算法，如回归、聚类、分类和协同过滤。此外，它还提供了用于数据预处理(如特征提取)和构建 ML 管道的工具。</li><li id="cfbb" class="ky kz in kc b kd lh kh li kl lj kp lk kt ll kx ld le lf lg bi translated"><strong class="kc io"> Spark Streaming — </strong>允许处理实时数据流。</li><li id="e15f" class="ky kz in kc b kd lh kh li kl lj kp lk kt ll kx ld le lf lg bi translated"><strong class="kc io"> Spark SQL 和 DataFrame— </strong>通过使用 SQL 查询支持数据操作和分析，提供结构化数据处理。当您使用另一种语言(比如 Python)运行 SQL 时，返回的结果将以数据集或数据框的形式出现。数据集是提供 RDDS 优势的分布式数据集合，而数据框是组织成命名列的数据集。这类似于熊猫的数据框。</li></ul><h2 id="3351" class="lm ln in bd lo lp lq dn lr ls lt dp lu kl lv lw lx kp ly lz ma kt mb mc md me bi translated">PySpark</h2><p id="a26b" class="pw-post-body-paragraph ka kb in kc b kd mf kf kg kh mg kj kk kl mh kn ko kp mi kr ks kt mj kv kw kx ig bi translated">PySpark 是 Python 中 Apache Spark 的一个接口。它是一个开源的分布式计算框架，由一组允许实时和大规模数据处理的库组成。作为一个分布式计算框架，它允许将一个任务分配成更小的任务，以便在一个机器网络中同时运行。</p><p id="dfd6" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">PySpark 支持大多数 Spark 特性，如 SparkSQL、数据帧、MLlib、Spark Core 和流。在本教程中，我们将只涵盖 Spark SQL 和数据帧，以及 MLlib。</p><p id="89f5" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">要使用 spark 执行任何分析，我们必须启动 Spark 会话。从 PySpark 导入 spark 会话，并如下所示启动它。如果没有安装 PySpark，可以使用 pip 安装。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="ec77" class="lm ln in mt b gy mx my l mz na"># import the spark session<br/>from pyspark.sql import SparkSession</span><span id="0d0d" class="lm ln in mt b gy nb my l mz na"># initiate the spark session<br/>spark = SparkSession.builder.appName("PySparkApp").getOrCreate()</span></pre><ul class=""><li id="3edf" class="ky kz in kc b kd ke kh ki kl la kp lb kt lc kx ld le lf lg bi translated">getOrCreate()函数创建一个新的会话或者在你的工作空间中检索现有的会话。</li><li id="0336" class="ky kz in kc b kd lh kh li kl lj kp lk kt ll kx ld le lf lg bi translated">PySparkApp 是所创建的应用程序的名称。</li><li id="82c5" class="ky kz in kc b kd lh kh li kl lj kp lk kt ll kx ld le lf lg bi translated">要查看关于您的会话的一些信息，您可以在笔记本中键入 spark，请参考下图。</li></ul><figure class="mo mp mq mr gt jo gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/2071f44fed23eca8446fae6a6c81215b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/format:webp/1*qImPGiMP8fGTPxp9DM8dvw.png"/></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">图 1</figcaption></figure><p id="c86f" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">现在我们可以继续分析我们的数据集了。</p><h2 id="93d6" class="lm ln in bd lo lp lq dn lr ls lt dp lu kl lv lw lx kp ly lz ma kt mb mc md me bi translated">PySpark 中常见的数据争论函数</h2><ul class=""><li id="bf06" class="ky kz in kc b kd mf kh mg kl mk kp ml kt mm kx ld le lf lg bi translated"><strong class="kc io">加载数据集</strong></li></ul><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="9a0d" class="lm ln in mt b gy mx my l mz na">#read csv file<br/>df = spark.read.csv("dataR2.csv", inferSchema=True, header=True)</span></pre><ul class=""><li id="ed9f" class="ky kz in kc b kd ke kh ki kl la kp lb kt lc kx ld le lf lg bi translated"><strong class="kc io">统计数据帧中的行数</strong></li></ul><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="294b" class="lm ln in mt b gy mx my l mz na"># count number of rows<br/>df.count()</span></pre><ul class=""><li id="eb94" class="ky kz in kc b kd ke kh ki kl la kp lb kt lc kx ld le lf lg bi translated"><strong class="kc io">从数据帧中选择一列</strong></li></ul><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="7299" class="lm ln in mt b gy mx my l mz na"># select a column<br/>df.select("Glucose").show(3)</span></pre><ul class=""><li id="fcd7" class="ky kz in kc b kd ke kh ki kl la kp lb kt lc kx ld le lf lg bi translated"><strong class="kc io">从数据框中选择列</strong></li></ul><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="7d06" class="lm ln in mt b gy mx my l mz na"># select columns<br/>df.select(["Age","BMI","Glucose"]).show(3)</span></pre><ul class=""><li id="6d67" class="ky kz in kc b kd ke kh ki kl la kp lb kt lc kx ld le lf lg bi translated"><strong class="kc io">显示表格的前 5 行</strong></li></ul><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="4203" class="lm ln in mt b gy mx my l mz na"># show the first five rows in the table<br/>df.show(5)</span></pre><ul class=""><li id="2415" class="ky kz in kc b kd ke kh ki kl la kp lb kt lc kx ld le lf lg bi translated"><strong class="kc io">检查数据类型</strong></li></ul><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="920b" class="lm ln in mt b gy mx my l mz na">df.printSchema()</span><span id="df38" class="lm ln in mt b gy nb my l mz na"># OR You can use the dtypes function</span><span id="5b03" class="lm ln in mt b gy nb my l mz na">df.dtypes</span></pre><p id="edeb" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">输出:</strong></p><figure class="mo mp mq mr gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi nd"><img src="../Images/8dcab18584be2fbfa71ad8a9c22e5149.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tg_ymu7eIPmTnQ2KSuLSxw.png"/></div></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">图 2</figcaption></figure><ul class=""><li id="6f5a" class="ky kz in kc b kd ke kh ki kl la kp lb kt lc kx ld le lf lg bi translated"><strong class="kc io">获取唯一值的计数</strong></li></ul><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="53ad" class="lm ln in mt b gy mx my l mz na"># counting the unique values in the Classification column<br/>df.groupBy("Classification").count().show()</span></pre><figure class="mo mp mq mr gt jo gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/0f135c5ee3570a61c2e1c41e6175a73b.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*rlKwBbZBG13mqQ93Sxk_1w.png"/></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">图 3</figcaption></figure><ul class=""><li id="7c27" class="ky kz in kc b kd ke kh ki kl la kp lb kt lc kx ld le lf lg bi translated"><strong class="kc io">删除一列</strong></li></ul><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="48ed" class="lm ln in mt b gy mx my l mz na">df.drop("Age")</span></pre><ul class=""><li id="9976" class="ky kz in kc b kd ke kh ki kl la kp lb kt lc kx ld le lf lg bi translated"><strong class="kc io">重命名列</strong></li></ul><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="f88d" class="lm ln in mt b gy mx my l mz na">df = df.withColumnRenamed("MCP.1", "MCP")</span></pre><ul class=""><li id="fdbe" class="ky kz in kc b kd ke kh ki kl la kp lb kt lc kx ld le lf lg bi translated"><strong class="kc io">获取汇总统计数据</strong></li></ul><p id="e91a" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated">要获得数据的汇总统计信息，使用 select 函数选择感兴趣的列，然后使用 describe 命令，如下所示。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="a279" class="lm ln in mt b gy mx my l mz na">df.select(["Age", "BMI"]).describe().show()</span></pre><p id="30b1" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">输出:</strong></p><figure class="mo mp mq mr gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi nf"><img src="../Images/10fe83862db2cbab1cc04f30bcca2dfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8sT0TCLpYewvdLoXPC53GA.png"/></div></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">图 4</figcaption></figure><h2 id="8210" class="lm ln in bd lo lp lq dn lr ls lt dp lu kl lv lw lx kp ly lz ma kt mb mc md me bi translated">处理空值</h2><ul class=""><li id="a588" class="ky kz in kc b kd mf kh mg kl mk kp ml kt mm kx ld le lf lg bi translated">如果某一行在任一列中包含空值，则删除该行</li></ul><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="d6f5" class="lm ln in mt b gy mx my l mz na">df = df.na.drop(how="any")</span></pre><ul class=""><li id="b9cb" class="ky kz in kc b kd ke kh ki kl la kp lb kt lc kx ld le lf lg bi translated">仅当所有列都有空值时删除一行</li></ul><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="11d8" class="lm ln in mt b gy mx my l mz na">df = df.na.drop(how='all')</span></pre><ul class=""><li id="7426" class="ky kz in kc b kd ke kh ki kl la kp lb kt lc kx ld le lf lg bi translated">使用`<strong class="kc io"> <em class="mn"> thres` </em> </strong>删除小于 thresh hold 非空值的行</li></ul><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="8bfd" class="lm ln in mt b gy mx my l mz na">df = df.na.drop(thres=2)</span></pre><ul class=""><li id="e769" class="ky kz in kc b kd ke kh ki kl la kp lb kt lc kx ld le lf lg bi translated">考虑列的子集，删除空值</li></ul><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="a12b" class="lm ln in mt b gy mx my l mz na">df = df.na.drop(how="any", subset=["Age", "BMI"])</span></pre><ul class=""><li id="5bbf" class="ky kz in kc b kd ke kh ki kl la kp lb kt lc kx ld le lf lg bi translated">用给定值替换空值</li></ul><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="b0c2" class="lm ln in mt b gy mx my l mz na">df = df.na.fill(value=10, subset=["Age"])</span></pre><ul class=""><li id="3f66" class="ky kz in kc b kd ke kh ki kl la kp lb kt lc kx ld le lf lg bi translated">使用火花输入器</li></ul><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="158e" class="lm ln in mt b gy mx my l mz na"># import imputer<br/>from pyspark.ml.feature import Imputer<br/># list of columns to impute<br/>cols=["Age", "BMI"]<br/># list of out put columns<br/>output_cols = ["Age", "BMI"]<br/>#initiate imputer<br/>imputer=Imputer(inputCols=cols,outputCols=output_cols).setStrategy("mean)<br/># fit the imputer to the data and transform it.<br/>df = imputer.fit(df).transform(df)</span></pre><h2 id="febe" class="lm ln in bd lo lp lq dn lr ls lt dp lu kl lv lw lx kp ly lz ma kt mb mc md me bi translated">过滤</h2><ul class=""><li id="4dbb" class="ky kz in kc b kd mf kh mg kl mk kp ml kt mm kx ld le lf lg bi translated">使用过滤器命令</li></ul><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="32fe" class="lm ln in mt b gy mx my l mz na">df.filter("Age &gt; 30")</span></pre><ul class=""><li id="fbe8" class="ky kz in kc b kd ke kh ki kl la kp lb kt lc kx ld le lf lg bi translated">使用 where 命令</li></ul><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="163b" class="lm ln in mt b gy mx my l mz na">df.where("Age &gt; 30")<br/># OR<br/>df.where(df["Age"] &gt; 30)</span></pre><ul class=""><li id="1a78" class="ky kz in kc b kd ke kh ki kl la kp lb kt lc kx ld le lf lg bi translated">基于两个条件的过滤</li></ul><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="7128" class="lm ln in mt b gy mx my l mz na">df.where((df["Age"] &gt;30) &amp; (df["class"]==0))</span></pre><h2 id="d416" class="lm ln in bd lo lp lq dn lr ls lt dp lu kl lv lw lx kp ly lz ma kt mb mc md me bi translated">根据给定值将数据分组</h2><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="8f1b" class="lm ln in mt b gy mx my l mz na">df.groupBy("Age").mean().select(["Age", "avg(BMI")]).show(5)</span></pre><h2 id="2369" class="lm ln in bd lo lp lq dn lr ls lt dp lu kl lv lw lx kp ly lz ma kt mb mc md me bi translated">分类数据</h2><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="785b" class="lm ln in mt b gy mx my l mz na"># import the ascending or descending function<br/>from pyspark.sql.functions import asc<br/>df1 = df.groupby("age").mean().select(["age", "avg(BMI")]).show(5)<br/># sort in ascending order<br/>df1.orderBy(asc("age")</span></pre><h2 id="5026" class="lm ln in bd lo lp lq dn lr ls lt dp lu kl lv lw lx kp ly lz ma kt mb mc md me bi translated">用 PySpark MLlib 构建机器学习模型</h2><ul class=""><li id="a23e" class="ky kz in kc b kd mf kh mg kl mk kp ml kt mm kx ld le lf lg bi translated">值得注意的是，PySpark 不接受特性列表，而是需要一个单独的列，其中每个条目都是特性列的向量。</li></ul><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="ccc5" class="lm ln in mt b gy mx my l mz na"># import the vector assembler<br/>from pyspark.ml.feature import VectorAssembler<br/># define feature columns<br/>x_cols = ["Age", "BMI"]<br/># initiate the assembler<br/>v_asmblr = VectorAssembler(inputCols=x_cols, outputCol="Fvec")<br/># apply the vector assembler to the data<br/>df=v_asmblr.transform(df)</span></pre><ul class=""><li id="81a6" class="ky kz in kc b kd ke kh ki kl la kp lb kt lc kx ld le lf lg bi translated">将数据集分为训练集和测试集</li></ul><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="14c3" class="lm ln in mt b gy mx my l mz na"><em class="mn"># Split the data into training and test sets (30% held out for testing)</em><br/>trainData, testData = df.randomSplit([0.7, 0.3])</span></pre><ul class=""><li id="ed32" class="ky kz in kc b kd ke kh ki kl la kp lb kt lc kx ld le lf lg bi translated">使用决策树分类器构建分类模型</li></ul><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="6c3f" class="lm ln in mt b gy mx my l mz na"># import the classifier<br/>from pyspark.ml.classification import DecisionTreeClassifier</span><span id="be38" class="lm ln in mt b gy nb my l mz na"># train decision tree model<br/>dt = DecisionTreeClassifier(labelCol="Classification", featuresCol="Fvec")<br/>dt_model = dt.fit(trainData)</span><span id="2638" class="lm ln in mt b gy nb my l mz na"># make predictions<br/>y_pred = dt_model.transform(testData)</span></pre><ul class=""><li id="2fa8" class="ky kz in kc b kd ke kh ki kl la kp lb kt lc kx ld le lf lg bi translated">将预测与原始数据集中的分类进行比较</li></ul><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="dbd1" class="lm ln in mt b gy mx my l mz na"># select example rows to display<br/>y_pred.select(['Classification', 'Fvec', 'rawPrediction', 'probability', 'prediction']).show()</span></pre><p id="b9ed" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">输出:</strong></p><figure class="mo mp mq mr gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi ng"><img src="../Images/18fbcd27b8c03569b82c2ad1ae49f865.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cGIkOxfg7yVWorVuxKM2ZQ.png"/></div></div><figcaption class="jv jw gj gh gi jx jy bd b be z dk translated">图 5</figcaption></figure><ul class=""><li id="10f4" class="ky kz in kc b kd ke kh ki kl la kp lb kt lc kx ld le lf lg bi translated"><strong class="kc io">模型评估</strong></li></ul><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="251c" class="lm ln in mt b gy mx my l mz na"># import the evaluator<br/>from pyspark.ml.evaluation import BinaryClassificationEvaluator</span><span id="cfe8" class="lm ln in mt b gy nb my l mz na"># Select (prediction, true label) and compute accuracy<br/>evaluator = MulticlassClassificationEvaluator(<br/>    labelCol="Classification", predictionCol="prediction", metricName="accuracy")</span><span id="7c76" class="lm ln in mt b gy nb my l mz na">accuracy = evaluator.evaluate(y_pred)<br/>print(accuracy)</span></pre><ul class=""><li id="8383" class="ky kz in kc b kd ke kh ki kl la kp lb kt lc kx ld le lf lg bi translated"><strong class="kc io">车型总结</strong></li></ul><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="8425" class="lm ln in mt b gy mx my l mz na"># print model summary<br/>treeModel = dt_model.toDebugString<br/>print(treeModel)</span></pre><p id="dc47" class="pw-post-body-paragraph ka kb in kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ig bi translated"><strong class="kc io">输出:</strong></p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="9d8b" class="lm ln in mt b gy mx my l mz na">DecisionTreeClassificationModel: uid=DecisionTreeClassifier_6369f0025818, depth=5, numNodes=29, numClasses=3, numFeatures=9<br/>  If (feature 2 &lt;= 93.5)<br/>   If (feature 7 &lt;= 12.85105)<br/>    If (feature 2 &lt;= 91.5)<br/>     If (feature 3 &lt;= 12.036000000000001)<br/>      Predict: 1.0<br/>     Else (feature 3 &gt; 12.036000000000001)<br/>      Predict: 2.0<br/>    Else (feature 2 &gt; 91.5)<br/>     If (feature 0 &lt;= 63.5)<br/>      If (feature 3 &lt;= 3.278)<br/>       Predict: 1.0<br/>      Else (feature 3 &gt; 3.278)<br/>       Predict: 2.0<br/>     Else (feature 0 &gt; 63.5)<br/>      Predict: 1.0<br/>   Else (feature 7 &gt; 12.85105)<br/>    If (feature 0 &lt;= 37.0)<br/>     Predict: 1.0<br/>    Else (feature 0 &gt; 37.0)<br/>     If (feature 0 &lt;= 74.5)<br/>      Predict: 2.0<br/>     Else (feature 0 &gt; 74.5)<br/>      Predict: 1.0<br/>  Else (feature 2 &gt; 93.5)<br/>   If (feature 0 &lt;= 75.5)<br/>    If (feature 1 &lt;= 35.95290733)<br/>     If (feature 5 &lt;= 54.08985)<br/>      Predict: 2.0<br/>     Else (feature 5 &gt; 54.08985)<br/>      If (feature 0 &lt;= 65.5)<br/>       Predict: 2.0<br/>      Else (feature 0 &gt; 65.5)<br/>       Predict: 1.0<br/>    Else (feature 1 &gt; 35.95290733)<br/>     If (feature 2 &lt;= 101.5)<br/>      Predict: 1.0<br/>     Else (feature 2 &gt; 101.5)<br/>      Predict: 2.0<br/>   Else (feature 0 &gt; 75.5)<br/>    If (feature 5 &lt;= 22.334200000000003)<br/>     Predict: 1.0<br/>    Else (feature 5 &gt; 22.334200000000003)<br/>     Predict: 2.0</span></pre><h2 id="02b7" class="lm ln in bd lo lp lq dn lr ls lt dp lu kl lv lw lx kp ly lz ma kt mb mc md me bi translated">结论:</h2><p id="529f" class="pw-post-body-paragraph ka kb in kc b kd mf kf kg kh mg kj kk kl mh kn ko kp mi kr ks kt mj kv kw kx ig bi translated">我希望这篇文章已经教会了你 PySpark 的基础知识。请随意发表评论或建议任何修改。如果你想了解更多，这里有一些参考资料</p><ul class=""><li id="ecb6" class="ky kz in kc b kd ke kh ki kl la kp lb kt lc kx ld le lf lg bi translated"><a class="ae jz" href="https://spark.apache.org/docs/1.5.2/ml-decision-tree.html" rel="noopener ugc nofollow" target="_blank">https://spark.apache.org/docs/1.5.2/ml-decision-tree.html</a></li><li id="4cca" class="ky kz in kc b kd lh kh li kl lj kp lk kt ll kx ld le lf lg bi translated"><a class="ae jz" href="https://spark.apache.org/docs/latest/api/python/" rel="noopener ugc nofollow" target="_blank">https://spark.apache.org/docs/latest/api/python/</a></li><li id="7dfa" class="ky kz in kc b kd lh kh li kl lj kp lk kt ll kx ld le lf lg bi translated"><a class="ae jz" href="https://sparkbyexamples.com/pyspark/pyspark-drop-rows-with-null-values/" rel="noopener ugc nofollow" target="_blank">https://sparkbyexamples.com</a></li><li id="db26" class="ky kz in kc b kd lh kh li kl lj kp lk kt ll kx ld le lf lg bi translated"><a class="ae jz" href="https://spark.apache.org/docs/latest/ml-guide.html" rel="noopener ugc nofollow" target="_blank">https://spark.apache.org/docs/latest/ml-guide.html</a></li></ul></div></div>    
</body>
</html>