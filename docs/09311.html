<html>
<head>
<title>Machine Learning Foundation : How Linear Regression Works?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习基础:线性回归如何工作？</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/machine-learning-foundation-how-linear-regression-works-6d01b32436f7?source=collection_archive---------16-----------------------#2022-08-13">https://blog.devgenius.io/machine-learning-foundation-how-linear-regression-works-6d01b32436f7?source=collection_archive---------16-----------------------#2022-08-13</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><figure class="gl gn jl jm jn jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi jk"><img src="../Images/e154240f92f5a7598689b089f74778a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jDLOgooZ_s6_ouEpVz13UQ.png"/></div></div></figure><p id="dfc8" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">你有没有开过网店，得到过你想要的商品的推荐？或者说，你在 YouTube 上看视频的时候，有没有感觉 YouTube 总是在你的主页上展示你感兴趣的视频，好像它们知道你想要什么，需要什么？是的，他们有。YouTube 知道我们喜欢什么，不喜欢什么。YouTube 知道我们需要什么，不需要什么。Youtube 之所以有这样的“魔力”，是因为机器学习。</p><h1 id="63ac" class="kt ku in bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">什么是机器学习？</h1><p id="30d2" class="pw-post-body-paragraph jv jw in jx b jy lr ka kb kc ls ke kf kg lt ki kj kk lu km kn ko lv kq kr ks ig bi translated">什么是机器学习？简而言之，机器学习是允许机器(电子设备)进行“学习”的科学领域。在上面的 youtube 案例中，使用了机器学习，以便 youtube 能够学习其用户的行为。了解用户喜欢什么，不喜欢什么，了解用户需要什么，不需要什么，等等。了解<a class="ae lw" href="https://medium.com/nerd-for-tech/a-machine-that-think-like-a-human-e710e95ead00" rel="noopener">更多关于机器学习的信息</a>。</p><h1 id="7551" class="kt ku in bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">是什么让机器学习变得重要？</h1><p id="0990" class="pw-post-body-paragraph jv jw in jx b jy lr ka kb kc ls ke kf kg lt ki kj kk lu km kn ko lv kq kr ks ig bi translated">让我们回到上面的 youtube 例子。YouTube 需要知道它的用户喜欢什么和不喜欢什么，以便能够提供相关的视频，符合用户意愿的视频。如果这行得通，那么用户就可以不间断地观看 youtube。此外，机器学习也很有用，以便 YouTube 可以向其用户显示相关广告，并通过他们的服务 Google Ads 实现收入最大化。这当然有利于 youtube 和那些使用谷歌广告的人。</p></div><div class="ab cl lx ly hr lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ig ih ii ij ik"><p id="25c6" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">在当今世界，机器学习已经影响了我们生活的许多方面。因此，机器学习成为一个非常有趣的讨论话题。在机器学习中，有各种各样的“神奇”算法，叫做模型。直到现在，已经有各种机器学习模型被应用来处理不同的问题。众所周知的机器学习模型之一是线性回归模型。</p><h1 id="6125" class="kt ku in bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">什么是线性回归？</h1><p id="13dc" class="pw-post-body-paragraph jv jw in jx b jy lr ka kb kc ls ke kf kg lt ki kj kk lu km kn ko lv kq kr ks ig bi translated">在你看来，是什么让房子变贵了？是这个尺寸吗？房子的位置？使用什么类型的建筑材料？</p><p id="b66d" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">一般来说，房子的面积越大，价格越贵。这是有道理的，考虑到房子的大面积需要更多的建材，所以房子的价格也更贵。房子的位置也影响价格。战略位置的房子往往更贵。比如离市中心越近的房子就越贵。所用材料的类型当然也会影响房子的价格。用的建材越贵，房子就越贵。</p><p id="87f5" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">通常情况下，房子的大小和价格是线性关系。什么是线性关系？简单地说，如果 x 和 y 的值总是有一个恒定的比值，那么可以说变量 x 和 y 具有线性关系。如果 x 值增加，则 y 值也增加，如果 x 值减少，则 y 值也以恒定速率减少。我们称这种类型的关系为 x 和 y 之间的线性关系。如果当我们绘制变量 x 和 y 之间的值时，我们将得到一个直线图(线性)。</p><p id="883c" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">这就是线性回归这个术语的由来。在机器学习的一些问题中，我们看到有一些变量是具有线性关系的，比如房子大小和房价的关系，学习时间和考试成绩的关系等等。不完全是线性的，但是足够接近了。</p><figure class="mf mg mh mi gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi me"><img src="../Images/cc2e861576ce17e4f354d60f0e0a3279.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*mVu9LdcM1rl4dzRj.png"/></div></div></figure><p id="d49d" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">问题是，如何利用这个线性性质来预测变量 x 的值？很简单。因为这些数据具有线性关系，所以我们可以创建一条能够代表几乎所有现有数据的线，这条线最适合这些数据点。这条线就是我们所说的回归线。这条回归线将是我们预测的结果。</p><p id="7a81" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">基于这条回归线，如果变量 x 是已知的，我们可以使用直线方程来估计变量 y 的值。我们知道，直线的方程式是:</p><figure class="mf mg mh mi gt jo gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/22a734abf949713041269b99880aedcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:258/format:webp/0*zYePte7CknhNVidR.png"/></div></figure><p id="27d3" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">例如，上图中蓝色圆点上方的图像显示的是实际数据，而红色线条显示的是预测数据。没那么难。我们需要做的就是找到最合适的线来描述上图中的数据分布。但是，怎么做呢？</p><h1 id="fa91" class="kt ku in bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">拟合回归线</h1><p id="97ce" class="pw-post-body-paragraph jv jw in jx b jy lr ka kb kc ls ke kf kg lt ki kj kk lu km kn ko lv kq kr ks ig bi translated">如上图所示，红线(我们称之为回归线)并不完全接触所有现有的点，这意味着红线并不能正确预测<strong class="jx io">上方图中其他点的存在</strong>。这一点非常清楚，因为数据并不是均匀分布形成一条完美的直线。即便如此，我们仍然可以通过<strong class="jx io">最小化误差来预测下一个数据 data。</strong></p><p id="e4fa" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">在这种情况下，误差是预测值和实际值之间的差异。误差函数这个术语就是由此而来的。误差函数是显示使用机器学习模型时产生多少误差的函数。最常用的误差函数公式是均方根误差(RMSE)函数。设 y_i 为 Y 在第 I 个数据点的值，Y_i 为 Y 在第 I 个数据点的预测值。那么线性回归模型的误差是:</p><figure class="mf mg mh mi gt jo gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/85374fc7d74a639db8766a23caef069b.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/0*WYXBd0Rk_1DFGdZI.png"/></div></figure><p id="05ef" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">如果将回归线方程 y = mx + c 代入上述方程，则:</p><figure class="mf mg mh mi gt jo gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/7f748e66351789e55fd7042fabbc726c.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/0*OB0SSlqPoabZVYg5.png"/></div></figure><p id="ef52" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">为了产生最适合数据的线，我们必须最小化误差函数值。通过最小化误差函数，我们的预测结果将更优。因为上式中的 m 和 c 是常数，其值是未知的，那么我们必须得到使误差最小的 m 和 c 的值。</p><blockquote class="mm mn mo"><p id="b31a" class="jv jw mp jx b jy jz ka kb kc kd ke kf mq kh ki kj mr kl km kn ms kp kq kr ks ig bi translated"><strong class="jx io"> <em class="in">趣闻:</em> </strong> <em class="in">这个最小化误差的问题也被称为优化问题</em></p></blockquote><p id="32ba" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">为了尽量减少错误，我们有两种方法可以做。第一种是普通的最小二乘法，第二种是梯度下降法。</p><h1 id="e908" class="kt ku in bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">普通最小二乘法</h1><p id="9bc1" class="pw-post-body-paragraph jv jw in jx b jy lr ka kb kc ls ke kf kg lt ki kj kk lu km kn ko lv kq kr ks ig bi translated">在普通的最小二乘法中，我们使用一个公式来计算 m 和 c 的值，该公式可以最小化回归线的误差。普通最小二乘法的公式为:</p><figure class="mf mg mh mi gt jo gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/73522eed0ed10b73866f15aa5928f3df.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*bgk9xK5ky3CdcOX7.png"/></div></figure><figure class="mf mg mh mi gt jo gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/2aa53f9969533b8bb5eb3b9f05f09678.png" data-original-src="https://miro.medium.com/v2/resize:fit:316/format:webp/0*rfu6ljEsjTlGk3wN.png"/></div></figure><p id="df0f" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">将 m 和 c 的值代入直线的一般方程，我们将得到最符合数据的回归线方程。有了这个回归线方程，不管后面 x 的值是多少，我们都能够预测 y 的值。</p><h1 id="3688" class="kt ku in bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">梯度下降法</h1><p id="ff17" class="pw-post-body-paragraph jv jw in jx b jy lr ka kb kc ls ke kf kg lt ki kj kk lu km kn ko lv kq kr ks ig bi translated">你记得我们的目标是什么吗？我们的目标是得到 m 和 c 的值，使得误差函数值最小。基本上，梯度下降法不能用于存在的每一种形式的误差函数。应用梯度下降法有两个条件:</p><ul class=""><li id="197c" class="mv mw in jx b jy jz kc kd kg mx kk my ko mz ks na nb nc nd bi translated">误差函数可以被微分</li><li id="3230" class="mv mw in jx b jy ne kc nf kg ng kk nh ko ni ks na nb nc nd bi translated">误差函数是凸函数</li></ul><p id="e2fe" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">那么，什么是梯度呢？梯度是曲线在某一点的斜率。函数 f(x)在 x = c 点的梯度是函数 f(x)在 x = c 点的导数，梯度下降法的工作原理是用变量在对应点的梯度减去变量的值。让我们举一个例子来计算 m 的值。如果我们看看线性回归的误差函数的曲线，它将看起来像下面的曲线:</p><figure class="mf mg mh mi gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi nj"><img src="../Images/40a802632394ec349f2b24d0325e9af9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/0*lNUf34nbTIDwKmS7.png"/></div></div></figure><p id="cc91" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">我们的目标是确定 m 的值，使得误差函数值最小。换句话说，我们必须确定函数 E(x)的最小全局值。全局最小值点是用绿色标记的点。我们如何到达那个点？方式是:</p><ol class=""><li id="b0b9" class="mv mw in jx b jy jz kc kd kg mx kk my ko mz ks nk nb nc nd bi translated">在图上随机选择一个点</li><li id="d3a2" class="mv mw in jx b jy ne kc nf kg ng kk nh ko ni ks nk nb nc nd bi translated">用 E(x)在该点的梯度减去 m 在该点的值</li><li id="ac77" class="mv mw in jx b jy ne kc nf kg ng kk nh ko ni ks nk nb nc nd bi translated">重复这个步骤，直到 m 的值变得恒定或者仅改变非常小的值。</li></ol><p id="9393" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">在上面的步骤 2 中，最近的 m 值成为减去学习率和梯度的乘积的先前 m 值。目前，学习率可以理解为梯度函数的一个尺度。</p><p id="32b9" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">以下是这些步骤的可视化:</p><figure class="mf mg mh mi gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi nl"><img src="../Images/8bc8cbd1db93b6fc167e38e9e999df62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xRlKvfCvZadH4thc.png"/></div></div></figure><p id="6d9d" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">用这种方法，我们可以确定使误差函数最小的 m 值。同样的步骤我们可以用来确定 c 的值。通过将 m 和 c 的值代入直线方程，我们将得到回归线方程。</p><p id="3031" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">这是线性回归模型中的数学过程。但是，我们需要为每个机器学习项目从头开始构建一切吗？不会。Python 和各种其他编程语言提供了各种实现了所有数学过程的框架，这样我们就可以更专注于开发过程，而不用担心数学步骤。</p><blockquote class="mm mn mo"><p id="2c6b" class="jv jw mp jx b jy jz ka kb kc kd ke kf mq kh ki kj mr kl km kn ms kp kq kr ks ig bi translated">让我们深入研究代码吧！</p></blockquote><h1 id="9edd" class="kt ku in bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">使用 Python 进行线性回归</h1><p id="3a94" class="pw-post-body-paragraph jv jw in jx b jy lr ka kb kc ls ke kf kg lt ki kj kk lu km kn ko lv kq kr ks ig bi translated">为了使用 python 应用线性回归模型，我们可以使用各种框架，其中之一是 scikit-learn 或 sklearn。</p><p id="a9b5" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">正如我们所讨论的，有两种方法可以用于线性回归模型中的优化问题，即普通最小二乘法和梯度下降法。默认情况下，scikit-learn 线性回归模型使用普通的最小二乘法。</p><p id="b703" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">要使用 scikit-learn，我们必须首先使用 python 包管理器 pip 安装它。</p><pre class="mf mg mh mi gt nm nn no np aw nq bi"><span id="54a5" class="nr ku in nn b gy ns nt l nu nv">pip install scikit-learn</span></pre><p id="779f" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">下一步是使用 import 关键字导入 scikit learn 框架。</p><pre class="mf mg mh mi gt nm nn no np aw nq bi"><span id="12e2" class="nr ku in nn b gy ns nt l nu nv">from sklearn.linear_model import LinearRegression</span></pre><p id="45cc" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">在 sklearn 框架中，有各种各样的机器学习模型可用，其中一种是线性模型。线性模型有许多实现方式，其中之一是线性回归。这也是我们从 sklearn.linear_model 导入 LinearRegression 模型的原因。</p><p id="1223" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">下一步是建立一个线性回归模型，我们将在后面使用。</p><pre class="mf mg mh mi gt nm nn no np aw nq bi"><span id="a32c" class="nr ku in nn b gy ns nt l nu nv">model = LinearRegression()</span></pre><p id="c183" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">记住，我们必须确定最符合可用数据的回归线。在对象模型上使用 fit()方法可以很容易地完成这个过程。默认情况下，该模型将使用普通的最小二乘法。</p><pre class="mf mg mh mi gt nm nn no np aw nq bi"><span id="03f6" class="nr ku in nn b gy ns nt l nu nv">model.fit(x, y)</span></pre><p id="d0e1" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">拟合过程完成后，我们可以使用 predict()方法预测 y 的值。</p><pre class="mf mg mh mi gt nm nn no np aw nq bi"><span id="6d71" class="nr ku in nn b gy ns nt l nu nv">y_predict = model.predict(new_x)</span></pre><p id="53f6" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">y_predict 是 y 相对于 new_x 的预测值。</p><h1 id="2e5c" class="kt ku in bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">例子</h1><p id="ffc7" class="pw-post-body-paragraph jv jw in jx b jy lr ka kb kc ls ke kf kg lt ki kj kk lu km kn ko lv kq kr ks ig bi translated">作为在线性回归上实现 scikit-learn 的一个例子，我们将使用<a class="ae lw" href="https://github.com/fikrinotes/Datasets/blob/main/LinearRegression/xypairs.csv" rel="noopener ugc nofollow" target="_blank">这个数据集</a>。</p><p id="bb70" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">首先，首先导入 pandas 库来查看和处理数据:</p><pre class="mf mg mh mi gt nm nn no np aw nq bi"><span id="bace" class="nr ku in nn b gy ns nt l nu nv">import pandas as pd</span><span id="9875" class="nr ku in nn b gy nw nt l nu nv">data = pd.read_csv ("https://raw.githubusercontent.com/fikrinotes/Datasets/main/LinearRegression/xypairs.csv")</span></pre><p id="7169" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">为了了解现有数据，我们可以使用以下命令:</p><pre class="mf mg mh mi gt nm nn no np aw nq bi"><span id="f009" class="nr ku in nn b gy ns nt l nu nv">data.head()</span></pre><figure class="mf mg mh mi gt jo gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/df6a79600f09f9c9bc1502b82f74c587.png" data-original-src="https://miro.medium.com/v2/resize:fit:398/format:webp/0*leuRHdyxLu8ZoaUv.png"/></div></figure><p id="ffb6" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">下一步是使用 scikit-learn 中的 linear regression 类构建一个线性回归模型:</p><pre class="mf mg mh mi gt nm nn no np aw nq bi"><span id="cbe7" class="nr ku in nn b gy ns nt l nu nv">model = LinearRegression()</span></pre><p id="d732" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">然后，拟合步骤是确定常数 m 和 c 的最合适的值</p><pre class="mf mg mh mi gt nm nn no np aw nq bi"><span id="8f6d" class="nr ku in nn b gy ns nt l nu nv">model.fit(data[["X"]], data[["Y"]])</span></pre><p id="e3e0" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">现在我们的模型已经可以使用了。假设我们想知道变量 x = data[["X"]]的 y 的近似值，那么:</p><pre class="mf mg mh mi gt nm nn no np aw nq bi"><span id="b51e" class="nr ku in nn b gy ns nt l nu nv">y_pred = model.predict(data[["X"]])</span></pre><p id="e0f1" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">y_pred 是数据的 y 的预测值[["X"]]。</p><p id="53f3" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">如果我们希望看到 y 的实际值与我们得到的预测值的比较，我们可以通过使用 matplotlib 库绘制这两个值来实现。</p><pre class="mf mg mh mi gt nm nn no np aw nq bi"><span id="0e3d" class="nr ku in nn b gy ns nt l nu nv">import matplotlib.pyplot as plt</span><span id="627e" class="nr ku in nn b gy nw nt l nu nv">plt.scatter(data.X[1:20], data.Y[1:20], s=2)</span><span id="3277" class="nr ku in nn b gy nw nt l nu nv">plt.plot(data.X[1:20], y_pred[1:20], c = 'orange')</span></pre><figure class="mf mg mh mi gt jo gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/2ff114e699f3afd7029ed801a93988b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/0*jOiVICYzU4ChwuRF.png"/></div></figure><p id="8269" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">橙色线显示我们的预测结果，蓝色点显示实际数据。我们可能意识到的第一件事是，我们的预测结果并不十分准确。但是是什么让它不那么准确呢？</p><p id="884f" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">如果我们查看所有现有的数据，我们会发现有些数据的位置与周围的数据相距甚远。有一个异常现象。</p><figure class="mf mg mh mi gt jo gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/2410dcc1767fdd642f0a0aa215b6c763.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/0*AZ_AzSZFJopl35VK.png"/></div></figure><p id="b71f" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">在数据科学中，类似这样的事情非常普遍。可能是因为输入数据时的错误、观察过程中使用测量单位时的错误等等。这足以增加我们的理解，即将用于机器学习模型的数据是原始数据，仍然需要首先进行处理和“清洗”。这个过程叫做<strong class="jx io">数据预处理</strong>。我们将在下一篇文章中讨论关于数据预处理的问题，<em class="mp">再见！</em></p></div></div>    
</body>
</html>