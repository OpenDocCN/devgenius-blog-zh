<html>
<head>
<title>Data Warehouse/Streaming Automation : MySQL to HDFS/Hive Using NiFi</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据仓库/流自动化:使用 NiFi 从 MySQL 到 HDFS/Hive</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/data-warehouse-automation-mysql-to-hdfs-hive-using-nifi-7076fffe23f7?source=collection_archive---------9-----------------------#2022-09-26">https://blog.devgenius.io/data-warehouse-automation-mysql-to-hdfs-hive-using-nifi-7076fffe23f7?source=collection_archive---------9-----------------------#2022-09-26</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><div class=""><h2 id="088d" class="pw-subtitle-paragraph jk im in bd b jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb dk translated">(使用 NiFi 将数据从 MySQL 传输到 Hive)</h2></div><h1 id="98d3" class="kc kd in bd ke kf kg kh ki kj kk kl km jt kn ju ko jw kp jx kq jz kr ka ks kt bi translated">背景</h1><p id="7d1e" class="pw-post-body-paragraph ku kv in kw b kx ky jo kz la lb jr lc ld le lf lg lh li lj lk ll lm ln lo lp ig bi translated">在办公室的一个美好的日子里，我被要求从 MySQL 到 hive 摄取一个表。我们使用 NiFi，所以我创建了一个流来接收该表。几天后，我被要求接收另一个表，我更新了流以包含另一个表。然后又有一个请求进来，我再次更新了流。然后这个过程一直持续到我觉得筋疲力尽才继续更新流量。所以我想，如果我可以建立一个 NiFi 流，它可以检测表列表(以摄取)并自动摄取。</p><h2 id="5e7c" class="lq kd in bd ke lr ls dn ki lt lu dp km ld lv lw ko lh lx ly kq ll lz ma ks mb bi translated">要求</h2><ul class=""><li id="2b34" class="mc md in kw b kx ky la lb ld me lh mf ll mg lp mh mi mj mk bi translated">建立一个数据传输服务，每 5 分钟从上游系统(MySQL)加载数据到 HDFS/Hive。</li><li id="947a" class="mc md in kw b kx ml la mm ld mn lh mo ll mp lp mh mi mj mk bi translated">它应该增量地(基于时间戳或整数值)或完全地接收表。</li><li id="1daf" class="mc md in kw b kx ml la mm ld mn lh mo ll mp lp mh mi mj mk bi translated">我们不想在每次新表到达时都构建一个接收作业。相反，它应该由表列表自动创建。</li></ul><h2 id="e99a" class="lq kd in bd ke lr ls dn ki lt lu dp km ld lv lw ko lh lx ly kq ll lz ma ks mb bi translated">这是如何工作的？</h2><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mq"><img src="../Images/c683e51a99a5473ba0eb111e9f79a17c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UOjRsb3tvNLOL0-cepLvBw.jpeg"/></div></div></figure><h2 id="f33b" class="lq kd in bd ke lr ls dn ki lt lu dp km ld lv lw ko lh lx ly kq ll lz ma ks mb bi translated">mysql_extractor 表的 Create table 语句:</h2><blockquote class="nc nd ne"><p id="6dc9" class="ku kv nf kw b kx ng jo kz la nh jr lc ni nj lf lg nk nl lj lk nm nn ln lo lp ig bi translated">创建表` MySQL _ extractor`<br/>(<br/>` table_id ` int(11)NOT NULL，<br/>` catalog ` varchar(50)DEFAULT NULL，<br/>` TABLE _ name ` varchar(50)DEFAULT NULL，<br/>` max _ value _ col ` varchar(50)DEFAULT NULL，<br/>主键(` TABLE _ id '，' catalog '，' TABLE _ name ')<br/>)；</p></blockquote><h2 id="dc3a" class="lq kd in bd ke lr ls dn ki lt lu dp km ld lv lw ko lh lx ly kq ll lz ma ks mb bi translated">流:` MySQL_To_Hive_Auto_Ingest '</h2><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi no"><img src="../Images/42803826a609f9a0349814956e4721f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*gEWGzu4tTgXBWHDZt0tPOw.gif"/></div></div></figure><p id="e21d" class="pw-post-body-paragraph ku kv in kw b kx ng jo kz la nh jr lc ld nj lf lg lh nl lj lk ll nn ln lo lp ig bi translated">让我们来看看它的实际应用:</p><p id="559b" class="pw-post-body-paragraph ku kv in kw b kx ng jo kz la nh jr lc ld nj lf lg lh nl lj lk ll nn ln lo lp ig bi translated">我们在 MySQL 中几乎没有表。我们希望接收一个表 salesmgr_dtl。</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi np"><img src="../Images/3d4f790f1fb551a1e9b9147f5e5b9230.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*bhlVRwIG8ypZLqwRxdH_rQ.gif"/></div></div></figure><p id="56aa" class="pw-post-body-paragraph ku kv in kw b kx ng jo kz la nh jr lc ld nj lf lg lh nl lj lk ll nn ln lo lp ig bi translated">现在让我们检查一下我们的 nifi 处理器是否启动并运行。</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi nq"><img src="../Images/30e6b27feb55edcbe0c06e38d9fbe7f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*-BCUF3rQoCBvToutVnRYgA.gif"/></div></div></figure><p id="d0d0" class="pw-post-body-paragraph ku kv in kw b kx ng jo kz la nh jr lc ld nj lf lg lh nl lj lk ll nn ln lo lp ig bi translated">现在让我们开始摄取。我们希望基于<strong class="kw io"> `salesmgr_id` </strong>列摄取表<strong class="kw io">` ugs . data _ lake . sales mgr _ DTL`</strong>。<br/>那么我们要做什么？我们将在提取表中创建一个条目(命名为<strong class="kw io"> `mysql_extractor` </strong>)。<br/>我们将提供` table_id`(PK 列)、` schema_name`(PK 列)、` table_name`(PK 列)和` max_value_col `(用于增量提取的列)。</p><p id="a6e2" class="pw-post-body-paragraph ku kv in kw b kx ng jo kz la nh jr lc ld nj lf lg lh nl lj lk ll nn ln lo lp ig bi translated">在我们开始讨论之前，让我告诉你在 HDFS 还没有这个表格的数据。</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mq"><img src="../Images/1256ca91d101ecf4e471b4ac3a43fa87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*YmRtPSg0TdOs7qxG7lqQAw.gif"/></div></div></figure><p id="9d5f" class="pw-post-body-paragraph ku kv in kw b kx ng jo kz la nh jr lc ld nj lf lg lh nl lj lk ll nn ln lo lp ig bi translated">看，有没有看到任何文件夹<strong class="kw io">"/ugs _ data _ lake/sales mgr _ DTL "</strong></p><p id="cef5" class="pw-post-body-paragraph ku kv in kw b kx ng jo kz la nh jr lc ld nj lf lg lh nl lj lk ll nn ln lo lp ig bi translated">让我们也检查一下蜂房。我们已经创建了一个分区表“salesmgr_dtl”。这里我们使用 dt 列进行分区，它不是 mysql 表中的列。它的值是我们的提取作业运行的日期。我们可以看到还没有可用的数据和分区。</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi nr"><img src="../Images/123183fba2351576fd1ab4fe43481a56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*GrJP1e68g-F_7MFKgkJGXQ.gif"/></div></div></figure><p id="b007" class="pw-post-body-paragraph ku kv in kw b kx ng jo kz la nh jr lc ld nj lf lg lh nl lj lk ll nn ln lo lp ig bi translated">现在，为了开始摄取，我们必须在“mysql_extractor”中插入一条记录。让我在“mysql_extractor”中插入一个表细节。我们希望每次添加新的销售经理时，该表都会自动接收。</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi ns"><img src="../Images/4737875e28cd32313ac0d6bf84437d39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*YSKLHsfLiLaUxfp00WmdOQ.gif"/></div></div></figure><p id="77c0" class="pw-post-body-paragraph ku kv in kw b kx ng jo kz la nh jr lc ld nj lf lg lh nl lj lk ll nn ln lo lp ig bi translated">现在让我们在 hdfs &amp; hive 中进行验证，并用 MySQL 进行验证。</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mq"><img src="../Images/0270976eba68a1c6e0a6383b7141d7ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*25rI9P4mli4sR5ABdrgFuw.gif"/></div></div></figure><p id="5568" class="pw-post-body-paragraph ku kv in kw b kx ng jo kz la nh jr lc ld nj lf lg lh nl lj lk ll nn ln lo lp ig bi translated">酷！成功了。我们可以看到 salesmgr_dtl 有了新的分区<strong class="kw io"> dt-2022-09-24 </strong>和 8 条与 MySQL 相同的记录。<br/>现在让我们通过添加一些新的销售经理来再试一次。我们在 2022-09-25 20:18 添加了新记录(salesmgr_id 9-16)</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mq"><img src="../Images/e38ab3db4f2d5dfd1d6d4f2abe898872.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*jkYpmiLQpZ02zQzn7Sgj2A.gif"/></div></div></figure><p id="693b" class="pw-post-body-paragraph ku kv in kw b kx ng jo kz la nh jr lc ld nj lf lg lh nl lj lk ll nn ln lo lp ig bi translated">让我们在 5 分钟后验证一下，我们可以看到 HDFS 新文件中的记录 9 到 16 是在 20:20 创建的。</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mq"><img src="../Images/98a90c9f00e7143d9f70a7501826cd13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*wUZcMq1EhgRlwzb0U--rFA.gif"/></div></div></figure><p id="9ef0" class="pw-post-body-paragraph ku kv in kw b kx ng jo kz la nh jr lc ld nj lf lg lh nl lj lk ll nn ln lo lp ig bi translated">让我们检查一下 Hive 表，我们应该有总共 16 条与 upstream(MySQL)相同的记录</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mq"><img src="../Images/095bdbeeefdef9299ea98ce7c6441b92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*ykrYuP0X9IRs6_TwvL3wbQ.gif"/></div></div></figure><p id="2098" class="pw-post-body-paragraph ku kv in kw b kx ng jo kz la nh jr lc ld nj lf lg lh nl lj lk ll nn ln lo lp ig bi translated">现在让我们试试另一张桌子。这一次，事务表有一百万条记录。<br/>我们想要基于列“date_id ”(一个时间戳列，带有添加/更新记录的时间戳值)摄取表“ugs_data_lake.sales_dtl”。<br/>我们把它插入 mysql_extractor 表中。</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mq"><img src="../Images/ccf898c8c61f30f947771476f8351f05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*j6H0lCf58-aKSg88qF78pw.gif"/></div></div></figure><p id="88e4" class="pw-post-body-paragraph ku kv in kw b kx ng jo kz la nh jr lc ld nj lf lg lh nl lj lk ll nn ln lo lp ig bi translated">让我们在 HDFS 验证一下。我们应该能在 5 分钟内得到数据。这里我们可以看到 10 个文件(每个 100000 条记录)，这是由于流配置。</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mq"><img src="../Images/0ce53afe719b3bf5be5c1e6e87e96d4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*GI9Rt5s2Pf0nqTYcMnGCtg.gif"/></div></div></figure><p id="f10d" class="pw-post-body-paragraph ku kv in kw b kx ng jo kz la nh jr lc ld nj lf lg lh nl lj lk ll nn ln lo lp ig bi translated">在配置单元中验证</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mq"><img src="../Images/c2c92ff0b2e8ddfaf3b0562deba3726d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*3qC9zgjN8elS8bH0pnBrvg.gif"/></div></div></figure><p id="98e7" class="pw-post-body-paragraph ku kv in kw b kx ng jo kz la nh jr lc ld nj lf lg lh nl lj lk ll nn ln lo lp ig bi translated">让我们在上游表中多摄取 100 万条记录。</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mq"><img src="../Images/2e48a20dc19c6252fac33a95842beb01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*z-hRDeJS33CBPHA-Waqk_g.gif"/></div></div></figure><p id="aa35" class="pw-post-body-paragraph ku kv in kw b kx ng jo kz la nh jr lc ld nj lf lg lh nl lj lk ll nn ln lo lp ig bi translated">我们去 HDFS 和蜂房看看</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mq"><img src="../Images/8d311338375e4350b60a6e3b7de7e2b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*h8C2GfbURn220qA5dMQQ2w.gif"/></div></div></figure><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mq"><img src="../Images/56051dc2695a3fe90a70a2e28f8f1411.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*tH-BIZy0fuo3XrcG-UtBew.gif"/></div></div></figure><h1 id="09a0" class="kc kd in bd ke kf kg kh ki kj kk kl km jt kn ju ko jw kp jx kq jz kr ka ks kt bi translated">摘要</h1><p id="55d7" class="pw-post-body-paragraph ku kv in kw b kx ky jo kz la lb jr lc ld le lf lg lh li lj lk ll lm ln lo lp ig bi translated">1.要获取一个表，您只需在表“mysql_extractor”中插入表的详细信息。一旦您提供详细信息，此表将每 5 分钟被 HDFS/Hive 接收一次。<br/> 2。如果您想要摄取整个表，请将 max_value_column 指定为 NULL。<br/> 3。对于配置单元，您必须事先在配置单元中手动建立表格。</p><h1 id="85ba" class="kc kd in bd ke kf kg kh ki kj kk kl km jt kn ju ko jw kp jx kq jz kr ka ks kt bi translated">注意</h1><p id="f52b" class="pw-post-body-paragraph ku kv in kw b kx ky jo kz la lb jr lc ld le lf lg lh li lj lk ll lm ln lo lp ig bi translated">这个流可以被更新以提供 orc 文件而不是文本(csv)。ORC 是 hive 中使用的最有效的列级格式。对于 hive 中的 CRUD，您的表应该在 ORC 文件格式之上。</p><p id="ad25" class="pw-post-body-paragraph ku kv in kw b kx ng jo kz la nh jr lc ld nj lf lg lh nl lj lk ll nn ln lo lp ig bi translated"><strong class="kw io">演职员表</strong></p><p id="f9c1" class="pw-post-body-paragraph ku kv in kw b kx ng jo kz la nh jr lc ld nj lf lg lh nl lj lk ll nn ln lo lp ig bi translated">帮我设计这个流程的 Shiv Kanaujiya 。</p></div></div>    
</body>
</html>