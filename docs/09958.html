<html>
<head>
<title>Introduction to Machine Learning: K Means</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习入门:K 的意思是</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/introduction-to-machine-learning-k-means-e1bb533bea8f?source=collection_archive---------14-----------------------#2022-09-25">https://blog.devgenius.io/introduction-to-machine-learning-k-means-e1bb533bea8f?source=collection_archive---------14-----------------------#2022-09-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="ca15" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">机器学习</h2><div class=""/><div class=""><h2 id="f01f" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">k 在 SKLearn 中的意思是</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/b45f51c48a1d56807f015652f8c23b57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_qZslcsfSVw6hINt"/></div></div></figure><p id="f8c5" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">欢迎来到我们关于机器学习的第四期。在本模块中，我们将讲述 K-Means。K-Means 是一种基于<a class="ae lw" href="https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)" rel="noopener ugc nofollow" target="_blank">超参数</a>“K”的<a class="ae lw" href="https://en.wikipedia.org/wiki/Cluster_analysis" rel="noopener ugc nofollow" target="_blank">聚类算法</a>，该超参数决定了将有多少个聚类。超参数只是一个我们可以调整的参数。每个集群都有一个“质心”或中心点，它将成为我们集群的锚。以下是 K 均值算法的步骤:</p><ol class=""><li id="aa51" class="lx ly iq lc b ld le lg lh lj lz ln ma lr mb lv mc md me mf bi translated">绘制数据点</li><li id="ebb1" class="lx ly iq lc b ld mg lg mh lj mi ln mj lr mk lv mc md me mf bi translated">随机绘制 K 个质心</li><li id="607f" class="lx ly iq lc b ld mg lg mh lj mi ln mj lr mk lv mc md me mf bi translated">计算从每个点到每个质心的距离</li><li id="a674" class="lx ly iq lc b ld mg lg mh lj mi ln mj lr mk lv mc md me mf bi translated">给每个点分配一个等于其聚类质心的标签</li><li id="3df4" class="lx ly iq lc b ld mg lg mh lj mi ln mj lr mk lv mc md me mf bi translated">计算每个聚类的中心→成为新的质心</li><li id="6b83" class="lx ly iq lc b ld mg lg mh lj mi ln mj lr mk lv mc md me mf bi translated">重复步骤 3 至 5，直到:</li><li id="cd12" class="lx ly iq lc b ld mg lg mh lj mi ln mj lr mk lv mc md me mf bi translated">质心停止移动</li><li id="8875" class="lx ly iq lc b ld mg lg mh lj mi ln mj lr mk lv mc md me mf bi translated">迭代后，不会给数据点分配新的质心</li><li id="dc24" class="lx ly iq lc b ld mg lg mh lj mi ln mj lr mk lv mc md me mf bi translated">我们达到了最大迭代次数</li></ol><p id="7b5e" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">视频教程:</p><p id="d47c" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><a class="ae lw" href="https://youtu.be/qONtUFyZ6II" rel="noopener ugc nofollow" target="_blank">https://youtu.be/qONtUFyZ6II</a></p><p id="2a8b" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在这个介绍性模块中，我们不打算手动实现 K-Means，我们只是要使用 Python 的 SKLearn 模块，它已经为我们实现了一个。让我们开始吧。开始使用 K-意味着我们必须安装一些库。我们将需要<code class="fe ml mm mn mo b">sklearn</code>库，它有 K-Means 的实现，我们可以直接使用，<code class="fe ml mm mn mo b">numpy</code>包含数值运算符，<code class="fe ml mm mn mo b">pandas</code>是 Python 事实上的数据组织库，<code class="fe ml mm mn mo b">matplotlib</code>我们已经多次使用，是 Python 最好的绘图库。我们可以在命令行中只使用一行代码来安装它们:</p><pre class="kp kq kr ks gt mp mo mq mr aw ms bi"><span id="6944" class="mt mu iq mo b gy mv mw l mx my">pip install sklearn numpy pandas matplotlib</span></pre><h1 id="973c" class="mz mu iq bd na nb nc nd ne nf ng nh ni kf nj kg nk ki nl kj nm kl nn km no np bi translated">随机生成的样本数据 K 均值</h1><p id="02e3" class="pw-post-body-paragraph la lb iq lc b ld nq ka lf lg nr kd li lj ns ll lm ln nt lp lq lr nu lt lu lv ij bi translated">好了，现在我们已经安装了我们的库，我们可以开始了。我们将在这里讨论两个不同的 K-Means 例子。第一个例子是一个有两个质心的随机生成数据的人为例子。例号 2 将出现在由<code class="fe ml mm mn mo b">sklearn</code>提供的<a class="ae lw" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html" rel="noopener ugc nofollow" target="_blank">数字数据集</a>上。一如既往，我们要做的第一件事是处理我们的进口。</p><pre class="kp kq kr ks gt mp mo mq mr aw ms bi"><span id="215b" class="mt mu iq mo b gy mv mw l mx my">import random<br/>import pandas as pd<br/>from sklearn.cluster import KMeans<br/>import matplotlib.pyplot as plt</span></pre><h1 id="8e41" class="mz mu iq bd na nb nc nd ne nf ng nh ni kf nj kg nk ki nl kj nm kl nn km no np bi translated">生成随机样本数据</h1><p id="876d" class="pw-post-body-paragraph la lb iq lc b ld nq ka lf lg nr kd li lj ns ll lm ln nt lp lq lr nu lt lu lv ij bi translated">接下来，我们将随机生成 100 个数据点。这些点将是两个集群，一个在<code class="fe ml mm mn mo b">(0,0)</code>周围，另一个在<code class="fe ml mm mn mo b">(5,5)</code>周围。我们将使用一个循环 100 次的 for 循环来实现这一点，并在偶数迭代中围绕<code class="fe ml mm mn mo b">(0,0)</code>生成一个集群，在奇数迭代中围绕<code class="fe ml mm mn mo b">(5,5)</code>生成一个集群。为了随机生成数据点，我们将创建两个值，一个<code class="fe ml mm mn mo b">x</code>和一个<code class="fe ml mm mn mo b">y</code>值。每个值都是通过生成一个介于-1 和 1 之间的均匀随机数，并将其与 0 或 5 相加而得到的。一旦我们生成了所有 100 个样本，我们将把我们的数据转换成一个<code class="fe ml mm mn mo b">pandas</code> <code class="fe ml mm mn mo b">DataFrame</code>对象，以便进一步处理。</p><pre class="kp kq kr ks gt mp mo mq mr aw ms bi"><span id="cc9e" class="mt mu iq mo b gy mv mw l mx my">samples = 100<br/>data = []<br/>for i in range(100):<br/>    if i%2 == 0:<br/>        base = 0<br/>    else:<br/>        base = 5<br/>    x = random.uniform(-1,1) + base<br/>    y = random.uniform(-1,1) + base<br/>    data.append([x,y])<br/>df_rand = pd.DataFrame(data)</span></pre><p id="fbb0" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">现在我们可以使用<code class="fe ml mm mn mo b">sklearn</code>来实现 2 个集群的 K-Means。首先我们将创建一个 K-Means 对象，然后在我们之前制作的<code class="fe ml mm mn mo b">DataFrame</code>上调用它的<code class="fe ml mm mn mo b">fit_predict</code>模块。一旦我们有了标签，我们将把带标签的数据分成两个单独的数据帧来绘制图表。</p><pre class="kp kq kr ks gt mp mo mq mr aw ms bi"><span id="fbb2" class="mt mu iq mo b gy mv mw l mx my">k2means = KMeans(n_clusters=2)<br/>label_rand=k2means.fit_predict(df_rand)<br/>flabels1 = df_rand[label_rand==1]<br/>flabels0 = df_rand[label_rand==0]</span></pre><h1 id="1a40" class="mz mu iq bd na nb nc nd ne nf ng nh ni kf nj kg nk ki nl kj nm kl nn km no np bi translated">绘制随机样本数据</h1><p id="c371" class="pw-post-body-paragraph la lb iq lc b ld nq ka lf lg nr kd li lj ns ll lm ln nt lp lq lr nu lt lu lv ij bi translated">现在我们所要做的就是用<code class="fe ml mm mn mo b">matplotlib</code>来散点图。我们还将使用之前创建的 K Means 对象的<code class="fe ml mm mn mo b">cluster_centers_</code>属性来获取质心。</p><pre class="kp kq kr ks gt mp mo mq mr aw ms bi"><span id="9b47" class="mt mu iq mo b gy mv mw l mx my">plt.scatter(flabels1[0], flabels1[1], label=0)<br/>plt.scatter(flabels0[0], flabels0[1], label=1)<br/>centroids_rand = k2means.cluster_centers_<br/>plt.scatter(centroids_rand[:,0], centroids_rand[:,1], s=80, color="black")<br/>plt.legend()<br/>plt.xlabel("X")<br/>plt.ylabel("Y")<br/>plt.title("Randomly Generated Two Centroid K Means")<br/>plt.show()</span></pre><p id="b303" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">一旦我们绘制了这些，我们应该会看到类似下图的东西。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/c94190d1cb6012cf3451dc500074944d.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/0*_GcyLgE4nOsEyvhK"/></div></figure><h1 id="6c56" class="mz mu iq bd na nb nc nd ne nf ng nh ni kf nj kg nk ki nl kj nm kl nn km no np bi translated">数字数据集 K 表示</h1><p id="00b5" class="pw-post-body-paragraph la lb iq lc b ld nq ka lf lg nr kd li lj ns ll lm ln nt lp lq lr nu lt lu lv ij bi translated">好了，现在我们已经看到了一个虚构的例子，让我们看看一个更真实的例子会是什么样的。对于本例，我们将对 digits 数据集运行 K 均值。和往常一样，我们将从导入我们的库开始。我们将从<code class="fe ml mm mn mo b">sklearn.datasets</code>导入<code class="fe ml mm mn mo b">load_digits</code>模块来加载数字数据集。我们将从分解中导入<code class="fe ml mm mn mo b">PCA</code>,将这个具有 64 个特征的数据集变成一个具有 2 个特征的数据集。PCA 是<a class="ae lw" href="https://pythonalgos.com/2021/11/12/intermediate-machine-learning-principal-component-analysis-pca/" rel="noopener ugc nofollow" target="_blank">主成分分析</a>，在本例中，我们将使用它进行降维。我们已经在上面导入了<code class="fe ml mm mn mo b">KMeans</code>和<code class="fe ml mm mn mo b">matplotlib.pyplot</code>,但是我把它们放在这里只是为了向你展示我们在这个例子中需要这些库。对于这个例子，我们还需要<code class="fe ml mm mn mo b">numpy</code>。</p><pre class="kp kq kr ks gt mp mo mq mr aw ms bi"><span id="0fea" class="mt mu iq mo b gy mv mw l mx my">from sklearn.datasets import load_digits<br/>from sklearn.decomposition import PCA<br/>from sklearn.cluster import KMeans<br/>import numpy as np<br/>import matplotlib.pyplot as plt</span></pre><p id="d898" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我们将从加载数字开始，并应用 PCA 将我们的 64 个特征数据集转换为 2 个特征数据集。</p><pre class="kp kq kr ks gt mp mo mq mr aw ms bi"><span id="de26" class="mt mu iq mo b gy mv mw l mx my">data = load_digits().data<br/>pca = PCA(n_components=2)<br/>df = pca.fit_transform(data)</span></pre><h1 id="b4dd" class="mz mu iq bd na nb nc nd ne nf ng nh ni kf nj kg nk ki nl kj nm kl nn km no np bi translated">加载和检查数据</h1><p id="b583" class="pw-post-body-paragraph la lb iq lc b ld nq ka lf lg nr kd li lj ns ll lm ln nt lp lq lr nu lt lu lv ij bi translated">一旦我们转换了数据，让我们应用<code class="fe ml mm mn mo b">KMeans</code>。因为有 10 个数字，所以我们给它 10 个簇。</p><pre class="kp kq kr ks gt mp mo mq mr aw ms bi"><span id="1039" class="mt mu iq mo b gy mv mw l mx my">kmeans = KMeans(n_clusters=10)<br/>label = kmeans.fit_predict(df)<br/>print(label)</span></pre><p id="c2dc" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">标签将是一个数字列表，其中每个数字是每个数据集的<code class="fe ml mm mn mo b">KMeans</code>预测。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/39804e7dc0e530d48dd175ebbd69132b.png" data-original-src="https://miro.medium.com/v2/resize:fit:368/0*8eYE95CKST4TcNeB"/></div></figure><p id="23ff" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这就是<code class="fe ml mm mn mo b">KMeans</code>的全部，让我们来看看当我们设计它的时候会是什么样子。我们将使用<code class="fe ml mm mn mo b">np</code>，我们的<code class="fe ml mm mn mo b">numpy</code>库的别名，从我们之前制作的标签中创建一组独特的标签。然后我们将使用我们创建的 K Means 对象中的<code class="fe ml mm mn mo b">cluster_centers_</code>来获取质心。现在，对于每个独特的标签，我们将在散点图上绘制对应于该标签的数据点。一旦我们绘制了所有的标签，我们将创建一个质心散点图。对于质心，你会注意到我传入了一个<code class="fe ml mm mn mo b">s</code>参数。这个参数说明了点的大小，一个常规点的大小是 72。</p><h1 id="42ee" class="mz mu iq bd na nb nc nd ne nf ng nh ni kf nj kg nk ki nl kj nm kl nn km no np bi translated">绘图数据</h1><p id="72f2" class="pw-post-body-paragraph la lb iq lc b ld nq ka lf lg nr kd li lj ns ll lm ln nt lp lq lr nu lt lu lv ij bi translated">一旦我们绘制了我们的数据，我们只需简单地标记我们的图表，添加一个图例，然后打印出来。</p><pre class="kp kq kr ks gt mp mo mq mr aw ms bi"><span id="91d6" class="mt mu iq mo b gy mv mw l mx my">unique_labels = np.unique(label)<br/>centroids = kmeans.cluster_centers_<br/>for i in unique_labels:<br/>    plt.scatter(df[label==i, 0], df[label==i, 1], label=i)<br/> <br/># s is a size indicator<br/> <br/>plt.scatter(centroids[:,0], centroids[:,1], s=80, color="black")<br/>plt.xlabel("X")<br/>plt.ylabel("Y")<br/>plt.title("Digits K Means")<br/>plt.legend()<br/>plt.show()</span></pre><p id="cf48" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我们的情节应该是这样的:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/efb4e31ea7d247f476ac779e3520e1f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/0*xjBpcGlXIVZoZNwl"/></div></figure><p id="ea68" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">就是这样，这就是 K-Means 的全部内容。非常简单，只用几行代码就可以实现。</p><p id="d3e1" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">如果你喜欢这篇文章，请在 Twitter 上分享！为了无限制地访问媒体文章，今天就注册成为<a class="ae lw" href="https://www.medium.com/@ytang07/membership" rel="noopener">媒体会员</a>！别忘了关注我，<a class="ae lw" href="https://www.medium.com/@ytang07" rel="noopener">唐</a>，获取更多关于增长、技术等方面的文章！</p><h1 id="e310" class="mz mu iq bd na nb nc nd ne nf ng nh ni kf nj kg nk ki nl kj nm kl nn km no np bi translated">进一步阅读</h1><ul class=""><li id="00b9" class="lx ly iq lc b ld nq lg nr lj ny ln nz lr oa lv ob md me mf bi translated"><a class="ae lw" href="https://pythonalgos.com/long-short-term-memory-lstm-in-keras/" rel="noopener ugc nofollow" target="_blank">长短期记忆(LSTM)在 Keras </a></li><li id="b65e" class="lx ly iq lc b ld mg lg mh lj mi ln mj lr mk lv ob md me mf bi translated"><a class="ae lw" href="https://pythonalgos.com/the-best-way-to-do-named-entity-recognition-ner/" rel="noopener ugc nofollow" target="_blank">识别命名实体的最佳方式</a></li><li id="3039" class="lx ly iq lc b ld mg lg mh lj mi ln mj lr mk lv ob md me mf bi translated"><a class="ae lw" href="https://pythonalgos.com/build-your-own-ai-text-summarizer-in-python/" rel="noopener ugc nofollow" target="_blank">构建你自己的人工智能文本摘要器</a></li><li id="072f" class="lx ly iq lc b ld mg lg mh lj mi ln mj lr mk lv ob md me mf bi translated"><a class="ae lw" href="https://pythonalgos.com/a-complete-guide-to-python-string-manipulation/" rel="noopener ugc nofollow" target="_blank">Python 字符串操作的完整指南</a></li><li id="acca" class="lx ly iq lc b ld mg lg mh lj mi ln mj lr mk lv ob md me mf bi translated"><a class="ae lw" href="https://pythonalgos.com/graph-algorithms-kruskals-algorithm-in-python/" rel="noopener ugc nofollow" target="_blank">Python 中带伪代码的克鲁斯卡尔算法</a></li></ul></div></div>    
</body>
</html>