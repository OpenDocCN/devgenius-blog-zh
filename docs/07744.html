<html>
<head>
<title>Feature Engineering in a nutshell: Frameworks for Feature Extraction and Feature Selection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">简而言之的特征工程:特征提取和特征选择的框架</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/feature-engineering-in-a-nutshell-frameworks-for-feature-extraction-and-feature-selection-616f9892b685?source=collection_archive---------2-----------------------#2022-04-20">https://blog.devgenius.io/feature-engineering-in-a-nutshell-frameworks-for-feature-extraction-and-feature-selection-616f9892b685?source=collection_archive---------2-----------------------#2022-04-20</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><h1 id="4566" class="jk jl in bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">0.摘要</h1><p id="8bf5" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">在这篇文章中，我将尝试使用一种很少使用但很实用的框架，对一些最重要的特征提取降维技术进行分类。我将尝试展示核心思想和技术背后的数学，尽可能多地保留必要的数学来更好地理解概念，尽可能少地保留可读性。我将关注这些技术的作用，以及在什么情况下使用它们更有意义。</p><p id="5700" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">其次，我将使用不同的框架对特征选择技术进行分类，并且解释一些最常用的算法。但首先要做一个简短的介绍，以提高对这个问题重要性的认识。</p><h1 id="6525" class="jk jl in bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">1.信息爆炸与维数灾难</h1><p id="5c17" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">当一个人刚开始学习数据科学时，他往往会觉得越多越好，包括数据和维度。这可能是真的，如果你从特征中获得新奇的信息，而事实并非总是如此。还有“没有免费的午餐定理”,意思是总有代价要付出，但你可以选择代价有多高，以及何时付出，可能在部署时，也可能更早……</p><p id="58e8" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">即使这个话题并不新鲜，但由于过去几十年的信息爆炸，它变得越来越不重要，这意味着数据生产的增长速度远远超过了我们分析数据的能力。这导致了一个分析缺口，为了弥补这个缺口，我们一直致力于减少功能数量，以提高速度和性能(如果可能的话)。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ll"><img src="../Images/699ee929d5f00d5998dd88935a197bee.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*L9arS9WLA7yw3sUetT8a4Q.png"/></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk translated">信息爆炸[2]</figcaption></figure><p id="6dfb" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">但是，很多在低维空间不出现的问题，在高维空间开始出现。这就是我所说的维度诅咒。您可以在 ML 中看到一个这样的例子，随机采样的高维向量通常非常稀疏，这增加了过度拟合的风险，使得在没有大量训练数据的情况下很难识别数据中的模式。</p><h1 id="193f" class="jk jl in bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">2.特征工程:为什么？我们有什么选择？</h1><p id="b38b" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">如前所述，您希望在训练数据中包含相关特征，而不是太多不相关的特征。想出一套好的相关特性是任何 ML 项目的关键部分，通常项目中投入的大部分时间都用于特性工程。这也是关于维数减少，因为我们想简化数据而不丢失太多的信息。我们可以获得的一些优势是:</p><ul class=""><li id="57c0" class="lx ly in kk b kl lg kp lh kt lz kx ma lb mb lf mc md me mf bi translated">如果我们将数据提供给另一个 ML 算法，它会运行得更快</li><li id="f81b" class="lx ly in kk b kl mg kp mh kt mi kx mj lb mk lf mc md me mf bi translated">数据将占用更少的内存空间</li><li id="5737" class="lx ly in kk b kl mg kp mh kt mi kx mj lb mk lf mc md me mf bi translated">如果我们消除冗余或无用的信息，它甚至可能表现得更好</li><li id="1d3f" class="lx ly in kk b kl mg kp mh kt mi kx mj lb mk lf mc md me mf bi translated">更好地可视化数据，深入了解最重要的功能</li></ul><p id="664d" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">一如既往，有一些缺点值得考虑:</p><ul class=""><li id="01b7" class="lx ly in kk b kl lg kp lh kt lz kx ma lb mb lf mc md me mf bi translated">信息丢失，性能可能下降</li><li id="3ecd" class="lx ly in kk b kl mg kp mh kt mi kx mj lb mk lf mc md me mf bi translated">可能计算量很大</li><li id="39d2" class="lx ly in kk b kl mg kp mh kt mi kx mj lb mk lf mc md me mf bi translated">ML 管道更加复杂</li><li id="8aef" class="lx ly in kk b kl mg kp mh kt mi kx mj lb mk lf mc md me mf bi translated">难以解释变形的特征</li></ul><p id="ec05" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">现在，你有什么选择。请参见下图，了解用于特征提取和特征选择的最重要可用选项的分类。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ml"><img src="../Images/319aa8be5c8c2092ad16a2513cad9254.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0BuIHICuDp14IMC1Jke9Lw.png"/></div></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk translated">FE 和 FS 方法的分类(图片由作者提供)</figcaption></figure><h1 id="0ae4" class="jk jl in bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">2.1.特征抽出</h1><p id="728e" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">特征提取的一个简单例子是当我们将几个相关的特征合并成一个时。如果汽车的里程与车龄非常相关，我们可以实现一个降维算法，将它们合并成一个表示汽车磨损的特征。然而，当我们有成百上千个特征时，手动地做这件事是不可行的。这里的主要降维技术涉及矩阵分解或邻居图[3]。</p><h1 id="6b18" class="jk jl in bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">2.1.1.矩阵分解</h1><p id="9030" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">这里的目标是将一个矩阵表示为两个较小矩阵的乘积。在下图中，蓝色矩阵是您的数据，其中每行是一个样本，每列是一个要素。原型是您将用来重构数据的最简单的形式。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi mq"><img src="../Images/71c0bfd03e41624558a6f93dd00ffaa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uLLwpdm7S4p5TIBFWTMfow.png"/></div></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk translated">矩阵分解[3]</figcaption></figure><p id="2705" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">你的一行数据将被表达为原型的线性组合。线性组合的系数(图中红色部分)是低维表示。这基本上就是矩阵分解。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi mr"><img src="../Images/0d65f39c581cc1ff54a1f51da3d32cc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*puIePoHTV_AoyYAdIXW_Pw.png"/></div></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk translated">行作为原型的线性组合[3]</figcaption></figure><p id="1c19" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">您可能已经看到了降维技术中的分类，这取决于数据转换，是线性的还是非线性的。正如你现在看到的，我们讨论的是线性的。</p><p id="b1e1" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">用数学符号表示，可以写成:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/393a44ac20d28102c038e1c87b34ffeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:322/format:webp/1*fGF5EX7Goi7jalc9AFEhAw.png"/></div></figure><p id="6b72" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">你可以看到它不是相等的，而是近似的。这是因为我们在一些约束条件下，尽量减少一些错误或损失。不同的矩阵分解技术基本上取决于这些约束。</p><h2 id="59ce" class="mt jl in bd jm mu mv dn jq mw mx dp ju kt my mz jy kx na nb kc lb nc nd kg ne bi translated">2.1.1.1.PCA:主成分分析</h2><p id="3a5b" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">如果你以前在降维方面做过一些工作，你肯定尝试过 PCA，因为它是一种标准。基本上，我们在这里做的是选择损失作为平方误差。如果你在统计学 101 的回归课上有闪回，那很好，因为本质上就是这样。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/c71063cb75191b3d579d724a5b868f35.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*K59jo3gcmaMERD_Uznph3g.png"/></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk translated">PCA 中最小化的函数[3]</figcaption></figure><p id="4576" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">在文献中，您可能已经看到 PCA 和类似的方法被归在“投影技术”下，这也是真实的，因为主成分只是形成超平面的轴，然后我们将数据投影到该超平面上。这些轴试图保持尽可能多的差异，或者换句话说，它试图最小化原始数据集与其投影之间的均方距离。所以同一个想法有不同的名字…最小化平方误差。</p><p id="c17d" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">还记得我们的矩阵乘法与原始数据不完全相等吗？这里值得一提的是近似误差，也称为“重建误差”，定义为原始数据和重建数据(压缩然后解压缩)之间的均方距离。有了<em class="ng"> sklearn </em>，我们只用几行代码就可以评估这一点:</p><pre class="lm ln lo lp gt nh ni nj nk aw nl bi"><span id="887a" class="mt jl in ni b gy nm nn l no np">pca = PCA(n_components = 154) <br/>X_reduced = pca.fit_transform(X_train) <br/>X_recovered = pca.inverse_transform(X_reduced)</span></pre><p id="8bda" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">将其应用于包含手写数字的 MNIST 数据集，我们可以看到如果我们希望保留 95%的方差，情况会是怎样:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi nq"><img src="../Images/20c3dd928c9d79be15364566d32699ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QGCLtW_UVgF8iGyqFoU0KA.png"/></div></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk translated">保留 95%方差的 MNIST 压缩[1]</figcaption></figure><p id="3f7e" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">这 95%被称为“解释方差比”，它基本上是数据集方差在每个主成分中所占的比例。</p><p id="81a9" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">如果你想知道什么时候使用 PCA，如果你不确定还有什么更好的方法，我建议你把它作为默认的降维技术。关于何时使用替代品，请查看以下章节。</p><h2 id="3aab" class="mt jl in bd jm mu mv dn jq mw mx dp ju kt my mz jy kx na nb kc lb nc nd kg ne bi translated">2.1.1.2.其他人</h2><p id="c3a4" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">除了经典的 PCA 之外，一些矩阵分解备选方案也值得一提:</p><ul class=""><li id="c912" class="lx ly in kk b kl lg kp lh kt lz kx ma lb mb lf mc md me mf bi translated">随机 PCA: <br/> -什么？我们不使用奇异值分解来寻找主成分，而是使用随机算法<br/> -什么时候？如果你想快点。这很容易比传统的 PCA 快 100 倍</li><li id="4645" class="lx ly in kk b kl mg kp mh kt mi kx mj lb mk lf mc md me mf bi translated">增量 PCA (IPCA): <br/> -什么？小批量处理数据集<br/> -何时？如果您有大量的训练集和/或在线应用 PCA。</li></ul><p id="edca" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">如果你对其他的感兴趣，你可以检查非负矩阵分解或概率 PCA。</p><h1 id="e19c" class="jk jl in bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">2.1.2.邻居图</h1><p id="782a" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">邻居图背后的核心思想是:构建图的方法和嵌入图的方法。在本节中，我将使用 S 数据集，它在解释流形学习时经常使用。基本思想可以在下一张图中看到，其中线性变换没有用，所以我们尝试构建一个连接这些点和它们的 k-最近邻的图。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/a3bd4c42da4d71ec6cc006e1bffdb9e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*3wryOQl5JqH5sURd2kLqtA.png"/></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk translated">来自 S 数据集的点</figcaption></figure><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/7c6313adcb0b80bfe8c480e232dac1e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*rKs0No96NTp-IneCstWVIw.png"/></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk translated">相同的图像，其中点与图形相链接[3]</figcaption></figure><p id="2cc2" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">值得一提的是，这些图片可能是欺骗性的，因为它们适用于 3D，但它们有助于获得对正在发生的事情的直觉。</p><p id="666e" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">在数学上更深入一点，这里是如何将流形的机制应用到一组有限的点上[5]:我们选择一个参数<em class="ng"> k </em>和<em class="ng">k</em>——一个点的最近邻居形成围绕<em class="ng"> x </em>的开放邻域。这些邻域将是坐标面片，并且与欧几里得空间中的开集微分同胚。有时选择邻域半径来计算开放邻域。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/e8658616e292e461c8b3789579f3b2df.png" data-original-src="https://miro.medium.com/v2/resize:fit:482/format:webp/1*OWvMzjYQJ9pzhZkmGeKb6Q.png"/></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk translated">从欧几里得空间提取的样本数据集[5]</figcaption></figure><p id="6133" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">我们假设数据点实际上位于一个 D 维子流形上，其中<em class="ng"> d &lt; D </em>。那么任务就是‘学习’流形。离散集合上的坐标面片通过计算每个点<em class="ng"> x </em>的<em class="ng"> k- </em>最近邻来构建。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/657f905b4f2c2d31ea76290e7c327b2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*0zwWGIQwRZSltg0Cysz0XA.png"/></div></figure><p id="fd69" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">给定这些坐标片，流形学习的目标是找到微分同胚</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/733a9cfa6e4cfbf78af4e5ab8ec54a3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/format:webp/1*HiCQJ9O-hpFWcWZb77nNoA.png"/></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk translated">微分同胚[5]</figcaption></figure><p id="4d97" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">这里的核心思想仍然是，我们想要构建一个图，根据我们如何构建它，以及我们如何将它嵌入到低维中，我们会得到不同的算法，正如我们接下来将看到的。</p><h2 id="c5e1" class="mt jl in bd jm mu mv dn jq mw mx dp ju kt my mz jy kx na nb kc lb nc nd kg ne bi translated">2.1.2.1.Isomap</h2><p id="35e5" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">为了在这里构建图形，我们试图保留实例之间的测地线距离(在<em class="ng">测地线空间</em>中的距离)。<em class="ng">测地线</em>是直线概念到弯曲流形的推广，它定义了流形上两点之间沿其弯曲几何形状的最短路径。在下面的图片中，我们可以将它表示为连接红点的红线。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/c37f5d4a3eb4cbf37d907d78ba472e98.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*YaxyocK5ICkbHBxfmbYIFg.png"/></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk translated">流形上两点之间的最短路径[3]</figcaption></figure><p id="7d18" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">如何把这个嵌入到一个更低维的空间？假设我们在欧几里得空间中，我们只是将权重放在形成所谓的<em class="ng">邻接</em>矩阵的点上。不需要深究数学，因为我们现在有一个矩阵，当我们有矩阵时，如何到达一个低维空间？如果你读到这里，你会知道我们又在讨论矩阵分解了😊</p><h2 id="d291" class="mt jl in bd jm mu mv dn jq mw mx dp ju kt my mz jy kx na nb kc lb nc nd kg ne bi translated">2.1.2.2.t-SNE</h2><p id="13b9" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">这个算法中的数学可能很复杂，但你可以理解为取<em class="ng"> k </em>最近邻，我们将改变内核的大小以匹配<em class="ng"> k </em>最近邻。为了将它嵌入到一个较低的维度中，该算法所做的是试图保持相似的实例靠近，而不相似的实例分开。</p><p id="c947" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">如果您想知道何时需要使用它，它主要用于可视化，特别是可视化高维空间中的实例集群。</p><h2 id="460f" class="mt jl in bd jm mu mv dn jq mw mx dp ju kt my mz jy kx na nb kc lb nc nd kg ne bi translated">2.1.2.3.LLE:局部线性嵌入</h2><p id="2beb" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">简而言之，该算法测量每个训练实例与其最近邻居之间的距离，并寻找训练集的低维表示，其中局部关系得到最佳保留。与前面类似，我们通过形成编码这些线性关系的权重矩阵来实现这一点。</p><p id="851e" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">为了将其映射到低维空间，我们希望保持权重固定，并在低维空间中找到实例图像的最佳位置。</p><p id="fa43" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">如果你想展开扭曲的流形，这种算法是好的，但是对于大数据集来说是不好的，因为这种算法伸缩性很差。</p><h2 id="2205" class="mt jl in bd jm mu mv dn jq mw mx dp ju kt my mz jy kx na nb kc lb nc nd kg ne bi translated">2.1 代码比较</h2><p id="34ed" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">当可视化这些算法的结果时，我们得到以下结果</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi nx"><img src="../Images/fd13317c3344213862d1d41c24bf9919.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3GOyBfwvKAUErp8hl4r-ZQ.png"/></div></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk translated">比较 S-dataset 中的不同算法[4]</figcaption></figure><p id="09f2" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">我们可以看到 SNE 霸王龙是最慢的，这种情况经常发生。我在这里没有谈到多维尺度(MDS)，但我们可以看到，它显示了良好的结果，虽然它需要很长的计算时间。还值得一提的是，对于这个特定的数据集，随机投影的效果出奇地好，并且比其他算法快得多。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ny"><img src="../Images/1adcf4829cc9dd2221a390a18c4f01ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eMMbKbLDiP2lnCOWZQY76g.png"/></div></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk translated">S 数据集中的随机投影[4]</figcaption></figure><h1 id="ff49" class="jk jl in bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">2.2.特征选择</h1><p id="1262" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">顾名思义，我们在这里选择最有用的功能进行训练。为了保持本文结构的一致性，我将首先对主要的特征选择技术进行分类，然后给出一些最重要算法的例子。如果您对特征选择技术的更深入的概述感兴趣，我鼓励您阅读[6]。</p><p id="c1d7" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">特征选择技术最初可以分为无监督的(如果这些方法忽略目标变量，而是使用例如相关性来去除冗余变量)和有监督的(使用目标变量来去除冗余特征)，但是在这两种情况下都使用 3 种主要算法:过滤方法、包装方法和嵌入方法。</p><h1 id="fa2a" class="jk jl in bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">2.2.1.过滤方法</h1><p id="3b04" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">这些算法应用统计测量来给特征分配分数。这个分数用于对特性进行排序，根据这个分数，我们将选择保留哪些特性，放弃哪些特性。</p><p id="2ca1" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">这些方法可以分为单变量(独立考虑特征)或多变量(考虑因变量)</p><h2 id="3145" class="mt jl in bd jm mu mv dn jq mw mx dp ju kt my mz jy kx na nb kc lb nc nd kg ne bi translated">2.2.1.1.检验</h2><p id="d1ca" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">它是在每个特征和特征之间计算的。选择具有最佳卡方得分的特征。为此，必须满足三个条件:</p><ul class=""><li id="d065" class="lx ly in kk b kl lg kp lh kt lz kx ma lb mb lf mc md me mf bi translated">该特征必须是分类的</li><li id="ade6" class="lx ly in kk b kl mg kp mh kt mi kx mj lb mk lf mc md me mf bi translated">它是独立采样的</li><li id="6b4e" class="lx ly in kk b kl mg kp mh kt mi kx mj lb mk lf mc md me mf bi translated">该特征的预期频率高于 5</li></ul><h2 id="b574" class="mt jl in bd jm mu mv dn jq mw mx dp ju kt my mz jy kx na nb kc lb nc nd kg ne bi translated">2.2.1.2.信息增益</h2><p id="23a5" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">在这里，我们正在计算转换的熵的减少。虽然熵的概念起源于热力学，作为一种分子无序的度量，它扩展到其他领域，如<em class="ng">信息论</em>，在那里它测量一条消息的平均信息内容。在 ML 中，当一个集合只包含一个类的实例时，它的熵为零。</p><h2 id="e210" class="mt jl in bd jm mu mv dn jq mw mx dp ju kt my mz jy kx na nb kc lb nc nd kg ne bi translated">2.2.1.3.相关系数得分</h2><p id="3ccb" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">寻找与其他要素不相关的要素。</p><h1 id="37d2" class="jk jl in bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">2.2.2.包装方法</h1><p id="b536" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">这里，一组特征的选择被认为是一个搜索问题，其中不同的组合被准备、评估和相互比较。为了评估特征的组合并基于模型准确度分配分数，使用了预测模型。根据搜索过程，可以对算法进行细分:</p><ul class=""><li id="9d5a" class="lx ly in kk b kl lg kp lh kt lz kx ma lb mb lf mc md me mf bi translated">有条不紊的搜索，如最佳优先搜索</li><li id="0269" class="lx ly in kk b kl mg kp mh kt mi kx mj lb mk lf mc md me mf bi translated">随机搜索，例如随机爬山算法</li><li id="01db" class="lx ly in kk b kl mg kp mh kt mi kx mj lb mk lf mc md me mf bi translated">使用类似向前和向后传递的试探法来添加/移除特征</li></ul><p id="dd79" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">值得一提的是，由于重复的学习步骤和交叉验证，包装器方法在计算上比过滤器方法更昂贵。从好的方面来看，这些方法比过滤方法更准确。</p><h2 id="c259" class="mt jl in bd jm mu mv dn jq mw mx dp ju kt my mz jy kx na nb kc lb nc nd kg ne bi translated">2.2.2.1.正向特征选择</h2><p id="af8e" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">它迭代地选择最佳性能特征。然后，它会选择与第一个特性结合使用时性能最佳的第二个特性。它不断迭代，直到达到期望的性能。</p><h2 id="7d14" class="mt jl in bd jm mu mv dn jq mw mx dp ju kt my mz jy kx na nb kc lb nc nd kg ne bi translated">2.2.2.2.向后特征选择</h2><p id="e444" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">这个想法基本上与前向特征选择相反。它开始考虑所有特征，并在每次迭代中删除最不重要的特征，从而提高模型的性能。</p><h2 id="2ebb" class="mt jl in bd jm mu mv dn jq mw mx dp ju kt my mz jy kx na nb kc lb nc nd kg ne bi translated">2.2.2.3.递归特征选择</h2><p id="ef48" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">这里的目标是通过递归地考虑较小的特征集来选择特征。给出了特征的权重估计。训练估计器，获得每个特征的重要性，然后从集合中删除最不重要的特征。该过程在删减集上递归重复，直到达到期望的特征数量。</p><h1 id="9e15" class="jk jl in bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">2.2.3.嵌入式方法</h1><p id="1cf3" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">在创建模型时，嵌入式方法可以了解哪些特性对模型的性能贡献最大。最常见的类型是正则化(或惩罚)方法，这种方法在预测算法的优化中引入了额外的约束，从而以更高的复杂性惩罚模型。</p><p id="da1b" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">与提到的替代方法相比，嵌入方法在计算上不如包装方法密集，但是它们具有学习模型特有的缺点。</p><h2 id="b01e" class="mt jl in bd jm mu mv dn jq mw mx dp ju kt my mz jy kx na nb kc lb nc nd kg ne bi translated">2.2.3.1.拉索正规化 L1</h2><p id="ec96" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">LASSO(最小绝对收缩和选择操作符)倾向于将权重向下推至零。这导致了稀疏模型，其中除了最重要的权重之外，所有的权重都为零。</p><h2 id="b5bb" class="mt jl in bd jm mu mv dn jq mw mx dp ju kt my mz jy kx na nb kc lb nc nd kg ne bi translated">2.2.3.2.弹性网</h2><p id="1ae8" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">LASSO 在某些情况下可能表现不稳定，例如，当几个要素高度相关或要素多于行时。在这种情况下，弹性网是首选，虽然它增加了一个额外的超参数。</p><h1 id="917e" class="jk jl in bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">3.结论</h1><p id="c5a5" class="pw-post-body-paragraph ki kj in kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ig bi translated">当谈到特征提取中的维度技术时，它有助于使用矩阵分解+邻居图的框架来尝试对任何遇到的技术进行分类。这可以帮助你理解几乎所有降维算法背后的核心思想。</p><p id="abba" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">我们也看到了一些维度技术，它们如何比较以及何时使用。最后，我们还看到了特征选择技术的分类以及一些最常见的算法。</p><p id="864e" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">这篇文章的续篇可以是关于通过收集新数据来创建新特性的文章，或者是关于自动编码器的文章。</p><p id="4c46" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">参考资料:</p><p id="ad6a" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">[1]使用 Scikit-Learn、Keras 和 TensorFlow 进行机器实践学习，第二版，作者 Aurelien Geron</p><p id="0d09" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">[2]由 Carlos Huertas (NxGTR)提出的维数约简— Kaggle Days SF</p><p id="a720" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">[3]利兰·麦金尼斯的《虚张声势者降维指南》</p><p id="6134" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">[4]Stefan küHN 的《用于数据可视化和特征工程的流形学习和降维》</p><p id="f49c" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">[5]贾努·维尔马著《流形学习》</p><p id="d08d" class="pw-post-body-paragraph ki kj in kk b kl lg kn ko kp lh kr ks kt li kv kw kx lj kz la lb lk ld le lf ig bi translated">[6]功能选择—详尽概述</p></div></div>    
</body>
</html>