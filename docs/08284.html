<html>
<head>
<title>Understanding the Nitty-Gritty Details of Regression Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解回归分析的本质细节</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/understanding-the-nitty-gritty-details-of-regression-analysis-e05a94fb47b5?source=collection_archive---------8-----------------------#2022-06-02">https://blog.devgenius.io/understanding-the-nitty-gritty-details-of-regression-analysis-e05a94fb47b5?source=collection_archive---------8-----------------------#2022-06-02</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><div class=""><h2 id="04f0" class="pw-subtitle-paragraph jk im in bd b jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb dk translated">用可视化方法探索 SST、SSE 和 SSR 的现实意义</h2></div><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi kc"><img src="../Images/faede5ee9036b03f52d8a70c40e4c03f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jVWspiAjMBJpfs4jtWq4qQ.jpeg"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated"><a class="ae ks" href="https://unsplash.com/@vinikhill?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> NIKHIL </a>在<a class="ae ks" href="https://unsplash.com/s/photos/patterns?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="5f14" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi lp translated"><span class="l lq lr ls bm lt lu lv lw lx di">如果</span>你刚刚开始尝试机器学习，那么你肯定遇到过线性回归。最基本的算法之一，但背后有丰富的概念。</p><p id="d6a0" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">但事情有那么简单吗？</p><p id="8e00" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">我来问你这个，<strong class="kv io">平方和误差(SSE) </strong> &amp; <strong class="kv io">平方和回归(SSR) </strong>有什么区别？第一个可能看起来更熟悉(感谢那里所有的课程！)但是其他的呢。</p><p id="17e2" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">除此之外，什么是<strong class="kv io">平方和(SST) </strong>？看，不再那么简单了，是吗？</p><p id="2822" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">不幸的是，这是使用图书馆学习基本概念的副作用。这会导致太多的抽象。</p><p id="4d05" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">当你在学习新的东西时，最好是在最低的水平上用最少的资源去做。随着时间的推移，这些东西复合在一起，形成了你思考 ML 问题的方式。</p><blockquote class="ly"><p id="8082" class="lz ma in bd mb mc md me mf mg mh lo dk translated">当你继续在基础上建造时，基础的 1 度移动会增加几十度。</p></blockquote><p id="5408" class="pw-post-body-paragraph kt ku in kv b kw mi jo ky kz mj jr lb lc mk le lf lg ml li lj lk mm lm ln lo ig bi translated">因此，了解基层的这些细节非常重要。</p></div><div class="ab cl mn mo hr mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="ig ih ii ij ik"><p id="3759" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">现在，让我们开始有趣的部分。</p><p id="ef8b" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">下图是两个变量<strong class="kv io"> X(自变量)</strong>和<strong class="kv io"> Y(因变量)</strong>之间因果关系的线性近似。</p><p id="b0df" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">没什么特别的，只是对一组数据点拟合一条线，直到最佳拟合。</p><blockquote class="mu mv mw"><p id="41eb" class="kt ku mx kv b kw kx jo ky kz la jr lb my ld le lf mz lh li lj na ll lm ln lo ig bi translated">公式:ŷ=b+w*x<br/>ŷ: y 预测值<br/> b: Y 截距(偏差因子)<br/> w:权重<br/> X:自变量 x<br/>y-bar:y 真实值的平均值</p></blockquote><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi nb"><img src="../Images/113cab85d182e9a702a5794485e3a867.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a4mU3yCPmNPnoLPndypi9Q.png"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">SST、SSE 和 SSR</figcaption></figure><blockquote class="mu mv mw"><p id="f00a" class="kt ku mx kv b kw kx jo ky kz la jr lb my ld le lf mz lh li lj na ll lm ln lo ig bi translated">从图中可以看出，任何回归模型的最终目标都是<strong class="kv io">解释总可变性中的最大可变性</strong>。也就是说，在某个阈值之前，具有高值的已解释变异性(SSR)和低值的未解释变异性(SSE)。</p></blockquote></div><div class="ab cl mn mo hr mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="ig ih ii ij ik"><h2 id="5be8" class="nc nd in bd ne nf ng dn nh ni nj dp nk lc nl nm nn lg no np nq lk nr ns nt nu bi translated">总平方和(SST):</h2><p id="18b6" class="pw-post-body-paragraph kt ku in kv b kw nv jo ky kz nw jr lb lc nx le lf lg ny li lj lk nz lm ln lo ig bi translated">总平方和是因变量 Y 的真值与其平均值之间的平方差之和。这表示当前数据集上的模型可以解释的总可变性。</p><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/97cc8028a230ae83a3208a324f638db2.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*ZbYE84-W9Bvv5-nZjgzLew.png"/></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">SST 公式</figcaption></figure><p id="3344" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">一些关键点:</p><ul class=""><li id="3701" class="ob oc in kv b kw kx kz la lc od lg oe lk of lo og oh oi oj bi translated">总可变性<strong class="kv io"> (SST) </strong> =已解释可变性<strong class="kv io"> (SSR) </strong> +未解释可变性<strong class="kv io"> (SSE) </strong></li><li id="3e34" class="ob oc in kv b kw ok kz ol lc om lg on lk oo lo og oh oi oj bi translated">也称为<strong class="kv io">平方和总和(TSS) </strong>。</li></ul></div><div class="ab cl mn mo hr mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="ig ih ii ij ik"><h2 id="0281" class="nc nd in bd ne nf ng dn nh ni nj dp nk lc nl nm nn lg no np nq lk nr ns nt nu bi translated">误差平方和(SSE):</h2><p id="05ac" class="pw-post-body-paragraph kt ku in kv b kw nv jo ky kz nw jr lb lc nx le lf lg ny li lj lk nz lm ln lo ig bi translated">误差平方和表示回归模型的估计能力。它表示训练后模型无法解释的差异量。</p><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/049114b2277f47f5256cae2fcca20189.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*A09GcwRlb_W0iXFgvXuSQw.png"/></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">上交所公式</figcaption></figure><p id="8940" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">一些关键点:</p><ul class=""><li id="71e1" class="ob oc in kv b kw kx kz la lc od lg oe lk of lo og oh oi oj bi translated">具有高估计能力的模型意味着预测值(ŷ)接近真实值(y)。因此，会导致更低的 SSE 值，这是我们的最终目标。</li><li id="93bc" class="ob oc in kv b kw ok kz ol lc om lg on lk oo lo og oh oi oj bi translated">由于真实值和预测值之间的差异是平方的，这导致较大的差异受到严重惩罚。</li><li id="8cbe" class="ob oc in kv b kw ok kz ol lc om lg on lk oo lo og oh oi oj bi translated">又称为<strong class="kv io">【残差平方和】</strong>。</li></ul><blockquote class="mu mv mw"><p id="1f36" class="kt ku mx kv b kw kx jo ky kz la jr lb my ld le lf mz lh li lj na ll lm ln lo ig bi translated"><strong class="kv io">误差&amp;残差有什么区别？</strong>答案取决于上下文。<br/>在机器学习的背景下，比如在这种情况下，它们可以互换使用，但在统计学中，它们有不同的含义。更多关于那个<a class="ae ks" href="https://stackoverflow.com/questions/56754560/what-is-the-exact-difference-between-error-and-residual" rel="noopener ugc nofollow" target="_blank">这里</a>。</p></blockquote></div><div class="ab cl mn mo hr mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="ig ih ii ij ik"><h2 id="affc" class="nc nd in bd ne nf ng dn nh ni nj dp nk lc nl nm nn lg no np nq lk nr ns nt nu bi translated">平方和回归(SSR):</h2><p id="3cd2" class="pw-post-body-paragraph kt ku in kv b kw nv jo ky kz nw jr lb lc nx le lf lg ny li lj lk nz lm ln lo ig bi translated">平方和回归解释了模型与数据点的拟合程度。它表示模型在训练后能够解释的差异量。</p><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/da42f718185111b6b2c865f4111e7ce6.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*jZvYSDVwIDH0lt-7M4Bjrw.png"/></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">SSR 公式</figcaption></figure><blockquote class="mu mv mw"><p id="c7d5" class="kt ku mx kv b kw kx jo ky kz la jr lb my ld le lf mz lh li lj na ll lm ln lo ig bi translated">用于计算回归模型拟合优度的<strong class="kv io"> R 平方度量</strong>只不过是<strong class="kv io">解释可变性(SSR) </strong>与<strong class="kv io">总可变性(SST) </strong>的比率。</p><p id="07ca" class="kt ku mx kv b kw kx jo ky kz la jr lb my ld le lf mz lh li lj na ll lm ln lo ig bi translated"><strong class="kv io"> R 平方= SSR/SST </strong></p><p id="f0b9" class="kt ku mx kv b kw kx jo ky kz la jr lb my ld le lf mz lh li lj na ll lm ln lo ig bi translated">其范围在[0，1]之间。<strong class="kv io">随着 SSR 接近 SST </strong>，<strong class="kv io">R 平方值接近 1。这表明该模型具有很好的拟合性，能够解释数据中的许多可变性。</strong></p></blockquote><p id="1233" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">一些关键点:</p><ul class=""><li id="ac80" class="ob oc in kv b kw kx kz la lc od lg oe lk of lo og oh oi oj bi translated">也被称为<strong class="kv io"/><strong class="kv io">【ESS】</strong></li></ul></div><div class="ab cl mn mo hr mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="ig ih ii ij ik"><h1 id="ede1" class="op nd in bd ne oq or os nh ot ou ov nk jt ow ju nn jw ox jx nq jz oy ka nt oz bi translated">感谢阅读！</h1><p id="2e19" class="pw-post-body-paragraph kt ku in kv b kw nv jo ky kz nw jr lb lc nx le lf lg ny li lj lk nz lm ln lo ig bi translated">这给了我很大的鼓励！😃<em class="mx">如果你觉得这个帖子很有意思，想看更多，可以考虑</em> <a class="ae ks" href="https://medium.com/subscribe/@pratik.pandav" rel="noopener"> <strong class="kv io"> <em class="mx">关注我</em></strong></a><strong class="kv io"><em class="mx"/></strong>🥁<em class="mx">。我每周发布与机器学习、统计和数据分析相关的主题。我喜欢通过可视化来学习，因此，我的帖子包含了大量的图表、模拟和代码示例。</em></p><p id="0b94" class="pw-post-body-paragraph kt ku in kv b kw kx jo ky kz la jr lb lc ld le lf lg lh li lj lk ll lm ln lo ig bi translated">简要说明一下，我尽最大努力将错误降到最低，但它们是我们学习的一部分，所以，如果你发现了什么，请指出来。最后，请随意提出你希望我写的主题。</p></div></div>    
</body>
</html>