# 卡夫卡溪流

> 原文：<https://blog.devgenius.io/kafka-streams-b928216a7413?source=collection_archive---------4----------------------->

我为什么要在乎？

> 我们以构建抽象概念为生。

是的，你可以自由地从 OOPs 中得出一个类比，但是我具体的意思是，一个工程师所做的就是在某个东西上建立一个抽象。

# 事件流

事件流是指**以**事件流**的形式从数据库、传感器、移动设备、云服务和软件应用等事件源实时**捕获数据的实践；持久地存储这些事件流以供以后检索；实时和追溯性地操纵、处理和**对事件流做出反应；并根据需要将事件流路由到不同的目的地技术。**

因此，事件流确保了数据的连续流动和解释，以便正确的信息在正确的时间出现在正确的位置。

事件流有很多用例，最近它们在**反应式微服务**中获得了越来越多的采用，其中**服务对服务通信**由事件流控制。

如今，事件流正在金融科技、电子商务、医疗保健、汽车、物联网等领域得到应用。你可以在这个链接中找到很多用例:[https://kafka.apache.org/powered-by](https://kafka.apache.org/powered-by)

事件流提倡使用构建事件驱动的系统。马丁·福勒关于事件驱动架构的精彩演讲:[https://www.youtube.com/watch?v=STKCRSUsyP0&VL = en](https://www.youtube.com/watch?v=STKCRSUsyP0&vl=en)

# 卡夫卡

Apache Kafka 是一个事件流平台。作为一个事件流平台的完整解决方案，Kafka 为我们提供了:
到**发布**(写)和**订阅**(读)事件流，包括从其他系统持续导入/导出您的数据。
到**持久可靠地存储**事件流，时间不限。
到**在事件发生时或事后处理**一系列事件。

Kafka(objective)最棒的地方在于它快如闪电([使用 OS Pagecache](https://kafka.apache.org/documentation/) )以及易用性。*(专业提示:有时将事情从堆中取出有利于获得更快的响应。)*

卡夫卡主要有两个组成部分:卡夫卡经纪人和卡夫卡客户。卡夫卡经纪人是卡夫卡的中枢神经，是保存所有讯息的地方。

**Kafka 客户端是 Kafka 提供的与其集成的抽象**或 API。

# 卡夫卡的客户

当谈到与 Kafka 的集成时，我们应该感谢 Kafka 的客户，因为他们提供了必要的*抽象*来这样做。否则，我们将会以编写大量带有大量序列化和解序列化代码的套接字相关代码而告终。

卡夫卡的客户有两种:生产者和消费者。生产者让我们在 卡夫卡上制作/发布数据 ***，同样，消费者让我们消费/接收来自*** 卡夫卡的 ***发布数据。***

让我们用一个用例来看看一个客户端的运行情况:
在 ZebraMart，每分钟都有来自全球各地的订单。当从 POS 机下订单时，原始订单信息被发布到主题上。让我们假设它有订单金额和客户用户名。我们的团队将为所有订单金额超过 100 万的用户创建定制的忠诚度计划。因此，我们的任务是向忠诚度团队提供这样的用户名。
我们大致可以实施三个步骤:
·消费订单详情。
过滤订单金额超过 100 万的记录
将这些订单的用户名发送到忠诚度计划的不同主题

因此，让我们观察我们将不得不使用香草卡夫卡生产者和消费者为此编写的代码:

这是一些纯粹的命令式代码。你可以清楚地看到，这种方法有很多危险信号，比如可读性、可维护性、整洁性、单元测试的简易性、无限循环等等。在所有这些中，更明显的一点是，业务逻辑在代码库中完全丢失了。不要担心，我也不建议用上述方法寻找业务逻辑，因为我们做得比这更好。

声明式编程让我们解决了上面提到的许多问题。为了以声明的方式编写，即编写*做什么而不是* *如何做，*我们转向函数式编程并采用 Kafka 流。

# 卡夫卡溪流

我们刚刚看到的相同代码可以被一行代码完全取代:

```
streamsBuilder.stream(*RAW_ORDER_TOPIC*)
        .filter(this::orderAmountIsGreaterThan1Mil)
        .mapValues(OrderDto::getUserName)
        .to(*ELIGIBLE_FOR_GIFTS*);
```

就是这样！

上面的代码也非常符合*的业务需求，*并且很酷。

正如我们所知，流是记录的无界、连续的实时流，我们可以清楚地看到，我们提供的所有操作都是在记录的“流”上进行的。一旦我们开始从流中获取数据，我们就过滤金额大于 1 百万的订单，然后我们使用 getUserName 方法将这些订单实例映射到 string 实例。一旦 order dto 被映射到 username 值，该值就被发布到另一个主题进行进一步处理。这当然是一个非常简单的用例，但是你可以想象你可以用 Kafka Streams 实现什么。

让我们来定义卡夫卡的作品:

> Kafka Streams 是一个客户端库，更像是 Kafka 客户端的 DSL

所以基本上卡夫卡流是对卡夫卡客户本身的抽象。使用 Kafka streams，我们可以对流进行不同类型的处理:
**无状态处理** **有状态处理**:如聚合、分组、连接等
**窗口操作**:给我最近 2 小时/天的记录等。当我们谈论记录流时，这更有意义。

但是等等，这还不是全部。Kafka Streams 具有弹性、高度可伸缩性和容错性。我们知道卡夫卡它本身是有弹性的和容错的，卡夫卡不保证关于处理或计算的逻辑是相同的。你必须将这些功能构建到你的应用程序中(因为这是处理记录的过程)。但是当涉及到 Kafka 流时，它保证了计算逻辑本身的弹性和容错性。它是从零开始构建的，以分布式计算为核心。当谈到纵向扩展计算能力时，您可以轻松地纵向和横向扩展。要扩大规模，你所要做的就是分配更多的流线程，这可以通过一个配置属性轻松完成。当你必须向外扩展时，你所要做的就是增加你的应用程序的实例，Kafka Streams 会处理剩下的事情。
***亲提示:(1)一个好的起点是拥有 app 的 n+1 个实例，*** *其中****n =卡夫卡主题的分区数。(2)如果您不想更改记录的关键字，请使用 mapValue 函数而不是 map 函数。更改 Kafka 流中的密钥可能会调用数据的重新分区。(3)还要记住卡夫卡式的消费者再平衡。你不想吃太多。***

在 Kafka Stream 的最新版本中，它还支持消息传递的“恰好一次”语义。

Kafka Streams 适用于从小到大的各种数据处理需求。就我个人而言，我测试过 Kafka Streams 在几分钟内处理数百万条记录。

编码快乐！