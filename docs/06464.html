<html>
<head>
<title>Twitter Sentiment Analysis with Traditional Machine Learning Algorithms Vs Deep Learning Algorithms(LSTM).</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用传统机器学习算法与深度学习算法的 Twitter 情感分析(LSTM)。</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/twitter-sentiment-analysis-with-traditional-machine-learning-algorithms-vs-deep-learning-b5fb7a4d8b00?source=collection_archive---------6-----------------------#2022-01-10">https://blog.devgenius.io/twitter-sentiment-analysis-with-traditional-machine-learning-algorithms-vs-deep-learning-b5fb7a4d8b00?source=collection_archive---------6-----------------------#2022-01-10</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><figure class="gl gn jl jm jn jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi jk"><img src="../Images/8aae1c26e537bba2f77d468aba8a202f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S-WKjg0686CNeIqunQR4cA.png"/></div></div></figure><p id="ebbc" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">在我的第一篇文章中，我们经历了使用 Tweepy 抓取推文的过程，也经历了清理和预处理推文的几个步骤。</p><p id="5895" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">下一篇文章的目标是使用传统的机器学习算法，如逻辑回归、支持向量机，以及深度学习算法，LSTM(长短期记忆)，一种特殊类型的 RNNs(递归神经网络)，对上一篇文章结束时获得的数据集进行建模。</p><p id="f19a" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">如果你还没有，你可以在这里阅读我以前的帖子；</p><div class="kt ku gp gr kv kw"><a href="https://medium.com/@bolarinwaoreoluwa24/preprocessing-tweets-for-twitter-sentiment-analysis-ced4b4963223" rel="noopener follow" target="_blank"><div class="kx ab fo"><div class="ky ab kz cl cj la"><h2 class="bd io gy z fp lb fr fs lc fu fw im bi translated">为推特情感分析预处理推文。</h2><div class="ld l"><h3 class="bd b gy z fp lb fr fs lc fu fw dk translated">在进行自然语言处理时，我们需要对数据进行预处理，原因有很多，比如脏…</h3></div><div class="le l"><p class="bd b dl z fp lb fr fs lc fu fw dk translated">medium.com</p></div></div><div class="lf l"><div class="lg l lh li lj lf lk jt kw"/></div></div></a></div></div><div class="ab cl ll lm hr ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ig ih ii ij ik"><h1 id="eea0" class="ls lt in bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated"><strong class="ak">用机器学习。</strong></h1><p id="2929" class="pw-post-body-paragraph jv jw in jx b jy mq ka kb kc mr ke kf kg ms ki kj kk mt km kn ko mu kq kr ks ig bi translated"><strong class="jx io">导入所需的库。</strong></p><figure class="mv mw mx my gt jo"><div class="bz fp l di"><div class="mz na l"/></div></figure><h2 id="82eb" class="nb lt in bd lu nc nd dn ly ne nf dp mc kg ng nh mg kk ni nj mk ko nk nl mo nm bi translated"><strong class="ak">加载我们的数据集。</strong></h2><pre class="mv mw mx my gt nn no np nq aw nr bi"><span id="ba12" class="nb lt in no b gy ns nt l nu nv"><strong class="no io">data = pd.read_csv("Tweets.csv")<br/>data.head()</strong></span></pre><figure class="mv mw mx my gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi nw"><img src="../Images/c3ba437e6586f8ac3ae1c6b3f0d79f1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*THN4QCVobsAmRYYBIO4Ymw.jpeg"/></div></div></figure><p id="291a" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">我们应该将原始数据框架分割成建模所需的数据框架；</p><pre class="mv mw mx my gt nn no np nq aw nr bi"><span id="74eb" class="nb lt in no b gy ns nt l nu nv">Data<strong class="no io">=</strong>data[['non_punctuated_tweets','sentiment']]<br/>Data</span></pre><figure class="mv mw mx my gt jo gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/55189fe049cb492eb9ab884076c780bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*HIr6FbNrUi7Vfji2KKJurA.png"/></div></figure><h2 id="646c" class="nb lt in bd lu nc nd dn ly ne nf dp mc kg ng nh mg kk ni nj mk ko nk nl mo nm bi translated"><strong class="ak">情感栏的标签编码。</strong></h2><pre class="mv mw mx my gt nn no np nq aw nr bi"><span id="ff30" class="nb lt in no b gy ns nt l nu nv">Data['sentiment'] <strong class="no io">=</strong> Data['sentiment']<strong class="no io">.</strong>replace('positive',1)<br/>Data['sentiment'] <strong class="no io">=</strong> Data['sentiment']<strong class="no io">.</strong>replace('negative',<strong class="no io">-</strong>1)<br/>Data['sentiment'] <strong class="no io">=</strong> Data['sentiment']<strong class="no io">.</strong>replace('neutral',0)<br/><br/><em class="ny">### code for encoding the meaning of positive negative and  neutral tweets</em></span></pre><p id="d48f" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">正值取值 1，负值取值-1，中性取值 0。因此，我们应该有 3 个独特的类在我们修改后的情绪栏。</p><pre class="mv mw mx my gt nn no np nq aw nr bi"><span id="2777" class="nb lt in no b gy ns nt l nu nv">Data['sentiment']<strong class="no io">.</strong>unique()<br/>#array([ 1, -1,  0], dtype=int64)<br/></span></pre><p id="f834" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">让我们来看看我们独特的类的数量计数；</p><pre class="mv mw mx my gt nn no np nq aw nr bi"><span id="9e19" class="nb lt in no b gy ns nt l nu nv">Data['sentiment']<strong class="no io">.</strong>value_counts()<br/> 1    12622<br/>-1     3823<br/> 0     1329<br/>Name: sentiment, dtype: int64</span></pre><p id="4f65" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">在这里，我们发现自己处于一个不平衡的数据集的情况下(即，正类远远大于负类和中性类)。</p><p id="6a11" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">有几种技术可以用来解决这个问题，但是我们将只使用一种技术。我们将利用<strong class="jx io">重采样技术(欠采样和过采样)。</strong></p><h1 id="abf8" class="ls lt in bd lu lv nz lx ly lz oa mb mc md ob mf mg mh oc mj mk ml od mn mo mp bi translated"><strong class="ak">推文的符号化。</strong></h1><pre class="mv mw mx my gt nn no np nq aw nr bi"><span id="b0d9" class="nb lt in no b gy ns nt l nu nv"><strong class="no io">from</strong> nltk.tokenize <strong class="no io">import</strong> TweetTokenizer<br/>  <br/><em class="ny"># Create a reference variable for Class TweetTokenizer</em><br/>tt <strong class="no io">=</strong> TweetTokenizer()<br/>Data['tokenized_tweets'] <strong class="no io">=</strong> Data['non_punctuated_tweets']<strong class="no io">.</strong>apply(tt<strong class="no io">.</strong>tokenize)<br/>Data['tokenized_tweets']<strong class="no io">.</strong>head()<br/>Data</span></pre><figure class="mv mw mx my gt jo gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/a8eec0051ef3752fc6057e08fa75435c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*PZ5qkM9O-EM4pVjfoqKVig.png"/></div></figure><h1 id="dc4a" class="ls lt in bd lu lv nz lx ly lz oa mb mc md ob mf mg mh oc mj mk ml od mn mo mp bi translated"><strong class="ak">推文词干。</strong></h1><pre class="mv mw mx my gt nn no np nq aw nr bi"><span id="a723" class="nb lt in no b gy ns nt l nu nv"><strong class="no io">import</strong> nltk<br/>st <strong class="no io">=</strong> nltk<strong class="no io">.</strong>PorterStemmer()<br/><strong class="no io">def</strong> stemming_on_text(data):<br/>    text <strong class="no io">=</strong> [st<strong class="no io">.</strong>stem(word) <strong class="no io">for</strong> word <strong class="no io">in</strong> data]<br/>    <strong class="no io">return</strong> text<br/>Data['stemmed_tweets']<strong class="no io">=</strong> Data['tokenized_tweets']<strong class="no io">.</strong>apply(<strong class="no io">lambda</strong> x: stemming_on_text(x))<br/>#Data['stemmed_tweets']<strong class="no io">.</strong>head()</span></pre><p id="ea7e" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">获取我们的输入数据和输出数据，同时还分成测试和训练数据；</p><figure class="mv mw mx my gt jo"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="2ab3" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">我们还需要将输入数据(文本格式)转换成我们的机器学习算法可以处理的数据，我们需要将它们转换成如下所示的向量；</p><h1 id="3824" class="ls lt in bd lu lv nz lx ly lz oa mb mc md ob mf mg mh oc mj mk ml od mn mo mp bi translated">使用 TF-IDF 将 tweets 转换为矢量。</h1><pre class="mv mw mx my gt nn no np nq aw nr bi"><span id="8eb9" class="nb lt in no b gy ns nt l nu nv">tfidf_vect <strong class="no io">=</strong> TfidfVectorizer()<br/>tfidf_vect<strong class="no io">.</strong>fit(X)<br/>X_train_tfidf <strong class="no io">=</strong>  tfidf_vect<strong class="no io">.</strong>transform(X_train)<br/>X_test_tfidf <strong class="no io">=</strong>  tfidf_vect<strong class="no io">.</strong>transform(X_test)</span></pre><h1 id="d094" class="ls lt in bd lu lv nz lx ly lz oa mb mc md ob mf mg mh oc mj mk ml od mn mo mp bi translated">数据建模。</h1><figure class="mv mw mx my gt jo"><div class="bz fp l di"><div class="mz na l"/></div></figure><h1 id="a001" class="ls lt in bd lu lv nz lx ly lz oa mb mc md ob mf mg mh oc mj mk ml od mn mo mp bi translated">而不需要重新采样不平衡的数据集。</h1><figure class="mv mw mx my gt jo"><div class="bz fp l di"><div class="mz na l"/></div></figure><figure class="mv mw mx my gt jo gh gi paragraph-image"><div class="gh gi of"><img src="../Images/0c4fa7d4792943fc854218216611a03f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*HZ9dkAJgCamnZ1ock8Yc6w.png"/></div></figure><p id="1804" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">以上是用不平衡数据建模的结果。从这里开始，我们将使用几种采样技术，看看它们的表现如何。</p><h1 id="ae25" class="ls lt in bd lu lv nz lx ly lz oa mb mc md ob mf mg mh oc mj mk ml od mn mo mp bi translated">随机过采样。</h1><figure class="mv mw mx my gt jo"><div class="bz fp l di"><div class="mz na l"/></div></figure><figure class="mv mw mx my gt jo gh gi paragraph-image"><div class="gh gi og"><img src="../Images/6d9709499fb234bcdcc0e642d254aae0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*ga8IROul9FYjS1uh489SwA.png"/></div></figure><h1 id="81af" class="ls lt in bd lu lv nz lx ly lz oa mb mc md ob mf mg mh oc mj mk ml od mn mo mp bi translated">打击过采样。</h1><figure class="mv mw mx my gt jo"><div class="bz fp l di"><div class="mz na l"/></div></figure><figure class="mv mw mx my gt jo gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/88cd2942b1dfd9b18b5ed59256c20403.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*3AtL56XgU6FTM3zsQhF5uQ.png"/></div></figure><h1 id="fd03" class="ls lt in bd lu lv nz lx ly lz oa mb mc md ob mf mg mh oc mj mk ml od mn mo mp bi translated">ADASYN:自适应合成采样(过采样)。</h1><figure class="mv mw mx my gt jo"><div class="bz fp l di"><div class="mz na l"/></div></figure><figure class="mv mw mx my gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi oi"><img src="../Images/fae2158065d1147ab1895334c733b71f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*vt1TDUoKcz64b_JuPnm4Xg.png"/></div></div></figure><h1 id="7c51" class="ls lt in bd lu lv nz lx ly lz oa mb mc md ob mf mg mh oc mj mk ml od mn mo mp bi translated">临界击打。</h1><figure class="mv mw mx my gt jo"><div class="bz fp l di"><div class="mz na l"/></div></figure><figure class="mv mw mx my gt jo gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/63f160167cdd2b2bd44c4417db3a877c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*rHSFiNWHxb3bejGT66BOoQ.png"/></div></figure><h1 id="49af" class="ls lt in bd lu lv nz lx ly lz oa mb mc md ob mf mg mh oc mj mk ml od mn mo mp bi translated">随机欠采样。</h1><figure class="mv mw mx my gt jo"><div class="bz fp l di"><div class="mz na l"/></div></figure><figure class="mv mw mx my gt jo gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/00f2ac47c22bfce8a483a77364215314.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*VbCnHhujyv7pXUiWg7j8sA.png"/></div></figure><h1 id="6b18" class="ls lt in bd lu lv nz lx ly lz oa mb mc md ob mf mg mh oc mj mk ml od mn mo mp bi translated">删除 Tomek 链接。</h1><figure class="mv mw mx my gt jo"><div class="bz fp l di"><div class="mz na l"/></div></figure><figure class="mv mw mx my gt jo gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/e83c0431052a6dc3a878615d23458c1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*Rm49JpdPgzdV9d2oEu8KKQ.png"/></div></figure><p id="dc4e" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">正如我们所看到的，当我们使用<strong class="jx io"> Tomek 链接移除</strong>欠采样技术<strong class="jx io">、</strong>时，我们的精度达到最高，大约为 71.95%</p><p id="9d61" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">当我们在没有对数据进行重采样的情况下建模时，精确度非常接近(71.5%)。</p><p id="c9ae" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">在本文接近尾声时，我们将把我们的结果与使用 LSTM 得到的结果进行比较。</p></div><div class="ab cl ll lm hr ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ig ih ii ij ik"><h1 id="17be" class="ls lt in bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated"><strong class="ak">用深度学习(LSTM)。</strong></h1><p id="8ef1" class="pw-post-body-paragraph jv jw in jx b jy mq ka kb kc mr ke kf kg ms ki kj kk mt km kn ko mu kq kr ks ig bi translated">也许你不知道 RNNs 和 LSTM，递归神经网络基本上是<strong class="jx io">特殊类型的人工神经网络，适用于时间序列数据或涉及序列的数据(在我们的情况下，文本数据)。</strong></p><p id="3251" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">文本可以说是单词的序列，对吗？</p><p id="eb08" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">另一方面，LSTM 可以说是 RNNs 的<strong class="jx io">高级版本</strong>，旨在解决常规 RNNs 遇到的消失梯度通常<strong class="jx io"> </strong>的问题<strong class="jx io">。</strong></p><p id="25e2" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">更多关于 RNN 和 LSTM 的知识和解释，点击<a class="ae om" href="https://purnasaigudikandula.medium.com/recurrent-neural-networks-and-lstm-explained-7f51c7f6bbb9" rel="noopener"> <strong class="jx io">这里</strong> </a> <strong class="jx io">。</strong></p><p id="f3c4" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">由于我们将利用 GPU 来更快地训练我们的模型，我决定将我的数据集(“Tweets.csv”)上传到 Kaggle，并利用 kaggle 内核。</p><h1 id="ea38" class="ls lt in bd lu lv nz lx ly lz oa mb mc md ob mf mg mh oc mj mk ml od mn mo mp bi translated"><strong class="ak">导入所需的库。</strong></h1><pre class="mv mw mx my gt nn no np nq aw nr bi"><span id="8eca" class="nb lt in no b gy ns nt l nu nv"><em class="ny"># Importing required libraries</em><br/><strong class="no io">import</strong> nltk<br/><strong class="no io">import</strong> numpy <strong class="no io">as</strong> np<br/><strong class="no io">import</strong> pandas <strong class="no io">as</strong> pd<br/><strong class="no io">from</strong> nltk.corpus <strong class="no io">import</strong> stopwords<br/><strong class="no io">from</strong> textblob <strong class="no io">import</strong> Word<br/><strong class="no io">from</strong> sklearn.preprocessing <strong class="no io">import</strong> LabelEncoder<br/><strong class="no io">from</strong> collections <strong class="no io">import</strong> Counter<br/><strong class="no io">import</strong> wordcloud<br/><strong class="no io">from</strong> sklearn.metrics <strong class="no io">import</strong> classification_report,confusion_matrix,accuracy_score<br/><strong class="no io">from</strong> tensorflow.keras.models <strong class="no io">import</strong> Sequential<br/><strong class="no io">from</strong> tensorflow.keras.preprocessing.text <strong class="no io">import</strong> Tokenizer<br/><strong class="no io">from</strong> tensorflow.keras.preprocessing.sequence <strong class="no io">import</strong> pad_sequences<br/><strong class="no io">from</strong> tensorflow.keras.layers <strong class="no io">import</strong> Dense, Embedding, LSTM, SpatialDropout1D<br/><strong class="no io">from</strong> sklearn.model_selection <strong class="no io">import</strong> train_test_split <br/><strong class="no io">import</strong> matplotlib.pyplot <strong class="no io">as</strong> plt<br/><strong class="no io">from</strong> nltk.tokenize <strong class="no io">import</strong> word_tokenize<br/><strong class="no io">from</strong> nltk.stem <strong class="no io">import</strong> WordNetLemmatizer<br/><strong class="no io">from</strong> tensorflow.keras.callbacks <strong class="no io">import</strong> EarlyStopping<br/><strong class="no io">import</strong> tensorflow_addons <strong class="no io">as</strong> tfa</span></pre><h1 id="38a6" class="ls lt in bd lu lv nz lx ly lz oa mb mc md ob mf mg mh oc mj mk ml od mn mo mp bi translated"><strong class="ak">数据</strong>。</h1><figure class="mv mw mx my gt jo"><div class="bz fp l di"><div class="mz na l"/></div></figure><pre class="mv mw mx my gt nn no np nq aw nr bi"><span id="7dbb" class="nb lt in no b gy ns nt l nu nv">Data</span></pre><figure class="mv mw mx my gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi on"><img src="../Images/79c46ce6958f501a3c98a09f4a97f385.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lPNx8f1qeF2axhyRgUpFbg.png"/></div></div></figure><h1 id="e8f3" class="ls lt in bd lu lv nz lx ly lz oa mb mc md ob mf mg mh oc mj mk ml od mn mo mp bi translated"><strong class="ak">标记化。</strong></h1><pre class="mv mw mx my gt nn no np nq aw nr bi"><span id="48c4" class="nb lt in no b gy ns nt l nu nv">Data['tokenized_tweets'] <strong class="no io">=</strong> Data<strong class="no io">.</strong>apply(<strong class="no io">lambda</strong> row : nltk<strong class="no io">.</strong>word_tokenize(str(row['non_punctuated_tweets'])),axis <strong class="no io">=</strong> 1)</span><span id="f020" class="nb lt in no b gy oo nt l nu nv">Data</span></pre><figure class="mv mw mx my gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi op"><img src="../Images/b1ea659913fde9faa15d1ce6a5b12976.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RPqsRKE6Z6gwDNfhLxrqwQ.png"/></div></div></figure><h2 id="4308" class="nb lt in bd lu nc nd dn ly ne nf dp mc kg ng nh mg kk ni nj mk ko nk nl mo nm bi translated">词汇化。</h2><pre class="mv mw mx my gt nn no np nq aw nr bi"><span id="c3f5" class="nb lt in no b gy ns nt l nu nv"><strong class="no io">def</strong> lemma(data):<br/>    <strong class="no io">return</strong> " "<strong class="no io">.</strong>join([Word(word)<strong class="no io">.</strong>lemmatize() <strong class="no io">for</strong> word <strong class="no io">in</strong> data])<br/>Data['lemmatized_tweets']<strong class="no io">=</strong> Data['tokenized_tweets']<strong class="no io">.</strong>apply(<strong class="no io">lambda</strong> x: lemma(x))</span><span id="6b46" class="nb lt in no b gy oo nt l nu nv">Data</span></pre><figure class="mv mw mx my gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi oq"><img src="../Images/a82baf0c02440bb7aaa9b1b53eaceeb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PTeZXLzr7RdaoTTq9jdVYg.png"/></div></div></figure><h1 id="4429" class="ls lt in bd lu lv nz lx ly lz oa mb mc md ob mf mg mh oc mj mk ml od mn mo mp bi translated"><strong class="ak">情感专栏的一个热门编码。</strong></h1><figure class="mv mw mx my gt jo"><div class="bz fp l di"><div class="mz na l"/></div></figure><pre class="mv mw mx my gt nn no np nq aw nr bi"><span id="e385" class="nb lt in no b gy ns nt l nu nv">Shape of label tensor: (17774, 3)</span></pre><p id="9f87" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">我们来看看 Y 现在是什么样子；</p><pre class="mv mw mx my gt nn no np nq aw nr bi"><span id="fbb4" class="nb lt in no b gy ns nt l nu nv">Y</span></pre><figure class="mv mw mx my gt jo"><div class="bz fp l di"><div class="mz na l"/></div></figure><h1 id="e2eb" class="ls lt in bd lu lv nz lx ly lz oa mb mc md ob mf mg mh oc mj mk ml od mn mo mp bi translated"><strong class="ak">将词条化的推文转换成向量。</strong></h1><figure class="mv mw mx my gt jo"><div class="bz fp l di"><div class="mz na l"/></div></figure><pre class="mv mw mx my gt nn no np nq aw nr bi"><span id="8e0e" class="nb lt in no b gy ns nt l nu nv">Found 22546 unique tokens.</span></pre><p id="f643" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">上面的代码块做了以下事情:</p><ul class=""><li id="a06f" class="or os in jx b jy jz kc kd kg ot kk ou ko ov ks ow ox oy oz bi translated">将词条化的推文矢量化。</li><li id="24ff" class="or os in jx b jy pa kc pb kg pc kk pd ko pe ks ow ox oy oz bi translated">将数据集限制在 500 个单词以内。</li><li id="1a82" class="or os in jx b jy pa kc pb kg pc kk pd ko pe ks ow ox oy oz bi translated">将每条推文的最大字数设置为 50。</li></ul><figure class="mv mw mx my gt jo"><div class="bz fp l di"><div class="mz na l"/></div></figure><pre class="mv mw mx my gt nn no np nq aw nr bi"><span id="6037" class="nb lt in no b gy ns nt l nu nv">Shape of data tensor: (17774, 50)</span></pre><h1 id="7550" class="ls lt in bd lu lv nz lx ly lz oa mb mc md ob mf mg mh oc mj mk ml od mn mo mp bi translated"><strong class="ak">列车试裂。</strong></h1><figure class="mv mw mx my gt jo"><div class="bz fp l di"><div class="mz na l"/></div></figure><pre class="mv mw mx my gt nn no np nq aw nr bi"><span id="c000" class="nb lt in no b gy ns nt l nu nv">(14219, 50) (14219, 3)<br/>(3555, 50) (3555, 3)</span></pre><h1 id="65bd" class="ls lt in bd lu lv nz lx ly lz oa mb mc md ob mf mg mh oc mj mk ml od mn mo mp bi translated"><strong class="ak">造型。</strong></h1><ul class=""><li id="af58" class="or os in jx b jy mq kc mr kg pf kk pg ko ph ks ow ox oy oz bi translated">第一层是嵌入层，使用 100 个长度向量来表示每个单词。</li><li id="23fb" class="or os in jx b jy pa kc pb kg pc kk pd ko pe ks ow ox oy oz bi translated">SpatialDropout1D 在 NLP 模型中执行变分丢失。</li><li id="b0a2" class="or os in jx b jy pa kc pb kg pc kk pd ko pe ks ow ox oy oz bi translated">下一层是具有 100 个存储单元的 LSTM 层。</li><li id="07a2" class="or os in jx b jy pa kc pb kg pc kk pd ko pe ks ow ox oy oz bi translated">输出层必须创建 3 个输出值，每个类一个。</li><li id="6296" class="or os in jx b jy pa kc pb kg pc kk pd ko pe ks ow ox oy oz bi translated">激活功能是 softmax 用于多类分类。</li><li id="8e2e" class="or os in jx b jy pa kc pb kg pc kk pd ko pe ks ow ox oy oz bi translated">因为这是一个多类分类问题，所以使用 categorical _ crossentropy 作为损失函数。</li></ul><figure class="mv mw mx my gt jo"><div class="bz fp l di"><div class="mz na l"/></div></figure><figure class="mv mw mx my gt jo"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="4ccb" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">经过 7 个时代的训练。</p><figure class="mv mw mx my gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi pi"><img src="../Images/b6d56473635367ebe5168f91a8dbd0d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1_5vsIoGYqG8v3wTDCSXcQ.png"/></div></div></figure><p id="bd22" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">训练准确率:76.64%</p><p id="a3e0" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">验证准确率:74.61 %</p><p id="6bab" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">让我们检查我们的测试数据；</p><figure class="mv mw mx my gt jo"><div class="bz fp l di"><div class="mz na l"/></div></figure><pre class="mv mw mx my gt nn no np nq aw nr bi"><span id="ba82" class="nb lt in no b gy ns nt l nu nv">112/112 [==============================] - 2s 14ms/step - loss: 0.6608 - accuracy: 0.7474 - f1_score: 0.7474<br/>Test set<br/>  Loss: 0.661<br/>  Accuracy: 0.747</span></pre><p id="5f4a" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">我们的测试数据大约有 75%的准确率..比我们使用机器学习算法的结果高出约 4 %。</p><p id="60ce" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">让我们想象一下我们的结果；</p><pre class="mv mw mx my gt nn no np nq aw nr bi"><span id="79a9" class="nb lt in no b gy ns nt l nu nv">plt<strong class="no io">.</strong>figure(figsize<strong class="no io">=</strong>(8, 8))<br/>plt<strong class="no io">.</strong>title('Loss')<br/>plt<strong class="no io">.</strong>plot(history_1<strong class="no io">.</strong>history['loss'], label<strong class="no io">=</strong>'train')<br/>plt<strong class="no io">.</strong>plot(history_1<strong class="no io">.</strong>history['val_loss'], label<strong class="no io">=</strong>'test')<br/>plt<strong class="no io">.</strong>legend()<br/>plt<strong class="no io">.</strong>show();</span></pre><figure class="mv mw mx my gt jo gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/7ce25d2bd4f27aead0d38b464366f499.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*LvKzRnAalNu-q8xuLbpXhg.png"/></div></figure><pre class="mv mw mx my gt nn no np nq aw nr bi"><span id="ffb7" class="nb lt in no b gy ns nt l nu nv">plt<strong class="no io">.</strong>figure(figsize<strong class="no io">=</strong>(8, 8))<br/>plt<strong class="no io">.</strong>title('Accuracy')<br/>plt<strong class="no io">.</strong>plot(history_1<strong class="no io">.</strong>history['accuracy'], label<strong class="no io">=</strong>'train')<br/>plt<strong class="no io">.</strong>plot(history_1<strong class="no io">.</strong>history['val_accuracy'], label<strong class="no io">=</strong>'test')<br/>plt<strong class="no io">.</strong>legend()<br/>plt<strong class="no io">.</strong>show();</span></pre><figure class="mv mw mx my gt jo gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/49b116db6533b54a32e31f0ae24c12b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*mEYEtVkk5oot0dRVRI8tiQ.png"/></div></figure><p id="bab1" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">从这个图中，我们可以说有一点过度拟合。获得更多的数据应该有助于改进我们的模型。</p><h1 id="76a3" class="ls lt in bd lu lv nz lx ly lz oa mb mc md ob mf mg mh oc mj mk ml od mn mo mp bi translated">用新推文测试。</h1><figure class="mv mw mx my gt jo"><div class="bz fp l di"><div class="mz na l"/></div></figure><pre class="mv mw mx my gt nn no np nq aw nr bi"><span id="d541" class="nb lt in no b gy ns nt l nu nv">[[0.1166406  0.02953981 0.8538196 ]] positive</span></pre><p id="1943" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">我们的新推文有一个积极的预测标签。</p><p id="05b2" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">本系列中使用的所有代码都可以在这个<a class="ae om" href="https://github.com/zhoroh/Sentiment_Analysis." rel="noopener ugc nofollow" target="_blank"> <strong class="jx io"> Github 资源库中找到。</strong> </a></p></div></div>    
</body>
</html>