<html>
<head>
<title>HuBERT Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">休伯特解释说</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/hubert-explained-6ec7c2bf71fc?source=collection_archive---------2-----------------------#2021-12-20">https://blog.devgenius.io/hubert-explained-6ec7c2bf71fc?source=collection_archive---------2-----------------------#2021-12-20</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><div class=""><h2 id="8b0c" class="pw-subtitle-paragraph jk im in bd b jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb dk translated">使用BERT从语音中学习语言</h2></div><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi kc"><img src="../Images/5a759bcb82921a7c4cb7a666c4073a02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PxWWE-VDLdMXyOEqUZrolQ.png"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">休伯特建筑——图片来自原始论文</figcaption></figure><p id="d6a0" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">自2019年wav2vec原始论文发布以来，自我监督学习现在越来越多地用于语音。休伯特在这方面迈出了下一步。</p><p id="ab1e" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">论文背后的主要思想是通过<strong class="ku io">隐藏单元</strong>对音频输入进行离散化，允许应用BERT模型。</p><p id="204f" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">在本帖中，我们将浏览该文件，解释该模型的组成部分，以及如何在您的项目中使用它。</p></div><div class="ab cl lo lp hr lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ig ih ii ij ik"><h1 id="383d" class="lv lw in bd lx ly lz ma mb mc md me mf jt mg ju mh jw mi jx mj jz mk ka ml mm bi translated">摘要</h1><ol class=""><li id="0176" class="mn mo in ku b kv mp ky mq lb mr lf ms lj mt ln mu mv mw mx bi translated">伯特简介</li><li id="43e5" class="mn mo in ku b kv my ky mz lb na lf nb lj nc ln mu mv mw mx bi translated">为什么我们不能在音频上使用NLP模型</li><li id="836a" class="mn mo in ku b kv my ky mz lb na lf nb lj nc ln mu mv mw mx bi translated">休伯特建筑与训练程序</li><li id="68ea" class="mn mo in ku b kv my ky mz lb na lf nb lj nc ln mu mv mw mx bi translated">如何在你的项目中使用休伯特</li><li id="9313" class="mn mo in ku b kv my ky mz lb na lf nb lj nc ln mu mv mw mx bi translated">结论</li></ol></div><div class="ab cl lo lp hr lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ig ih ii ij ik"><h1 id="d629" class="lv lw in bd lx ly lz ma mb mc md me mf jt mg ju mh jw mi jx mj jz mk ka ml mm bi translated">伯特简介</h1><p id="4623" class="pw-post-body-paragraph ks kt in ku b kv mp jo kx ky mq jr la lb nd ld le lf ne lh li lj nf ll lm ln ig bi translated">BERT是基于transformer架构的双向自监督NLP模型。</p><p id="5b13" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">让我们一步一步来</p><p id="bc92" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">transformer架构是一个基于自我关注机制的深度学习架构，但解释它超出了这篇文章的范围，要了解更多，你可以阅读这个<a class="ae ng" href="http://peterbloem.nl/blog/transformers" rel="noopener ugc nofollow" target="_blank">伟大的指南</a>。</p><p id="2c96" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">双向意味着模型一次接收整个单词序列，因此能够根据单词之前和之后的单词来学习单词的上下文。</p><p id="068a" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">自我监督意味着BERT可以利用未标记的数据进行学习，为此它使用了两种机制，<strong class="ku io">掩蔽语言建模</strong>和<strong class="ku io">下一句预测。</strong></p><p id="b382" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated"><strong class="ku io">屏蔽语言建模</strong>包括用[屏蔽]标记替换一定比例的输入标记，并训练模型预测屏蔽输入的原始值。</p><p id="efea" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated"><strong class="ku io">下一个句子预测</strong>包括输入模型2的句子，并训练它学习它们是否在原始文档中是后续的。</p><p id="7838" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">关于BERT更深入的解释可以在这篇伟大的TDS文章中看到，也可以阅读原文[1]</p></div><div class="ab cl lo lp hr lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ig ih ii ij ik"><h1 id="107f" class="lv lw in bd lx ly lz ma mb mc md me mf jt mg ju mh jw mi jx mj jz mk ka ml mm bi translated">为什么我们不能在音频上使用NLP模型</h1><p id="7294" class="pw-post-body-paragraph ks kt in ku b kv mp jo kx ky mq jr la lb nd ld le lf ne lh li lj nf ll lm ln ig bi translated">当试图对语音数据应用BERT或其他NLP模型时，存在3个主要问题:</p><ol class=""><li id="6d1d" class="mn mo in ku b kv kw ky kz lb nh lf ni lj nj ln mu mv mw mx bi translated">每个输入表达式中有多个声音单元</li><li id="fcb9" class="mn mo in ku b kv my ky mz lb na lf nb lj nc ln mu mv mw mx bi translated">没有离散声音单位的词典</li><li id="f984" class="mn mo in ku b kv my ky mz lb na lf nb lj nc ln mu mv mw mx bi translated">声音单元具有可变的长度，并且没有明确的分段</li></ol><p id="64c9" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">问题1阻止了诸如实例分类之类的技术的使用，这些技术在计算机视觉中用于预训练。</p><p id="13be" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">问题2阻碍了预测损失的使用，因为没有可靠的目标来比较预测。</p><p id="2883" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">最后，由于声音单元之间的未知边界，问题3使掩蔽预测预训练变得复杂。</p><p id="71fe" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">为了解决这些问题，作者提出了休伯特。</p></div><div class="ab cl lo lp hr lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ig ih ii ij ik"><h1 id="7e23" class="lv lw in bd lx ly lz ma mb mc md me mf jt mg ju mh jw mi jx mj jz mk ka ml mm bi translated">休伯特建筑与训练程序</h1><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi kc"><img src="../Images/5a759bcb82921a7c4cb7a666c4073a02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PxWWE-VDLdMXyOEqUZrolQ.png"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">休伯特建筑——图片来自原始论文</figcaption></figure><p id="985b" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">HuBERT模型架构遵循wav2vec 2.0架构，包括:</p><ul class=""><li id="3f95" class="mn mo in ku b kv kw ky kz lb nh lf ni lj nj ln nk mv mw mx bi translated">卷积编码器</li><li id="ed84" class="mn mo in ku b kv my ky mz lb na lf nb lj nc ln nk mv mw mx bi translated">伯特编码器</li><li id="5d5a" class="mn mo in ku b kv my ky mz lb na lf nb lj nc ln nk mv mw mx bi translated">投影层</li><li id="2125" class="mn mo in ku b kv my ky mz lb na lf nb lj nc ln nk mv mw mx bi translated">代码嵌入层</li></ul><p id="81f7" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">这些组件的数量在基础、大和x-大变量之间变化。</p><p id="daf8" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">在解释训练循环时，将更好地解释每个组件及其任务。</p><p id="cb5a" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">培训包括2个步骤:</p><h1 id="bb4d" class="lv lw in bd lx ly nl ma mb mc nm me mf jt nn ju mh jw no jx mj jz np ka ml mm bi translated">生成隐藏单元</h1><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi nq"><img src="../Images/018ea41f1e600d50fe59f43e8dd469e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5vvukxrVk5cYhCnc5fFOgQ.png"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">休伯特初始聚类步骤-按作者分类的图像</figcaption></figure><p id="c720" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">第一个训练步骤包括发现隐藏单元，该过程从从音频波形中提取<a class="ae ng" href="https://en.wikipedia.org/wiki/Mel-frequency_cepstrum" rel="noopener ugc nofollow" target="_blank"> MFCCs(Mel频率倒谱)</a>开始。</p><p id="cc0c" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">这些是用于表示语音的原始声学特征。</p><p id="1fba" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">然后，每个音频片段被传递给<a class="ae ng" href="https://pt.wikipedia.org/wiki/K-means" rel="noopener ugc nofollow" target="_blank"> K均值聚类算法</a>，并被分配给K个聚类中的一个。</p><p id="26ba" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">所有的音频帧将根据它们所属的簇被标记，这些是<strong class="ku io">隐藏单元。</strong></p><p id="b50d" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">之后，这些单元被转换成嵌入向量，用于训练的步骤B。</p><p id="e01c" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">在第一训练步骤之后，模型本身可以生成比MFCCs更好的表示，这是通过使用来自前一迭代的BERT编码器的中间层的输出来完成的:</p><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="gh gi nr"><img src="../Images/2411187c8582bb544b12bff6fa4555ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ywo3yGf9EOqKRR4rvoFggg.png"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">休伯特后续聚类步骤-按作者分类的图像</figcaption></figure><h1 id="212d" class="lv lw in bd lx ly nl ma mb mc nm me mf jt nn ju mh jw no jx mj jz np ka ml mm bi translated">掩蔽预测</h1><figure class="kd ke kf kg gt kh gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/c19e7fa32fada585b57c7fee38fb976e.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*5IcPQgczWKHYvz9xUe9jsw.png"/></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk translated">休伯特预测步骤-作者图片</figcaption></figure><p id="0b98" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">第二步类似于原始BERT模型的训练，使用屏蔽语言建模。</p><p id="c2cf" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">CNN负责从原始音频中生成特征，然后随机屏蔽并馈入BERT编码器。</p><p id="72f2" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">BERT编码器输出特征序列，填充屏蔽的记号。该输出然后被投影到较低的维度以匹配标签，并且计算这些输出和在步骤a中生成的每个隐藏单元嵌入之间的余弦相似性</p><p id="c70b" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">交叉熵损失然后被用于逻辑上以惩罚错误的预测。</p></div><div class="ab cl lo lp hr lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ig ih ii ij ik"><h1 id="a8a4" class="lv lw in bd lx ly lz ma mb mc md me mf jt mg ju mh jw mi jx mj jz mk ka ml mm bi translated">如何在你的项目中使用休伯特</h1><p id="e9c8" class="pw-post-body-paragraph ks kt in ku b kv mp jo kx ky mq jr la lb nd ld le lf ne lh li lj nf ll lm ln ig bi translated">Huggingface在变形金刚库中提供了HuBERT的所有官方版本(基本、大和X-large ):</p><figure class="kd ke kf kg gt kh"><div class="bz fp l di"><div class="nt nu l"/></div></figure></div><div class="ab cl lo lp hr lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ig ih ii ij ik"><h1 id="356a" class="lv lw in bd lx ly lz ma mb mc md me mf jt mg ju mh jw mi jx mj jz mk ka ml mm bi translated">结论</h1><p id="4a49" class="pw-post-body-paragraph ks kt in ku b kv mp jo kx ky mq jr la lb nd ld le lf ne lh li lj nf ll lm ln ig bi translated">现在您已经知道了HuBERT模型是如何工作的，您可以在您的语音识别项目中使用它了！如果您愿意，甚至可以为其他下游任务进行微调！</p><p id="d67e" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">要了解关于模型和结果的更多信息，请查看原始论文！</p><p id="3313" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated"><strong class="ku io">参考文献:</strong></p><p id="f6c6" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">[1] Hsu，Wei-Ning，et al .<a class="ae ng" href="https://arxiv.org/abs/2106.07447" rel="noopener ugc nofollow" target="_blank">“HuBERT:隐单元掩蔽预测的自监督语音表征学习”</a> (2021)。</p><p id="fe44" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">彼得·布鲁姆的《从零开始的变形金刚》</p><p id="fe12" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">[3] <a class="ae ng" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> Devlin，Jacob，等《Bert:用于语言理解的深度双向转换器的预训练》</a> (2018)。</p><p id="7fa6" class="pw-post-body-paragraph ks kt in ku b kv kw jo kx ky kz jr la lb lc ld le lf lg lh li lj lk ll lm ln ig bi translated">【4】<a class="ae ng" href="https://arxiv.org/abs/2006.11477" rel="noopener ugc nofollow" target="_blank">巴耶夫斯基，阿列克谢，等，“wav2vec 2.0:语音表征的自我监督学习框架”</a> (2020)。</p></div></div>    
</body>
</html>