<html>
<head>
<title>Types of Loss Functions in Deep Learning explained with Keras.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Keras 解释深度学习中损失函数的类型。</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/types-of-loss-functions-in-deep-learning-explained-with-keras-70b79acdbe5d?source=collection_archive---------5-----------------------#2022-10-09">https://blog.devgenius.io/types-of-loss-functions-in-deep-learning-explained-with-keras-70b79acdbe5d?source=collection_archive---------5-----------------------#2022-10-09</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><p id="1e9f" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">损失函数是让 ANN(人工神经网络)了解哪里出了问题以及如何达到黄金精度范围的东西，就像损失让你珍惜利润并识别出哪里出了问题一样。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/41d2cc001dc990f44a6f0194a98c3eee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*5Ve6QF3Hjijub1Su32HC7g.jpeg"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">因为利润会随之而来，对吗？..对吗？？</figcaption></figure><p id="6f6a" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">现在来看损失函数的<strong class="jm io">数学</strong>定义，它用来衡量预测值(^y)和实际标签(y)之间的不一致性。它是一个非负值，模型的稳健性随着损失函数值的减小而增加。</p><p id="e088" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">实际上很简单，它基本上是你的算法对数据集建模的好坏，如果它的预测完全错误，你的损失函数将输出一个更高的数字。如果它很好，它会输出一个较低的数字。</p><p id="6224" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">在这篇文章中，他们被分为<strong class="jm io">三种</strong>类型</p><ol class=""><li id="4c2e" class="ku kv in jm b jn jo jr js jv kw jz kx kd ky kh kz la lb lc bi translated"><strong class="jm io">分类亏损</strong></li><li id="f9c9" class="ku kv in jm b jn ld jr le jv lf jz lg kd lh kh kz la lb lc bi translated"><strong class="jm io">回归损失</strong></li><li id="dc8b" class="ku kv in jm b jn ld jr le jv lf jz lg kd lh kh kz la lb lc bi translated"><strong class="jm io">车型特定损失</strong></li></ol><p id="c166" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">让我们深入讨论一下，</p><h1 id="81a9" class="li lj in bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">分类损失</h1><p id="a677" class="pw-post-body-paragraph jk jl in jm b jn mg jp jq jr mh jt ju jv mi jx jy jz mj kb kc kd mk kf kg kh ig bi translated">在这方面，又有各种不同的类别，</p><h2 id="6121" class="ml lj in bd lk mm mn dn lo mo mp dp ls jv mq mr lw jz ms mt ma kd mu mv me mw bi translated">二元交叉熵损失/对数损失</h2><p id="4747" class="pw-post-body-paragraph jk jl in jm b jn mg jp jq jr mh jt ju jv mi jx jy jz mj kb kc kd mk kf kg kh ig bi translated">该损失函数主要用于二进制分类器的训练，它将每个预测概率与可能为 0 或 1 的实际类输出进行比较。</p><p id="50c7" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">然后，它根据与期望值的距离计算惩罚概率的分数。</p><p id="9379" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">其背后的<strong class="jm io">数学</strong>在下面讨论，</p><p id="7771" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">这意味着离实际值有多近或多远。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi mx"><img src="../Images/12ed1d809cb893983b819f5e17f585bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PjAjUStLjZWvGoByL4O5Aw.png"/></div></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">原木损失方程</figcaption></figure><p id="6ae1" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">这里 pi 是类 1 的概率，(1-pi)是类 0 的概率。</p><p id="6f37" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">当观察值属于类别 1 时，公式的第一部分变为有效，第二部分消失，反之亦然，此时观察值的实际类别为 0。</p><p id="7422" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">Keras 中的实现如下所示</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">批次大小为 1 和样本数量为 4 时的 BCE(Keras)</figcaption></figure><h2 id="48ab" class="ml lj in bd lk mm mn dn lo mo mp dp ls jv mq mr lw jz ms mt ma kd mu mv me mw bi translated"><strong class="ak">铰链损耗</strong></h2><p id="f753" class="pw-post-body-paragraph jk jl in jm b jn mg jp jq jr mh jt ju jv mi jx jy jz mj kb kc kd mk kf kg kh ig bi translated">它用于分类问题，是交叉熵的替代方案，主要是为支持向量机(SVM)开发的，铰链损失和交叉熵损失之间的区别在于，前者来自于<strong class="jm io">试图最大化我们的决策边界和数据点之间的差距。</strong></p><p id="3c87" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">数学上</strong>等式如下所示，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/109739a077d7424774fe325f629b1a26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*c-nomqMJ9LKJsXv_b-TeVQ.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">铰链损耗方程</figcaption></figure><p id="efba" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">在<strong class="jm io"> Keras </strong>中的实现如下:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">Keras 中的铰链损耗</figcaption></figure><h2 id="8c26" class="ml lj in bd lk mm mn dn lo mo mp dp ls jv mq mr lw jz ms mt ma kd mu mv me mw bi translated">范畴交叉熵</h2><p id="ed0b" class="pw-post-body-paragraph jk jl in jm b jn mg jp jq jr mh jt ju jv mi jx jy jz mj kb kc kd mk kf kg kh ig bi translated">这是一个 SoftMax <strong class="jm io">激活</strong>加上<strong class="jm io">交叉熵损失</strong>。如果我们使用这个损失，我们将使用 CNN 训练一个图像分类器来预测每一个存在的类别，例如一个鸟图像分类器属于哪个鸟类别。</p><p id="920d" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">用于<strong class="jm io">多级分类</strong>。</p><p id="4a98" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">由于本文没有讨论激活函数，如果你不了解它，我有一篇文章介绍了深度学习中使用 Keras 的激活函数，链接如下…</p><div class="nf ng gp gr nh ni"><a href="https://medium.com/@tripathiadityaprakash/types-of-activation-functions-in-deep-learning-explained-with-keras-cd1e0b85e003" rel="noopener follow" target="_blank"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd io gy z fp nn fr fs no fu fw im bi translated">用 Keras 解释深度学习中激活函数的类型</h2><div class="np l"><h3 class="bd b gy z fp nn fr fs no fu fw dk translated">激活是指点击激活你的车(如果它有，当然)，同样的概念，但在…</h3></div><div class="nq l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">medium.com</p></div></div><div class="nr l"><div class="ns l nt nu nv nr nw ko ni"/></div></div></a></div><p id="47e2" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">数学上，</strong>公式如下</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi nx"><img src="../Images/e875f1bcba7fb81dd429a87baa53735a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wM2hLqEmylkmh7rConpfpA.png"/></div></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">分类交叉熵损失方程</figcaption></figure><p id="e19b" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">在<strong class="jm io"> Keras </strong>的实现如下所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">Keras 中的范畴交叉熵损失</figcaption></figure><h2 id="6b2e" class="ml lj in bd lk mm mn dn lo mo mp dp ls jv mq mr lw jz ms mt ma kd mu mv me mw bi translated"><strong class="ak">稀疏分类交叉熵损失</strong></h2><p id="ed80" class="pw-post-body-paragraph jk jl in jm b jn mg jp jq jr mh jt ju jv mi jx jy jz mj kb kc kd mk kf kg kh ig bi translated">稀疏 CCEL(分类交叉熵损失)和 CCEL 都计算分类交叉熵，唯一的区别是目标/标签应该如何编码。</p><p id="45ae" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">当使用稀疏分类交叉熵时，目标由类别的索引表示(从 0 开始)。</p><p id="4b3d" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">您的输出具有 4x2 的形状，这意味着您有两个类别。因此，目标应该是一个条目为 0 或 1 的四维向量。这与分类交叉熵相反，在分类交叉熵中，标签应该是一位热编码的。</p><p id="d232" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">数学上</strong>，等式是</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/d32c4058f843167ddd883aca641bf826.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*oi82EWYo32sIfhKTcmY4eQ.jpeg"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">稀疏交叉熵损失方程</figcaption></figure><p id="fe56" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">何时使用哪个，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi nz"><img src="../Images/ac4829a8b52ddeb675c5c0ea0c698d1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZQxj8dQgqP2FWjM8rWN-Aw.png"/></div></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">什么时候使用分类交叉熵，什么时候使用稀疏</figcaption></figure><p id="5619" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">在<strong class="jm io"> Keras </strong>中的实现如下所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">稀疏分类交叉熵的 Keras 实现</figcaption></figure><h2 id="f812" class="ml lj in bd lk mm mn dn lo mo mp dp ls jv mq mr lw jz ms mt ma kd mu mv me mw bi translated">库尔贝克-莱布勒发散损失</h2><p id="da51" class="pw-post-body-paragraph jk jl in jm b jn mg jp jq jr mh jt ju jv mi jx jy jz mj kb kc kd mk kf kg kh ig bi translated">简单地说，它是一个概率分布与另一个概率分布的不同程度的度量。</p><p id="c65c" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">数学上的</strong>，可以用以下公式解释</p><p id="47cf" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">两个分布 P 和 q 之间的 KL 散度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/d1c0efb551ce04403fef48942901ca47.png" data-original-src="https://miro.medium.com/v2/resize:fit:270/format:webp/1*elyWV_nZAiGxeqw0d0TUgw.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">库尔贝克-莱布勒方程</figcaption></figure><p id="e69a" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">“||”运算符表示从 q 开始的“<em class="ob">发散</em>”或 Ps 发散。</p><p id="b24d" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">KL 散度可以计算为 P 中每个事件的概率的负和乘以 Q 中事件的概率对 P 中事件的概率的对数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/a84fb82a6808bbfa9f17bc21ca916628.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*kMUaC13_D_OilZrLjHvZoQ.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">给定事件的分流</figcaption></figure><p id="e02f" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">在<strong class="jm io"> Keras </strong>中实现如下:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><h1 id="4d69" class="li lj in bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">回归损失</h1><p id="d056" class="pw-post-body-paragraph jk jl in jm b jn mg jp jq jr mh jt ju jv mi jx jy jz mj kb kc kd mk kf kg kh ig bi translated">在这方面，又有各种不同的类别，</p><h2 id="1b89" class="ml lj in bd lk mm mn dn lo mo mp dp ls jv mq mr lw jz ms mt ma kd mu mv me mw bi translated">均方误差/二次损耗/ L2 损耗</h2><p id="145f" class="pw-post-body-paragraph jk jl in jm b jn mg jp jq jr mh jt ju jv mi jx jy jz mj kb kc kd mk kf kg kh ig bi translated">它是观察值和预测值之间的平均平方差。例如，在线性回归中，我们找到最好地描述所提供的数据点的直线。许多线可以描述数据点，但是使用 MSE 可以找到选择哪条线最好地描述它。</p><p id="3904" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">在下图中，预测值在直线上，实际值用小圆圈表示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi od"><img src="../Images/414cf10a7167b0e1d78cf216e408964f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-AgB42TN8qCJtcl9HotSOQ.png"/></div></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">显示线性回归的图表</figcaption></figure><p id="b676" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">误差是数据点和拟合线之间的距离。</p><p id="9a84" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">数学上</strong>，可以表述为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/468d6086752643a81a5c6a795ed75da8.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*xlDDnsnRISGQ6O3X9H2u4w.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">均方差方程</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/8d868a3ce481bf593df68ccdb138c492.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*NJvEhyi71B8MooJfgPQDag.png"/></div></figure><p id="3507" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">在<strong class="jm io"> Keras </strong>中的实现如下所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">Keras 中的均方误差</figcaption></figure><h2 id="b797" class="ml lj in bd lk mm mn dn lo mo mp dp ls jv mq mr lw jz ms mt ma kd mu mv me mw bi translated">平均绝对误差/ L1 损耗</h2><p id="69e1" class="pw-post-body-paragraph jk jl in jm b jn mg jp jq jr mh jt ju jv mi jx jy jz mj kb kc kd mk kf kg kh ig bi translated">现在我们继续讨论绝对值别闹了，现在集合！</p><p id="6f72" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">它计算标记数据和预测数据之间的误差的均方值。它计算当前输出和预期输出之间的绝对差值除以输出数量。</p><p id="a256" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">与均方误差不同，它对异常值不敏感。</p><p id="595a" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">数学上的</strong>，可以写成</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/8f4c57735e8107824bb76ce49e02bf48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*CyxisErQIN3igHXeA3liWg.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">均方差方程</figcaption></figure><p id="2e55" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">如公式所示，增加了模块，仅给出了误差的绝对值。</p><p id="79e8" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">在<strong class="jm io"> Keras </strong>的实现如下所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">Keras 中的平均绝对误差</figcaption></figure><h2 id="6a7c" class="ml lj in bd lk mm mn dn lo mo mp dp ls jv mq mr lw jz ms mt ma kd mu mv me mw bi translated">Huber 损失/平滑平均值</h2><p id="4645" class="pw-post-body-paragraph jk jl in jm b jn mg jp jq jr mh jt ju jv mi jx jy jz mj kb kc kd mk kf kg kh ig bi translated">两全其美，iOS 和 Android 两款手机一起？不了..</p><p id="ea50" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">它的 Huber 损失函数，它结合了 MSE 和 MAE，如果损失值大于δ使用 MAE，如果小于δ使用 MSE。</p><p id="55e2" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">数学上的</strong>，可以写成</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/ae2b32a66feeac088062959b4f1c5568.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*Vx7otH8Vzkkw_Gxzt9xpgg.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">胡伯损失方程</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/5be911c3f968d6d031819676524fc335.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*rOL8u0yJj3iAHRA70zHvIQ.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">Hubber(绿色)、MAE(红色)和 MSE(蓝色)的比较</figcaption></figure><p id="228d" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">Keras 的实施情况如下</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">胡伯损失的 Keras 实现</figcaption></figure><p id="1d51" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">现在来看<strong class="jm io">模型特定损耗</strong>它们与下面使用它们的模型一起被提及，</p><h2 id="8394" class="ml lj in bd lk mm mn dn lo mo mp dp ls jv mq mr lw jz ms mt ma kd mu mv me mw bi translated">最小最大值(GAN 损耗)</h2><p id="d925" class="pw-post-body-paragraph jk jl in jm b jn mg jp jq jr mh jt ju jv mi jx jy jz mj kb kc kd mk kf kg kh ig bi translated">生成敌对网络利用了这种特殊的损失，这很简单(他们是这么说的)。</p><p id="e437" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">在这次损失之前，需要对 GAN 有一点了解，所以让我们在下面的总结中描述一下</p><ul class=""><li id="b62f" class="ku kv in jm b jn jo jr js jv kw jz kx kd ky kh oj la lb lc bi translated">涉及到两个神经网络。</li><li id="1851" class="ku kv in jm b jn ld jr le jv lf jz lg kd lh kh oj la lb lc bi translated">其中一个网络，即生成器，从随机数据分布开始，并试图复制特定类型的分布。</li><li id="24f9" class="ku kv in jm b jn ld jr le jv lf jz lg kd lh kh oj la lb lc bi translated">另一个网络，鉴别器，通过随后的训练，可以更好地将伪造的分布从真实的分布中区分出来。</li><li id="c672" class="ku kv in jm b jn ld jr le jv lf jz lg kd lh kh oj la lb lc bi translated">这两个网络都在玩一个最小-最大游戏，其中一个试图智胜另一个。</li></ul><p id="1e0b" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">现在回到最小-最大损失，<strong class="jm io">数学上</strong>它的等式是</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/e25d8541b4df293ea8cb73526b9643a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*uSj6CiR3wfQH1vTlmSdmdw.jpeg"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">最小—最大损失方程</figcaption></figure><p id="f644" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">发生器试图最小化该函数，而鉴别器试图最大化该函数，它对发生器饱和，使其停止训练它落后于鉴别器… <em class="ob"> sad 发生器噪声</em> …。</p><p id="b187" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">这可以进一步分为两部分 1) <strong class="jm io">鉴别器</strong>损耗和 2) <strong class="jm io">发电机</strong>损耗</p><p id="aa07" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">鉴别器损耗</strong></p><p id="2910" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">鉴别器训练 by 对来自发生器的真实数据和虚假数据进行分类。</p><p id="29ba" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">通过最大化下面提到的函数，它惩罚自己将一个真实的实例误分类为假，或者将一个假实例(由生成器创建)误分类为真实。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi ol"><img src="../Images/45072b439f51a5b757572c9bd34ec445.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7a13QnNknnf3TOBSvChsVw.jpeg"/></div></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">鉴频器损耗公式</figcaption></figure><ul class=""><li id="b586" class="ku kv in jm b jn jo jr js jv kw jz kx kd ky kh oj la lb lc bi translated"><strong class="jm io"> log(D(x)) </strong>指生成器正确分类真实图像的概率，</li><li id="06df" class="ku kv in jm b jn ld jr le jv lf jz lg kd lh kh oj la lb lc bi translated">最大化<strong class="jm io"> log(1-D(G(z))) </strong>将帮助它正确标记来自生成器的假图像。</li></ul><p id="ca43" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">发电机损耗</strong></p><p id="a8d9" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">当生成器被训练时，它对随机噪声进行采样，并从该噪声中产生输出。然后，输出通过鉴别器，并根据鉴别器的辨别能力被分类为“真”或“假”。</p><p id="1099" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">然后，根据鉴别器的分类计算发电机损耗——如果成功骗过鉴别器，将获得奖励，否则将受到惩罚。</p><p id="7f95" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">下面的等式被最小化以训练发生器，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/b1d0f99035ab5ab92ed639cac606bef5.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*Vuyh-s05JJZpqtXzQeNTww.jpeg"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">最小化以训练发电机</figcaption></figure><p id="507a" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">深层 GAN 网络的整个实现以及下面提到的损耗，因为损耗是相对于模型定义的，所以需要参考整个实现。</p><div class="nf ng gp gr nh ni"><a href="https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/dcgan.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd io gy z fp nn fr fs no fu fw im bi translated">主 tensorflow/docs 上的 docs/dcgan.ipynb</h2><div class="np l"><h3 class="bd b gy z fp nn fr fs no fu fw dk translated">张量流文档。通过在 GitHub 上创建帐户，为 tensorflow/docs 的开发做出贡献。</h3></div><div class="nq l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">github.com</p></div></div><div class="nr l"><div class="on l nt nu nv nr nw ko ni"/></div></div></a></div><h2 id="8870" class="ml lj in bd lk mm mn dn lo mo mp dp ls jv mq mr lw jz ms mt ma kd mu mv me mw bi translated">焦点丢失(物体检测)</h2><p id="9ad0" class="pw-post-body-paragraph jk jl in jm b jn mg jp jq jr mh jt ju jv mi jx jy jz mj kb kc kd mk kf kg kh ig bi translated">这是交叉熵损失函数的扩展，它降低了简单例子的权重，并集中训练硬否定。</p><p id="f976" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">焦点损失将调制项应用于交叉熵损失，以便将学习集中在硬错误分类的例子上。这是一种动态缩放的交叉熵损失，其中缩放因子随着正确类别的置信度增加而衰减到零。</p><p id="6b0e" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">数学马上就来，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/0bb48c98e444088cc21f6a05af7be16d.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*aTaBZkEF8-Ddyesh19UsVA.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">焦点损失方程</figcaption></figure><p id="213b" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">焦点损失给标准交叉熵标准增加了一个因子(1−pt)^γ。设置γ&gt;0 会减少分类良好的示例(pt&gt;.5)的相对损失，从而将更多注意力放在困难的、错误分类的示例上。这里有可调的<em class="ob">聚焦</em>参数γ≥0。</p><p id="b177" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">在<strong class="jm io"> Keras </strong>的实现如下所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">焦点损失函数的实现</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi op"><img src="../Images/34386a19cb8be9c526600cef8227dd2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pt5mcp4yJ2dnNQP_jQOamQ.png"/></div></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">上述损失函数的输出</figcaption></figure><p id="1c90" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">每项损失实施的整个笔记本如下所述，</p><div class="nf ng gp gr nh ni"><a href="https://colab.research.google.com/drive/1XIZBm8eQ-tTVV0D6xqa2QI6Jnvo_ATXP?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd io gy z fp nn fr fs no fu fw im bi translated">谷歌联合实验室</h2><div class="np l"><h3 class="bd b gy z fp nn fr fs no fu fw dk translated">编辑描述</h3></div><div class="nq l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">colab.research.google.com</p></div></div><div class="nr l"><div class="oq l nt nu nv nr nw ko ni"/></div></div></a></div><blockquote class="or os ot"><p id="0a37" class="jk jl ob jm b jn jo jp jq jr js jt ju ou jw jx jy ov ka kb kc ow ke kf kg kh ig bi translated"><strong class="jm io">参考文献</strong></p><p id="7e79" class="jk jl ob jm b jn jo jp jq jr js jt ju ou jw jx jy ov ka kb kc ow ke kf kg kh ig bi translated"><a class="ae ox" href="https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3" rel="noopener" target="_blank">https://towards data science . com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23 e0e F3 e 14d 3</a></p><p id="20f1" class="jk jl ob jm b jn jo jp jq jr js jt ju ou jw jx jy ov ka kb kc ow ke kf kg kh ig bi translated"><a class="ae ox" href="https://medium.com/nerd-for-tech/what-loss-function-to-use-for-machine-learning-project-b5c5bd4a151e" rel="noopener">https://medium . com/nerd-for-tech/what-loss-function-to-use-for-machine-learning-project-b5 C5 BD 4a 151 e</a></p><p id="29ab" class="jk jl ob jm b jn jo jp jq jr js jt ju ou jw jx jy ov ka kb kc ow ke kf kg kh ig bi translated"><a class="ae ox" href="https://neptune.ai/blog/gan-loss-functions" rel="noopener ugc nofollow" target="_blank">https://neptune.ai/blog/gan-loss-functions</a></p><p id="5297" class="jk jl ob jm b jn jo jp jq jr js jt ju ou jw jx jy ov ka kb kc ow ke kf kg kh ig bi translated"><a class="ae ox" href="https://www.analyticsvidhya.com/blog/2020/08/a-beginners-guide-to-focal-loss-in-object-detection/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2020/08/a-初学者指南-焦点丢失物体检测/ </a></p></blockquote><p id="cb3e" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">如果你喜欢这篇文章，并想阅读更多，你可以访问我的媒体简介，如果你想连接这里是我的 LinkedIn 简介</p><p id="9271" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><a class="ae ox" href="https://www.linkedin.com/in/tripathiadityaprakash/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/tripathiadityaprakash/</a></p></div></div>    
</body>
</html>