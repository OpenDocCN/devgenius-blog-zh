# 深度学习入门:2

> 原文：<https://blog.devgenius.io/getting-started-with-deep-learning-2-4dfee5a631ab?source=collection_archive---------10----------------------->

在本文中，我们将学习激活功能。

## 激活/转移功能

这些功能决定了神经元是否应该被激活。它将来自节点的加权输入之和转换为输出值，作为输出提供给下一层。

![](img/a4b50ff381ebc61e5f90e2ea9f7cc387.png)

## 激活功能的需求

*   向神经网络添加非线性；因为一次又一次地增加输入只会意味着输入之间的线性关系，而不会有学习。
*   激活函数的选择对神经网络的能力和性能有很大的影响，并且不同的激活函数可以用于模型的不同部分。

# 激活功能的类型

1.  二元阶跃函数

![](img/1b8941b33f9c8c5b3ce4c611475a033b.png)

二元阶跃函数依赖于决定神经元是否应该被激活的阈值。
馈入激活函数的输入与某个阈值进行比较；如果输入大于它，那么神经元被激活，否则它被停用，这意味着它的输出不会传递到下一个隐藏层。阶跃函数的梯度为零，这在反向传播过程中造成阻碍。

2.二元 Sigmoid 函数

![](img/81842ea7fc5809d99e74eb831bd232d9.png)

二元 Sigmoid 图

也称为物流/挤压功能。该函数的范围从 0 到 1，呈 S 形。它用于 DNN 的输出图层，并用于基于概率的输出。因为任何事情的概率只存在于 0 和 1 之间，所以 sigmoid 是正确的选择，因为它的范围。

![](img/2606bbd51fb96df0ace3fdc3a08fecf8.png)

二元 Sigmoid 函数

缺点:反向传播期间的急剧阻尼梯度、梯度饱和、缓慢收敛以及非零中心输出，从而导致梯度更新在不同方向上传播。

![](img/6a0d981056cfd383ebbf96150711d9fc.png)

*函数的导数为***f '(x)= sigmoid(x)*(1-sigmoid(x))**

从上图中我们可以看出，梯度值仅在**范围-3 到 3** 内有意义，在其他区域图形变得更加平坦。这意味着对于大于 3 或小于-3 的值，函数将具有非常小的梯度。当梯度值接近零时，网络停止学习并遭受 ***消失梯度*** 问题。

3.双极 Sigmoid 函数

![](img/dec34b70f172192be24699b7016419f0.png)

sigmoid 函数的值的范围可以根据应用而变化。但是，最常用的范围是(-1，+1)。

4.双曲正切函数

![](img/b050208a75f596513377a62b0cf1aceb.png)

它具有相同的 S 形(S 形/逻辑激活函数),输出范围的差异为-1 比 1。在 tanh 中，输入越大(越正)，输出值越接近 1.0，而输入越小(越负)，输出越接近-1.0。它本质上是两极的。这是一种被称为**反向传播网络的特殊类型的神经网络的广泛采用的激活函数。**

![](img/3951cf4bb5da5376b34fb9544387b991.png)

tanh 函数

由于此函数以零为中心，因此更容易对具有强负值、中性值和强正值的输入进行建模。

![](img/6e14ebedbfb631ec0b10822d5ae0e958.png)

双曲正切函数的导数

5.ReLU 函数

![](img/0b7ee69b6c8b89b562b3ee2f5817f236.png)

Relu 函数

深度学习中使用最广泛的激活函数。它在性能和通用性方面更胜一筹，同时提高了计算和通信的整体速度，因为它不计算指数和除法。它很容易优化，因此在实践中它总是优于 Sigmoid 函数。

缺点:它还面临着类似于 sigmoid 激活函数的消失梯度问题。

![](img/136691b88a63fbc6c485a58afe1cec6a.png)

**濒死热卢问题:**图形的负边使渐变值为零。由于这个原因，在反向传播过程中，一些神经元的权重和偏差没有被更新。这会产生永远不会被激活的死亡神经元。所有负输入值立即变为零，这降低了模型根据数据进行适当拟合或训练的能力。

6.泄漏 ReLU 函数

该函数是 ReLUto solve 的改进版本，用于解决**死亡 ReLU 问题**，因为它在负区域也有小的正斜率。它支持负值的反向传播，并且与 ReLU 一样具有优势。

![](img/375e36bf0830b412880cd1d7738d3e0d.png)

泄漏 ReLU 函数

通过对负输入值进行这种微小的修改，图形左侧的梯度变成了一个非零值。因此，我们在那个区域不会再遇到死亡的神经元。

![](img/35460d4401d46def055ec2c3e04756fd.png)

泄漏 ReLU 的推导

7.参数 ReLU 函数

另一个旨在解决**将死的 ReLU 问题的函数。**该函数提供函数负部分的斜率作为参数 a。*通过执行反向传播，学习到最合适的 a 值。*

![](img/86107127af39345bb84ebf81da1e723c.png)

参数 ReLU 函数: **f(x) = max(ax，x)**

当泄漏 ReLU 函数在解决死神经元问题上仍然失败，并且相关信息没有成功传递到下一层时，使用参数化 ReLU 函数。

![](img/6dd2c2dd3818df639a8510f549edecfd.png)

参数 ReLU 的导数

该功能的局限性在于，根据斜率参数 ***a.*** 的值，对于不同的问题会有不同的表现

8.指数线性单位

![](img/d12556661e79756a8cb0a8c7757fad8c.png)

ELU 函数

它通过在训练期间将平均激活推向零来减少偏差，ELU 代表了 ReLU 的一个很好的替代方案。ELU 的一个局限性是 ELU 不会将值集中在零上。

![](img/ea9e101be11418be06af1f8e10a2bdc5.png)

ELU 函数的导数

由于以下优势，ELU 是 ReLU 的强有力替代产品:

*   ELU 慢慢变得平滑，直到其输出等于-α，而 RELU 急剧平滑。
*   通过引入输入负值的对数曲线，避免了死 ReLU 问题。它有助于网络向正确的方向推动权重和偏好。

9.Softmax 函数

它是一个函数，将 K 个实数值的向量转换为 K 个实数值的向量，其和为 1。输入值可以是正数、负数、零或大于 1 的值，但是 softmax 会将它们转换为介于 0 和 1 之间的值，以便可以将它们解释为概率。

如果其中一个输入很小或为负，softmax 会将其转换为小概率，如果输入很大，则会将其转换为大概率，但它将始终保持在 0 和 1 之间。在多类分类的情况下，它最常用作神经网络最后一层的激活函数。

它计算相对概率。类似于 sigmoid/logistic 激活函数，SoftMax 函数返回每个类的概率。

![](img/4006ef569b9ebfca4afd2fd11d2c7dc4.png)

此处指数->指数

softmax 输出小于 1，softmax 功能的输出总和为 1。因此，可以说 softmax 函数的输出是一个**概率分布**。由于这一特性，Softmax 函数被认为是神经网络和算法(如多项式逻辑回归)中的激活函数。注意，对于二元逻辑回归，使用的激活函数是 sigmoid 函数。

10.Swish 函数

![](img/e474e427be118e6f9ed3f92e12f27ac2.png)

Swish 函数

它是由*s 形激活函数*和输入函数组合提出的第一个复合*激活函数*之一，以实现一个*混合激活函数*。Swish 函数的性质包括*光滑，非单调，下面有界*，上限无界。

![](img/3eec3a46f6fd28d497f3afb1b925b097.png)

Swish 函数的导数

11.高斯误差线性单位

![](img/80a76f8df26536de04eed14716dc1ae5.png)

GELU 函数

高斯误差线性单元(GELU)激活函数与 BERT、ROBERTa、ALBERT 和其他顶级 NLP 模型兼容。这个激活函数是由 dropout、zoneout 和 ReLUs 的属性组合而成的。

![](img/d23ba005e307962a3f9f072f4e976f53.png)

格律派生词

12.标度指数线性单位

![](img/db84ed118eba83cd0dff9f0960aca056.png)

SELU 函数

SELU 是在自归一化网络中定义的，并负责内部归一化，这意味着每一层都保留了前一层的均值和方差。SELU 通过调整均值和方差来实现这种标准化。

SELU 有正值和负值来移动平均值，这对于 ReLU 激活功能是不可能的，因为它不能输出负值。

梯度可以用来调整方差。激活函数需要梯度大于 1 的区域来增加它。SELU 具有预定义的α值和λ值。

以下是 SELU 相对于 ReLU 的主要优势:

*   内部归一化比外部归一化更快，这意味着网络收敛更快。
*   SELU 是一个相对较新的激活函数，需要更多关于架构的论文，如 CNN 和 RNNs，在那里它被比较深入地研究。

![](img/ca794deeed60a67f0e94373d3a3cb189.png)

SELU 的衍生物