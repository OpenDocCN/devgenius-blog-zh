# ç®€å•çš„äººå·¥æ™ºèƒ½äººè„¸å’Œæƒ…æ„Ÿè¯†åˆ«ä¸ååº”

> åŸæ–‡ï¼š<https://blog.devgenius.io/simple-ai-face-and-emotion-recognition-with-react-da2921e6075e?source=collection_archive---------0----------------------->

![](img/853bc0edba13bdbf00d53f6e79cd2cb3.png)

ä½ å–œæ¬¢è‰¾å—ï¼Ÿå› ä¸ºæˆ‘ç¡®å®çŸ¥é“ã€‚ä»Šå¤©ï¼Œä½ å°†ä½¿ç”¨äººå·¥æ™ºèƒ½å’Œé¢éƒ¨è¯†åˆ«æ¥é€šè¿‡ç½‘ç»œæ‘„åƒå¤´å®æ—¶ç¡®å®šä½ çš„æƒ…ç»ªã€‚

ä¸ºäº†å®Œæˆè¿™ä¸ªäººè„¸è¯†åˆ«ï¼Œæ‚¨å°†ä½¿ç”¨ä¸€ä¸ªåä¸º [face-api.js](https://www.npmjs.com/package/face-api.js) çš„åº“ï¼Œå®ƒæ˜¯ tensor-flow çš„åŒ…è£…å™¨ï¼Œæ˜¯ç›®å‰æœ€æµè¡Œçš„æœºå™¨å­¦ä¹ åº“ä¹‹ä¸€(æˆªè‡³æœ¬æ–‡å‘è¡¨æ—¶)ã€‚åœ¨æµè§ˆå™¨ä¸­è®¾ç½®å’Œè¿è¡Œå®ƒçœŸçš„å¾ˆå®¹æ˜“ã€‚

è¿™æ˜¯æœ¬æ–‡ GitHub åº“çš„[é“¾æ¥ã€‚](https://github.com/Exclusiveideas/ai-face-detection)

å½“ä½ åœ¨æˆ‘çš„ä»“åº“æ—¶ï¼Œè¯·ç•™ä¸‹ä¸€é¢—æ˜Ÿæ˜Ÿâœ¨ã€‚

# æˆ‘ä»¬å¼€å§‹å§

Bootstrap ä¸€ä¸ªæ–°çš„ react åº”ç”¨ç¨‹åºï¼Œæ¯”å¦‚äººå·¥æ™ºèƒ½-äººè„¸æ£€æµ‹

`npx create-react-app ai-face-detection`

è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œåœ¨ react åº”ç”¨ç¨‹åºä¸­ä¸‹è½½ [face-api.js](https://www.npmjs.com/package/face-api.js) åº“

```
//using npm
npm i face-api.js//using yarn
yarn add face-api.js
```

è¿™ä¸ªé¡¹ç›®éœ€è¦çš„æ‰€æœ‰æ¨¡å‹éƒ½å¯ä»¥åœ¨æˆ‘çš„[èµ„æºåº“](https://github.com/Exclusiveideas/ai-face-detection/tree/main/public/models)ä¸­æ‰¾åˆ°ã€‚

å°†æ‚¨ä»æˆ‘çš„[å­˜å‚¨åº“](https://github.com/Exclusiveideas/ai-face-detection/tree/main/public/models)è·å¾—çš„æ¨¡å‹æ–‡ä»¶å¤¹æ”¾åœ¨ react åº”ç”¨ç¨‹åºçš„ public æ–‡ä»¶å¤¹ä¸‹ã€‚ä½ çš„ **app ç»“æ„**åº”è¯¥æ˜¯è¿™æ ·çš„:

```
AI-FACE-DETECTION
|- public
   |- models
|- src
   |- App.css
   |- App.js
   |- index.css
   |- index.js
```

è¿è¡Œ`npm start`å¯åŠ¨æ‚¨çš„åº”ç”¨ç¨‹åºã€‚

è®©æˆ‘ä»¬ç»™æˆ‘ä»¬çš„ **index.css** æ·»åŠ ä¸€äº›æ ·å¼:

```
body {
  margin: 0;
  padding: 0;
  width: 100vw;
  height: 100vh;
  display: flex;
  justify-content: center;
  align-items: center;
}
```

æˆ‘ä»¬çš„ App.js æ–‡ä»¶å°†æ˜¯æœ¬æ–‡çš„é‡ç‚¹ã€‚ç°åœ¨åœ¨ App é‡Œ**ã€‚js** ï¼Œæˆ‘ä»¬è¦æ·»åŠ ä¸€ä¸ªæ ‡é¢˜å’Œè§†é¢‘å…ƒç´ ã€‚

**App.js**

```
import { useEffect, useRef } from 'react';const App = () => {
  const videoRef = useRef();return (
    <div>
       <video crossOrigin='anonymous' ref={videoRef} autoPlay>  
       </video>
    </div>
  )
};export default App;
```

æˆ‘æ·»åŠ äº†ä¸€ä¸ª *useRef* é’©å­æ¥è·Ÿè¸ªè§†é¢‘æµã€‚æˆ‘ä»¬éœ€è¦è®¿é—®ç½‘ç»œæ‘„åƒå¤´å¹¶è¾“å‡ºè§†é¢‘ï¼Œä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªå‡½æ•° *startVideo* ï¼Œå®ƒå°†åŒ…å«è®¿é—®ç½‘ç»œæ‘„åƒå¤´å¹¶å°†æµå­˜å‚¨åœ¨ *videoRefã€*ä¸­çš„è¯­æ³•

**App.js**

```
useEffect(() => {
  startVideo();
}, []);const startVideo = () => {
   navigator.mediaDevices.getUserMedia({ video: true    })
   .then((currentStream) => {
           videoRef.current.srcObject = currentStream;
        }).catch((err) => {
           console.error(err)
   });
}
```

*useeffecthhook*åœ¨åº”ç”¨å®‰è£…åè¿è¡Œï¼Œå¹¶é€šè¿‡è°ƒç”¨ *startVideo* å‡½æ•°åˆå§‹åŒ–è§†é¢‘æµã€‚ç°åœ¨ï¼Œå¦‚æœä½ ä¿å­˜ï¼Œä½ åº”è¯¥å¯ä»¥åœ¨æµè§ˆå™¨ä¸­çœ‹åˆ°ä½ çš„æ‘„åƒå¤´è¾“å‡ºã€‚

ç°åœ¨è®©æˆ‘ä»¬å¼€å§‹ä½¿ç”¨æˆ‘ä»¬çš„ face-api.js åº“ã€‚

å°† face-api.js åº“å¯¼å…¥åˆ° index.js ä¸­ï¼Œå¹¶ä» models æ–‡ä»¶å¤¹ä¸­åŠ è½½æ¨¡å‹

**App.js**

```
useEffect(() => {
   startVideo(); videoRef && loadModels();
}, []);const loadModels = () => {
   Promise.all([
     faceapi.nets.tinyFaceDetector.loadFromUri('/models'),
     faceapi.nets.faceLandmark68Net.loadFromUri('/models'),
     faceapi.nets.faceRecognitionNet.loadFromUri('/models'),
     faceapi.nets.faceExpressionNet.loadFromUri('/models'),
   ]).then(() => {
    faceDetection();
   })
};const faceDetection = () => {
   const detections = await faceapi.detectAllFaces 
       (videoRef.current, new faceapi.TinyFaceDetectorOptions())
        .withFaceLandmarks()
        .withFaceExpressions();
   console.log(detections);
}
```

å¥½å§ï¼Œè®©æˆ‘è§£é‡Šä¸€ä¸‹ï¼Œ

*   åœ¨æ·»åŠ åˆ° *useEffect* é’©å­çš„ä»£ç ç‰‡æ®µä¸­ï¼Œ

`videoRef && loadModels();`

åªæœ‰å½“ç½‘ç»œæ‘„åƒå¤´è¢«åŠ è½½æ—¶ï¼Œæˆ‘ä»¬æ‰è°ƒç”¨ *loadModels* å‡½æ•°

*   *loadModels* å‡½æ•°ä½¿ç”¨ä¸€ä¸ª *Promise.all ä»æˆ‘ä»¬çš„æ¨¡å‹æ–‡ä»¶å¤¹ä¸­åŠ è½½æ‰€æœ‰éœ€è¦çš„æ¨¡å‹(ç”¨äºè¿è¡Œå¤šä¸ªæ‰¿è¯ºå¹¶è¿”å›ä¸€ä¸ªåŒ…å«è¾“å…¥æ‰¿è¯ºç»“æœçš„æ‰¿è¯º)ã€‚*
*   åŠ è½½æ¨¡å‹åï¼Œå®ƒè°ƒç”¨ *faceDetection* å‡½æ•°ï¼Œè¯¥å‡½æ•°æ£€æµ‹æ‘„åƒå¤´è§†å›¾ä¸­çš„ä»»ä½•äººè„¸ï¼Œå¹¶å°†å…¶æ£€æµ‹å€¼æ‰“å°åˆ°æ§åˆ¶å°ã€‚

ç°åœ¨ï¼Œå¦‚æœæ‚¨å°†æ‚¨çš„è„¸æ”¾åœ¨ç½‘ç»œæ‘„åƒå¤´è§†å›¾ä¸­ï¼Œæ‚¨åº”è¯¥ä¼šåœ¨æµè§ˆå™¨æ§åˆ¶å°ä¸­çœ‹åˆ°ä¸€ä¸ªæ—¥å¿—ï¼Œå…¶ä¸­åŒ…å«æœ‰å…³æ£€æµ‹åˆ°çš„è„¸çš„ä¿¡æ¯ã€‚

æ³¨æ„:ä¸ºäº†æ­£ç¡®æ£€æµ‹ï¼Œä½ åº”è¯¥åœ¨ä¸€ä¸ªå…‰çº¿å……è¶³çš„ç¯å¢ƒä¸­ã€‚

ä¸ºäº†åœ¨è§†é¢‘è¾“å‡ºä¸­æ˜¾ç¤ºæ£€æµ‹ç»“æœï¼Œæˆ‘ä»¬å°†åœ¨ App.js ä¸­æ·»åŠ ä¸€ä¸ª canvas å…ƒç´ ï¼Œä»¥è·Ÿè¸ªæ£€æµ‹åˆ°çš„äººè„¸çš„ä½ç½®ï¼Œå¹¶åˆ›å»ºä¸€ä¸ª *canvasRef* æ¥è·Ÿè¸ªç”»å¸ƒã€‚

**App.js**

```
const canvasRef = useRef();return (
  <div  className="app">
    <h1> AI FACE DETECTION</h1>
    <div className='app__video'> 
       <video crossOrigin='anonymous' ref={videoRef} autoPlay >
       </video>
    </div> 
    <canvas ref={canvasRef} width="940" height="650"
     className='app__canvas' />
</div>
);
```

å¯¹äºä½ç½®çš„æ ·å¼ï¼Œæˆ‘ä»¬å°†ç¼–è¾‘æˆ‘ä»¬çš„ App.css

**App.css**

```
.app {
   display: flex;
   width: 100vw; 
   height: 100vh;
   flex-direction: column;
   align-items: center;
   justify-content: space-between;
}.app__video {
   display: flex;
   align-items: center;
}.app__canvas {
   position: absolute;
   top: 100px;
}
```

ç°åœ¨ï¼Œä¸ºäº†å°†æ¥è‡ªæ£€æµ‹çš„æ•°æ®ä¼ è¾“åˆ°ç”»å¸ƒä¸Šï¼Œæˆ‘ä»¬å°†æŠŠæˆ‘ä»¬çš„ *faceDetection* å‡½æ•°ç¼–è¾‘æˆ:

**App.js**

```
const faceDetection = async () => {
   setInterval(async() => {
   const detections = await faceapi.detectAllFaces
         (videoRef.current,new faceapi.TinyFaceDetectorOptions())
          .withFaceLandmarks()
          .withFaceExpressions();canvasRef.current.innerHtml = faceapi.createCanvasFromMedia
                              (videoRef.current);faceapi.matchDimensions(canvasRef.current, {
  width: 940,
  height: 650,
})const resized = faceapi.resizeResults(detections, {
  width: 940,
  height: 650,
});// to draw the detection onto the detected face i.e the box
faceapi.draw.drawDetections(canvasRef.current, resized);//to draw the the points onto the detected face
faceapi.draw.drawFaceLandmarks(canvasRef.current, resized);//to analyze and output the current expression by the detected face
faceapi.draw.drawFaceExpressions(canvasRef.current, resized);}, 1000);};
```

è¿™æ ·æˆ‘ä»¬å°±å®Œæˆäº†äººè„¸å’Œè¡¨æƒ…è¯†åˆ« appï¼Œä½ å¯ä»¥åœ¨ ***App.css*** æ–‡ä»¶ä¸­ä¿®æ”¹ç”»å¸ƒçš„ä½ç½®ã€‚

æ‰€ä»¥æˆ‘ä»¬å®Œæ•´çš„ **App.js** æ–‡ä»¶åº”è¯¥æ˜¯è¿™æ ·çš„:

```
import { useRef, useEffect } from 'react';
import './App.css';
import * as faceapi from "face-api.js";function App() {
  const videoRef = useRef();
  const canvasRef = useRef();

  useEffect(() => {
    startVideo(); videoRef && loadModels();
}, []); const loadModels = () => {
     Promise.all([
         faceapi.nets.tinyFaceDetector.loadFromUri('/models'),
         faceapi.nets.faceLandmark68Net.loadFromUri('/models'),
         faceapi.nets.faceRecognitionNet.loadFromUri('/models'),
         faceapi.nets.faceExpressionNet.loadFromUri('/models'),
     ]).then(() => {
         faceDetection();
        })
}; const startVideo = () => {
     navigator.mediaDevices.getUserMedia({ video: true })
     .then((currentStream) => {
          videoRef.current.srcObject = currentStream;
      }).catch((err) => {
         console.error(err)
         });
} const faceDetection = async () => {
    setInterval(async() => {
      const detections = await faceapi.detectAllFaces
           (videoRef.current, new faceapi.TinyFaceDetectorOptions())
           .withFaceLandmarks().withFaceExpressions();canvasRef.current.innerHtml = faceapi.
     createCanvasFromMedia(videoRef.current);faceapi.matchDimensions(canvasRef.current, {
    width: 940,
    height: 650,
})const resized = faceapi.resizeResults(detections, {
    width: 940,
    height: 650,
});// to draw the detection onto the detected face i.e the box
faceapi.draw.drawDetections(canvasRef.current, resized);//to draw the the points onto the detected face
faceapi.draw.drawFaceLandmarks(canvasRef.current, resized);//to analyze and output the current expression by the detected face
faceapi.draw.drawFaceExpressions(canvasRef.current, resized);}, 1000)}return (
  <div  className="app">
     <h1> AI FACE DETECTION</h1>
     <div className='app__video'>
        <video crossOrigin='anonymous' ref={videoRef} autoPlay 
        </video>
     </div>
     <canvas ref={canvasRef} width="940" height="650"
     className='app__canvas' />
 </div>
);
}export default App;
```

# ç»“è®º

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†ä½¿ç”¨ [face-api.js](https://www.npmjs.com/package/face-api.js) åº“æ¥è®¾ç½®äººè„¸è¯†åˆ«åº”ç”¨ç¨‹åºæ˜¯å¤šä¹ˆå®¹æ˜“ã€‚æ‚¨å¯ä»¥é€šè¿‡é˜…è¯»å®ƒä»¬çš„æ–‡æ¡£æ¥é€‰æ‹©å®ç°æ›´å¤šçš„ç‰¹æ€§ã€‚

æ„Ÿè°¢é˜…è¯»ï¼Œåˆ«å¿˜äº†é¼“æŒğŸ‘å¹¶åœ¨æˆ‘çš„ä»“åº“é‡Œç•™ä¸‹ä¸€é¢—âœ¨æ˜Ÿã€‚