<html>
<head>
<title>“Finally Machines can answers all your questions”.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">“最后，机器可以回答你所有的问题”。</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/finally-machines-can-answers-all-your-questions-65d5d0747411?source=collection_archive---------11-----------------------#2022-06-05">https://blog.devgenius.io/finally-machines-can-answers-all-your-questions-65d5d0747411?source=collection_archive---------11-----------------------#2022-06-05</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><p id="fdf8" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">最近我一直在研究 Coveo 搜索 ML 模型，以增强我们电子商务网站的搜索体验。在探索 Coveo ML 模型的特性时，我开始了解 BERT(变压器的双向编码器表示)的功能，我对其功能和 BERT 预训练模型的性能感到非常惊讶。BERT 是一种基于转换器的机器学习技术，用于自然语言处理预训练，由 Google 开发。</p><p id="164c" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">在本文中，我试图展示任何人都可以通过在 GCP 云上使用英伟达 NGC 容器轻松使用 BERT 预训练模型，并使用您自己的问题/答案上下文进行训练。本教程有 3 个部分，将帮助您理解以下主题，以及您可以建立自己的 BERT 模型:</p><p id="52bc" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">1.<strong class="jm io">英伟达 NGC 目录</strong></p><p id="2d7a" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">2.<strong class="jm io">自然语言处理的伯特模型</strong></p><p id="0a3a" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">3.<strong class="jm io">使用 GCP 的 BERT 问题/答案示例</strong></p><p id="f2d0" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">开始吧！</p><h1 id="cf8b" class="ki kj in bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">1.<strong class="ak">英伟达 NGC 目录</strong></h1><p id="4408" class="pw-post-body-paragraph jk jl in jm b jn lg jp jq jr lh jt ju jv li jx jy jz lj kb kc kd lk kf kg kh ig bi translated">英伟达 NGC 目录是一款软件，由预训练模型、HPC 和 GPU 支持的各种机器学习和深度学习框架组成，如 Tensor flow、PyTorch、MXNet、NLTK 等。它还包括 docker 运行时，NVidia 驱动程序和舵图表，用于这些模型的生产部署。</p><p id="4116" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">简而言之，它部署了性能优化的 AI/HPC 软件容器、预训练的 AI 模型和 Jupyter 笔记本电脑，可在任何基于 GPU 的内部、云和边缘系统上加速 AI 开发和 HPC 工作负载。</p><p id="04f4" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">以下是英伟达 NGC 的软件堆栈:</strong></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/3e65db8ab0aa8c8268d3fdd65f32072e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UK1_YLAxo7pYZEbB2GZkhw.png"/></div></div></figure><h1 id="86b0" class="ki kj in bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated"><strong class="ak"> 2。自然语言处理的 BERT 模型</strong></h1><p id="e120" class="pw-post-body-paragraph jk jl in jm b jn lg jp jq jr lh jt ju jv li jx jy jz lj kb kc kd lk kf kg kh ig bi translated">BERT 是 Transformers 双向编码器表示的缩写，是用于自然语言处理的机器学习(ML)模型。它是由谷歌人工智能语言的研究人员在 2018 年开发的，可作为最常见语言任务的角色模型解决方案，如情感分析和命名实体识别。它可以处理一些最常见的语言任务，如下所述:</p><p id="ba2f" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">搜索、问题/答案、情感分析、文本分类、词性(POS)、识别命名实体(NER)、情感分类、文本生成、摘要和相似性匹配。</strong></p><p id="d735" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">伯特非常重要和关键原因有两个:</p><ol class=""><li id="4344" class="lx ly in jm b jn jo jr js jv lz jz ma kd mb kh mc md me mf bi translated"><strong class="jm io">数据集大小</strong>:语言杂乱、复杂，对计算机来说比识别图像要难学得多。他们需要更多的数据来更好地识别语言模式，识别单词和短语之间的关系。像最新的 GPT-3 这样的模型是在 45TB 的数据上训练的，包含 1750 亿个参数。这些是巨大的数字，所以很少有人甚至组织有资源来训练这些类型的模型。如果每个人都必须训练自己的 BERT，如果研究人员不利用这些模型的力量，我们将会看到很少的进展。进展将是缓慢的，并且仅限于几个大玩家。</li><li id="6490" class="lx ly in jm b jn mg jr mh jv mi jz mj kd mk kh mc md me mf bi translated"><strong class="jm io">微调:</strong>预先训练的模型有双重好处，它们可以“现成”使用，即无需任何更改，企业只需将 BERT 插入他们的管道，并与聊天机器人或其他应用程序一起使用。但这也意味着这些模型可以针对具体任务进行微调，而无需太多数据或模型调整。对于 BERT 来说，你所需要的只是几千个例子，你可以根据你的数据进行微调。</li></ol><p id="5ed3" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">让我们更多地了解 BERT，以及它为什么非常成功。</p><h2 id="8913" class="ml kj in bd kk mm mn dn ko mo mp dp ks jv mq mr kw jz ms mt la kd mu mv le mw bi translated">变压器架构:</h2><p id="0d0f" class="pw-post-body-paragraph jk jl in jm b jn lg jp jq jr lh jt ju jv li jx jy jz lj kb kc kd lk kf kg kh ig bi translated">为了对 Transformer 架构以及它与 BERT 等模型的关系有一个大致的了解，本文将让。</p><p id="a434" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">我们知道，最初的变压器纸叫做“<a class="ae mx" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="jm io">注意就是你所需要的</strong> </a>”。名称本身很重要，因为它指出了它与以前的方法有什么不同。早期的 NLP 模型(如 ELMo)采用 RNNs 以类似循环的方式顺序处理文本，如下所述:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi my"><img src="../Images/5ba551a8fc39a81b08001b47a8e69dc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*f1e1fhK76zt-FnwQ"/></div><figcaption class="mz na gj gh gi nb nc bd b be z dk translated"><em class="nd">具有序列对序列的 RNNs 按顺序接近处理过的文本，直到它们到达句子结束标记(&lt; eos &gt;)。在这个例子中，请求“ABC”被映射到回复“WXYZ”。当模型接收到&lt; eos &gt;令牌时，模型的隐藏状态存储了前面文本序列的整个上下文。来源:</em> <a class="ae mx" href="https://arxiv.org/pdf/1506.05869.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="nd">一个神经对话模型</em> </a></figcaption></figure><p id="3b77" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">现在想一个简单的句子，像“<strong class="jm io"> <em class="ne">狗在街上追猫的时候猫跑了</em> </strong>”。对于一个人来说，这是一个容易理解的句子，但是如果你想按顺序处理它，实际上有很多困难。一旦你到了“<em class="ne"> it </em>”部分，你怎么知道它指的是什么？您可能需要存储一些状态来识别这个句子中的主角是“<em class="ne">猫</em>”。然后，当你继续阅读这个句子的时候，你必须找到一些方法把"<em class="ne"> it </em>"和"<em class="ne"> cat </em>"联系起来。</p><p id="91b8" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">现在想象这个句子的长度可以是任意数量的单词，试着想想当你处理越来越多的文本时，你将如何跟踪被引用的内容。</p><p id="cd71" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">这是序列模型遇到的问题，就像前伯特模型一样。</p><p id="67c9" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">它们是有限的。他们只能对最近处理过的单词的重要性进行排序。随着他们继续沿着句子前进，前面单词的重要性或相关性开始减弱。</p><p id="f4c9" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">你可以把它想象成在处理每个新单词时向列表中添加信息。您处理的单词越多，就越难引用列表开头的单词。本质上，你需要一次一个元素，一个单词一个单词地往回移动，直到你找到更早的单词，然后看看那些实体是否相关。</p><p id="e414" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">“<em class="ne">它</em>是指“<em class="ne">猫</em>”吗？这就是所谓的“<a class="ae mx" href="https://www.superdatascience.com/blogs/recurrent-neural-networks-rnn-the-vanishing-gradient-problem" rel="noopener ugc nofollow" target="_blank"> <strong class="jm io">消失梯度</strong> </a>”问题，ELMo 使用了一种称为长短期记忆网络(LSTMs)的特殊网络来缓解这种现象的后果。LSTMs 确实解决了这个问题，但没有消除它。</p><p id="2a65" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">最终，他们无法创造一种有效的方式来“聚焦”每个句子中的重要单词。<strong class="jm io">这是变压器网络通过使用我们已知的“注意”机制解决的问题。</strong></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi nf"><img src="../Images/cecf016207a561faa3dbab31c61758de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*j4D311WXxSEifQ-Y.gif"/></div></div><figcaption class="mz na gj gh gi nb nc bd b be z dk translated"><em class="nd">这张 gif 来自一篇关于理解变形金刚中注意力的博文。底部的绿色向量代表编码输入，即编码成向量的输入文本。顶部的深绿色向量表示输入 1 的输出。对每个输入重复该过程，以生成输出向量，该输出向量具有输入中每个单词的“重要性”的注意力权重，该权重与正在处理的当前单词相关。它通过从输入中导出的键、值和查询矩阵之间的一系列乘法运算来实现这一点。来源:</em> <a class="ae mx" href="https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a" rel="noopener" target="_blank"> <em class="nd">图文并茂自我关注</em> </a> <em class="nd">。</em></figcaption></figure><p id="0dc4" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">“<strong class="jm io">注意力是你所需要的全部</strong>”这篇论文利用注意力来提高机器翻译的性能。他们创建了一个包含两个主要部分的模型:</p><ol class=""><li id="43e8" class="lx ly in jm b jn jo jr js jv lz jz ma kd mb kh mc md me mf bi translated"><strong class="jm io">编码器</strong>:这部分的“<strong class="jm io">注意力是你所需要的全部</strong>”模型处理输入文本，寻找重要部分，并根据与句子中<strong class="jm io">其他单词的相关性为每个单词创建嵌入。</strong></li><li id="ecc0" class="lx ly in jm b jn mg jr mh jv mi jz mj kd mk kh mc md me mf bi translated"><strong class="jm io">解码器</strong>:获取编码器的<strong class="jm io">输出，这是一个嵌入，然后将该嵌入转换回文本输出<strong class="jm io">，即输入文本的翻译版本</strong>。</strong></li></ol><p id="bc01" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">然而，论文的关键部分不是编码器或解码器，而是用于创建它们的层。具体来说，编码器和解码器都不像传统的 RNNs 那样使用任何递归或循环。相反，他们使用了“T6”注意力“T7”的层次，信息通过这些层次线性传递。它没有多次循环输入，而是通过多个关注层传递输入。</p><p id="c03d" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">你可以把每一个注意力层想象成“<strong class="jm io">学习</strong>”更多关于输入的东西，也就是看着句子的不同部分，试图发现更多的语义或句法信息。这在我们之前提到的渐变消失问题中是很重要的。</p><p id="b947" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">随着句子长度的增加，rnn 处理它们和学习更多信息变得越来越困难。每一个新单词都意味着要存储更多的数据，并且更难检索这些数据来理解句子中的上下文。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ng"><img src="../Images/6a9e6bab1f55a255b3a1d0d9f7eed96f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Qe-HBBevpQVc1ZGf"/></div></div><figcaption class="mz na gj gh gi nb nc bd b be z dk translated"><em class="nd">这看起来很可怕，事实上，一开始理解这是如何工作的有点让人不知所措。所以现在不要担心理解它。这里的主要要点是，Transformer 不是循环，而是多次并行使用缩放的点积注意机制，即它添加更多的注意机制，然后并行处理每个机制中的输入。这类似于在 RNN 中多次循环一个图层。来源:</em> <a class="ae mx" href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" rel="noopener ugc nofollow" target="_blank"> <em class="nd">又一个很棒的帖子关注</em> </a></figcaption></figure><p id="5bac" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">作为比较，最大的 BERT 模型由 24 个注意层组成。GPT 2 号有 12 个关注层，GPT 3 号有 96 个关注层。</p><h1 id="f836" class="ki kj in bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated"><strong class="ak"> 3。使用 GCP 的 BERT 问题/答案示例</strong></h1><p id="3c58" class="pw-post-body-paragraph jk jl in jm b jn lg jp jq jr lh jt ju jv li jx jy jz lj kb kc kd lk kf kg kh ig bi translated">BERT 代表来自 Transformer 的双向编码器表示，是一种方法/预训练模型，可以重新训练或用于各种自然语言处理任务或用例。在这种情况下，我们将使用 Nvidia NGC 目录在 GCP 顶点人工智能上运行 BERT 问题/答案。</p><p id="e4fd" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">本笔记本演示:</strong></p><ul class=""><li id="ed01" class="lx ly in jm b jn jo jr js jv lz jz ma kd mb kh nh md me mf bi translated">基于 BERT 大模型的问答任务推理</li><li id="ce7c" class="lx ly in jm b jn mg jr mh jv mi jz mj kd mk kh nh md me mf bi translated">使用/下载经过微调的 NVIDIA BERT 型号</li><li id="2862" class="lx ly in jm b jn mg jr mh jv mi jz mj kd mk kh nh md me mf bi translated">使用混合精度进行推理</li></ul><p id="bf2b" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">系统要求:</strong></p><ul class=""><li id="2756" class="lx ly in jm b jn jo jr js jv lz jz ma kd mb kh nh md me mf bi translated"><em class="ne">英伟达 NGC 目录预训练模型伯特</em></li><li id="a9f9" class="lx ly in jm b jn mg jr mh jv mi jz mj kd mk kh nh md me mf bi translated"><em class="ne">针对 BERT 的 NVidia docker 图像</em></li><li id="0b93" class="lx ly in jm b jn mg jr mh jv mi jz mj kd mk kh nh md me mf bi translated"><em class="ne">用于 Tensorflow 应用框架 v.1.15.5 的 NVidia docker 图像</em></li><li id="c62b" class="lx ly in jm b jn mg jr mh jv mi jz mj kd mk kh nh md me mf bi translated"><em class="ne"> NVidia docker 运行时</em></li><li id="a9aa" class="lx ly in jm b jn mg jr mh jv mi jz mj kd mk kh nh md me mf bi translated"><em class="ne">谷歌顶点 AI 工作台</em></li><li id="02a0" class="lx ly in jm b jn mg jr mh jv mi jz mj kd mk kh nh md me mf bi translated"><em class="ne">英伟达特斯拉 V100 GPU — 1 </em></li><li id="1182" class="lx ly in jm b jn mg jr mh jv mi jz mj kd mk kh nh md me mf bi translated">英伟达</li><li id="91c3" class="lx ly in jm b jn mg jr mh jv mi jz mj kd mk kh nh md me mf bi translated"><em class="ne"> 250 GB 固态硬盘</em></li><li id="b39c" class="lx ly in jm b jn mg jr mh jv mi jz mj kd mk kh nh md me mf bi translated">Ubuntu 20.x</li></ul><p id="80ba" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">要满足以上所有要求，最简单的方法就是在 https://catalog.ngc.nvidia.com/<a class="ae mx" href="https://catalog.ngc.nvidia.com/" rel="noopener ugc nofollow" target="_blank">英伟达 NGC</a>创建一个账户并注册。</p><p id="4da6" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">登录后进入 BERT for tensor flow Jupyter Notebook，点击 deploy it to Vertex AI。只是为了确定你需要 tensorflow v1.15.5 来运行这个模型。其余的步骤如下所述，非常简单。</p><p id="12ed" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io"> BERT 型号配置:</strong></p><ul class=""><li id="10bd" class="lx ly in jm b jn jo jr js jv lz jz ma kd mb kh nh md me mf bi translated"><em class="ne">型号:伯特大型</em></li><li id="eb79" class="lx ly in jm b jn mg jr mh jv mi jz mj kd mk kh nh md me mf bi translated"><em class="ne">隐藏层:24 个编码器</em></li><li id="aef4" class="lx ly in jm b jn mg jr mh jv mi jz mj kd mk kh nh md me mf bi translated"><em class="ne">隐藏单元尺寸:1024 </em></li><li id="a8ef" class="lx ly in jm b jn mg jr mh jv mi jz mj kd mk kh nh md me mf bi translated"><em class="ne">注意头:16 </em></li><li id="8481" class="lx ly in jm b jn mg jr mh jv mi jz mj kd mk kh nh md me mf bi translated"><em class="ne">前馈滤波器尺寸:4x1024 </em></li><li id="a602" class="lx ly in jm b jn mg jr mh jv mi jz mj kd mk kh nh md me mf bi translated"><em class="ne">最大序列长度:512 </em></li><li id="343a" class="lx ly in jm b jn mg jr mh jv mi jz mj kd mk kh nh md me mf bi translated"><em class="ne">参数:330 米</em></li></ul><p id="b944" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">在众多可用配置中，我们将下载以下两种之一:</p><ul class=""><li id="cf5b" class="lx ly in jm b jn jo jr js jv lz jz ma kd mb kh nh md me mf bi translated"><strong class="jm io">b</strong>ert _ TF _ ckpt _ large _ QA _ SQuaD 2 _ amp _ 384 这些都是在<a class="ae mx" href="https://rajpurkar.github.io/SQuAD-explorer/" rel="noopener ugc nofollow" target="_blank"> SQuaD 2.0 数据集</a>上训练出来的。</li></ul><p id="0de4" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">运行下面这段代码:</strong></p><p id="aff6" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">use_mixed_precision_model = True</p><p id="73d0" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">#收集数据和模型目录:</strong></p><p id="6e53" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io"># Bert _ TF _ ckpt _ large _ QA _ squad 2 _ amp _ 384</strong><br/>DATA _ DIR _ FT = '/workspace/Bert/DATA/fine tuned _ large _ model _ squad 2.0 '<br/>！mkdir -p $DATA_DIR_FT <br/> <br/>！wget-content-disposition-O $ DATA _ DIR _ FT/Bert _ TF _ ckpt _ large _ QA _ squad 2 _ amp _ 384 _ 19 . 03 . 1 . zip \<br/><a class="ae mx" href="https://api.ngc.nvidia.com/v2/models/nvidia/bert_tf_ckpt_large_qa_squad2_amp_384/versions/19.03.1/zip" rel="noopener ugc nofollow" target="_blank">https://API . NGC . NVIDIA . com/v2/models/NVIDIA/Bert _ TF _ ckpt _ large _ QA _ squad 2 _ amp _ 384/versions/19 . 03 . 1/zip</a>\<br/>&amp;&amp;unzip-n</p><p id="cf2e" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io"> #下载 BERT 助手脚本</strong> <br/>！wget-NC—show-progress-O Bert _ scripts . zip \<br/><a class="ae mx" href="https://api.ngc.nvidia.com/v2/recipes/nvidia/bert_for_tensorflow/versions/1/zip" rel="noopener ugc nofollow" target="_blank">https://API . NGC . NVIDIA . com/v2/recipes/NVIDIA/Bert _ for _ tensor flow/versions/1/zip</a><br/>！mkdir -p /workspace/bert <br/>！unzip-n-d/workspace/Bert Bert _ scripts . zip</p><p id="6e4f" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io"> #BERT 配置</strong></p><p id="237b" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io"> #下载 BERT vocab 文件</strong> <br/>！mkdir-p/workspace/Bert/config . QA<br/>！wget-NC<a class="ae mx" href="https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt" rel="noopener ugc nofollow" target="_blank">https://S3 . Amazon AWS . com/models . hugging face . co/Bert/Bert-base-un cased-vocab . txt</a>\<br/>-O/workspace/Bert/config . QA/vocab . txt</p><pre class="lm ln lo lp gt ni nj nk nl aw nm bi"><span id="c9b3" class="ml kj in nj b gy nn no l np nq"><strong class="nj io">#Writing /workspace/bert/config.qa/bert_config.json</strong></span></pre><p id="00d4" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">% % writefile/workspace/Bert/config . QA/Bert _ config . JSON<br/>{<br/>" attention _ probs _ dropout _ prob ":0.1，<br/> "hidden_act": "gelu "，<br/> "hidden_dropout_prob": 0.1，<br/> "hidden_size": 1024，<br/> "initializer_range": 0.02，<br/> "intermediate_size": 4096，<br/>" max _ position _ embeddings ":512，<br/></p><p id="65cf" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io"><em class="ne">#助手功能:</em> </strong></p><p id="161a" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io"> #根据用户输入创建动态 JSON 文件</strong><br/>def write _ input _ file(context，qinputs，predict_file):</p><p id="924f" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io"> #为有效的 JSON</strong><br/>context = context . replace(“”，“”)删除文本中的引号和换行符。替换(' \n '，'')</p><p id="d4df" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io"> #创建 JSON dict 写</strong><br/>JSON _ dict = {<br/>" data ":[<br/>{<br/>" title ":" BERT QA "，<br/>" paragraphs ":[<br/>{<br/>" context ":context，<br/>" QAS ":Qin puts<br/>}]]}</p><p id="2925" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io"> #编写 JSON 到输入文件</strong> <br/>用 open(predict_file，' w ')作为 json_file: <br/>导入 json <br/> json.dump(json_dict，json_file，indent=2) <br/> <br/> <strong class="jm io"> #显示推理结果为 HTML 表</strong><br/>def Display _ Results(predict _ file，output _ predict _ file):<br/>从 IPython.display 导入 json <br/>导入显示，HTML</p><p id="2c01" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io"> #这里我们只显示预测结果，nbest 预测在输出目录中也是可用的</strong></p><p id="bf1c" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">结果= "" <br/> with open(predict_file，' r ')as query _ file:<br/>queries = JSON . load(query _ file)<br/>input_data = queries[" data "]<br/>with open(output _ prediction _ file，' r ')as result _ file:<br/>data = JSON . load(result _ file)<br/>对于 input _ data 中的条目:<br/>对于条目中的段落["段落"]: <br/>对于段落["qas"]中的 QA:<br/>格式(qa["id"]，qa["问题"]，数据[qa["id"]])</p><p id="d488" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">显示(HTML("<table><tr><th>Id</th><th>问题</th> <th>回答</th> </tr> {} </table>)。格式(结果)))</p><h1 id="a47c" class="ki kj in bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">伯特推理:问题回答</h1><p id="31ae" class="pw-post-body-paragraph jk jl in jm b jn lg jp jq jr lh jt ju jv li jx jy jz lj kb kc kd lk kf kg kh ig bi translated">我们可以在一个微调过的 BERT 模型上运行推理来完成像回答问题这样的任务。</p><p id="9186" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">在这里，我们使用了在<a class="ae mx" href="https://rajpurkar.github.io/SQuAD-explorer/" rel="noopener ugc nofollow" target="_blank"> SQuaD 2.0 数据集</a>上微调的 BERT 模型，该数据集包含 500 多篇文章的 100，000 多个问答对，以及超过 50，000 个新的、无法回答的问题。</p><p id="d61b" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"># <strong class="jm io">创建包含(1)上下文和(2)根据上下文回答的问题的 BERT 输入文件——非常重要</strong></p><p id="fd7b" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">predict _ file = '/workspace/Bert/config . QA/input . JSON '</p><p id="bf65" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">% % writefile $ predict _ file<br/>{ " data ":<br/>[<br/>{ " title ":" Littelfuse QA "，<br/>" paragraphs ":[<br/>{ " context ":" Littelfuse 提供多种电路保护装置，保护乘用车上的电路。我们的汽车产品，包括我们的新型可复位设备，都是行业标准。我们的高质量产品可供广泛选择，使您能够寻找满足您特定需求的最佳解决方案。在下面浏览我们的产品。”、<br/>" QAS ":[<br/>{ " question ":" Littelfuse 是否提供乘用车产品？"、<br/>" id ":" Q1 "<br/>}<br/>}]}]}</p><h1 id="9602" class="ki kj in bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">运行问题/答案推理</h1><p id="cca0" class="pw-post-body-paragraph jk jl in jm b jn lg jp jq jr lh jt ju jv li jx jy jz lj kb kc kd lk kf kg kh ig bi translated">要运行 QA 推理，我们将使用以下参数启动脚本 run_squad.py:</p><p id="0e23" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">导入操作系统</p><p id="add5" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io"> #这指定了模型架构。</strong>T12】Bert _ config _ file = '/workspace/Bert/config . QA/Bert _ config . JSON '</p><p id="1c18" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io"> #训练 BERT 模型的词汇文件。</strong><br/>vocab _ file = '/workspace/Bert/config . QA/vocab . txt '</p><p id="25f7" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io"> #向微调后的 BERT 大型模型发起检查点</strong><br/>init _ check point = OS . path . join('/workspace/BERT/data/fine tuned _ Large _ model _ squad 2.0/model . ckpt ')</p><p id="31f5" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io"> #创建保存所有结果的输出目录。</strong><br/>output _ dir = '/workspace/Bert/results '<br/>output _ prediction _ file = OS . path . join(output _ dir，' predictions . JSON ')<br/><br/><strong class="jm io">#是否对输入进行小写——对未装箱模型为 True，对装箱模型为 False。</strong><br/>do _ lower _ case = True<br/><br/><strong class="jm io">#预测总批量</strong> <br/> predict_batch_size = 8</p><p id="e283" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io"> #是否在开发集上运行 eval。</strong> <br/> do_predict = True</p><p id="4727" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io"> #当将一个长文档分割成多个块时，块之间的步幅是多少。</strong> <br/> doc_stride = 128</p><p id="cc9e" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io"> #分词后最大总输入序列长度。<br/> #长于此长度的序列将被截断，短于此长度的序列将被填充。</strong> <br/> max_seq_length = 384</p><h1 id="cc3c" class="ki kj in bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">运行推理</h1><p id="b7ab" class="pw-post-body-paragraph jk jl in jm b jn lg jp jq jr lh jt ju jv li jx jy jz lj kb kc kd lk kf kg kh ig bi translated"><strong class="jm io"> #问伯特问题</strong> <br/>！python/workspace/Bert/run _ squad . py \<br/>—Bert _ config _ file = $ Bert _ config _ file \<br/>—vocab _ file = $ vocab _ file \<br/>—init _ check point = $ init _ check point \<br/>—output _ dir = $ output _ dir \<br/>—do _ predict = $ do _ predict \<br/>—predict _ file = $ predict _ file \<br/>—predict _ batch _ size = $ predict _ batch _ size \<br/>—doc</p><h1 id="5b7f" class="ki kj in bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">显示结果:</h1><p id="b10b" class="pw-post-body-paragraph jk jl in jm b jn lg jp jq jr lh jt ju jv li jx jy jz lj kb kc kd lk kf kg kh ig bi translated"><strong class="jm io">问题:【Littelfuse 是否提供乘用车产品？</strong></p><p id="f9f2" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><strong class="jm io">答:</strong> Littelfuse 提供多种电路保护装置来保护客车上的电路</p><p id="7cd1" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">现在，您可以添加任何上下文和多个问题，并尝试将其写入 input.json 文件，然后运行推理命令，该命令将为您提供答案。到目前为止，我已经尝试了许多问答，它给出了非常快速和准确的结果。我也将很快在 github 上发布代码。</p></div></div>    
</body>
</html>