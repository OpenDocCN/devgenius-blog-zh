<html>
<head>
<title>Web scraping with Couchbase Server</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Couchbase服务器进行网页抓取</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/web-scraping-with-couchbase-server-629da1e92c43?source=collection_archive---------13-----------------------#2021-12-14">https://blog.devgenius.io/web-scraping-with-couchbase-server-629da1e92c43?source=collection_archive---------13-----------------------#2021-12-14</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><figure class="gl gn jl jm jn jo gh gi paragraph-image"><div class="gh gi jk"><img src="../Images/0dbd97ed550d29b5dd2251cf6bc80e63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*2d42vUVSAvThMLrnjIkv8A.jpeg"/></div></figure><p id="ff39" class="pw-post-body-paragraph jr js in jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ig bi translated">本文旨在概述如何通过三个简单的步骤将抓取的数据从web存储到couchbase服务器中。</p><h1 id="d44a" class="kp kq in bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">什么是网页抓取？</h1><p id="1eac" class="pw-post-body-paragraph jr js in jt b ju ln jw jx jy lo ka kb kc lp ke kf kg lq ki kj kk lr km kn ko ig bi translated">它是从网络上收集和解析数据的过程。我们利用一个非常流行的Python库BeautifulSoup从web上抓取数据。它与您喜欢的解析器一起工作，提供导航、搜索和修改解析树的惯用方式。它通常为程序员节省数小时或数天的工作。网上大量的结构化和非结构化数据可以在各种决策过程中使用。</p><h1 id="a262" class="kp kq in bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">什么是Couchbase服务器？</h1><p id="e70e" class="pw-post-body-paragraph jr js in jt b ju ln jw jx jy lo ka kb kc lp ke kf kg lq ki kj kk lr km kn ko ig bi translated">它是一个开放源码的分布式多模型NoSQL面向文档的数据库软件包，为交互式应用程序进行了优化。所以所有抓取的数据都将被插入/存储到couchbase服务器中。Couchbase管理JSON文档，消除了数据库中硬编码模式的需要。JSON中可用的应用程序对象定义是由开发人员控制的模式。</p><h1 id="def2" class="kp kq in bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">入门指南</h1><h2 id="2928" class="ls kq in bd kr lt lu dn kv lv lw dp kz kc lx ly ld kg lz ma lh kk mb mc ll md bi translated">1 .装置</h2><ul class=""><li id="b05e" class="me mf in jt b ju ln jy lo kc mg kg mh kk mi ko mj mk ml mm bi translated"><a class="ae mn" href="https://www.couchbase.com/downloads" rel="noopener ugc nofollow" target="_blank">下载</a>couch base服务器，并通过输入集群名称、用户名和密码来设置集群。</li><li id="e9ae" class="me mf in jt b ju mo jy mp kc mq kg mr kk ms ko mj mk ml mm bi translated">从存储桶部分创建一个存储桶</li><li id="861c" class="me mf in jt b ju mo jy mp kc mq kg mr kk ms ko mj mk ml mm bi translated">安装图书馆<a class="ae mn" href="https://pypi.org/project/beautifulsoup4/" rel="noopener ugc nofollow" target="_blank">美索</a>和<a class="ae mn" href="https://pypi.org/project/couchbase/" rel="noopener ugc nofollow" target="_blank">沙发座</a></li></ul><h2 id="d28d" class="ls kq in bd kr lt lu dn kv lv lw dp kz kc lx ly ld kg lz ma lh kk mb mc ll md bi translated">2.密码</h2><ul class=""><li id="abe2" class="me mf in jt b ju ln jy lo kc mg kg mh kk mi ko mj mk ml mm bi translated">导入所需的库</li></ul><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="725b" class="ls kq in my b gy nc nd l ne nf"><strong class="my io">import</strong> requests<br/><strong class="my io">from</strong> bs4 <strong class="my io">import</strong> BeautifulSoup<br/><strong class="my io">from</strong> couchbase.cluster <strong class="my io">import</strong> Cluster, ClusterOptions<br/><strong class="my io">from</strong> couchbase.auth <strong class="my io">import</strong> PasswordAuthenticator</span></pre><ul class=""><li id="0e3f" class="me mf in jt b ju jv jy jz kc ng kg nh kk ni ko mj mk ml mm bi translated">通过指定URL、用户名和密码连接到couchbase服务器集群。选择需要存储抓取数据的存储桶</li></ul><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="3b7c" class="ls kq in my b gy nc nd l ne nf"><strong class="my io">def couchbase_connection():</strong><br/>    <br/>    cluster = Cluster('couchbase://localhost', ClusterOptions(<br/>    PasswordAuthenticator('Administrator', 'password')))<br/>    bucket = cluster.bucket('Premier_League')<br/>    collection = bucket.default_collection()<br/>    <strong class="my io">return</strong> collection</span></pre><ul class=""><li id="b30c" class="me mf in jt b ju jv jy jz kc ng kg nh kk ni ko mj mk ml mm bi translated">通过选择适当的类名或选择HTML元素，检查需要删除数据的网页。一旦提取了所需的元素，通过指定键和值将抓取的数据插入couchbase集合</li></ul><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="d76b" class="ls kq in my b gy nc nd l ne nf"><strong class="my io">def scrape():</strong></span><span id="e219" class="ls kq in my b gy nj nd l ne nf">    url = '<a class="ae mn" href="https://www.sportskeeda.com/go/epl/standings'" rel="noopener ugc nofollow" target="_blank">https://www.sportskeeda.com/go/epl/standings'</a><br/>    collection = couchbase_connection()<br/>    req = requests.get(url)<br/>    soup = BeautifulSoup(req.text,"html.parser")<br/>    contents = soup.find('table',class_='keeda_points_table')<br/>    <br/>    trs = contents.find_all('tr')<br/>    for tr in trs[1:]:<br/>        tds = tr.find_all('td')<br/>        row = [td.text.replace('\n', '') for td in tds]<br/>        <br/>        document ={"Team":row[1],"Matches Played":row[2],<br/>                   "Won":row[3],"Draw":row[4],"Lost":row[5],<br/>                   "Goal Difference":row[6],"Points":row[7]}<br/>        collection.insert(row[0],document)</span></pre><h2 id="933c" class="ls kq in bd kr lt lu dn kv lv lw dp kz kc lx ly ld kg lz ma lh kk mb mc ll md bi translated">3.验证插入的文档</h2><p id="bbc2" class="pw-post-body-paragraph jr js in jt b ju ln jw jx jy lo ka kb kc lp ke kf kg lq ki kj kk lr km kn ko ig bi translated">导航到couchbase服务器并点击documents部分，验证抓取的数据是否被插入到正确的集合中。</p><figure class="mt mu mv mw gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi nk"><img src="../Images/42313e18dd9dbc2560620b6364e1f798.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AzSDasmmoNx-qVr47IIJpg.png"/></div></div></figure><h1 id="8c37" class="kp kq in bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated"><strong class="ak">完整代码</strong></h1><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="e348" class="ls kq in my b gy nc nd l ne nf"><strong class="my io">import</strong> requests<br/><strong class="my io">from</strong> bs4 <strong class="my io">import</strong> BeautifulSoup<br/><strong class="my io">from</strong> couchbase.cluster <strong class="my io">import</strong> Cluster, ClusterOptions<br/><strong class="my io">from</strong> couchbase.auth <strong class="my io">import</strong> PasswordAuthenticator</span><span id="ce25" class="ls kq in my b gy nj nd l ne nf"><strong class="my io">def couchbase_connection():</strong><br/>    <br/>    cluster = Cluster('couchbase://localhost', ClusterOptions(<br/>    PasswordAuthenticator('Administrator', 'password')))<br/>    bucket = cluster.bucket('Premier_League')<br/>    collection = bucket.default_collection()<br/>    <strong class="my io">return</strong> collection<br/></span><span id="e332" class="ls kq in my b gy nj nd l ne nf"><strong class="my io">def scrape():</strong></span><span id="6854" class="ls kq in my b gy nj nd l ne nf">    url = '<a class="ae mn" href="https://www.sportskeeda.com/go/epl/standings'" rel="noopener ugc nofollow" target="_blank">https://www.sportskeeda.com/go/epl/standings'</a><br/>    collection = couchbase_connection()<br/>    req = requests.get(url)<br/>    soup = BeautifulSoup(req.text,"html.parser")<br/>    contents = soup.find('table',class_='keeda_points_table')<br/>    <br/>    trs = contents.find_all('tr')<br/>    for tr in trs[1:]:<br/>        tds = tr.find_all('td')<br/>        row = [td.text.replace('\n', '') for td in tds]<br/>        <br/>        document ={"Team":row[1],"Matches Played":row[2],<br/>                   "Won":row[3],"Draw":row[4],"Lost":row[5],<br/>                   "Goal Difference":row[6],"Points":row[7]}<br/>        collection.insert(row[0],document)<br/></span><span id="095f" class="ls kq in my b gy nj nd l ne nf"><strong class="my io">if</strong> __name__=="<strong class="my io">__main__</strong>":<br/>    scrape()</span></pre><h1 id="111e" class="kp kq in bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">结论</h1><p id="a792" class="pw-post-body-paragraph jr js in jt b ju ln jw jx jy lo ka kb kc lp ke kf kg lq ki kj kk lr km kn ko ig bi translated">连接到couchbase的便利性及其不需要严格模式的好处使得将抓取的数据存储到couchbase变得毫不费力。</p><p id="eafc" class="pw-post-body-paragraph jr js in jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ig bi translated">希望你已经理解了网络抓取的概念，现在可以根据你的需要从不同的网站抓取数据，并将它们存储或插入couchbase服务器。</p><p id="774d" class="pw-post-body-paragraph jr js in jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ig bi translated">感谢阅读！</p><p id="2705" class="pw-post-body-paragraph jr js in jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ig bi translated"><a class="ae mn" href="https://github.com/ashish-mj/Scraping_Couchbase" rel="noopener ugc nofollow" target="_blank"> Github </a> <a class="ae mn" href="https://ashishmj.vercel.app/" rel="noopener ugc nofollow" target="_blank">网站</a> <a class="ae mn" href="https://www.linkedin.com/in/ashish-mj/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a></p></div></div>    
</body>
</html>