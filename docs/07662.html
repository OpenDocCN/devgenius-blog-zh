<html>
<head>
<title>Efficient Nets — Rethinking Model Scaling</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">高效网络——重新思考模型缩放</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/efficient-nets-rethinking-model-scaling-f342ee7bb021?source=collection_archive---------10-----------------------#2022-04-13">https://blog.devgenius.io/efficient-nets-rethinking-model-scaling-f342ee7bb021?source=collection_archive---------10-----------------------#2022-04-13</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><figure class="gl gn jl jm jn jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi jk"><img src="../Images/da96b1acded657e2b9286ff955d94635.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jytGmWnR8XJDAP3W"/></div></div></figure><p id="496b" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">有各种神经网络可用；但是每一种都有一些缺点，这促使开发者创建新的和改进的网络。一个这样的网络叫做“高效网络”。与所有其他神经网络一样，设计 CNN 的关键问题之一是模型缩放，即决定如何增加模型大小以提供更好的准确性。</p><p id="4705" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">高效网络是一种先进的神经网络架构和缩放方法，使用<strong class="jx io">复合系数</strong>统一缩放深度/宽度/分辨率的所有维度。与现有 CNN 相比，EfficientNet 模型实现了更高的精度和更好的效率，将参数大小和 FLOPS 减少了一个数量级。这是一个轻量级模型(可以在 android 应用程序中使用)，即使使用较少的参数也能提供更高的准确性。</p><blockquote class="kt ku kv"><p id="f7bd" class="jv jw kw jx b jy jz ka kb kc kd ke kf kx kh ki kj ky kl km kn kz kp kq kr ks ig bi translated"><strong class="jx io"> FLOPS </strong>测量一个冻结的深度学习网络的操作次数。</p><p id="1e2d" class="jv jw kw jx b jy jz ka kb kc kd ke kf kx kh ki kj ky kl km kn kz kp kq kr ks ig bi translated"><strong class="jx io"> <em class="in"> FLOPS =每秒浮点运算次数。</em>T9】</strong></p></blockquote><p id="9367" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">在普通 CNN 中，只进行<strong class="jx io">深度缩放</strong>。深度缩放意味着增加深度，即网络中的层数，以获得更好的精度。模型深度的不断增加逐渐导致<strong class="jx io">消失渐变的问题。</strong>这个问题通过使用另一个神经网络——ResNet 来解决。EfficientNet 模型用于实现比现有 CNN 更高的精度和效率，将参数大小和 FLOPS 减少一个数量级。</p><figure class="lb lc ld le gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi la"><img src="../Images/035957f058e02eaea8b892753efe0e9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*eMDT0yHDdVaLBuVb.png"/></div></div></figure><blockquote class="kt ku kv"><p id="5387" class="jv jw kw jx b jy jz ka kb kc kd ke kf kx kh ki kj ky kl km kn kz kp kq kr ks ig bi translated"><em class="in">要了解更多关于</em> <strong class="jx io"> <em class="in">消失渐变</em> </strong> <em class="in">问题和</em><strong class="jx io"><em class="in">ResNet</em></strong><em class="in">架构，敬请参考我的文章</em> <a class="ae lf" rel="noopener ugc nofollow" target="_blank" href="/resnet50-6b42934db431"> <strong class="jx io"> <em class="in">这里</em> </strong> </a> <em class="in">。</em></p></blockquote><p id="614c" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">在 EfficientNet 中，我们扩展了所有 3 个组件，即</p><ol class=""><li id="0825" class="lg lh in jx b jy jz kc kd kg li kk lj ko lk ks ll lm ln lo bi translated">深度</li><li id="c1b1" class="lg lh in jx b jy lp kc lq kg lr kk ls ko lt ks ll lm ln lo bi translated">宽度</li><li id="b1a6" class="lg lh in jx b jy lp kc lq kg lr kk ls ko lt ks ll lm ln lo bi translated">解决</li></ol><p id="d9e0" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated"><strong class="jx io">宽度缩放</strong>是指改变输入图像的宽度。图像越宽，特征图/通道的数量就越多，因此可用于处理的信息就越多。这反过来导致更好的结论和预测。</p><figure class="lb lc ld le gt jo gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/5c4546d4b523da8184d7a9402538a63d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*wPW_KZ8jdcJFfBJiyfrLxw.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk translated">原创</figcaption></figure><figure class="lb lc ld le gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi lz"><img src="../Images/326f5bb79a8f90027a60457cc1af2522.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ay8UqI4zJ63NVeJbKuwBgQ.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk translated">宽度缩放后</figcaption></figure><p id="6616" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">因此，图像越宽，图像中出现的通道数量就越多。</p><p id="420e" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">但是，随着宽度的增加，出现了一些不重要的特征图。就像，深蓝色的特征地图只限制了道路和背景墙，而不是汽车。</p><p id="fcef" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated"><strong class="jx io">分辨率缩放</strong>是指改变图像的分辨率。图像的 DPI(每英寸点数)越大，分辨率就越高。简单来说，更好的分辨率指的是图像中像素数量的增加。</p><figure class="lb lc ld le gt jo gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/b8cb632bae23ee86431e057535173a47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/0*5NtJw-9J4Mh8op0w.jpg"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk translated">分辨率缩放</figcaption></figure><p id="587b" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">分辨率越高的图像提供的信息越详细，因此卷积模型的结果越好。</p><p id="af72" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">为了处理高分辨率图像，我们需要深度神经网络(深度缩放)。即使图像的分辨率很高，特征图的数量也较少(宽度缩放)，捕获的信息也较少(对于大量的特征图，由于大量的计算，模型变得较慢)。因此，深度、宽度和分辨率缩放都是相互依赖的实体。</p><p id="8385" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">在论文- <strong class="jx io"> <em class="kw">中对卷积神经网络的模型缩放进行再思考</em> </strong> 2 观察到:</p><ol class=""><li id="0b95" class="lg lh in jx b jy jz kc kd kg li kk lj ko lk ks ll lm ln lo bi translated">扩大网络的任何维度——宽度、深度或分辨率——都会提高精度，但<strong class="jx io"> <em class="kw">对于更大的模型</em> </strong>(消失梯度问题)，精度增益会降低。</li></ol><p id="d350" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">2.为了追求更好的精度和效率，关键是<strong class="jx io"> <em class="kw">在缩放(<strong class="jx io">复合缩放</strong>)时平衡网络</em> </strong>的所有维度。</p><p id="fea5" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">为了缩放三维，需要一个<strong class="jx io">基线模型</strong>—<strong class="jx io">efficient net B0</strong></p><p id="a590" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated"><strong class="jx io"> EfficientNet B0 </strong>由<strong class="jx io"> NAS(神经架构搜索)</strong>设计，其中神经网络生成新的和改进的神经网络。</p><h1 id="f9fd" class="mb mc in bd md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my bi translated">复合缩放</h1><figure class="lb lc ld le gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi mz"><img src="../Images/a1a5ddfb963d21c14e497372482f8d5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*GnTl9lkOFWfmWONZ.png"/></div></div></figure><p id="f98f" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">复合缩放指的是缩放所有 3 个组件同时保持它们之间平衡的方法。</p><figure class="lb lc ld le gt jo gh gi paragraph-image"><div class="gh gi na"><img src="../Images/3d83df954f35589b2c4f2ac41cbe8cb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:530/format:webp/1*-9qUCAdlwxUHXrZAFGB7uw.png"/></div></figure><p id="b500" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">在哪里，</p><p id="612f" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">d =深度</p><p id="8c1d" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">w =宽度</p><p id="85b3" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">r =分辨率</p><p id="f48a" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">ϕ=compound 系数</p><p id="3e21" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">d、w、r 的值是固定的，ϕ的值由<strong class="jx io">网格搜索技术提供。</strong></p><p id="8a3d" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated"><strong class="jx io">α= 1.2；β = 1.1;ϒ = 1.15;</strong></p><p id="1dd2" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">让我们，ϕ=1</p><p id="56be" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">这意味着，如果我们增加 20%的深度，那么宽度应该增加 10%，分辨率增加 15%，以保持平衡。</p><p id="b6f3" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">从 B0 到 B7，共有 7 种高效网络模式。B0 是基线模型。</p><p id="be9e" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">输入图像尺寸因型号而异。随着模型级别的增加，图像的输入大小也增加。</p><figure class="lb lc ld le gt jo gh gi paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="gh gi nb"><img src="../Images/596964427cf84269f6425e4ceabadf97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*lX8IwMIki6KiBhkwFlEt_g.png"/></div></div></figure><p id="8980" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated">使用这种技术，作者能够创建比以前的 ConvNets 精度更高的模型，同时极大地减少了总体失败次数和模型大小。这种缩放策略是灵活的，并且它可以用于有效地缩放卷积神经网络，并且提高各种架构的准确性。</p></div><div class="ab cl nc nd hr ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ig ih ii ij ik"><p id="6542" class="pw-post-body-paragraph jv jw in jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ig bi translated"><em class="kw">感谢阅读！如有任何疑问，请随时联系我的</em><a class="ae lf" href="http://aditi2507rastogi@gmail.com/" rel="noopener ugc nofollow" target="_blank"><strong class="jx io"><em class="kw">Gmail</em></strong></a><strong class="jx io"><em class="kw"/></strong><em class="kw">或我的</em> <a class="ae lf" href="https://www.linkedin.com/in/aditi-rastogi-961789191/" rel="noopener ugc nofollow" target="_blank"> <strong class="jx io"> <em class="kw"> LinkedIn 个人资料</em></strong></a><strong class="jx io"><em class="kw"/></strong><em class="kw">或</em><a class="ae lf" href="https://github.com/AditiRastogi250701" rel="noopener ugc nofollow" target="_blank"><strong class="jx io"><em class="kw">GitHub</em></strong></a><em class="kw">个人资料。</em></p></div></div>    
</body>
</html>