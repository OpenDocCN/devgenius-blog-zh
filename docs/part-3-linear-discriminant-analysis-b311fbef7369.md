# 第三部分:线性判别分析

> 原文：<https://blog.devgenius.io/part-3-linear-discriminant-analysis-b311fbef7369?source=collection_archive---------9----------------------->

![](img/c2da15d2907d1ec395239704ffd14415.png)

[TDS](https://towardsdatascience.com/probabilistic-linear-discriminant-analysis-plda-explained-253b5effb96) 的 LDA 与非 LDA 投影

线性判别式分析(LDA)是 Fisher 线性判别式的推广，Fisher 线性判别式是一种用于统计、模式识别和机器学习的技术，用于寻找表征或分离两类或更多类对象或事件的特征的线性组合。线性判别分析(LDA)最常用作模型分类和机器学习应用的预处理降维技术。虽然 PCA 和逻辑回归等其他降维技术也被广泛使用，但在一些特定的用例中，线性判别分析更为合适。在本文之后，我们讨论了一种称为 LDA 的受控降维方法，当逻辑回归失败以及处理两个或更多类时，可以进一步将其用作分类器。

LDA 已经成功地用于各种应用中，在某种程度上，问题变成了分类问题，这种方法是可以实现的。得到的组合可以用作线性分类器，或者更常见的是，在进一步分类之前用于降维。例如，可以将类划分为多个部分，并且可以使用标准的 Fisher 判别式或 LDA 来对每个部分进行排序。LDA 和 Fisher 鉴别器可以通过内核技巧扩展用于非线性分类。

另一方面，LDA 是一种监督算法，它使用输入标签和类别标签来查找线性判别式，从而最大化多个类别之间的分离。LDA 的一般方法非常类似于主成分分析(有关 PCA 的更多信息，请参见上一篇文章 Python 中主成分分析(PCA)的分步实施)，但是除了找到最大化我们的数据方差(PCA)的成分轴之外，我们还对最大化多个类之间的分离(LDA)的轴感兴趣。LDA 还与主成分分析(PCA)和因子分析密切相关，因为两者都寻找最佳解释数据的变量的线性组合。线性判别分析(LDA)和主成分分析(PCA)都是广泛使用的线性变换降维方法(都是数据矩阵分解方法)。

LDA 与方差分析(ANOVA)和回归分析密切相关，后者也试图将因变量表示为其他特征或维度的线性组合。逻辑回归是一种重要的线性分类算法，但它也有一些限制，导致需要使用替代的线性分类算法。另一种替代用途是在使用非线性分类算法之前减小尺寸。即使假设被轻微违反，线性判别分析也能很好地工作，并且当使用二分变量时仍然是可靠的(尽管在这种情况下多变量正态假设往往被违反)。

1936 年，Ronald A. Fisher 首次提出了线性判别式，并展示了其作为分类器的一些实际应用。它被描述为两类问题，随后由 CRRao 在 1948 年概括为多类线性判别分析或多重判别分析。在 LDA 中，我们基本上试图决定哪一组参数能够最好地描述一个类的组关联，以及哪一个是分离这些组的最佳分类模型。