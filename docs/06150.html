<html>
<head>
<title>Create a Neural Network in Sci-Kit Learn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Sci-Kit学习中创建神经网络</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/create-a-neural-network-in-sci-kit-learn-943c928a306d?source=collection_archive---------3-----------------------#2021-12-17">https://blog.devgenius.io/create-a-neural-network-in-sci-kit-learn-943c928a306d?source=collection_archive---------3-----------------------#2021-12-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e4d6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用sklearn的MLPClassifier在不到40行Python代码中轻松创建一个神经网络</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/44f2f1a48a2d07e3f79a745ca59c25be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7I5CcDPaQiVg6JqA.jpg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="http://stackoverflow.com/questions/34725495/can-i-use-a-neural-network-for-regression-when-input-has-multiple-output-values" rel="noopener ugc nofollow" target="_blank">来自堆栈溢出的图像</a></figcaption></figure><p id="e25c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">神经网络是21世纪应用机器学习兴起的支柱。虽然它们是在20世纪后期发明的，但当时的计算能力不足以发挥神经网络的全部能力。神经网络的基本技术是梯度下降、反向传播和激活函数。梯度下降和反向传播都需要对多元微积分有所了解。谢天谢地，像<code class="fe ls lt lu lv b">sklearn</code>这样的机器学习库已经为我们抽象出了这一点。在本帖中，我们将讲述如何使用<code class="fe ls lt lu lv b">sklearn</code>构建自己的神经网络，而无需深入多变量微积分。</p><p id="b161" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将使用SKLearn的“多层感知器分类器”的<code class="fe ls lt lu lv b">MLPClassifier</code>。多层感知器只是神经网络的一个花哨词，反之亦然。神经网络由许多感知器组成，这些感知器也可以称为“节点”或“神经元”。感知器是一个简单的函数表示，它对一些输入进行数学运算并返回结果。感知器也是典型的“二元分类器”,这意味着它们返回0或1。经典的数学是某种分类函数，如sigmoid函数。</p><h1 id="8cfd" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">如何使用SKLearn构建神经网络的概述</h1><ol class=""><li id="f948" class="mo mp iq ky b kz mq lc mr lf ms lj mt ln mu lr mv mw mx my bi translated">导入正确的<code class="fe ls lt lu lv b">sklearn</code>库</li><li id="7b9c" class="mo mp iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated">获取和转换MNIST数据集</li><li id="bcce" class="mo mp iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated">对数据执行训练/测试分割</li><li id="7263" class="mo mp iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated">通过MLP分类器建立我们的神经网络</li><li id="f66e" class="mo mp iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated">训练神经网络</li><li id="daad" class="mo mp iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated">在MNIST数据集上检验我们的神经网络的性能</li></ol><h1 id="1467" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">导入正确的<code class="fe ls lt lu lv b">sklearn</code>库</h1><p id="c143" class="pw-post-body-paragraph kw kx iq ky b kz mq jr lb lc mr ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">用<code class="fe ls lt lu lv b">sklearn</code>构建神经网络很简单。我们真正需要的唯一函数是来自<code class="fe ls lt lu lv b">sklearn.neural_network</code>的<code class="fe ls lt lu lv b">MLPClassifier</code>函数。然而，对我们来说，我们将导入多个助手函数。我们将从<code class="fe ls lt lu lv b">sklearn.exceptions</code>导入<code class="fe ls lt lu lv b">warnings</code>库和<code class="fe ls lt lu lv b">ConvergenceWarning</code>，因为我们不能确定模型会收敛。对于这个例子，我们将只运行15次迭代，所以我们不应该期望模型收敛。我们还将从<code class="fe ls lt lu lv b">sklearn.datasets</code>导入<code class="fe ls lt lu lv b">fetch_openml</code>，以导入MNIST数据集来训练和测试我们的神经网络。</p><pre class="kg kh ki kj gt nh lv ni nj aw nk bi"><span id="31ae" class="nl lx iq lv b gy nm nn l no np">import warnings</span><span id="cfcb" class="nl lx iq lv b gy nq nn l no np">from sklearn.datasets import fetch_openml</span><span id="22bf" class="nl lx iq lv b gy nq nn l no np">from sklearn.exceptions import ConvergenceWarning</span><span id="4a08" class="nl lx iq lv b gy nq nn l no np">from sklearn.neural_network import MLPClassifier</span></pre><h1 id="dbd0" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">获取和转换MNIST数据集</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/895066a7e87308c221e5aeec94338b7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/0*3W5A39DqtVg5v77E.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://en.wikipedia.org/wiki/MNIST_database" rel="noopener ugc nofollow" target="_blank">图片来自维基百科</a></figcaption></figure><p id="1a2c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了训练神经网络，我们需要数据。重要的是，我们不只是获得任何数据，而是获得合适的数据来进行训练和测试。数据必须被清理并缩放到正确的尺寸。由于我们自己收集、清理和缩放数据进行训练会非常困难，所以我们将使用OpenML中现有的数据集。</p><p id="75c0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">处理完导入后，我们将下载并转换MNIST数据集。要获取数据集，我们需要做的就是调用我们导入的库之一<code class="fe ls lt lu lv b">fetch_openml</code>，并向它传递正确的参数。第一个参数是MNIST数据集的名称，指定我们想要28x28像素的图像(因此是784)。我们还指定我们需要第一个版本，并且我们希望它以<code class="fe ls lt lu lv b">X, y</code>格式返回。然后，我们将X数据除以255，缩放到<code class="fe ls lt lu lv b">[0,1]</code>范围。</p><pre class="kg kh ki kj gt nh lv ni nj aw nk bi"><span id="86a0" class="nl lx iq lv b gy nm nn l no np"># load MNIST data from fetch_openml</span><span id="965f" class="nl lx iq lv b gy nq nn l no np">X, y = fetch_openml("mnist_784", <em class="ns">version</em>=1, <em class="ns">return_X_y</em>=True)</span><span id="9919" class="nl lx iq lv b gy nq nn l no np">X = X/255.0</span></pre><h1 id="b8e8" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">对数据执行训练/测试分割</h1><p id="96be" class="pw-post-body-paragraph kw kx iq ky b kz mq jr lb lc mr ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">对数据进行训练/测试分割是常见的做法。由于我们使用已经清理过的现有数据，这有助于我们确保我们的训练和测试数据是相同的格式。这对于有效测量我们神经网络的准确性是很重要的。</p><p id="c19b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">来自<a class="ae kv" href="https://www.openml.org/d/554" rel="noopener ugc nofollow" target="_blank"> OpenML </a>的<code class="fe ls lt lu lv b">mnist_784</code>数据集中有70，000张图片。为了测试我们的神经网络，我们要把它分成两组。一组用于训练神经网络，另一组用于测试神经网络。对于本例，我们将使用前60，000个数据点进行训练，其余的(另外10，000)进行测试。</p><pre class="kg kh ki kj gt nh lv ni nj aw nk bi"><span id="7d76" class="nl lx iq lv b gy nm nn l no np"># get train/test split</span><span id="2207" class="nl lx iq lv b gy nq nn l no np">X_train, X_test = X[:60000], X[60000:]</span><span id="3184" class="nl lx iq lv b gy nq nn l no np">y_train, y_test = y[:60000], y[60000:]</span></pre><h1 id="8116" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">通过MLP分类器建立我们的神经网络</h1><p id="6bd4" class="pw-post-body-paragraph kw kx iq ky b kz mq jr lb lc mr ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">现在我们已经得到了数据，是时候为我们的神经网络设置超参数了。我们将为<code class="fe ls lt lu lv b">MLPClassifier</code>设置七个参数。我们将设置的第一个参数是<code class="fe ls lt lu lv b">hidden_layer_sizes</code>。此参数控制层的数量和层的大小。对于这个例子，我们将只使用一个大小为50的隐藏层。我们要设置的下一个参数是<code class="fe ls lt lu lv b">max_iter</code>参数。这是我们想要训练神经网络的最大迭代次数或时期数。在这个例子中，我们将它设置为15。</p><p id="d692" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们将设置<code class="fe ls lt lu lv b">alpha</code>，L2惩罚或正则项。这个术语有助于减少过度拟合。α越大，神经网络的最佳权重越接近0。这里我们只使用默认值0.0001。请注意，从技术上讲，我们不必设置这个值，因为我们使用的是默认值，这只是为了清楚和详细起见。选择<code class="fe ls lt lu lv b">alpha</code>后，我们将设置<code class="fe ls lt lu lv b">solver</code>，这是我们正在使用的梯度下降。对于这个例子，我们将使用<code class="fe ls lt lu lv b">sgd</code>或随机梯度下降解算器。我们只设置了<code class="fe ls lt lu lv b">random_state </code>参数来使我们的结果可重复，这与状态设置成什么无关。在这里，我们将它设置为1。</p><p id="7de2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我们将设置我们的<code class="fe ls lt lu lv b">learning_rate_init</code>，这是初始学习率。如果将<code class="fe ls lt lu lv b">learning_rate</code>设置为自适应，一旦损失增加，<code class="fe ls lt lu lv b">MLPClassifier</code>将自动改变学习率。我们在这里没有这样做。我们将初始学习率设置为0.1，较大的学习率允许更快的收敛，但是太大了，模型将不会收敛。<code class="fe ls lt lu lv b">learning_rate</code>参数仅用于<code class="fe ls lt lu lv b">sgd</code>解算器。</p><pre class="kg kh ki kj gt nh lv ni nj aw nk bi"><span id="33c9" class="nl lx iq lv b gy nm nn l no np"># set up MLP Classifier</span><span id="8ffa" class="nl lx iq lv b gy nq nn l no np">mlp = MLPClassifier(</span><span id="f04e" class="nl lx iq lv b gy nq nn l no np"><em class="ns">    hidden_layer_sizes</em>=(50,),</span><span id="ed7a" class="nl lx iq lv b gy nq nn l no np"><em class="ns">    max_iter</em>=15,</span><span id="f7e6" class="nl lx iq lv b gy nq nn l no np"><em class="ns">    alpha</em>=1e-4,</span><span id="3a57" class="nl lx iq lv b gy nq nn l no np"><em class="ns">    solver</em>="sgd",</span><span id="60b1" class="nl lx iq lv b gy nq nn l no np"><em class="ns">    verbose</em>=True,</span><span id="a363" class="nl lx iq lv b gy nq nn l no np"><em class="ns">    random_state</em>=1,</span><span id="1f91" class="nl lx iq lv b gy nq nn l no np"><em class="ns">    learning_rate_init</em>=0.1</span><span id="6cae" class="nl lx iq lv b gy nq nn l no np">)</span></pre><h1 id="68a5" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">训练神经网络</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/8321d46fb252cab2082a70be9f15f7d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/0*qxmzdsgxrUSlB0To"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="http://www.techsavvyed.net/archives/4155" rel="noopener ugc nofollow" target="_blank">图片来自TechSavvyEd </a></figcaption></figure><p id="5b83" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">训练基于<code class="fe ls lt lu lv b">sklearn </code>的神经网络相当容易。我们所做的就是使用我们的<code class="fe ls lt lu lv b">MLPClassifier</code>来调用训练数据上的<code class="fe ls lt lu lv b">fit</code>函数。我们将它包装在一个<code class="fe ls lt lu lv b">warnings.catch_warnings()</code>中，然后调用<code class="fe ls lt lu lv b">warnings.filterwarnings</code>函数来忽略来自<code class="fe ls lt lu lv b">sklearn</code>模块的收敛警告，这样我们就不会在网络不收敛时被责骂。就是这样，在这之后，神经网络就完成了训练。</p><pre class="kg kh ki kj gt nh lv ni nj aw nk bi"><span id="cf14" class="nl lx iq lv b gy nm nn l no np"># We probably won't converge so we'll catch the warnings</span><span id="f880" class="nl lx iq lv b gy nq nn l no np">with warnings.catch_warnings():</span><span id="2e3d" class="nl lx iq lv b gy nq nn l no np">    warnings.filterwarnings("ignore", <em class="ns">category</em>=ConvergenceWarning, <em class="ns">module</em>="sklearn")</span><span id="5826" class="nl lx iq lv b gy nq nn l no np">    mlp.fit(X_train, y_train)</span></pre><h1 id="ba62" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">在MNIST数据集上检验我们的神经网络的性能</h1><p id="0ee8" class="pw-post-body-paragraph kw kx iq ky b kz mq jr lb lc mr ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">现在我们的神经网络已经训练好了，我们可以测试它了。我们将使用MNIST数据集中没有训练过的其余数据来测试我们的神经网络。<code class="fe ls lt lu lv b">score</code>函数返回一个介于0和1之间的值，表示被正确分类的数据点的比例。</p><pre class="kg kh ki kj gt nh lv ni nj aw nk bi"><span id="5293" class="nl lx iq lv b gy nm nn l no np"># print out the model scores</span><span id="45ec" class="nl lx iq lv b gy nq nn l no np">print(f"Training set score: {mlp.score(X_train, y_train)}")</span><span id="5691" class="nl lx iq lv b gy nq nn l no np">print(f"Test set score: {mlp.score(X_test, y_test)}")</span></pre><p id="fcf1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当我们运行我们的函数时，我们应该看到下面的输出。每次都应该是一样的，因为我们在<code class="fe ls lt lu lv b">MLPClassifier</code>中使用了<code class="fe ls lt lu lv b">random_state</code>参数。我们可以看到，在15次迭代之后，我们在测试集上看到了大约99.4%的分类率和97.36%的分类率。这很好。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/fe83df1e64eace4d5c24b8aa018479d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*21FYnkT_lcJuuwx0"/></div></figure><h1 id="e013" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">我们如何提高神经网络的分数？</h1><p id="e968" class="pw-post-body-paragraph kw kx iq ky b kz mq jr lb lc mr ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">97.36%的准确率已经相当不错了，但是我们实际上可以让我们的神经网络变得更好。怎么会？我们可以调整超参数或者训练更多的迭代。当然，总是存在过度拟合的可能性，我们可能需要对超参数进行相当多的调整。在这一点上，你必须通过检查你的用例来问你自己这是否值得。以数字识别为例？很可能不值得。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/c25d4e84f77c311658ee44492da43014.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/0*lDCS68evbUCAiL6U"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="http://stats.stackexchange.com/questions/131233/neural-network-over-fitting/131234" rel="noopener ugc nofollow" target="_blank">来自堆栈交换的图像</a></figcaption></figure><p id="e596" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了让我们的神经网络停止在一个特定的精度，我们需要一个验证集。验证数据集就是我们在训练之后和测试之前运行“验证”的一组数据。这让我们对神经网络的表现有所了解。我们还使用验证分数来确定何时停止训练。我们使用验证分数而不是训练分数，因为只要误差变小，神经网络就会持续训练。这是有问题的，因为它可能导致过度拟合。</p><h1 id="31d1" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">回顾:用SKLearn的MLP分类器构建一个神经网络</h1><p id="a62f" class="pw-post-body-paragraph kw kx iq ky b kz mq jr lb lc mr ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">在这篇文章中，我们回顾了如何使用<code class="fe ls lt lu lv b">MLPClassifier</code>来训练一个完全连接的3层深度神经网络。我们的示例使用MNIST数据集进行训练和测试。MNIST数据集是由70，000幅28x28图像组成的数据集。我们讨论了如何设置神经网络每层的数量和大小，如何设置学习率和L2惩罚，如何选择求解器和最大迭代次数，以及如何使用<code class="fe ls lt lu lv b">random_state</code>参数来确保每次都得到相同的结果。</p><h1 id="0b3a" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">进一步阅读</h1><p id="57be" class="pw-post-body-paragraph kw kx iq ky b kz mq jr lb lc mr ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">要了解更多关于神经网络的信息，请阅读Python 3 中关于如何<a class="ae kv" href="https://pythonalgos.com/2021/12/06/create-a-neural-network-from-scratch-in-python-3/" rel="noopener ugc nofollow" target="_blank">从头开始构建自己的神经网络的内容。为了进一步探索机器学习，请查看关于NLP的</a><a class="ae kv" href="https://pythonalgos.com/2021/12/10/introduction-to-nlp-core-concepts/" rel="noopener ugc nofollow" target="_blank">介绍:核心概念</a>和用于计算机视觉的<a class="ae kv" href="https://link.medium.com/fm2XSFOj4lb" rel="noopener">深度学习</a>。</p><p id="e80b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你喜欢这个或者它对你有帮助，请在Twitter或LinkedIn上分享它！要无限制地访问媒体的信息宝库，今天就成为<a class="ae kv" href="https://medium.com/subscribe/@ytang07" rel="noopener">媒体会员</a>！更多关于机器学习、软件、成长的帖子，记得关注我，<a class="ae kv" href="https://www.medium.com/@ytang07" rel="noopener">唐</a>！</p></div></div>    
</body>
</html>