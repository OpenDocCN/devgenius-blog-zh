<html>
<head>
<title>Spark Structured API</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Spark 结构化 API</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/spark-structured-api-6a931407ff1e?source=collection_archive---------14-----------------------#2022-11-27">https://blog.devgenius.io/spark-structured-api-6a931407ff1e?source=collection_archive---------14-----------------------#2022-11-27</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><p id="60c7" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">spark 大数据架构从 Map Reduce 等其他大数据框架中脱颖而出，因为 Spark 的基本核心数据结构 RDD(弹性分布式数据集)内存处理。因为 RDD 本身可以成为讨论的话题，所以基本上 rdd 是不可变的分布式对象集合，它们通常不受模式的限制。在某种意义上，rdd 有助于处理非结构化数据。酪当结构化数据可用时，由于采用了无模式方法，RDD API 很难使用。为了使开发变得容易和直观，Spark 结构化 API 来拯救。在本文中，我们将看到不同的 spark 结构的 API。</p><p id="34bf" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">结构化数据包括模式，数据以行和列的形式存储。结构化 API 是低级 RDD API 的高级抽象(在某种程度上，结构化 API 建立在 RDD 数据结构之上)。</p><p id="ee91" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">Spark 支持三种类型的结构化 API。</p><ol class=""><li id="9cab" class="ki kj in jm b jn jo jr js jv kk jz kl kd km kh kn ko kp kq bi translated">DataFrames:这个数据集合以表格的形式组织，表格中有行和列，允许处理大量的结构化数据。数据帧中的数据是按行和列组织的，而在 rdd 中则不是这样。但是，DataFrame 没有编译时类型安全。</li><li id="b1e9" class="ki kj in jm b jn kr jr ks jv kt jz ku kd kv kh kn ko kp kq bi translated">数据集:这种结构是数据帧的扩展，包括数据帧和 rdd 的特性。数据集为安全处理数据提供了面向对象的接口。面向对象的接口指的是这样一种接口，其中所有的实体都被视为一个对象，要访问它们，必须调用该对象。请注意，数据集仅在基于 Java 虚拟机(JVM)的语言中可用，如 Scala 和 Java，但在 Python 和 r 中不可用。</li><li id="b373" class="ki kj in jm b jn kr jr ks jv kt jz ku kd kv kh kn ko kp kq bi translated">SQL 表和视图(Spark SQL):使用 Spark SQL，您可以对组织到数据库中的视图或表运行类似 SQL 的查询。</li></ol></div><div class="ab cl kw kx hr ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ig ih ii ij ik"><p id="dca3" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">数据帧:</p><p id="9f52" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">DataFrame API 是 Spark SQL 模块的一部分。Spark 中的数据帧与 Pandas 中的数据帧非常相似，除了它们也遵循 rdd 之类的惰性求值。数据以列和行的形式提供，数据帧中的每一列都有与之相关联的特定数据类型。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ld"><img src="../Images/b44da6b08e6876549822317747b7bb53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*M1bXqVFiI2v2pqhf.png"/></div></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk translated"><a class="ae lt" href="https://phoenixnap.com/kb/spark-dataframe#:~:text=What%20is%20a%20DataFrame%3F,and%20columns%20of%20different%20datatypes." rel="noopener ugc nofollow" target="_blank">图片由</a>提供</figcaption></figure><p id="2650" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">数据帧的优势。</p><p id="c49d" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">高级 API:数据帧抽象由于它的结构与 Pandas 中的数据帧非常相似，所以代码可读性很高，并且易于编写。</p><p id="67b1" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">优化器:DataFrames 可以使用 Catalyst optimizer 或自适应查询执行引擎之类的优化器在内部优化代码。</p><p id="2a74" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">内存:更好的内存管理和堆外内存的使用<a class="ae lt" href="https://www.databricks.com/session/deep-dive-apache-spark-memory-management" rel="noopener ugc nofollow" target="_blank">存储</a>。</p><p id="486a" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">Dataframe 对象的工作方式有点类似于类似熊猫的数据框。</p><pre class="le lf lg lh gt lu lv lw bn lx ly bi"><span id="3083" class="lz ma in lv b be mb mc l md me"># To initiate the spark session:<br/>from pyspark.sql import SparkSession<br/>spark = SparkSession.builder.appName("Name of the app").getOrCreate()</span></pre><pre class="mf lu lv lw bn lx ly bi"><span id="900b" class="lz ma in lv b be mb mc l md me"># As Dataframe is a structured datatype code to check the scheme. <br/>df.printSchema()<br/>root<br/> |-- name: string (nullable = true)<br/> |-- salary: double (nullable = true)<br/> |-- age: double (nullable = true)</span></pre><pre class="mf lu lv lw bn lx ly bi"><span id="b7f9" class="lz ma in lv b be mb mc l md me"># To print the dataframe.<br/>df.show()<br/>+-------+--------+----+<br/>|   name|  salary| age|<br/>+-------+--------+----+<br/>|Michael| 40000.0|29.0|<br/>|   Andy| 50000.0|30.0|<br/>| Justin| 30000.0|19.0|<br/>|Michael| 45000.0|45.0|<br/>|  Sandy| 57000.0|37.0|<br/>|  Rocky| 88000.0|60.0|<br/>+-------+--------+----+</span></pre><p id="c986" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">dataframe 支持广泛的 API，用于过滤、分组和摆弄列和行(<a class="ae lt" href="https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html" rel="noopener ugc nofollow" target="_blank">官方文档</a>)。</p></div><div class="ab cl kw kx hr ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ig ih ii ij ik"><p id="84a2" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">熊猫 API:</p><p id="04f8" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">熊猫为 Python 语言在数据科学领域的出名做出了巨大贡献。<a class="ae lt" href="https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html" rel="noopener ugc nofollow" target="_blank">熊猫 API </a>启用 python 用户</p><p id="0bb4" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">在星火上经营熊猫。通常开发人员更喜欢亲自使用 pandas 而不是 pyspark，所以为了避免开发人员学习 pysprk，pandas API over spark(也称为 pandas-on-spark)有助于编写代码 pandas API</p><p id="a542" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">(类似于本地 pandas 的语法)但是在 pyspark 集群上执行。</p><p id="6a08" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">熊猫 API 与本土熊猫的区别如下。</p><ul class=""><li id="0c6e" class="ki kj in jm b jn jo jr js jv kk jz kl kd km kh mg ko kp kq bi translated">节点:Pandas API 运行在 Spark 上，与本机 Pandas 的单个节点相比，Spark 提供了多个执行节点。</li><li id="9b75" class="ki kj in jm b jn kr jr ks jv kt jz ku kd kv kh mg ko kp kq bi translated">执行:与原生 Pandas 相比，Pandas API 允许延迟执行，而原生 Pandas 是为急切执行而提供的。</li><li id="c6e9" class="ki kj in jm b jn kr jr ks jv kt jz ku kd kv kh mg ko kp kq bi translated">可伸缩性:与本地 Pandas 相比，Pandas API 通过向集群中添加更多的节点，实现了简单的可伸缩性。</li><li id="2a75" class="ki kj in jm b jn kr jr ks jv kt jz ku kd kv kh mg ko kp kq bi translated">类型支持:Pandas API 有类型支持，允许在 Pandas/PySpark 之间转换。将 pandas-on-Spark 数据帧转换为 PySpark 数据帧时，数据类型会自动转换为适当的类型。</li></ul><pre class="le lf lg lh gt lu lv lw bn lx ly bi"><span id="e8d1" class="lz ma in lv b be mb mc l md me"># Importing pandas on pyspark as <br/>import pyspark.pandas as ps</span></pre><pre class="mf lu lv lw bn lx ly bi"><span id="3ae2" class="lz ma in lv b be mb mc l md me"># Pandas API for creating a series <br/>panAPI_series = ps.Series([1, 3, 5, np.nan, 6, 8])</span></pre><pre class="mf lu lv lw bn lx ly bi"><span id="5879" class="lz ma in lv b be mb mc l md me"># Pandas API for creating dataframe<br/>panAPI_dataframe = ps.DataFrame(<br/>{'a': [1, 2, 3, 4, 5, 6],<br/>'b': [100, 200, 300, 400, 500, 600],<br/>'c': ["one", "two", "three", "four", "five", "six"]},<br/>index=[10, 20, 30, 40, 50, 60])</span></pre><p id="a6fc" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">Spark 和 pandas-API 一起支持数据帧的内部对话。</p><pre class="le lf lg lh gt lu lv lw bn lx ly bi"><span id="e7e1" class="lz ma in lv b be mb mc l md me"># Existing Spark DataFrame can be converted to native pandas DataFrame.<br/>spark_topandas_dataframe = spark_dataframe.toPandas()</span></pre><pre class="mf lu lv lw bn lx ly bi"><span id="934a" class="lz ma in lv b be mb mc l md me"># Existing native pandas DataFrame can be converted to pandas-on-spark <br/># DataFrame.<br/>pandasAPI_datframe = ps.from_pandas(spark_topandas_dataframe)</span></pre></div><div class="ab cl kw kx hr ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ig ih ii ij ik"><p id="f645" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">Spark SQL:</p><p id="ead6" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">借助 Spark SQL 模块，Spark 能够在数据帧和数据集上运行类似 SQL 的查询。<br/> Spark SQL 为 Spark 带来了对 SQL 的本地支持，并简化了查询存储在 rdd(Spark 的分布式数据集)和外部数据源中的数据的过程。</p><p id="8442" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">要在数据帧上运行 SQL 查询，需要将其转换为表。</p><pre class="le lf lg lh gt lu lv lw bn lx ly bi"><span id="fae0" class="lz ma in lv b be mb mc l md me">createOrReplaceTempView("table_name")</span></pre><p id="28b2" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">示例:</p><pre class="le lf lg lh gt lu lv lw bn lx ly bi"><span id="c4ec" class="lz ma in lv b be mb mc l md me">data = spark.read.options(inferSchema='True',header='True').csv("./&lt;file&gt;.csv")<br/>data.createOrReplaceTempView('table_name')<br/>spark.sql("select * from table_name limit 10").show()</span></pre></div><div class="ab cl kw kx hr ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ig ih ii ij ik"><p id="6218" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">参考资料:</p><p id="8732" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><a class="ae lt" href="https://phoenixnap.com/kb/spark-dataframe#:~:text=What%20is%20a%20DataFrame%3F,and%20columns%20of%20different%20datatypes" rel="noopener ugc nofollow" target="_blank">https://phoenix nap . com/kb/spark-data frame #:~:text = What % 20 is % 20a % 20 data frame % 3F，and % 20 columns % 20 of % 20 different % 20 data types</a>。</p><p id="e022" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><a class="ae lt" href="https://www.databricks.com/glossary/what-is-spark-sql#:~:text=Spark%20SQL%20is%20a%20Spark,on%20existing%20deployments%20and%20data" rel="noopener ugc nofollow" target="_blank">https://www . data bricks . com/glossary/what-is-Spark-SQL #:~:text = Spark % 20 SQL % 20 is % 20a % 20 Spark，on % 20 existing % 20 deployment % 20 和%20data </a>。</p><p id="49e9" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">火花官方文档。</p></div></div>    
</body>
</html>