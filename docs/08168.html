<html>
<head>
<title>A brief intro to PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark 简介</h1>
<blockquote>原文：<a href="https://blog.devgenius.io/a-brief-intro-to-pyspark-552e3be1bd60?source=collection_archive---------11-----------------------#2022-05-22">https://blog.devgenius.io/a-brief-intro-to-pyspark-552e3be1bd60?source=collection_archive---------11-----------------------#2022-05-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="f937" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">PySpark 是 Spark 的 Python API，通常用于大容量数据应用。它的分布式计算框架通常跨云平台使用，尽管它具有可扩展性，但它不像 Pandas 这样的框架那样受欢迎。您可能从未遇到过需要昂贵的 Spark 集群来运行计算的情况，但是您仍然可以在本地使用该框架并进行尝试。在本文中，我将向您展示一个使用 PySpark 进行数据探索的简单练习，让您感受一下 PySpark 和 Pandas 之间的语法和差异。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/6f6b6e25f50e73310bf14f214ff636ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WuBKaJzMThPdtmyqZVmuSA.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk translated">来源:<a class="ae lb" href="http://www.databricks.comglossary/pyspark" rel="noopener ugc nofollow" target="_blank">www . data bricks . com glossary/py spark</a></figcaption></figure><h1 id="964e" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">安装和会话初始化</h1><p id="bef1" class="pw-post-body-paragraph jn jo iq jp b jq ma js jt ju mb jw jx jy mc ka kb kc md ke kf kg me ki kj kk ij bi translated">您可以通过 PyPi 安装 PySpark，方法是打开终端窗口(在 VS 代码中，您可以通过按 Ctrl +')并使用以下命令:</p><pre class="km kn ko kp gt mf mg mh mi aw mj bi"><span id="8609" class="mk ld iq mg b gy ml mm l mn mo">pip install pyspark</span></pre><p id="715a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第一步是初始化一个会话。从文档来看，“SparkSession 可以用来创建一个<strong class="jp ir">数据帧</strong>，将<strong class="jp ir">数据帧</strong>注册为表，对表执行 SQL，缓存表，以及读取 parquet 文件”。</p><p id="8c0b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我的例子中，我将分析花旗自行车的数据，这是纽约的一个自行车共享系统，因此得名“自行车”。</p><pre class="km kn ko kp gt mf mg mh mi aw mj bi"><span id="b546" class="mk ld iq mg b gy ml mm l mn mo">#Initialize Spark Session<br/>from pyspark.sql import SparkSession</span><span id="b76b" class="mk ld iq mg b gy mp mm l mn mo">spark = SparkSession \<br/>    .builder \<br/>    .appName("Bikes") \<br/>    .config("spark.some.config.option", "some-value") \<br/>    .getOrCreate()</span></pre><h1 id="0c2d" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">获取数据</h1><p id="9e21" class="pw-post-body-paragraph jn jo iq jp b jq ma js jt ju mb jw jx jy mc ka kb kc md ke kf kg me ki kj kk ij bi translated">我将使用的数据可以来自花旗自行车网站。<a class="ae lb" href="https://ride.citibikenyc.com/system-data" rel="noopener ugc nofollow" target="_blank">在这里</a>你可以访问历史数据。在我的例子中，我选择了几个小文件，因为这只是为了演示。我下载了 CSV 格式的文件，并以这种格式阅读。如果您想下载 S3 桶中的所有数据，您可能需要编写更复杂的管道来确保您不会得到重复的数据。</p><pre class="km kn ko kp gt mf mg mh mi aw mj bi"><span id="9031" class="mk ld iq mg b gy ml mm l mn mo">df_main = spark.read.format("csv").option("header","true").load('nyc/07-04-2022-202108-nyc.csv')</span><span id="ca24" class="mk ld iq mg b gy mp mm l mn mo">df_main.columns</span><span id="05d2" class="mk ld iq mg b gy mp mm l mn mo">Output: ['ride_id',  'rideable_type',  'started_at',  'ended_at',  'start_station_name',  'start_station_id',  'end_station_name',  'end_station_id',  'start_lat',  'start_lng',  'end_lat',  'end_lng',  'member_casual']</span></pre><p id="0f9d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我想分析的第一件事是我们每天实际乘坐的次数，以及这个月的变化情况。此外，我们所看到的日期间隔是多少</p><p id="4845" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，让我们选择足够的列来使用<code class="fe mq mr ms mg b">.select()</code>方法执行我们的查询。这是熊猫相当于写作<code class="fe mq mr ms mg b">dataframe[‘ColumnName’]</code>。然后，让我们在旅程开始时订购，以便更轻松地可视化数据。</p><pre class="km kn ko kp gt mf mg mh mi aw mj bi"><span id="5061" class="mk ld iq mg b gy ml mm l mn mo">df_light = df_main.select('ride_id', 'started_at','ended_at').orderBy('started_at')<br/>df_light.show(5)</span></pre><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/482c48c00cd185f3896f80df8cce0e3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*C_y83Q8Oo2VNA3VwVVMcNQ.png"/></div></figure><p id="dc81" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们想计算每天有多少行，我们不能使用时间戳。但是，我们可以添加一个列- <code class="fe mq mr ms mg b">.withColumn()-</code>，并使用<code class="fe mq mr ms mg b">to_date()</code>将等价日期传递给它。</p><pre class="km kn ko kp gt mf mg mh mi aw mj bi"><span id="d2d4" class="mk ld iq mg b gy ml mm l mn mo">from pyspark.sql.functions import to_date</span><span id="24fb" class="mk ld iq mg b gy mp mm l mn mo">df_light = df_light.withColumn('date',to_date(df_light.started_at))<br/>df_light.show(5)</span></pre><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mu"><img src="../Images/2f3ac7be898da2be002dd637b8d53d50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AAlcZsom-50FW_h8Ndj58g.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk translated">添加了新的日期列。</figcaption></figure><p id="a858" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们也可以用下面几行得到最小和最大日期。因为我们不能在 Pyspark 中对 Column 对象执行<code class="fe mq mr ms mg b">max()</code>,所以我们本质上是将 Dataframe】到一列<code class="fe mq mr ms mg b">row </code>对象中，并使用索引查询来查找字符串值。请注意，数据框已经订购。</p><pre class="km kn ko kp gt mf mg mh mi aw mj bi"><span id="edd2" class="mk ld iq mg b gy ml mm l mn mo"># Getting Minimum and Maximun date in the Interval</span><span id="7968" class="mk ld iq mg b gy mp mm l mn mo">max_value = df_light.agg({"date": "max"}).collect()[0][0]<br/>min_value = df_light.agg({"date": "min"}).collect()[0][0]</span><span id="127b" class="mk ld iq mg b gy mp mm l mn mo">print('min, max : ' , min_value,max_value)<br/>Output: min, max :  2021-08-01 2021-08-31</span></pre><p id="f66d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们现在可以在<code class="fe mq mr ms mg b">.count()</code>上<code class="fe mq mr ms mg b">.groupBy()</code>日期，以获得每日乘车次数。</p><pre class="km kn ko kp gt mf mg mh mi aw mj bi"><span id="f37e" class="mk ld iq mg b gy ml mm l mn mo">df_light_agg = df_light.select('date').groupBy('date').count().orderBy('date')</span><span id="ce96" class="mk ld iq mg b gy mp mm l mn mo">df_light_agg.show()<br/>+----------+-----+ <br/>|      date|count| <br/>+----------+-----+ <br/>|2021-08-01| 2727| <br/>|2021-08-02| 2915| <br/>|2021-08-03| 3092|</span></pre><p id="2ab1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下一个合乎逻辑的步骤是想象这一趋势。为了绘图，我使用<code class="fe mq mr ms mg b">.toPandas() </code>方法将 PySpark 数据帧转换为 Pandas，并使用 Plotly 绘图。使用 Plotly，如果您转换您的数据帧，它会容易得多，因为该框架与 Spark 数据帧不兼容。</p><pre class="km kn ko kp gt mf mg mh mi aw mj bi"><span id="c61b" class="mk ld iq mg b gy ml mm l mn mo">import plotly.express as px</span><span id="9a32" class="mk ld iq mg b gy mp mm l mn mo">fig = px.scatter(df_light_agg.toPandas(), x="date", y="count")<br/>fig.show()</span></pre><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mv"><img src="../Images/3b07a875e5ef84b36b21d1c923171052.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q-2Om_aS_OuVvBe1FSooQg.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk translated">八月份每日乘车次数。</figcaption></figure><p id="7be8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，进行第二轮分析，让我们了解我们的数据中有多少个站，哪些路线最繁忙。首先，选择必填字段。</p><pre class="km kn ko kp gt mf mg mh mi aw mj bi"><span id="6cc1" class="mk ld iq mg b gy ml mm l mn mo">df_stations = df_main.select('ride_id',<br/> 'started_at',<br/> 'start_station_name',<br/> 'end_station_name', 'start_lat',<br/> 'start_lng',<br/> 'end_lat',<br/> 'end_lng')</span></pre><p id="d4de" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">按组合数分组，即按包含起点站和终点站的路线分组。</p><pre class="km kn ko kp gt mf mg mh mi aw mj bi"><span id="ae8f" class="mk ld iq mg b gy ml mm l mn mo">df_stations_combinations = df_stations.select('start_station_name',<br/> 'end_station_name', 'start_lat',<br/> 'start_lng',<br/> 'end_lat',<br/> 'end_lng').groupBy('start_station_name',<br/> 'end_station_name','start_lat',<br/> 'start_lng',<br/> 'end_lat',<br/> 'end_lng').count().orderBy('count', ascending=False)</span></pre><p id="2e73" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意到前面语句的输出，我注意到有一条路线记录了不寻常的乘车次数(是第二多的路线的 3 倍)。这条路线的起点和终点都有相同的车站，这意味着这可能是自行车不能正确停靠和计算多次行程的问题。使用<code class="fe mq mr ms mg b">.where()</code>方法，我们可以选择具有不同起点和终点的路线。使用<code class="fe mq mr ms mg b">.col()</code>方法对数据帧列执行逻辑运算。</p><pre class="km kn ko kp gt mf mg mh mi aw mj bi"><span id="2bf1" class="mk ld iq mg b gy ml mm l mn mo">df_stations_combinations = df_stations_combinations.where(col('start_station_name') != col('end_station_name')).orderBy('count',ascending=False)</span><span id="d63d" class="mk ld iq mg b gy mp mm l mn mo">df_stations_combinations.select('start_station_name','end_station_name', 'start_lat','end_lat','count').show(5)</span><span id="e0a4" class="mk ld iq mg b gy mp mm l mn mo">df_stations_combinations.count()<br/>output : 4798 #count of distinct journeys</span></pre><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mw"><img src="../Images/478f77d96c9880befe4e428c244be633.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O1hvhmDYy_1FZvyZNnxRwQ.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk translated">显而易见，两条主要的旅游轴线位于霍博肯码头和霍博肯大道之间，以及 14St 渡轮和南海滨之间(双向)。</figcaption></figure><p id="08eb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，为了获得网格中的站点数量(对于我们正在处理的数据集)，因为我们已经采取了所有路线的组合，这意味着我们有一个包含所有站点(作为终点或起点站点)的小样本行。通过用<code class="fe mq mr ms mg b">.union()</code>合并这两列并选择不同的值，我们可以推断网格中<code class="fe mq mr ms mg b">distinct </code>站的数量。</p><pre class="km kn ko kp gt mf mg mh mi aw mj bi"><span id="40f3" class="mk ld iq mg b gy ml mm l mn mo">df_unique_stations = df_stations_combinations.select('start_station_name','start_lat','start_lng').union(df_stations_combinations.select('end_station_name','end_lat','end_lng')).distinct()</span><span id="f229" class="mk ld iq mg b gy mp mm l mn mo">df_unique_stations.count()<br/>output: 176<br/>df_unique_stations.show()</span></pre><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mx"><img src="../Images/05e7813e2e473ca2df2213a467242534.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ES_Py3GgGMdDSzuTbZdsBw.png"/></div></div></figure><p id="af30" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们有了一个站点列表和站点之间的乘车次数，我们可以潜在地使用它们来绘图或进一步分析。</p><p id="f58e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是对 PySpark 的一个非常简短的调查，但是我希望它能让你有兴趣自己去看看。我想再写一些关于 PySpark 更高级方面的文章，并且可能写一些非常重要的流水线方面的文章。</p><p id="6778" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我真的很喜欢 PySpark，因为它让我可以编写带有某种 SQL 风格的 Python。在本教程中，当使用像<code class="fe mq mr ms mg b">.select()</code>、<code class="fe mq mr ms mg b">.groupBy()</code>、<code class="fe mq mr ms mg b">.where()</code>和<code class="fe mq mr ms mg b">.distinct()</code>这样的命令时，这一点很清楚。</p><p id="a502" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你喜欢它并且想要更多这些文章，请在评论中告诉我。</p><p id="cfc5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">祝你愉快，</p><p id="1591" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">若昂</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi my"><img src="../Images/b3fe53f0d19fe2fa5f49d6c8b0d406b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*22mcPPPKt1EsIAprrfoiqA.png"/></div></figure></div></div>    
</body>
</html>